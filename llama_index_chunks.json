[
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/types.py",
    "filename": "types.py",
    "relpath": "types.py",
    "start_line": 1,
    "end_line": 175,
    "length": 175,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "parse",
      "format",
      "_format_message",
      "format_messages",
      "__get_pydantic_core_schema__",
      "__get_pydantic_json_schema__",
      "output_cls",
      "__call__",
      "acall",
      "stream_call",
      "astream_call",
      "__init__"
    ],
    "chunk_class_names": [
      "BaseOutputParser",
      "BasePydanticProgram",
      "for",
      "PydanticProgramMode",
      "Thread"
    ],
    "document_function_names": [
      "parse",
      "format",
      "_format_message",
      "format_messages",
      "__get_pydantic_core_schema__",
      "__get_pydantic_json_schema__",
      "output_cls",
      "__call__",
      "acall",
      "stream_call",
      "astream_call",
      "__init__"
    ],
    "document_class_names": [
      "BaseOutputParser",
      "BasePydanticProgram",
      "for",
      "PydanticProgramMode",
      "Thread"
    ],
    "content": "import threading\nfrom abc import ABC, abstractmethod\nfrom contextvars import copy_context\nfrom enum import Enum\nfrom functools import partial\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Dict,\n    Generator,\n    Generic,\n    List,\n    Optional,\n    TYPE_CHECKING,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole, TextBlock\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    GetCoreSchemaHandler,\n    GetJsonSchemaHandler,\n)\nfrom llama_index.core.bridge.pydantic_core import CoreSchema, core_schema\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\n\nModel = TypeVar(\"Model\", bound=BaseModel)\n\nTokenGen = Generator[str, None, None]\nTokenAsyncGen = AsyncGenerator[str, None]\nRESPONSE_TEXT_TYPE = Union[BaseModel, str, TokenGen, TokenAsyncGen]\n\nif TYPE_CHECKING:\n    from llama_index.core.program.utils import FlexibleModel\n\n\n# TODO: move into a `core` folder\n# NOTE: this is necessary to make it compatible with pydantic\nclass BaseOutputParser(DispatcherSpanMixin, ABC):\n    \"\"\"Output parser class.\"\"\"\n\n    @abstractmethod\n    def parse(self, output: str) -> Any:\n        \"\"\"Parse, validate, and correct errors programmatically.\"\"\"\n\n    def format(self, query: str) -> str:\n        \"\"\"Format a query with structured output formatting instructions.\"\"\"\n        return query\n\n    def _format_message(self, message: ChatMessage) -> ChatMessage:\n        text_blocks: list[tuple[int, TextBlock]] = [\n            (idx, block)\n            for idx, block in enumerate(message.blocks)\n            if isinstance(block, TextBlock)\n        ]\n\n        # add text to the last text block, or add a new text block\n        format_text = \"\"\n        if text_blocks:\n            format_idx = text_blocks[-1][0]\n            format_text = text_blocks[-1][1].text\n\n            if format_idx != -1:\n                # this should always be a text block\n                assert isinstance(message.blocks[format_idx], TextBlock)\n                message.blocks[format_idx].text = self.format(format_text)  # type: ignore\n        else:\n            message.blocks.append(TextBlock(text=self.format(format_text)))\n\n        return message\n\n    def format_messages(self, messages: List[ChatMessage]) -> List[ChatMessage]:\n        \"\"\"Format a list of messages with structured output formatting instructions.\"\"\"\n        # NOTE: apply output parser to either the first message if it's a system message\n        #       or the last message\n        if messages:\n            if messages[0].role == MessageRole.SYSTEM:\n                # get text from the last text blocks\n                messages[0] = self._format_message(messages[0])\n            else:\n                messages[-1] = self._format_message(messages[-1])\n\n        return messages\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: Type[Any], handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        return core_schema.any_schema()\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> Dict[str, Any]:\n        json_schema = handler(core_schema)\n        return handler.resolve_ref_schema(json_schema)\n\n\nclass BasePydanticProgram(DispatcherSpanMixin, ABC, Generic[Model]):\n    \"\"\"A base class for LLM-powered function that return a pydantic model.\n\n    Note: this interface is not yet stable.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def output_cls(self) -> Type[Model]:\n        pass\n\n    @abstractmethod\n    def __call__(self, *args: Any, **kwargs: Any) -> Union[Model, List[Model]]:\n        pass\n\n    async def acall(self, *args: Any, **kwargs: Any) -> Union[Model, List[Model]]:\n        return self(*args, **kwargs)\n\n    def stream_call(\n        self, *args: Any, **kwargs: Any\n    ) -> Generator[\n        Union[Model, List[Model], \"FlexibleModel\", List[\"FlexibleModel\"]], None, None\n    ]:\n        raise NotImplementedError(\"stream_call is not supported by default.\")\n\n    async def astream_call(\n        self, *args: Any, **kwargs: Any\n    ) -> AsyncGenerator[\n        Union[Model, List[Model], \"FlexibleModel\", List[\"FlexibleModel\"]], None\n    ]:\n        raise NotImplementedError(\"astream_call is not supported by default.\")\n\n\nclass PydanticProgramMode(str, Enum):\n    \"\"\"Pydantic program mode.\"\"\"\n\n    DEFAULT = \"default\"\n    OPENAI = \"openai\"\n    LLM = \"llm\"\n    FUNCTION = \"function\"\n    GUIDANCE = \"guidance\"\n    LM_FORMAT_ENFORCER = \"lm-format-enforcer\"\n\n\nclass Thread(threading.Thread):\n    \"\"\"\n    A wrapper for threading.Thread that copies the current context and uses the copy to run the target.\n    \"\"\"\n\n    def __init__(\n        self,\n        group: Optional[Any] = None,\n        target: Optional[Callable[..., Any]] = None,\n        name: Optional[str] = None,\n        args: Tuple[Any, ...] = (),\n        kwargs: Optional[Dict[str, Any]] = None,\n        *,\n        daemon: Optional[bool] = None\n    ) -> None:\n        if target is not None:\n            args = (\n                partial(target, *args, **(kwargs if isinstance(kwargs, dict) else {})),\n            )\n        else:\n            args = ()\n\n        super().__init__(\n            group=group,\n            target=copy_context().run,\n            name=name,\n            args=args,\n            daemon=daemon,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/service_context.py",
    "filename": "service_context.py",
    "relpath": "service_context.py",
    "start_line": 1,
    "end_line": 46,
    "length": 46,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "set_global_service_context"
    ],
    "chunk_class_names": [
      "ServiceContext"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "set_global_service_context"
    ],
    "document_class_names": [
      "ServiceContext"
    ],
    "content": "from typing import Any, Optional\n\n\nclass ServiceContext:\n    \"\"\"Service Context container.\n\n    NOTE: Deprecated, use llama_index.settings.Settings instead or pass in\n    modules to local functions/methods/interfaces.\n\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        raise ValueError(\n            \"ServiceContext is deprecated. Use llama_index.settings.Settings instead, \"\n            \"or pass in modules to local functions/methods/interfaces.\\n\"\n            \"See the docs for updated usage/migration: \\n\"\n            \"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\"\n        )\n\n    @classmethod\n    def from_defaults(\n        cls,\n        **kwargs: Any,\n    ) -> \"ServiceContext\":\n        \"\"\"Create a ServiceContext from defaults.\n\n        NOTE: Deprecated, use llama_index.settings.Settings instead or pass in\n        modules to local functions/methods/interfaces.\n\n        \"\"\"\n        raise ValueError(\n            \"ServiceContext is deprecated. Use llama_index.settings.Settings instead, \"\n            \"or pass in modules to local functions/methods/interfaces.\\n\"\n            \"See the docs for updated usage/migration: \\n\"\n            \"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\"\n        )\n\n\ndef set_global_service_context(service_context: Optional[ServiceContext]) -> None:\n    \"\"\"Helper function to set the global service context.\"\"\"\n    raise ValueError(\n        \"ServiceContext is deprecated. Use llama_index.settings.Settings instead, \"\n        \"or pass in modules to local functions/methods/interfaces.\\n\"\n        \"See the docs for updated usage/migration: \\n\"\n        \"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\"\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/img_utils.py",
    "filename": "img_utils.py",
    "relpath": "img_utils.py",
    "start_line": 1,
    "end_line": 20,
    "length": 20,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "img_2_b64",
      "b64_2_img"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "img_2_b64",
      "b64_2_img"
    ],
    "document_class_names": [],
    "content": "\"\"\"Utils for manipulating images.\"\"\"\nimport base64\nfrom io import BytesIO\nfrom typing import cast\n\nfrom PIL import Image\nfrom PIL.ImageFile import ImageFile\n\n\ndef img_2_b64(image: ImageFile, format: str = \"JPEG\") -> str:\n    \"\"\"Convert a PIL.Image to a base64 encoded image str.\"\"\"\n    buff = BytesIO()\n    image.save(buff, format=format)\n    return cast(str, base64.b64encode(buff.getvalue()))\n\n\ndef b64_2_img(data: str) -> ImageFile:\n    \"\"\"Convert base64 encoded image str to a PIL.Image.\"\"\"\n    buff = BytesIO(base64.b64decode(data))\n    return cast(ImageFile, Image.open(buff))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/async_utils.py",
    "filename": "async_utils.py",
    "relpath": "async_utils.py",
    "start_line": 1,
    "end_line": 150,
    "length": 150,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "asyncio_module",
      "asyncio_run",
      "run_async_tasks",
      "_tqdm_gather",
      "_gather",
      "chunks",
      "batch_gather",
      "get_asyncio_module",
      "run_jobs",
      "worker"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "asyncio_module",
      "asyncio_run",
      "run_async_tasks",
      "_tqdm_gather",
      "_gather",
      "chunks",
      "batch_gather",
      "get_asyncio_module",
      "run_jobs",
      "worker"
    ],
    "document_class_names": [],
    "content": "\"\"\"Async utils.\"\"\"\n\nimport asyncio\nfrom itertools import zip_longest\nfrom typing import Any, Coroutine, Iterable, List, Optional, TypeVar\n\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\ndef asyncio_module(show_progress: bool = False) -> Any:\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n\n        module = tqdm_asyncio\n    else:\n        module = asyncio\n\n    return module\n\n\ndef asyncio_run(coro: Coroutine) -> Any:\n    \"\"\"Gets an existing event loop to run the coroutine.\n\n    If there is no existing event loop, creates a new one.\n    \"\"\"\n    try:\n        # Check if there's an existing event loop\n        loop = asyncio.get_event_loop()\n\n        # If we're here, there's an existing loop but it's not running\n        return loop.run_until_complete(coro)\n\n    except RuntimeError as e:\n        # If we can't get the event loop, we're likely in a different thread, or its already running\n        try:\n            return asyncio.run(coro)\n        except RuntimeError as e:\n            raise RuntimeError(\n                \"Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.\"\n                \"Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.\"\n            )\n\n\ndef run_async_tasks(\n    tasks: List[Coroutine],\n    show_progress: bool = False,\n    progress_bar_desc: str = \"Running async tasks\",\n) -> List[Any]:\n    \"\"\"Run a list of async tasks.\"\"\"\n    tasks_to_execute: List[Any] = tasks\n    if show_progress:\n        try:\n            import nest_asyncio\n            from tqdm.asyncio import tqdm\n\n            # jupyter notebooks already have an event loop running\n            # we need to reuse it instead of creating a new one\n            nest_asyncio.apply()\n            loop = asyncio.get_event_loop()\n\n            async def _tqdm_gather() -> List[Any]:\n                return await tqdm.gather(*tasks_to_execute, desc=progress_bar_desc)\n\n            tqdm_outputs: List[Any] = loop.run_until_complete(_tqdm_gather())\n            return tqdm_outputs\n        # run the operation w/o tqdm on hitting a fatal\n        # may occur in some environments where tqdm.asyncio\n        # is not supported\n        except Exception:\n            pass\n\n    async def _gather() -> List[Any]:\n        return await asyncio.gather(*tasks_to_execute)\n\n    outputs: List[Any] = asyncio_run(_gather())\n    return outputs\n\n\ndef chunks(iterable: Iterable, size: int) -> Iterable:\n    args = [iter(iterable)] * size\n    return zip_longest(*args, fillvalue=None)\n\n\nasync def batch_gather(\n    tasks: List[Coroutine], batch_size: int = 10, verbose: bool = False\n) -> List[Any]:\n    output: List[Any] = []\n    for task_chunk in chunks(tasks, batch_size):\n        task_chunk = (task for task in task_chunk if task is not None)\n        output_chunk = await asyncio.gather(*task_chunk)\n        output.extend(output_chunk)\n        if verbose:\n            print(f\"Completed {len(output)} out of {len(tasks)} tasks\")\n    return output\n\n\ndef get_asyncio_module(show_progress: bool = False) -> Any:\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n\n        module = tqdm_asyncio\n    else:\n        module = asyncio\n\n    return module\n\n\nDEFAULT_NUM_WORKERS = 4\n\nT = TypeVar(\"T\")\n\n\n@dispatcher.span\nasync def run_jobs(\n    jobs: List[Coroutine[Any, Any, T]],\n    show_progress: bool = False,\n    workers: int = DEFAULT_NUM_WORKERS,\n    desc: Optional[str] = None,\n) -> List[T]:\n    \"\"\"Run jobs.\n\n    Args:\n        jobs (List[Coroutine]):\n            List of jobs to run.\n        show_progress (bool):\n            Whether to show progress bar.\n\n    Returns:\n        List[Any]:\n            List of results.\n    \"\"\"\n    semaphore = asyncio.Semaphore(workers)\n\n    @dispatcher.span\n    async def worker(job: Coroutine) -> Any:\n        async with semaphore:\n            return await job\n\n    pool_jobs = [worker(job) for job in jobs]\n\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n\n        results = await tqdm_asyncio.gather(*pool_jobs, desc=desc)\n    else:\n        results = await asyncio.gather(*pool_jobs)\n\n    return results"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/constants.py",
    "filename": "constants.py",
    "relpath": "constants.py",
    "start_line": 1,
    "end_line": 36,
    "length": 36,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Set of constants.\"\"\"\n\nDEFAULT_TEMPERATURE = 0.1\nDEFAULT_CONTEXT_WINDOW = 3900  # tokens\nDEFAULT_NUM_OUTPUTS = 256  # tokens\nDEFAULT_NUM_INPUT_FILES = 10  # files\n\nDEFAULT_EMBED_BATCH_SIZE = 10\n\nDEFAULT_CHUNK_SIZE = 1024  # tokens\nDEFAULT_CHUNK_OVERLAP = 20  # tokens\nDEFAULT_SIMILARITY_TOP_K = 2\nDEFAULT_IMAGE_SIMILARITY_TOP_K = 2\n\n# NOTE: for text-embedding-ada-002\nDEFAULT_EMBEDDING_DIM = 1536\n\n# context window size for llm predictor\nCOHERE_CONTEXT_WINDOW = 2048\nAI21_J2_CONTEXT_WINDOW = 8192\n\n\nTYPE_KEY = \"__type__\"\nDATA_KEY = \"__data__\"\nVECTOR_STORE_KEY = \"vector_store\"\nIMAGE_STORE_KEY = \"image_store\"\nGRAPH_STORE_KEY = \"graph_store\"\nINDEX_STORE_KEY = \"index_store\"\nDOC_STORE_KEY = \"doc_store\"\nPG_STORE_KEY = \"property_graph_store\"\n\n# llama-cloud constants\nDEFAULT_PIPELINE_NAME = \"default\"\nDEFAULT_PROJECT_NAME = \"Default\"\nDEFAULT_BASE_URL = \"https://api.cloud.llamaindex.ai\"\nDEFAULT_APP_URL = \"https://cloud.llamaindex.ai\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/image_retriever.py",
    "filename": "image_retriever.py",
    "relpath": "image_retriever.py",
    "start_line": 1,
    "end_line": 104,
    "length": 104,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "text_to_image_retrieve",
      "_text_to_image_retrieve",
      "image_to_image_retrieve",
      "_image_to_image_retrieve",
      "atext_to_image_retrieve",
      "_atext_to_image_retrieve",
      "aimage_to_image_retrieve",
      "_aimage_to_image_retrieve"
    ],
    "chunk_class_names": [
      "BaseImageRetriever"
    ],
    "document_function_names": [
      "text_to_image_retrieve",
      "_text_to_image_retrieve",
      "image_to_image_retrieve",
      "_image_to_image_retrieve",
      "atext_to_image_retrieve",
      "_atext_to_image_retrieve",
      "aimage_to_image_retrieve",
      "_aimage_to_image_retrieve"
    ],
    "document_class_names": [
      "BaseImageRetriever"
    ],
    "content": "from abc import abstractmethod\nfrom typing import List\n\nfrom llama_index.core.indices.query.schema import QueryBundle, QueryType\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.prompts.mixin import PromptMixin\nfrom llama_index.core.schema import NodeWithScore\n\n\nclass BaseImageRetriever(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base Image Retriever Abstraction.\"\"\"\n\n    def text_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve image nodes given query or single image input.\n\n        Args:\n            str_or_query_bundle (QueryType): a query text\n            string or a QueryBundle object.\n        \"\"\"\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(query_str=str_or_query_bundle)\n        return self._text_to_image_retrieve(str_or_query_bundle)\n\n    @abstractmethod\n    def _text_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve image nodes or documents given query text.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    def image_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve image nodes given single image input.\n\n        Args:\n            str_or_query_bundle (QueryType): a image path\n            string or a QueryBundle object.\n        \"\"\"\n        if isinstance(str_or_query_bundle, str):\n            # leave query_str as empty since we are using image_path for image retrieval\n            str_or_query_bundle = QueryBundle(\n                query_str=\"\", image_path=str_or_query_bundle\n            )\n        return self._image_to_image_retrieve(str_or_query_bundle)\n\n    @abstractmethod\n    def _image_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve image nodes or documents given image.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    # Async Methods\n    async def atext_to_image_retrieve(\n        self,\n        str_or_query_bundle: QueryType,\n    ) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(query_str=str_or_query_bundle)\n        return await self._atext_to_image_retrieve(str_or_query_bundle)\n\n    @abstractmethod\n    async def _atext_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Async retrieve image nodes or documents given query text.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    async def aimage_to_image_retrieve(\n        self,\n        str_or_query_bundle: QueryType,\n    ) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            # leave query_str as empty since we are using image_path for image retrieval\n            str_or_query_bundle = QueryBundle(\n                query_str=\"\", image_path=str_or_query_bundle\n            )\n        return await self._aimage_to_image_retrieve(str_or_query_bundle)\n\n    @abstractmethod\n    async def _aimage_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Async retrieve image nodes or documents given image.\n\n        Implemented by the user.\n\n        \"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/schema.py",
    "filename": "schema.py",
    "relpath": "schema.py",
    "start_line": 1,
    "end_line": 478,
    "length": 478,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__get_pydantic_json_schema__",
      "class_name",
      "json",
      "custom_model_dump",
      "dict",
      "__getstate__",
      "__setstate__",
      "to_dict",
      "to_json",
      "from_dict",
      "from_json",
      "__call__",
      "acall",
      "class_name",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "hash",
      "node_id",
      "node_id",
      "source_node",
      "prev_node",
      "next_node",
      "parent_node",
      "child_nodes",
      "ref_doc_id",
      "extra_info",
      "extra_info",
      "__str__",
      "get_embedding",
      "as_related_node_info"
    ],
    "chunk_class_names": [
      "from",
      "BaseComponent",
      "names",
      "name",
      "name",
      "name",
      "had",
      "TransformComponent",
      "for",
      "NodeRelationship",
      "ObjectType",
      "Modality",
      "MetadataMode",
      "RelatedNodeInfo",
      "BaseNode"
    ],
    "document_function_names": [
      "__get_pydantic_json_schema__",
      "class_name",
      "json",
      "custom_model_dump",
      "dict",
      "__getstate__",
      "__setstate__",
      "to_dict",
      "to_json",
      "from_dict",
      "from_json",
      "__call__",
      "acall",
      "class_name",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "hash",
      "node_id",
      "node_id",
      "source_node",
      "prev_node",
      "next_node",
      "parent_node",
      "child_nodes",
      "ref_doc_id",
      "extra_info",
      "extra_info",
      "__str__",
      "get_embedding",
      "as_related_node_info",
      "validate_data",
      "validate_mimetype",
      "hash",
      "class_name",
      "get_type",
      "get_content",
      "set_content",
      "hash",
      "__init__",
      "class_name",
      "hash",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "get_node_info",
      "get_text",
      "node_info",
      "__init__",
      "get_type",
      "class_name",
      "resolve_image",
      "hash",
      "dict",
      "from_text_node",
      "from_dict",
      "get_type",
      "class_name",
      "__str__",
      "get_score",
      "class_name",
      "node_id",
      "id_",
      "text",
      "metadata",
      "embedding",
      "get_text",
      "get_content",
      "get_embedding",
      "__init__",
      "custom_model_dump",
      "text",
      "get_type",
      "doc_id",
      "doc_id",
      "__str__",
      "get_doc_id",
      "to_langchain_format",
      "from_langchain_format",
      "to_haystack_format",
      "from_haystack_format",
      "to_embedchain_format",
      "from_embedchain_format",
      "to_semantic_kernel_format",
      "from_semantic_kernel_format",
      "to_vectorflow",
      "example",
      "class_name",
      "to_cloud_document",
      "from_cloud_document",
      "__init__",
      "image",
      "image",
      "image_path",
      "image_path",
      "image_url",
      "image_url",
      "image_mimetype",
      "image_mimetype",
      "text_embedding",
      "text_embedding",
      "class_name",
      "resolve_image",
      "embedding_strs",
      "embedding_image",
      "__str__"
    ],
    "document_class_names": [
      "from",
      "BaseComponent",
      "names",
      "name",
      "name",
      "name",
      "had",
      "TransformComponent",
      "for",
      "NodeRelationship",
      "ObjectType",
      "Modality",
      "MetadataMode",
      "RelatedNodeInfo",
      "BaseNode",
      "MediaResource",
      "for",
      "represents",
      "Node",
      "TextNode",
      "ImageNode",
      "IndexNode",
      "NodeWithScore",
      "Document",
      "ImageDocument",
      "class",
      "contains"
    ],
    "content": "\"\"\"Base schema for data structures.\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport json\nimport logging\nimport pickle\nimport textwrap\nimport uuid\nfrom abc import abstractmethod\nfrom binascii import Error as BinasciiError\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom hashlib import sha256\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Annotated,\n    Any,\n    Dict,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Union,\n)\n\nimport filetype\nimport requests\nfrom dataclasses_json import DataClassJsonMixin\nfrom deprecated import deprecated\nfrom typing_extensions import Self\n\nfrom llama_index.core.bridge.pydantic import (\n    AnyUrl,\n    BaseModel,\n    ConfigDict,\n    Field,\n    GetJsonSchemaHandler,\n    JsonSchemaValue,\n    PlainSerializer,\n    SerializationInfo,\n    SerializeAsAny,\n    SerializerFunctionWrapHandler,\n    ValidationInfo,\n    field_validator,\n    model_serializer,\n)\nfrom llama_index.core.bridge.pydantic_core import CoreSchema\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.utils import SAMPLE_TEXT, truncate_text\n\nif TYPE_CHECKING:  # pragma: no cover\n    from haystack.schema import Document as HaystackDocument  # type: ignore\n    from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n    from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n    from llama_index.core.bridge.langchain import Document as LCDocument  # type: ignore\n\n\nDEFAULT_TEXT_NODE_TMPL = \"{metadata_str}\\n\\n{content}\"\nDEFAULT_METADATA_TMPL = \"{key}: {value}\"\n# NOTE: for pretty printing\nTRUNCATE_LENGTH = 350\nWRAP_WIDTH = 70\n\nImageType = Union[str, BytesIO]\n\nlogger = logging.getLogger(__name__)\n\nEnumNameSerializer = PlainSerializer(\n    lambda e: e.value, return_type=\"str\", when_used=\"always\"\n)\n\n\nclass BaseComponent(BaseModel):\n    \"\"\"Base component object to capture class names.\"\"\"\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> JsonSchemaValue:\n        json_schema = handler(core_schema)\n        json_schema = handler.resolve_ref_schema(json_schema)\n\n        # inject class name to help with serde\n        if \"properties\" in json_schema:\n            json_schema[\"properties\"][\"class_name\"] = {\n                \"title\": \"Class Name\",\n                \"type\": \"string\",\n                \"default\": cls.class_name(),\n            }\n        return json_schema\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"\n        Get the class name, used as a unique ID in serialization.\n\n        This provides a key that makes serialization robust against actual class\n        name changes.\n        \"\"\"\n        return \"base_component\"\n\n    def json(self, **kwargs: Any) -> str:\n        return self.to_json(**kwargs)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n        data = handler(self)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        return self.model_dump(**kwargs)\n\n    def __getstate__(self) -> Dict[str, Any]:\n        state = super().__getstate__()\n\n        # remove attributes that are not pickleable -- kind of dangerous\n        keys_to_remove = []\n        for key, val in state[\"__dict__\"].items():\n            try:\n                pickle.dumps(val)\n            except Exception:\n                keys_to_remove.append(key)\n\n        for key in keys_to_remove:\n            logging.warning(f\"Removing unpickleable attribute {key}\")\n            del state[\"__dict__\"][key]\n\n        # remove private attributes if they aren't pickleable -- kind of dangerous\n        keys_to_remove = []\n        private_attrs = state.get(\"__pydantic_private__\", None)\n        if private_attrs:\n            for key, val in state[\"__pydantic_private__\"].items():\n                try:\n                    pickle.dumps(val)\n                except Exception:\n                    keys_to_remove.append(key)\n\n            for key in keys_to_remove:\n                logging.warning(f\"Removing unpickleable private attribute {key}\")\n                del state[\"__pydantic_private__\"][key]\n\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]) -> None:\n        # Use the __dict__ and __init__ method to set state\n        # so that all variables initialize\n        try:\n            self.__init__(**state[\"__dict__\"])  # type: ignore\n        except Exception:\n            # Fall back to the default __setstate__ method\n            # This may not work if the class had unpickleable attributes\n            super().__setstate__(state)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = self.dict(**kwargs)\n        data[\"class_name\"] = self.class_name()\n        return data\n\n    def to_json(self, **kwargs: Any) -> str:\n        data = self.to_dict(**kwargs)\n        return json.dumps(data)\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        # In SimpleKVStore we rely on shallow coping. Hence, the data will be modified in the store directly.\n        # And it is the same when the user is passing a dictionary to create a component. We can't modify the passed down dictionary.\n        data = dict(data)\n        if isinstance(kwargs, dict):\n            data.update(kwargs)\n        data.pop(\"class_name\", None)\n        return cls(**data)\n\n    @classmethod\n    def from_json(cls, data_str: str, **kwargs: Any) -> Self:  # type: ignore\n        data = json.loads(data_str)\n        return cls.from_dict(data, **kwargs)\n\n\nclass TransformComponent(BaseComponent, DispatcherSpanMixin):\n    \"\"\"Base class for transform components.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:\n        \"\"\"Transform nodes.\"\"\"\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -> Sequence[BaseNode]:\n        \"\"\"Async transform nodes.\"\"\"\n        return self.__call__(nodes, **kwargs)\n\n\nclass NodeRelationship(str, Enum):\n    \"\"\"Node relationships used in `BaseNode` class.\n\n    Attributes:\n        SOURCE: The node is the source document.\n        PREVIOUS: The node is the previous node in the document.\n        NEXT: The node is the next node in the document.\n        PARENT: The node is the parent node in the document.\n        CHILD: The node is a child node in the document.\n\n    \"\"\"\n\n    SOURCE = auto()\n    PREVIOUS = auto()\n    NEXT = auto()\n    PARENT = auto()\n    CHILD = auto()\n\n\nclass ObjectType(str, Enum):\n    TEXT = auto()\n    IMAGE = auto()\n    INDEX = auto()\n    DOCUMENT = auto()\n    MULTIMODAL = auto()\n\n\nclass Modality(str, Enum):\n    TEXT = auto()\n    IMAGE = auto()\n    AUDIO = auto()\n    VIDEO = auto()\n\n\nclass MetadataMode(str, Enum):\n    ALL = \"all\"\n    EMBED = \"embed\"\n    LLM = \"llm\"\n    NONE = \"none\"\n\n\nclass RelatedNodeInfo(BaseComponent):\n    node_id: str\n    node_type: Annotated[ObjectType, EnumNameSerializer] | str | None = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    hash: Optional[str] = None\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"RelatedNodeInfo\"\n\n\nRelatedNodeType = Union[RelatedNodeInfo, List[RelatedNodeInfo]]\n\n\n# Node classes for indexes\nclass BaseNode(BaseComponent):\n    \"\"\"Base node Object.\n\n    Generic abstract interface for retrievable nodes\n\n    \"\"\"\n\n    # hash is computed on local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n\n    id_: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\"\n    )\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"Embedding of the node.\"\n    )\n\n    \"\"\"\"\n    metadata fields\n    - injected as part of the text shown to LLMs as context\n    - injected as part of the text for generating embeddings\n    - used by vector DBs for metadata filtering\n\n    \"\"\"\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"A flat dictionary of metadata fields\",\n        alias=\"extra_info\",\n    )\n    excluded_embed_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the embed model.\",\n    )\n    excluded_llm_metadata_keys: List[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the LLM.\",\n    )\n    relationships: Dict[\n        Annotated[NodeRelationship, EnumNameSerializer],\n        RelatedNodeType,\n    ] = Field(\n        default_factory=dict,\n        description=\"A mapping of relationships to other node information.\",\n    )\n    metadata_template: str = Field(\n        default=DEFAULT_METADATA_TMPL,\n        description=(\n            \"Template for how metadata is formatted, with {key} and \"\n            \"{value} placeholders.\"\n        ),\n    )\n    metadata_separator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n        alias=\"metadata_seperator\",\n    )\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Object type.\"\"\"\n\n    @abstractmethod\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -> str:\n        \"\"\"Get object content.\"\"\"\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n        \"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_separator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    @abstractmethod\n    def set_content(self, value: Any) -> None:\n        \"\"\"Set the content of the node.\"\"\"\n\n    @property\n    @abstractmethod\n    def hash(self) -> str:\n        \"\"\"Get hash of node.\"\"\"\n\n    @property\n    def node_id(self) -> str:\n        return self.id_\n\n    @node_id.setter\n    def node_id(self, value: str) -> None:\n        self.id_ = value\n\n    @property\n    def source_node(self) -> Optional[RelatedNodeInfo]:\n        \"\"\"Source object node.\n\n        Extracted from the relationships field.\n\n        \"\"\"\n        if NodeRelationship.SOURCE not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.SOURCE]\n        if isinstance(relation, list):\n            raise ValueError(\"Source object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def prev_node(self) -> Optional[RelatedNodeInfo]:\n        \"\"\"Prev node.\"\"\"\n        if NodeRelationship.PREVIOUS not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PREVIOUS]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Previous object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def next_node(self) -> Optional[RelatedNodeInfo]:\n        \"\"\"Next node.\"\"\"\n        if NodeRelationship.NEXT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.NEXT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Next object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def parent_node(self) -> Optional[RelatedNodeInfo]:\n        \"\"\"Parent node.\"\"\"\n        if NodeRelationship.PARENT not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.PARENT]\n        if not isinstance(relation, RelatedNodeInfo):\n            raise ValueError(\"Parent object must be a single RelatedNodeInfo object\")\n        return relation\n\n    @property\n    def child_nodes(self) -> Optional[List[RelatedNodeInfo]]:\n        \"\"\"Child nodes.\"\"\"\n        if NodeRelationship.CHILD not in self.relationships:\n            return None\n\n        relation = self.relationships[NodeRelationship.CHILD]\n        if not isinstance(relation, list):\n            raise ValueError(\"Child objects must be a list of RelatedNodeInfo objects.\")\n        return relation\n\n    @property\n    def ref_doc_id(self) -> Optional[str]:  # pragma: no cover\n        \"\"\"Deprecated: Get ref doc id.\"\"\"\n        source_node = self.source_node\n        if source_node is None:\n            return None\n        return source_node.node_id\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self) -> dict[str, Any]:  # pragma: no coverde\n        return self.metadata\n\n    @extra_info.setter\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'extra_info' is deprecated, use 'metadata' instead.\",\n    )\n    def extra_info(self, extra_info: dict[str, Any]) -> None:  # pragma: no coverde\n        self.metadata = extra_info\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Node ID: {self.node_id}\\n{source_text_wrapped}\"\n\n    def get_embedding(self) -> List[float]:\n        \"\"\"Get embedding.\n\n        Errors if embedding is None.\n\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    def as_related_node_info(self) -> RelatedNodeInfo:\n        \"\"\"Get node as RelatedNodeInfo.\"\"\"\n        return RelatedNodeInfo(\n            node_id=self.node_id,\n            node_type=self.get_type(),\n            metadata=self.metadata,\n            hash=self.hash,\n        )\n\n\nEmbeddingKind = Literal[\"sparse\", \"dense\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/schema.py",
    "filename": "schema.py",
    "relpath": "schema.py",
    "start_line": 478,
    "end_line": 913,
    "length": 436,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "validate_data",
      "validate_mimetype",
      "hash",
      "class_name",
      "get_type",
      "get_content",
      "set_content",
      "hash",
      "__init__",
      "class_name",
      "hash",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "get_node_info",
      "get_text",
      "node_info",
      "__init__",
      "get_type",
      "class_name",
      "resolve_image",
      "hash",
      "dict",
      "from_text_node",
      "from_dict",
      "get_type",
      "class_name"
    ],
    "chunk_class_names": [
      "MediaResource",
      "for",
      "represents",
      "Node",
      "TextNode",
      "ImageNode",
      "IndexNode"
    ],
    "document_function_names": [
      "__get_pydantic_json_schema__",
      "class_name",
      "json",
      "custom_model_dump",
      "dict",
      "__getstate__",
      "__setstate__",
      "to_dict",
      "to_json",
      "from_dict",
      "from_json",
      "__call__",
      "acall",
      "class_name",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "hash",
      "node_id",
      "node_id",
      "source_node",
      "prev_node",
      "next_node",
      "parent_node",
      "child_nodes",
      "ref_doc_id",
      "extra_info",
      "extra_info",
      "__str__",
      "get_embedding",
      "as_related_node_info",
      "validate_data",
      "validate_mimetype",
      "hash",
      "class_name",
      "get_type",
      "get_content",
      "set_content",
      "hash",
      "__init__",
      "class_name",
      "hash",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "get_node_info",
      "get_text",
      "node_info",
      "__init__",
      "get_type",
      "class_name",
      "resolve_image",
      "hash",
      "dict",
      "from_text_node",
      "from_dict",
      "get_type",
      "class_name",
      "__str__",
      "get_score",
      "class_name",
      "node_id",
      "id_",
      "text",
      "metadata",
      "embedding",
      "get_text",
      "get_content",
      "get_embedding",
      "__init__",
      "custom_model_dump",
      "text",
      "get_type",
      "doc_id",
      "doc_id",
      "__str__",
      "get_doc_id",
      "to_langchain_format",
      "from_langchain_format",
      "to_haystack_format",
      "from_haystack_format",
      "to_embedchain_format",
      "from_embedchain_format",
      "to_semantic_kernel_format",
      "from_semantic_kernel_format",
      "to_vectorflow",
      "example",
      "class_name",
      "to_cloud_document",
      "from_cloud_document",
      "__init__",
      "image",
      "image",
      "image_path",
      "image_path",
      "image_url",
      "image_url",
      "image_mimetype",
      "image_mimetype",
      "text_embedding",
      "text_embedding",
      "class_name",
      "resolve_image",
      "embedding_strs",
      "embedding_image",
      "__str__"
    ],
    "document_class_names": [
      "from",
      "BaseComponent",
      "names",
      "name",
      "name",
      "name",
      "had",
      "TransformComponent",
      "for",
      "NodeRelationship",
      "ObjectType",
      "Modality",
      "MetadataMode",
      "RelatedNodeInfo",
      "BaseNode",
      "MediaResource",
      "for",
      "represents",
      "Node",
      "TextNode",
      "ImageNode",
      "IndexNode",
      "NodeWithScore",
      "Document",
      "ImageDocument",
      "class",
      "contains"
    ],
    "content": "class MediaResource(BaseModel):\n    \"\"\"A container class for media content.\n\n    This class represents a generic media resource that can be stored and accessed\n    in multiple ways - as raw bytes, on the filesystem, or via URL. It also supports\n    storing vector embeddings for the media content.\n\n    Attributes:\n        embeddings: Multi-vector dict representation of this resource for embedding-based search/retrieval\n        text: Plain text representation of this resource\n        data: Raw binary data of the media content\n        mimetype: The MIME type indicating the format/type of the media content\n        path: Local filesystem path where the media content can be accessed\n        url: URL where the media content can be accessed remotely\n    \"\"\"\n\n    embeddings: dict[EmbeddingKind, list[float]] | None = Field(\n        default=None, description=\"Vector representation of this resource.\"\n    )\n    data: bytes | None = Field(\n        default=None,\n        exclude=True,\n        description=\"base64 binary representation of this resource.\",\n    )\n    text: str | None = Field(\n        default=None, description=\"Text representation of this resource.\"\n    )\n    path: Path | None = Field(\n        default=None, description=\"Filesystem path of this resource.\"\n    )\n    url: AnyUrl | None = Field(default=None, description=\"URL to reach this resource.\")\n    mimetype: str | None = Field(\n        default=None, description=\"MIME type of this resource.\"\n    )\n\n    model_config = {\n        # This ensures validation runs even for None values\n        \"validate_default\": True\n    }\n\n    @field_validator(\"data\", mode=\"after\")\n    @classmethod\n    def validate_data(cls, v: bytes | None, info: ValidationInfo) -> bytes | None:\n        \"\"\"If binary data was passed, store the resource as base64 and guess the mimetype when possible.\n\n        In case the model was built passing binary data but without a mimetype,\n        we try to guess it using the filetype library. To avoid resource-intense\n        operations, we won't load the path or the URL to guess the mimetype.\n        \"\"\"\n        if v is None:\n            return v\n\n        try:\n            # Check if data is already base64 encoded.\n            # b64decode() can succeed on random binary data, so we\n            # pass verify=True to make sure it's not a false positive\n            decoded = base64.b64decode(v, validate=True)\n        except BinasciiError:\n            # b64decode failed, return encoded\n            return base64.b64encode(v)\n\n        # Good as is, return unchanged\n        return v\n\n    @field_validator(\"mimetype\", mode=\"after\")\n    @classmethod\n    def validate_mimetype(cls, v: str | None, info: ValidationInfo) -> str | None:\n        if v is not None:\n            return v\n\n        # Since this field validator runs after the one for `data`\n        # then the contents of `data` should be encoded already\n        b64_data = info.data.get(\"data\")\n        if b64_data:  # encoded bytes\n            decoded_data = base64.b64decode(b64_data)\n            if guess := filetype.guess(decoded_data):\n                return guess.mime\n\n        # guess from path\n        rpath: str | None = info.data[\"path\"]\n        if rpath:\n            extension = Path(rpath).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                return ftype.mime\n\n        return v\n\n    @property\n    def hash(self) -> str:\n        \"\"\"Generate a hash to uniquely identify the media resource.\n\n        The hash is generated based on the available content (data, path, text or url).\n        Returns an empty string if no content is available.\n        \"\"\"\n        bits: list[str] = []\n        if self.text is not None:\n            bits.append(self.text)\n        if self.data is not None:\n            # Hash the binary data if available\n            bits.append(str(sha256(self.data).hexdigest()))\n        if self.path is not None:\n            # Hash the file path if provided\n            bits.append(str(sha256(str(self.path).encode(\"utf-8\")).hexdigest()))\n        if self.url is not None:\n            # Use the URL string as basis for hash\n            bits.append(str(sha256(str(self.url).encode(\"utf-8\")).hexdigest()))\n\n        doc_identity = \"\".join(bits)\n        if not doc_identity:\n            return \"\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n\nclass Node(BaseNode):\n    text_resource: MediaResource | None = Field(\n        default=None, description=\"Text content of the node.\"\n    )\n    image_resource: MediaResource | None = Field(\n        default=None, description=\"Image content of the node.\"\n    )\n    audio_resource: MediaResource | None = Field(\n        default=None, description=\"Audio content of the node.\"\n    )\n    video_resource: MediaResource | None = Field(\n        default=None, description=\"Video content of the node.\"\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text_resource is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Node\"\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Object type.\"\"\"\n        return ObjectType.MULTIMODAL\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n        \"\"\"Get the text content for the node if available.\n\n        Provided for backward compatibility, use self.text_resource directly instead.\n        \"\"\"\n        if self.text_resource:\n            return self.text_template.format(\n                content=self.text_resource.text or \"\",\n                metadata_str=self.get_metadata_str(metadata_mode),\n            ).strip()\n        return \"\"\n\n    def set_content(self, value: str) -> None:\n        \"\"\"Set the text content of the node.\n\n        Provided for backward compatibility, set self.text_resource instead.\n        \"\"\"\n        self.text_resource = MediaResource(text=value)\n\n    @property\n    def hash(self) -> str:\n        doc_identities = []\n        if self.audio_resource is not None:\n            doc_identities.append(self.audio_resource.hash)\n        if self.image_resource is not None:\n            doc_identities.append(self.image_resource.hash)\n        if self.text_resource is not None:\n            doc_identities.append(self.text_resource.hash)\n        if self.video_resource is not None:\n            doc_identities.append(self.video_resource.hash)\n\n        doc_identity = \"-\".join(doc_identities)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n\nclass TextNode(BaseNode):\n    \"\"\"Provided for backward compatibility.\n\n    Note: we keep the field with the typo \"seperator\" to maintain backward compatibility for\n    serialized objects.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Make TextNode forward-compatible with Node by supporting 'text_resource' in the constructor.\"\"\"\n        if \"text_resource\" in kwargs:\n            tr = kwargs.pop(\"text_resource\")\n            if isinstance(tr, MediaResource):\n                kwargs[\"text\"] = tr.text\n            else:\n                kwargs[\"text\"] = tr[\"text\"]\n        super().__init__(*args, **kwargs)\n\n    text: str = Field(default=\"\", description=\"Text content of the node.\")\n    mimetype: str = Field(\n        default=\"text/plain\", description=\"MIME type of the node content.\"\n    )\n    start_char_idx: Optional[int] = Field(\n        default=None, description=\"Start char index of the node.\"\n    )\n    end_char_idx: Optional[int] = Field(\n        default=None, description=\"End char index of the node.\"\n    )\n    metadata_seperator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n    )\n    text_template: str = Field(\n        default=DEFAULT_TEXT_NODE_TMPL,\n        description=(\n            \"Template for how text is formatted, with {content} and \"\n            \"{metadata_str} placeholders.\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TextNode\"\n\n    @property\n    def hash(self) -> str:\n        doc_identity = str(self.text) + str(self.metadata)\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Object type.\"\"\"\n        return ObjectType.TEXT\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n        \"\"\"Get object content.\"\"\"\n        metadata_str = self.get_metadata_str(mode=metadata_mode).strip()\n        if not metadata_str:\n            return self.text\n\n        return self.text_template.format(\n            content=self.text, metadata_str=metadata_str\n        ).strip()\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -> str:\n        \"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        usable_metadata_keys = set(self.metadata.keys())\n        if mode == MetadataMode.LLM:\n            for key in self.excluded_llm_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n        elif mode == MetadataMode.EMBED:\n            for key in self.excluded_embed_metadata_keys:\n                if key in usable_metadata_keys:\n                    usable_metadata_keys.remove(key)\n\n        return self.metadata_seperator.join(\n            [\n                self.metadata_template.format(key=key, value=str(value))\n                for key, value in self.metadata.items()\n                if key in usable_metadata_keys\n            ]\n        )\n\n    def set_content(self, value: str) -> None:\n        \"\"\"Set the content of the node.\"\"\"\n        self.text = value\n\n    def get_node_info(self) -> Dict[str, Any]:\n        \"\"\"Get node info.\"\"\"\n        return {\"start\": self.start_char_idx, \"end\": self.end_char_idx}\n\n    def get_text(self) -> str:\n        return self.get_content(metadata_mode=MetadataMode.NONE)\n\n    @property\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'node_info' is deprecated, use 'get_node_info' instead.\",\n    )\n    def node_info(self) -> Dict[str, Any]:\n        \"\"\"Deprecated: Get node info.\"\"\"\n        return self.get_node_info()\n\n\nclass ImageNode(TextNode):\n    \"\"\"Node with image.\"\"\"\n\n    # TODO: store reference instead of actual image\n    # base64 encoded image str\n    image: Optional[str] = None\n    image_path: Optional[str] = None\n    image_url: Optional[str] = None\n    image_mimetype: Optional[str] = None\n    text_embedding: Optional[List[float]] = Field(\n        default=None,\n        description=\"Text embedding of image node, if text field is filled out\",\n    )\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Make ImageNode forward-compatible with Node by supporting 'image_resource' in the constructor.\"\"\"\n        if \"image_resource\" in kwargs:\n            ir = kwargs.pop(\"image_resource\")\n            if isinstance(ir, MediaResource):\n                kwargs[\"image_path\"] = ir.path.as_posix() if ir.path else None\n                kwargs[\"image_url\"] = ir.url\n                kwargs[\"image_mimetype\"] = ir.mimetype\n            else:\n                kwargs[\"image_path\"] = ir.get(\"path\", None)\n                kwargs[\"image_url\"] = ir.get(\"url\", None)\n                kwargs[\"image_mimetype\"] = ir.get(\"mimetype\", None)\n\n        mimetype = kwargs.get(\"image_mimetype\")\n        if not mimetype and kwargs.get(\"image_path\") is not None:\n            # guess mimetype from image_path\n            extension = Path(kwargs[\"image_path\"]).suffix.replace(\".\", \"\")\n            if ftype := filetype.get_type(ext=extension):\n                kwargs[\"image_mimetype\"] = ftype.mime\n\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.IMAGE\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageNode\"\n\n    def resolve_image(self) -> ImageType:\n        \"\"\"Resolve an image such that PIL can read it.\"\"\"\n        if self.image is not None:\n            import base64\n\n            return BytesIO(base64.b64decode(self.image))\n        elif self.image_path is not None:\n            return self.image_path\n        elif self.image_url is not None:\n            # load image from URL\n            import requests\n\n            response = requests.get(self.image_url)\n            return BytesIO(response.content)\n        else:\n            raise ValueError(\"No image found in node.\")\n\n    @property\n    def hash(self) -> str:\n        \"\"\"Get hash of node.\"\"\"\n        # doc identity depends on if image, image_path, or image_url is set\n        image_str = self.image or \"None\"\n        image_path_str = self.image_path or \"None\"\n        image_url_str = self.image_url or \"None\"\n        image_text = self.text or \"None\"\n        doc_identity = f\"{image_str}-{image_path_str}-{image_url_str}-{image_text}\"\n        return str(sha256(doc_identity.encode(\"utf-8\", \"surrogatepass\")).hexdigest())\n\n\nclass IndexNode(TextNode):\n    \"\"\"Node with reference to any object.\n\n    This can include other indices, query engines, retrievers.\n\n    This can also include other nodes (though this is overlapping with `relationships`\n    on the Node class).\n\n    \"\"\"\n\n    index_id: str\n    obj: Any = None\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        from llama_index.core.storage.docstore.utils import doc_to_json\n\n        data = super().dict(**kwargs)\n\n        try:\n            if self.obj is None:\n                data[\"obj\"] = None\n            elif isinstance(self.obj, BaseNode):\n                data[\"obj\"] = doc_to_json(self.obj)\n            elif isinstance(self.obj, BaseModel):\n                data[\"obj\"] = self.obj.model_dump()\n            else:\n                data[\"obj\"] = json.dumps(self.obj)\n        except Exception:\n            raise ValueError(\"IndexNode obj is not serializable: \" + str(self.obj))\n\n        return data\n\n    @classmethod\n    def from_text_node(\n        cls,\n        node: TextNode,\n        index_id: str,\n    ) -> IndexNode:\n        \"\"\"Create index node from text node.\"\"\"\n        # copy all attributes from text node, add index id\n        return cls(\n            **node.dict(),\n            index_id=index_id,\n        )\n\n    # TODO: return type here not supported by current mypy version\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        output = super().from_dict(data, **kwargs)\n\n        obj = data.get(\"obj\")\n        parsed_obj = None\n\n        if isinstance(obj, str):\n            parsed_obj = TextNode(text=obj)\n        elif isinstance(obj, dict):\n            from llama_index.core.storage.docstore.utils import json_to_doc\n\n            # check if its a node, else assume stringable\n            try:\n                parsed_obj = json_to_doc(obj)  # type: ignore[assignment]\n            except Exception:\n                parsed_obj = TextNode(text=str(obj))\n\n        output.obj = parsed_obj\n\n        return output\n\n    @classmethod\n    def get_type(cls) -> str:\n        return ObjectType.INDEX\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"IndexNode\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/schema.py",
    "filename": "schema.py",
    "relpath": "schema.py",
    "start_line": 913,
    "end_line": 1343,
    "length": 431,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__str__",
      "get_score",
      "class_name",
      "node_id",
      "id_",
      "text",
      "metadata",
      "embedding",
      "get_text",
      "get_content",
      "get_embedding",
      "__init__",
      "custom_model_dump",
      "text",
      "get_type",
      "doc_id",
      "doc_id",
      "__str__",
      "get_doc_id",
      "to_langchain_format",
      "from_langchain_format",
      "to_haystack_format",
      "from_haystack_format",
      "to_embedchain_format",
      "from_embedchain_format",
      "to_semantic_kernel_format",
      "from_semantic_kernel_format",
      "to_vectorflow",
      "example",
      "class_name",
      "to_cloud_document",
      "from_cloud_document",
      "__init__",
      "image",
      "image",
      "image_path",
      "image_path",
      "image_url",
      "image_url",
      "image_mimetype",
      "image_mimetype",
      "text_embedding",
      "text_embedding",
      "class_name",
      "resolve_image",
      "embedding_strs",
      "embedding_image",
      "__str__"
    ],
    "chunk_class_names": [
      "NodeWithScore",
      "Document",
      "ImageDocument",
      "class",
      "contains"
    ],
    "document_function_names": [
      "__get_pydantic_json_schema__",
      "class_name",
      "json",
      "custom_model_dump",
      "dict",
      "__getstate__",
      "__setstate__",
      "to_dict",
      "to_json",
      "from_dict",
      "from_json",
      "__call__",
      "acall",
      "class_name",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "hash",
      "node_id",
      "node_id",
      "source_node",
      "prev_node",
      "next_node",
      "parent_node",
      "child_nodes",
      "ref_doc_id",
      "extra_info",
      "extra_info",
      "__str__",
      "get_embedding",
      "as_related_node_info",
      "validate_data",
      "validate_mimetype",
      "hash",
      "class_name",
      "get_type",
      "get_content",
      "set_content",
      "hash",
      "__init__",
      "class_name",
      "hash",
      "get_type",
      "get_content",
      "get_metadata_str",
      "set_content",
      "get_node_info",
      "get_text",
      "node_info",
      "__init__",
      "get_type",
      "class_name",
      "resolve_image",
      "hash",
      "dict",
      "from_text_node",
      "from_dict",
      "get_type",
      "class_name",
      "__str__",
      "get_score",
      "class_name",
      "node_id",
      "id_",
      "text",
      "metadata",
      "embedding",
      "get_text",
      "get_content",
      "get_embedding",
      "__init__",
      "custom_model_dump",
      "text",
      "get_type",
      "doc_id",
      "doc_id",
      "__str__",
      "get_doc_id",
      "to_langchain_format",
      "from_langchain_format",
      "to_haystack_format",
      "from_haystack_format",
      "to_embedchain_format",
      "from_embedchain_format",
      "to_semantic_kernel_format",
      "from_semantic_kernel_format",
      "to_vectorflow",
      "example",
      "class_name",
      "to_cloud_document",
      "from_cloud_document",
      "__init__",
      "image",
      "image",
      "image_path",
      "image_path",
      "image_url",
      "image_url",
      "image_mimetype",
      "image_mimetype",
      "text_embedding",
      "text_embedding",
      "class_name",
      "resolve_image",
      "embedding_strs",
      "embedding_image",
      "__str__"
    ],
    "document_class_names": [
      "from",
      "BaseComponent",
      "names",
      "name",
      "name",
      "name",
      "had",
      "TransformComponent",
      "for",
      "NodeRelationship",
      "ObjectType",
      "Modality",
      "MetadataMode",
      "RelatedNodeInfo",
      "BaseNode",
      "MediaResource",
      "for",
      "represents",
      "Node",
      "TextNode",
      "ImageNode",
      "IndexNode",
      "NodeWithScore",
      "Document",
      "ImageDocument",
      "class",
      "contains"
    ],
    "content": "class NodeWithScore(BaseComponent):\n    node: SerializeAsAny[BaseNode]\n    score: Optional[float] = None\n\n    def __str__(self) -> str:\n        score_str = \"None\" if self.score is None else f\"{self.score: 0.3f}\"\n        return f\"{self.node}\\nScore: {score_str}\\n\"\n\n    def get_score(self, raise_error: bool = False) -> float:\n        \"\"\"Get score.\"\"\"\n        if self.score is None:\n            if raise_error:\n                raise ValueError(\"Score not set.\")\n            else:\n                return 0.0\n        else:\n            return self.score\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"NodeWithScore\"\n\n    ##### pass through methods to BaseNode #####\n    @property\n    def node_id(self) -> str:\n        return self.node.node_id\n\n    @property\n    def id_(self) -> str:\n        return self.node.id_\n\n    @property\n    def text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.text\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    @property\n    def metadata(self) -> Dict[str, Any]:\n        return self.node.metadata\n\n    @property\n    def embedding(self) -> Optional[List[float]]:\n        return self.node.embedding\n\n    def get_text(self) -> str:\n        if isinstance(self.node, TextNode):\n            return self.node.get_text()\n        else:\n            raise ValueError(\"Node must be a TextNode to get text.\")\n\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.NONE) -> str:\n        return self.node.get_content(metadata_mode=metadata_mode)\n\n    def get_embedding(self) -> List[float]:\n        return self.node.get_embedding()\n\n\n# Document Classes for Readers\n\n\nclass Document(Node):\n    \"\"\"Generic interface for a data document.\n\n    This document connects to data sources.\n    \"\"\"\n\n    def __init__(self, **data: Any) -> None:\n        \"\"\"Keeps backward compatibility with old 'Document' versions.\n\n        If 'text' was passed, store it in 'text_resource'.\n        If 'doc_id' was passed, store it in 'id_'.\n        If 'extra_info' was passed, store it in 'metadata'.\n        \"\"\"\n        if \"doc_id\" in data:\n            value = data.pop(\"doc_id\")\n            if \"id_\" in data:\n                msg = \"'doc_id' is deprecated and 'id_' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"id_\"] = value\n\n        if \"extra_info\" in data:\n            value = data.pop(\"extra_info\")\n            if \"metadata\" in data:\n                msg = \"'extra_info' is deprecated and 'metadata' will be used instead\"\n                logging.warning(msg)\n            else:\n                data[\"metadata\"] = value\n\n        if \"text\" in data:\n            text = data.pop(\"text\")\n            if \"text_resource\" in data:\n                text_resource = (\n                    data[\"text_resource\"]\n                    if isinstance(data[\"text_resource\"], MediaResource)\n                    else MediaResource.model_validate(data[\"text_resource\"])\n                )\n                if (text_resource.text or \"\").strip() != text.strip():\n                    msg = (\n                        \"'text' is deprecated and 'text_resource' will be used instead\"\n                    )\n                    logging.warning(msg)\n            else:\n                data[\"text_resource\"] = MediaResource(text=text)\n\n        super().__init__(**data)\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(\n        self, handler: SerializerFunctionWrapHandler, info: SerializationInfo\n    ) -> Dict[str, Any]:\n        \"\"\"For full backward compatibility with the text field, we customize the model serializer.\"\"\"\n        data = super().custom_model_dump(handler, info)\n        exclude_set = set(info.exclude or [])\n        if \"text\" not in exclude_set:\n            data[\"text\"] = self.text\n        return data\n\n    @property\n    def text(self) -> str:\n        \"\"\"Provided for backward compatibility, it returns the content of text_resource.\"\"\"\n        return self.get_content()\n\n    @classmethod\n    def get_type(cls) -> str:\n        \"\"\"Get Document type.\"\"\"\n        return ObjectType.DOCUMENT\n\n    @property\n    def doc_id(self) -> str:\n        \"\"\"Get document ID.\"\"\"\n        return self.id_\n\n    @doc_id.setter\n    def doc_id(self, id_: str) -> None:\n        self.id_ = id_\n\n    def __str__(self) -> str:\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Doc ID: {self.doc_id}\\n{source_text_wrapped}\"\n\n    @deprecated(\n        version=\"0.12.2\",\n        reason=\"'get_doc_id' is deprecated, access the 'id_' property instead.\",\n    )\n    def get_doc_id(self) -> str:  # pragma: nocover\n        return self.id_\n\n    def to_langchain_format(self) -> LCDocument:\n        \"\"\"Convert struct to LangChain document format.\"\"\"\n        from llama_index.core.bridge.langchain import (\n            Document as LCDocument,  # type: ignore\n        )\n\n        metadata = self.metadata or {}\n        return LCDocument(page_content=self.text, metadata=metadata, id=self.id_)\n\n    @classmethod\n    def from_langchain_format(cls, doc: LCDocument) -> Document:\n        \"\"\"Convert struct from LangChain document format.\"\"\"\n        if doc.id:\n            return cls(text=doc.page_content, metadata=doc.metadata, id_=doc.id)\n        return cls(text=doc.page_content, metadata=doc.metadata)\n\n    def to_haystack_format(self) -> HaystackDocument:\n        \"\"\"Convert struct to Haystack document format.\"\"\"\n        from haystack import Document as HaystackDocument  # type: ignore\n\n        return HaystackDocument(\n            content=self.text, meta=self.metadata, embedding=self.embedding, id=self.id_\n        )\n\n    @classmethod\n    def from_haystack_format(cls, doc: HaystackDocument) -> Document:\n        \"\"\"Convert struct from Haystack document format.\"\"\"\n        return cls(\n            text=doc.content, metadata=doc.meta, embedding=doc.embedding, id_=doc.id\n        )\n\n    def to_embedchain_format(self) -> Dict[str, Any]:\n        \"\"\"Convert struct to EmbedChain document format.\"\"\"\n        return {\n            \"doc_id\": self.id_,\n            \"data\": {\"content\": self.text, \"meta_data\": self.metadata},\n        }\n\n    @classmethod\n    def from_embedchain_format(cls, doc: Dict[str, Any]) -> Document:\n        \"\"\"Convert struct from EmbedChain document format.\"\"\"\n        return cls(\n            text=doc[\"data\"][\"content\"],\n            metadata=doc[\"data\"][\"meta_data\"],\n            id_=doc[\"doc_id\"],\n        )\n\n    def to_semantic_kernel_format(self) -> MemoryRecord:\n        \"\"\"Convert struct to Semantic Kernel document format.\"\"\"\n        import numpy as np\n        from semantic_kernel.memory.memory_record import MemoryRecord  # type: ignore\n\n        return MemoryRecord(\n            id=self.id_,\n            text=self.text,\n            additional_metadata=self.get_metadata_str(),\n            embedding=np.array(self.embedding) if self.embedding else None,\n        )\n\n    @classmethod\n    def from_semantic_kernel_format(cls, doc: MemoryRecord) -> Document:\n        \"\"\"Convert struct from Semantic Kernel document format.\"\"\"\n        return cls(\n            text=doc._text,\n            metadata={\"additional_metadata\": doc._additional_metadata},\n            embedding=doc._embedding.tolist() if doc._embedding is not None else None,\n            id_=doc._id,\n        )\n\n    def to_vectorflow(self, client: Any) -> None:\n        \"\"\"Send a document to vectorflow, since they don't have a document object.\"\"\"\n        # write document to temp file\n        import tempfile\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(self.text.encode(\"utf-8\"))\n            f.flush()\n            client.embed(f.name)\n\n    @classmethod\n    def example(cls) -> Document:\n        return Document(\n            text=SAMPLE_TEXT,\n            metadata={\"filename\": \"README.md\", \"category\": \"codebase\"},\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"Document\"\n\n    def to_cloud_document(self) -> CloudDocument:\n        \"\"\"Convert to LlamaCloud document type.\"\"\"\n        from llama_cloud.types.cloud_document import CloudDocument  # type: ignore\n\n        return CloudDocument(\n            text=self.text,\n            metadata=self.metadata,\n            excluded_embed_metadata_keys=self.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=self.excluded_llm_metadata_keys,\n            id=self.id_,\n        )\n\n    @classmethod\n    def from_cloud_document(\n        cls,\n        doc: CloudDocument,\n    ) -> Document:\n        \"\"\"Convert from LlamaCloud document type.\"\"\"\n        return Document(\n            text=doc.text,\n            metadata=doc.metadata,\n            excluded_embed_metadata_keys=doc.excluded_embed_metadata_keys,\n            excluded_llm_metadata_keys=doc.excluded_llm_metadata_keys,\n            id_=doc.id,\n        )\n\n\nclass ImageDocument(Document):\n    \"\"\"Backward compatible wrapper around Document containing an image.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        image = kwargs.pop(\"image\", None)\n        image_path = kwargs.pop(\"image_path\", None)\n        image_url = kwargs.pop(\"image_url\", None)\n        image_mimetype = kwargs.pop(\"image_mimetype\", None)\n        text_embedding = kwargs.pop(\"text_embedding\", None)\n\n        if image:\n            kwargs[\"image_resource\"] = MediaResource(\n                data=image, mimetype=image_mimetype\n            )\n        elif image_path:\n            kwargs[\"image_resource\"] = MediaResource(\n                path=image_path, mimetype=image_mimetype\n            )\n        elif image_url:\n            kwargs[\"image_resource\"] = MediaResource(\n                url=image_url, mimetype=image_mimetype\n            )\n\n        super().__init__(**kwargs)\n\n    @property\n    def image(self) -> str | None:\n        if self.image_resource and self.image_resource.data:\n            return self.image_resource.data.decode(\"utf-8\")\n        return None\n\n    @image.setter\n    def image(self, image: str) -> None:\n        self.image_resource = MediaResource(data=image.encode(\"utf-8\"))\n\n    @property\n    def image_path(self) -> str | None:\n        if self.image_resource and self.image_resource.path:\n            return str(self.image_resource.path)\n        return None\n\n    @image_path.setter\n    def image_path(self, image_path: str) -> None:\n        self.image_resource = MediaResource(path=Path(image_path))\n\n    @property\n    def image_url(self) -> str | None:\n        if self.image_resource and self.image_resource.url:\n            return str(self.image_resource.url)\n        return None\n\n    @image_url.setter\n    def image_url(self, image_url: str) -> None:\n        self.image_resource = MediaResource(url=AnyUrl(url=image_url))\n\n    @property\n    def image_mimetype(self) -> str | None:\n        if self.image_resource:\n            return self.image_resource.mimetype\n        return None\n\n    @image_mimetype.setter\n    def image_mimetype(self, image_mimetype: str) -> None:\n        if self.image_resource:\n            self.image_resource.mimetype = image_mimetype\n\n    @property\n    def text_embedding(self) -> list[float] | None:\n        if self.text_resource and self.text_resource.embeddings:\n            return self.text_resource.embeddings.get(\"dense\")\n        return None\n\n    @text_embedding.setter\n    def text_embedding(self, embeddings: list[float]) -> None:\n        if self.text_resource:\n            if self.text_resource.embeddings is None:\n                self.text_resource.embeddings = {}\n            self.text_resource.embeddings[\"dense\"] = embeddings\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImageDocument\"\n\n    def resolve_image(self, as_base64: bool = False) -> BytesIO:\n        \"\"\"Resolve an image such that PIL can read it.\n\n        Args:\n            as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n        \"\"\"\n        if self.image_resource is None:\n            return BytesIO()\n\n        if self.image_resource.data is not None:\n            if as_base64:\n                return BytesIO(self.image_resource.data)\n            return BytesIO(base64.b64decode(self.image_resource.data))\n        elif self.image_resource.path is not None:\n            img_bytes = self.image_resource.path.read_bytes()\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        elif self.image_resource.url is not None:\n            # load image from URL\n            response = requests.get(str(self.image_resource.url))\n            img_bytes = response.content\n            if as_base64:\n                return BytesIO(base64.b64encode(img_bytes))\n            return BytesIO(img_bytes)\n        else:\n            raise ValueError(\"No image found in the chat message!\")\n\n\n@dataclass\nclass QueryBundle(DataClassJsonMixin):\n    \"\"\"\n    Query bundle.\n\n    This dataclass contains the original query string and associated transformations.\n\n    Args:\n        query_str (str): the original user-specified query string.\n            This is currently used by all non embedding-based queries.\n        custom_embedding_strs (list[str]): list of strings used for embedding the query.\n            This is currently used by all embedding-based queries.\n        embedding (list[float]): the stored embedding for the query.\n    \"\"\"\n\n    query_str: str\n    # using single image path as query input\n    image_path: Optional[str] = None\n    custom_embedding_strs: Optional[List[str]] = None\n    embedding: Optional[List[float]] = None\n\n    @property\n    def embedding_strs(self) -> List[str]:\n        \"\"\"Use custom embedding strs if specified, otherwise use query str.\"\"\"\n        if self.custom_embedding_strs is None:\n            if len(self.query_str) == 0:\n                return []\n            return [self.query_str]\n        else:\n            return self.custom_embedding_strs\n\n    @property\n    def embedding_image(self) -> List[ImageType]:\n        \"\"\"Use image path for image retrieval.\"\"\"\n        if self.image_path is None:\n            return []\n        return [self.image_path]\n\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        return self.query_str\n\n\nQueryType = Union[str, QueryBundle]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/__init__.py",
    "filename": "__init__.py",
    "relpath": "__init__.py",
    "start_line": 1,
    "end_line": 162,
    "length": 162,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file of LlamaIndex.\"\"\"\n\n__version__ = \"0.12.22\"\n\nimport logging\nfrom logging import NullHandler\nfrom typing import Callable, Optional\n\ntry:\n    # Force pants to install eval_type_backport on 3.9\n    import eval_type_backport  # noqa  # type: ignore\nexcept ImportError:\n    pass\n\n# response\nfrom llama_index.core.base.response.schema import Response\n\n# import global eval handler\nfrom llama_index.core.callbacks.global_handlers import set_global_handler\nfrom llama_index.core.data_structs.struct_type import IndexStructType\nfrom llama_index.core.embeddings.mock_embed_model import MockEmbedding\n\n# indices\n# loading\nfrom llama_index.core.indices import (\n    ComposableGraph,\n    DocumentSummaryIndex,\n    GPTDocumentSummaryIndex,\n    GPTKeywordTableIndex,\n    GPTListIndex,\n    GPTRAKEKeywordTableIndex,\n    GPTSimpleKeywordTableIndex,\n    GPTTreeIndex,\n    GPTVectorStoreIndex,\n    KeywordTableIndex,\n    KnowledgeGraphIndex,\n    ListIndex,\n    PropertyGraphIndex,\n    RAKEKeywordTableIndex,\n    SimpleKeywordTableIndex,\n    SummaryIndex,\n    TreeIndex,\n    VectorStoreIndex,\n    load_graph_from_storage,\n    load_index_from_storage,\n    load_indices_from_storage,\n)\n\n# structured\nfrom llama_index.core.indices.common.struct_store.base import (\n    SQLDocumentContextBuilder,\n)\n\n# prompt helper\nfrom llama_index.core.indices.prompt_helper import PromptHelper\n\n# prompts\nfrom llama_index.core.prompts import (\n    BasePromptTemplate,\n    ChatPromptTemplate,\n    # backwards compatibility\n    Prompt,\n    PromptTemplate,\n    SelectorPromptTemplate,\n)\nfrom llama_index.core.readers import SimpleDirectoryReader, download_loader\n\n# Response Synthesizer\nfrom llama_index.core.response_synthesizers.factory import get_response_synthesizer\nfrom llama_index.core.schema import Document, QueryBundle\nfrom llama_index.core.service_context import (\n    ServiceContext,\n    set_global_service_context,\n)\n\n# global settings\nfrom llama_index.core.settings import Settings\n\n# storage\nfrom llama_index.core.storage.storage_context import StorageContext\n\n# sql wrapper\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\n\n# global tokenizer\nfrom llama_index.core.utils import get_tokenizer, set_global_tokenizer\n\n# best practices for library logging:\n# https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library\nlogging.getLogger(__name__).addHandler(NullHandler())\n\n__all__ = [\n    \"StorageContext\",\n    \"ServiceContext\",\n    \"ComposableGraph\",\n    # indices\n    \"SummaryIndex\",\n    \"VectorStoreIndex\",\n    \"SimpleKeywordTableIndex\",\n    \"KeywordTableIndex\",\n    \"RAKEKeywordTableIndex\",\n    \"TreeIndex\",\n    \"DocumentSummaryIndex\",\n    \"KnowledgeGraphIndex\",\n    \"PropertyGraphIndex\",\n    # indices - legacy names\n    \"GPTKeywordTableIndex\",\n    \"GPTKnowledgeGraphIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTListIndex\",\n    \"ListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTVectorStoreIndex\",\n    \"GPTDocumentSummaryIndex\",\n    \"Prompt\",\n    \"PromptTemplate\",\n    \"BasePromptTemplate\",\n    \"ChatPromptTemplate\",\n    \"SelectorPromptTemplate\",\n    \"SummaryPrompt\",\n    \"TreeInsertPrompt\",\n    \"TreeSelectPrompt\",\n    \"TreeSelectMultiplePrompt\",\n    \"RefinePrompt\",\n    \"QuestionAnswerPrompt\",\n    \"KeywordExtractPrompt\",\n    \"QueryKeywordExtractPrompt\",\n    \"Response\",\n    \"Document\",\n    \"SimpleDirectoryReader\",\n    \"VellumPredictor\",\n    \"VellumPromptRegistry\",\n    \"MockEmbedding\",\n    \"SQLDatabase\",\n    \"SQLDocumentContextBuilder\",\n    \"SQLContextBuilder\",\n    \"PromptHelper\",\n    \"IndexStructType\",\n    \"download_loader\",\n    \"load_graph_from_storage\",\n    \"load_index_from_storage\",\n    \"load_indices_from_storage\",\n    \"QueryBundle\",\n    \"get_response_synthesizer\",\n    \"set_global_service_context\",\n    \"set_global_handler\",\n    \"set_global_tokenizer\",\n    \"get_tokenizer\",\n    \"Settings\",\n]\n\n# eval global toggle\nfrom llama_index.core.callbacks.base_handler import BaseCallbackHandler\n\nglobal_handler: Optional[BaseCallbackHandler] = None\n\n# NOTE: keep for backwards compatibility\nSQLContextBuilder = SQLDocumentContextBuilder\n\n# global tokenizer\nglobal_tokenizer: Optional[Callable[[str], list]] = None"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/utils.py",
    "filename": "utils.py",
    "relpath": "utils.py",
    "start_line": 1,
    "end_line": 538,
    "length": 538,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "stopwords",
      "encode",
      "set_global_tokenizer",
      "get_tokenizer",
      "get_new_id",
      "get_new_int_id",
      "temp_set_attrs",
      "retry_on_exceptions_with_backoff",
      "aretry_on_exceptions_with_backoff",
      "get_retry_on_exceptions_with_backoff_decorator",
      "decorator",
      "wrapper",
      "awrapper",
      "foo",
      "truncate_text",
      "iter_batch",
      "concat_dirs",
      "get_tqdm_iterable",
      "count_tokens",
      "get_transformer_tokenizer_fn",
      "get_cache_dir",
      "add_sync_version",
      "_wrapper",
      "get_color_mapping",
      "_get_colored_text",
      "print_text"
    ],
    "chunk_class_names": [
      "from",
      "GlobalsHelper",
      "Tokenizer",
      "for",
      "class"
    ],
    "document_function_names": [
      "__init__",
      "stopwords",
      "encode",
      "set_global_tokenizer",
      "get_tokenizer",
      "get_new_id",
      "get_new_int_id",
      "temp_set_attrs",
      "retry_on_exceptions_with_backoff",
      "aretry_on_exceptions_with_backoff",
      "get_retry_on_exceptions_with_backoff_decorator",
      "decorator",
      "wrapper",
      "awrapper",
      "foo",
      "truncate_text",
      "iter_batch",
      "concat_dirs",
      "get_tqdm_iterable",
      "count_tokens",
      "get_transformer_tokenizer_fn",
      "get_cache_dir",
      "add_sync_version",
      "_wrapper",
      "get_color_mapping",
      "_get_colored_text",
      "print_text",
      "infer_torch_device",
      "unit_generator",
      "async_unit_generator",
      "resolve_binary"
    ],
    "document_class_names": [
      "from",
      "GlobalsHelper",
      "Tokenizer",
      "for",
      "class"
    ],
    "content": "\"\"\"General utils functions.\"\"\"\n\nimport asyncio\nimport base64\nimport os\nimport random\nimport sys\nimport time\nimport traceback\nimport uuid\nfrom binascii import Error as BinasciiError\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom io import BytesIO\nfrom itertools import islice\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    List,\n    Optional,\n    Protocol,\n    Set,\n    Type,\n    Union,\n    runtime_checkable,\n)\n\nimport requests\n\n\nclass GlobalsHelper:\n    \"\"\"Helper to retrieve globals.\n\n    Helpful for global caching of certain variables that can be expensive to load.\n    (e.g. tokenization)\n\n    \"\"\"\n\n    _stopwords: Optional[List[str]] = None\n    _nltk_data_dir: Optional[str] = None\n\n    def __init__(self) -> None:\n        \"\"\"Initialize NLTK stopwords and punkt.\"\"\"\n        import nltk\n\n        self._nltk_data_dir = os.environ.get(\n            \"NLTK_DATA\",\n            os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"_static/nltk_cache\",\n            ),\n        )\n\n        if self._nltk_data_dir not in nltk.data.path:\n            nltk.data.path.append(self._nltk_data_dir)\n\n        # ensure access to data is there\n        try:\n            nltk.data.find(\"corpora/stopwords\")\n        except LookupError:\n            nltk.download(\"stopwords\", download_dir=self._nltk_data_dir, quiet=True)\n\n        try:\n            nltk.data.find(\"tokenizers/punkt_tab\")\n        except LookupError:\n            nltk.download(\"punkt_tab\", download_dir=self._nltk_data_dir, quiet=True)\n\n    @property\n    def stopwords(self) -> List[str]:\n        \"\"\"Get stopwords.\"\"\"\n        if self._stopwords is None:\n            try:\n                import nltk\n                from nltk.corpus import stopwords\n            except ImportError:\n                raise ImportError(\n                    \"`nltk` package not found, please run `pip install nltk`\"\n                )\n\n            try:\n                nltk.data.find(\"corpora/stopwords\", paths=[self._nltk_data_dir])\n            except LookupError:\n                nltk.download(\"stopwords\", download_dir=self._nltk_data_dir, quiet=True)\n            self._stopwords = stopwords.words(\"english\")\n        return self._stopwords\n\n\nglobals_helper = GlobalsHelper()\n\n\n# Global Tokenizer\n@runtime_checkable\nclass Tokenizer(Protocol):\n    def encode(self, text: str, *args: Any, **kwargs: Any) -> List[Any]:\n        ...\n\n\ndef set_global_tokenizer(tokenizer: Union[Tokenizer, Callable[[str], list]]) -> None:\n    import llama_index.core\n\n    if isinstance(tokenizer, Tokenizer):\n        llama_index.core.global_tokenizer = tokenizer.encode\n    else:\n        llama_index.core.global_tokenizer = tokenizer\n\n\ndef get_tokenizer() -> Callable[[str], List]:\n    import llama_index.core\n\n    if llama_index.core.global_tokenizer is None:\n        tiktoken_import_err = (\n            \"`tiktoken` package not found, please run `pip install tiktoken`\"\n        )\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(tiktoken_import_err)\n\n        # set tokenizer cache temporarily\n        should_revert = False\n        if \"TIKTOKEN_CACHE_DIR\" not in os.environ:\n            should_revert = True\n            os.environ[\"TIKTOKEN_CACHE_DIR\"] = os.path.join(\n                os.path.dirname(os.path.abspath(__file__)),\n                \"_static/tiktoken_cache\",\n            )\n\n        enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n        tokenizer = partial(enc.encode, allowed_special=\"all\")\n        set_global_tokenizer(tokenizer)\n\n        if should_revert:\n            del os.environ[\"TIKTOKEN_CACHE_DIR\"]\n\n    assert llama_index.core.global_tokenizer is not None\n    return llama_index.core.global_tokenizer\n\n\ndef get_new_id(d: Set) -> str:\n    \"\"\"Get a new ID.\"\"\"\n    while True:\n        new_id = str(uuid.uuid4())\n        if new_id not in d:\n            break\n    return new_id\n\n\ndef get_new_int_id(d: Set) -> int:\n    \"\"\"Get a new integer ID.\"\"\"\n    while True:\n        new_id = random.randint(0, sys.maxsize)\n        if new_id not in d:\n            break\n    return new_id\n\n\n@contextmanager\ndef temp_set_attrs(obj: Any, **kwargs: Any) -> Generator:\n    \"\"\"Temporary setter.\n\n    Utility class for setting a temporary value for an attribute on a class.\n    Taken from: https://tinyurl.com/2p89xymh\n\n    \"\"\"\n    prev_values = {k: getattr(obj, k) for k in kwargs}\n    for k, v in kwargs.items():\n        setattr(obj, k, v)\n    try:\n        yield\n    finally:\n        for k, v in prev_values.items():\n            setattr(obj, k, v)\n\n\n@dataclass\nclass ErrorToRetry:\n    \"\"\"Exception types that should be retried.\n\n    Args:\n        exception_cls (Type[Exception]): Class of exception.\n        check_fn (Optional[Callable[[Any]], bool]]):\n            A function that takes an exception instance as input and returns\n            whether to retry.\n\n    \"\"\"\n\n    exception_cls: Type[Exception]\n    check_fn: Optional[Callable[[Any], bool]] = None\n\n\ndef retry_on_exceptions_with_backoff(\n    lambda_fn: Callable,\n    errors_to_retry: List[ErrorToRetry],\n    max_tries: int = 10,\n    min_backoff_secs: float = 0.5,\n    max_backoff_secs: float = 60.0,\n) -> Any:\n    \"\"\"Execute lambda function with retries and exponential backoff.\n\n    Args:\n        lambda_fn (Callable): Function to be called and output we want.\n        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n            At least one needs to be provided.\n        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n            Defaults to 0.5.\n        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n            Defaults to 60.\n\n    \"\"\"\n    if not errors_to_retry:\n        raise ValueError(\"At least one error to retry needs to be provided\")\n\n    error_checks = {\n        error_to_retry.exception_cls: error_to_retry.check_fn\n        for error_to_retry in errors_to_retry\n    }\n    exception_class_tuples = tuple(error_checks.keys())\n\n    backoff_secs = min_backoff_secs\n    tries = 0\n\n    while True:\n        try:\n            return lambda_fn()\n        except exception_class_tuples as e:\n            traceback.print_exc()\n            tries += 1\n            if tries >= max_tries:\n                raise\n            check_fn = error_checks.get(e.__class__)\n            if check_fn and not check_fn(e):\n                raise\n            time.sleep(backoff_secs)\n            backoff_secs = min(backoff_secs * 2, max_backoff_secs)\n\n\nasync def aretry_on_exceptions_with_backoff(\n    async_fn: Callable,\n    errors_to_retry: List[ErrorToRetry],\n    max_tries: int = 10,\n    min_backoff_secs: float = 0.5,\n    max_backoff_secs: float = 60.0,\n) -> Any:\n    \"\"\"Execute lambda function with retries and exponential backoff.\n\n    Args:\n        async_fn (Callable): Async Function to be called and output we want.\n        errors_to_retry (List[ErrorToRetry]): List of errors to retry.\n            At least one needs to be provided.\n        max_tries (int): Maximum number of tries, including the first. Defaults to 10.\n        min_backoff_secs (float): Minimum amount of backoff time between attempts.\n            Defaults to 0.5.\n        max_backoff_secs (float): Maximum amount of backoff time between attempts.\n            Defaults to 60.\n\n    \"\"\"\n    if not errors_to_retry:\n        raise ValueError(\"At least one error to retry needs to be provided\")\n\n    error_checks = {\n        error_to_retry.exception_cls: error_to_retry.check_fn\n        for error_to_retry in errors_to_retry\n    }\n    exception_class_tuples = tuple(error_checks.keys())\n\n    backoff_secs = min_backoff_secs\n    tries = 0\n\n    while True:\n        try:\n            return await async_fn()\n        except exception_class_tuples as e:\n            traceback.print_exc()\n            tries += 1\n            if tries >= max_tries:\n                raise\n            check_fn = error_checks.get(e.__class__)\n            if check_fn and not check_fn(e):\n                raise\n            time.sleep(backoff_secs)\n            backoff_secs = min(backoff_secs * 2, max_backoff_secs)\n\n\ndef get_retry_on_exceptions_with_backoff_decorator(\n    *retry_args: Any, **retry_kwargs: Any\n) -> Callable:\n    \"\"\"Return a decorator that retries with exponential backoff on provided exceptions.\"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        @wraps(func)\n        def wrapper(*func_args: Any, **func_kwargs: Any) -> Any:\n            return retry_on_exceptions_with_backoff(\n                lambda: func(*func_args, **func_kwargs), *retry_args, **retry_kwargs\n            )\n\n        @wraps(func)\n        async def awrapper(*func_args: Any, **func_kwargs: Any) -> Any:\n            async def foo() -> Any:\n                return await func(*func_args, **func_kwargs)\n\n            return await aretry_on_exceptions_with_backoff(\n                foo, *retry_args, **retry_kwargs\n            )\n\n        return awrapper if asyncio.iscoroutinefunction(func) else wrapper\n\n    return decorator\n\n\ndef truncate_text(text: str, max_length: int) -> str:\n    \"\"\"Truncate text to a maximum length.\"\"\"\n    if len(text) <= max_length:\n        return text\n    return text[: max_length - 3] + \"...\"\n\n\ndef iter_batch(iterable: Union[Iterable, Generator], size: int) -> Iterable:\n    \"\"\"Iterate over an iterable in batches.\n\n    >>> list(iter_batch([1,2,3,4,5], 3))\n    [[1, 2, 3], [4, 5]]\n    \"\"\"\n    source_iter = iter(iterable)\n    while source_iter:\n        b = list(islice(source_iter, size))\n        if len(b) == 0:\n            break\n        yield b\n\n\ndef concat_dirs(dirname: str, basename: str) -> str:\n    \"\"\"\n    Append basename to dirname, avoiding backslashes when running on windows.\n\n    os.path.join(dirname, basename) will add a backslash before dirname if\n    basename does not end with a slash, so we make sure it does.\n    \"\"\"\n    dirname += \"/\" if dirname[-1] != \"/\" else \"\"\n    return os.path.join(dirname, basename)\n\n\ndef get_tqdm_iterable(items: Iterable, show_progress: bool, desc: str) -> Iterable:\n    \"\"\"\n    Optionally get a tqdm iterable. Ensures tqdm.auto is used.\n    \"\"\"\n    _iterator = items\n    if show_progress:\n        try:\n            from tqdm.auto import tqdm\n\n            return tqdm(items, desc=desc)\n        except ImportError:\n            pass\n    return _iterator\n\n\ndef count_tokens(text: str) -> int:\n    tokenizer = get_tokenizer()\n    tokens = tokenizer(text)\n    return len(tokens)\n\n\ndef get_transformer_tokenizer_fn(model_name: str) -> Callable[[str], List[str]]:\n    \"\"\"\n    Args:\n        model_name(str): the model name of the tokenizer.\n                        For instance, fxmarty/tiny-llama-fast-tokenizer.\n    \"\"\"\n    try:\n        from transformers import AutoTokenizer  # pants: no-infer-dep\n    except ImportError:\n        raise ValueError(\n            \"`transformers` package not found, please run `pip install transformers`\"\n        )\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return tokenizer.tokenize\n\n\ndef get_cache_dir() -> str:\n    \"\"\"Locate a platform-appropriate cache directory for llama_index,\n    and create it if it doesn't yet exist.\n    \"\"\"\n    # User override\n    if \"LLAMA_INDEX_CACHE_DIR\" in os.environ:\n        path = Path(os.environ[\"LLAMA_INDEX_CACHE_DIR\"])\n\n    # Linux, Unix, AIX, etc.\n    elif os.name == \"posix\" and sys.platform != \"darwin\":\n        path = Path(\"/tmp/llama_index\")\n\n    # Mac OS\n    elif sys.platform == \"darwin\":\n        path = Path(os.path.expanduser(\"~\"), \"Library/Caches/llama_index\")\n\n    # Windows (hopefully)\n    else:\n        local = os.environ.get(\"LOCALAPPDATA\", None) or os.path.expanduser(\n            \"~\\\\AppData\\\\Local\"\n        )\n        path = Path(local, \"llama_index\")\n\n    if not os.path.exists(path):\n        os.makedirs(\n            path, exist_ok=True\n        )  # prevents https://github.com/jerryjliu/llama_index/issues/7362\n    return str(path)\n\n\ndef add_sync_version(func: Any) -> Any:\n    \"\"\"Decorator for adding sync version of an async function. The sync version\n    is added as a function attribute to the original function, func.\n\n    Args:\n        func(Any): the async function for which a sync variant will be built.\n    \"\"\"\n    assert asyncio.iscoroutinefunction(func)\n\n    @wraps(func)\n    def _wrapper(*args: Any, **kwds: Any) -> Any:\n        return asyncio.get_event_loop().run_until_complete(func(*args, **kwds))\n\n    func.sync = _wrapper\n    return func\n\n\n# Sample text from llama_index's readme\nSAMPLE_TEXT = \"\"\"\nContext\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\nThey are pre-trained on large amounts of publicly available data.\nHow do we best augment LLMs with our own private data?\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\n\nProposed Solution\nThat's where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\nyou build LLM  apps. It provides the following tools:\n\nOffers data connectors to ingest your existing data sources and data formats\n(APIs, PDFs, docs, SQL, etc.)\nProvides ways to structure your data (indices, graphs) so that this data can be\neasily used with LLMs.\nProvides an advanced retrieval/query interface over your data:\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\nAllows easy integrations with your outer application framework\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\nLlamaIndex provides tools for both beginner users and advanced users.\nOur high-level API allows beginner users to use LlamaIndex to ingest and\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\nreranking modules), to fit their needs.\n\"\"\"\n\n_LLAMA_INDEX_COLORS = {\n    \"llama_pink\": \"38;2;237;90;200\",\n    \"llama_blue\": \"38;2;90;149;237\",\n    \"llama_turquoise\": \"38;2;11;159;203\",\n    \"llama_lavender\": \"38;2;155;135;227\",\n}\n\n_ANSI_COLORS = {\n    \"red\": \"31\",\n    \"green\": \"32\",\n    \"yellow\": \"33\",\n    \"blue\": \"34\",\n    \"magenta\": \"35\",\n    \"cyan\": \"36\",\n    \"pink\": \"38;5;200\",\n}\n\n\ndef get_color_mapping(\n    items: List[str], use_llama_index_colors: bool = True\n) -> Dict[str, str]:\n    \"\"\"\n    Get a mapping of items to colors.\n\n    Args:\n        items (List[str]): List of items to be mapped to colors.\n        use_llama_index_colors (bool, optional): Flag to indicate\n        whether to use LlamaIndex colors or ANSI colors.\n            Defaults to True.\n\n    Returns:\n        Dict[str, str]: Mapping of items to colors.\n    \"\"\"\n    if use_llama_index_colors:\n        color_palette = _LLAMA_INDEX_COLORS\n    else:\n        color_palette = _ANSI_COLORS\n\n    colors = list(color_palette.keys())\n    return {item: colors[i % len(colors)] for i, item in enumerate(items)}\n\n\ndef _get_colored_text(text: str, color: str) -> str:\n    \"\"\"\n    Get the colored version of the input text.\n\n    Args:\n        text (str): Input text.\n        color (str): Color to be applied to the text.\n\n    Returns:\n        str: Colored version of the input text.\n    \"\"\"\n    all_colors = {**_LLAMA_INDEX_COLORS, **_ANSI_COLORS}\n\n    if color not in all_colors:\n        return f\"\\033[1;3m{text}\\033[0m\"  # just bolded and italicized\n\n    color = all_colors[color]\n\n    return f\"\\033[1;3;{color}m{text}\\033[0m\"\n\n\ndef print_text(text: str, color: Optional[str] = None, end: str = \"\") -> None:\n    \"\"\"\n    Print the text with the specified color.\n\n    Args:\n        text (str): Text to be printed.\n        color (str, optional): Color to be applied to the text. Supported colors are:\n            llama_pink, llama_blue, llama_turquoise, llama_lavender,\n            red, green, yellow, blue, magenta, cyan, pink.\n        end (str, optional): String appended after the last character of the text.\n\n    Returns:\n        None\n    \"\"\"\n    text_to_print = _get_colored_text(text, color) if color is not None else text\n    print(text_to_print, end=end)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/utils.py",
    "filename": "utils.py",
    "relpath": "utils.py",
    "start_line": 538,
    "end_line": 637,
    "length": 100,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "infer_torch_device",
      "unit_generator",
      "async_unit_generator",
      "resolve_binary"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "stopwords",
      "encode",
      "set_global_tokenizer",
      "get_tokenizer",
      "get_new_id",
      "get_new_int_id",
      "temp_set_attrs",
      "retry_on_exceptions_with_backoff",
      "aretry_on_exceptions_with_backoff",
      "get_retry_on_exceptions_with_backoff_decorator",
      "decorator",
      "wrapper",
      "awrapper",
      "foo",
      "truncate_text",
      "iter_batch",
      "concat_dirs",
      "get_tqdm_iterable",
      "count_tokens",
      "get_transformer_tokenizer_fn",
      "get_cache_dir",
      "add_sync_version",
      "_wrapper",
      "get_color_mapping",
      "_get_colored_text",
      "print_text",
      "infer_torch_device",
      "unit_generator",
      "async_unit_generator",
      "resolve_binary"
    ],
    "document_class_names": [
      "from",
      "GlobalsHelper",
      "Tokenizer",
      "for",
      "class"
    ],
    "content": "def infer_torch_device() -> str:\n    \"\"\"Infer the input to torch.device.\"\"\"\n    try:\n        has_cuda = torch.cuda.is_available()\n    except NameError:\n        import torch  # pants: no-infer-dep\n\n        has_cuda = torch.cuda.is_available()\n    if has_cuda:\n        return \"cuda\"\n    if torch.backends.mps.is_available():\n        return \"mps\"\n    return \"cpu\"\n\n\ndef unit_generator(x: Any) -> Generator[Any, None, None]:\n    \"\"\"A function that returns a generator of a single element.\n\n    Args:\n        x (Any): the element to build yield\n\n    Yields:\n        Any: the single element\n    \"\"\"\n    yield x\n\n\nasync def async_unit_generator(x: Any) -> AsyncGenerator[Any, None]:\n    \"\"\"A function that returns a generator of a single element.\n\n    Args:\n        x (Any): the element to build yield\n\n    Yields:\n        Any: the single element\n    \"\"\"\n    yield x\n\n\ndef resolve_binary(\n    raw_bytes: Optional[bytes] = None,\n    path: Optional[Union[str, Path]] = None,\n    url: Optional[str] = None,\n    as_base64: bool = False,\n) -> BytesIO:\n    \"\"\"Resolve binary data from various sources into a BytesIO object.\n\n    Args:\n        raw_bytes: Raw bytes data\n        path: File path to read bytes from\n        url: URL to fetch bytes from\n        as_base64: Whether to base64 encode the output bytes\n\n    Returns:\n        BytesIO object containing the binary data\n\n    Raises:\n        ValueError: If no valid source is provided\n    \"\"\"\n    if raw_bytes is not None:\n        # check if raw_bytes is base64 encoded\n        try:\n            decoded_bytes = base64.b64decode(raw_bytes)\n        except Exception:\n            decoded_bytes = raw_bytes\n\n        try:\n            # Check if raw_bytes is already base64 encoded.\n            # b64decode() can succeed on random binary data, so we\n            # pass verify=True to make sure it's not a false positive\n            decoded_bytes = base64.b64decode(raw_bytes, validate=True)\n        except BinasciiError:\n            # b64decode failed, leave as is\n            decoded_bytes = raw_bytes\n\n        if as_base64:\n            return BytesIO(base64.b64encode(decoded_bytes))\n        return BytesIO(decoded_bytes)\n\n    elif path is not None:\n        path = Path(path) if isinstance(path, str) else path\n        data = path.read_bytes()\n        if as_base64:\n            return BytesIO(base64.b64encode(data))\n        return BytesIO(data)\n\n    elif url is not None:\n        headers = {\n            \"User-Agent\": \"LlamaIndex/0.0 (https://llamaindex.ai; info@llamaindex.ai) llama-index-core/0.0\"\n        }\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        if as_base64:\n            return BytesIO(base64.b64encode(response.content))\n        return BytesIO(response.content)\n\n    raise ValueError(\"No valid source provided to resolve binary data!\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/exec_utils.py",
    "filename": "exec_utils.py",
    "relpath": "exec_utils.py",
    "start_line": 1,
    "end_line": 180,
    "length": 180,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_restricted_import",
      "_get_restricted_globals",
      "__init__",
      "visit_Name",
      "visit_Attribute",
      "_contains_protected_access",
      "_verify_source_safety",
      "safe_eval",
      "safe_exec"
    ],
    "chunk_class_names": [
      "DunderVisitor"
    ],
    "document_function_names": [
      "_restricted_import",
      "_get_restricted_globals",
      "__init__",
      "visit_Name",
      "visit_Attribute",
      "_contains_protected_access",
      "_verify_source_safety",
      "safe_eval",
      "safe_exec"
    ],
    "document_class_names": [
      "DunderVisitor"
    ],
    "content": "import ast\nimport copy\nfrom types import CodeType, ModuleType\nfrom typing import Any, Dict, Mapping, Sequence, Union\n\nALLOWED_IMPORTS = {\n    \"math\",\n    \"time\",\n    \"datetime\",\n    \"pandas\",\n    \"scipy\",\n    \"numpy\",\n    \"matplotlib\",\n    \"plotly\",\n    \"seaborn\",\n}\n\n\ndef _restricted_import(\n    name: str,\n    globals: Union[Mapping[str, object], None] = None,\n    locals: Union[Mapping[str, object], None] = None,\n    fromlist: Sequence[str] = (),\n    level: int = 0,\n) -> ModuleType:\n    if name in ALLOWED_IMPORTS:\n        return __import__(name, globals, locals, fromlist, level)\n    raise ImportError(f\"Import of module '{name}' is not allowed\")\n\n\nALLOWED_BUILTINS = {\n    \"abs\": abs,\n    \"all\": all,\n    \"any\": any,\n    \"ascii\": ascii,\n    \"bin\": bin,\n    \"bool\": bool,\n    \"bytearray\": bytearray,\n    \"bytes\": bytes,\n    \"chr\": chr,\n    \"complex\": complex,\n    \"divmod\": divmod,\n    \"enumerate\": enumerate,\n    \"filter\": filter,\n    \"float\": float,\n    \"format\": format,\n    \"frozenset\": frozenset,\n    \"hash\": hash,\n    \"hex\": hex,\n    \"int\": int,\n    \"isinstance\": isinstance,\n    \"issubclass\": issubclass,\n    \"len\": len,\n    \"list\": list,\n    \"map\": map,\n    \"max\": max,\n    \"min\": min,\n    \"oct\": oct,\n    \"ord\": ord,\n    \"pow\": pow,\n    \"print\": print,\n    \"range\": range,\n    \"repr\": repr,\n    \"reversed\": reversed,\n    \"round\": round,\n    \"set\": set,\n    \"slice\": slice,\n    \"sorted\": sorted,\n    \"str\": str,\n    \"sum\": sum,\n    \"tuple\": tuple,\n    \"type\": type,\n    \"zip\": zip,\n    # Constants\n    \"True\": True,\n    \"False\": False,\n    \"None\": None,\n    \"__import__\": _restricted_import,\n}\n\n\ndef _get_restricted_globals(__globals: Union[dict, None]) -> Any:\n    restricted_globals = copy.deepcopy(ALLOWED_BUILTINS)\n    if __globals:\n        restricted_globals.update(__globals)\n    return restricted_globals\n\n\nvulnerable_code_snippets = [\n    \"os.\",\n]\n\n\nclass DunderVisitor(ast.NodeVisitor):\n    def __init__(self) -> None:\n        self.has_access_to_private_entity = False\n        self.has_access_to_disallowed_builtin = False\n\n        builtins = globals()[\"__builtins__\"].keys()\n        self._builtins = builtins\n\n    def visit_Name(self, node: ast.Name) -> None:\n        if node.id.startswith(\"_\"):\n            self.has_access_to_private_entity = True\n        if node.id not in ALLOWED_BUILTINS and node.id in self._builtins:\n            self.has_access_to_disallowed_builtin = True\n        self.generic_visit(node)\n\n    def visit_Attribute(self, node: ast.Attribute) -> None:\n        if node.attr.startswith(\"_\"):\n            self.has_access_to_private_entity = True\n        if node.attr not in ALLOWED_BUILTINS and node.attr in self._builtins:\n            self.has_access_to_disallowed_builtin = True\n        self.generic_visit(node)\n\n\ndef _contains_protected_access(code: str) -> bool:\n    # do not allow imports\n    imports_modules = False\n    tree = ast.parse(code)\n    for node in ast.iter_child_nodes(tree):\n        if isinstance(node, ast.Import):\n            imports_modules = True\n        elif isinstance(node, ast.ImportFrom):\n            imports_modules = True\n        else:\n            continue\n\n    dunder_visitor = DunderVisitor()\n    dunder_visitor.visit(tree)\n\n    for vulnerable_code_snippet in vulnerable_code_snippets:\n        if vulnerable_code_snippet in code:\n            dunder_visitor.has_access_to_disallowed_builtin = True\n\n    return (\n        dunder_visitor.has_access_to_private_entity\n        or dunder_visitor.has_access_to_disallowed_builtin\n        or imports_modules\n    )\n\n\ndef _verify_source_safety(__source: Union[str, bytes, CodeType]) -> None:\n    \"\"\"\n    Verify that the source is safe to execute. For now, this means that it\n    does not contain any references to private or dunder methods.\n    \"\"\"\n    if isinstance(__source, CodeType):\n        raise RuntimeError(\"Direct execution of CodeType is forbidden!\")\n    if isinstance(__source, bytes):\n        __source = __source.decode()\n    if _contains_protected_access(__source):\n        raise RuntimeError(\n            \"Execution of code containing references to private or dunder methods, \"\n            \"disallowed builtins, or any imports, is forbidden!\"\n        )\n\n\ndef safe_eval(\n    __source: Union[str, bytes, CodeType],\n    __globals: Union[Dict[str, Any], None] = None,\n    __locals: Union[Mapping[str, object], None] = None,\n) -> Any:\n    \"\"\"\n    eval within safe global context.\n    \"\"\"\n    _verify_source_safety(__source)\n    return eval(__source, _get_restricted_globals(__globals), __locals)\n\n\ndef safe_exec(\n    __source: Union[str, bytes, CodeType],\n    __globals: Union[Dict[str, Any], None] = None,\n    __locals: Union[Mapping[str, object], None] = None,\n) -> None:\n    \"\"\"\n    eval within safe global context.\n    \"\"\"\n    _verify_source_safety(__source)\n    return exec(__source, _get_restricted_globals(__globals), __locals)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/settings.py",
    "filename": "settings.py",
    "relpath": "settings.py",
    "start_line": 1,
    "end_line": 248,
    "length": 248,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "llm",
      "llm",
      "pydantic_program_mode",
      "pydantic_program_mode",
      "embed_model",
      "embed_model",
      "global_handler",
      "global_handler",
      "callback_manager",
      "callback_manager",
      "tokenizer",
      "tokenizer",
      "node_parser",
      "node_parser",
      "chunk_size",
      "chunk_size",
      "chunk_overlap",
      "chunk_overlap",
      "text_splitter",
      "text_splitter",
      "prompt_helper",
      "prompt_helper",
      "num_output",
      "num_output",
      "context_window",
      "context_window",
      "transformations",
      "transformations"
    ],
    "chunk_class_names": [
      "from",
      "class"
    ],
    "document_function_names": [
      "llm",
      "llm",
      "pydantic_program_mode",
      "pydantic_program_mode",
      "embed_model",
      "embed_model",
      "global_handler",
      "global_handler",
      "callback_manager",
      "callback_manager",
      "tokenizer",
      "tokenizer",
      "node_parser",
      "node_parser",
      "chunk_size",
      "chunk_size",
      "chunk_overlap",
      "chunk_overlap",
      "text_splitter",
      "text_splitter",
      "prompt_helper",
      "prompt_helper",
      "num_output",
      "num_output",
      "context_window",
      "context_window",
      "transformations",
      "transformations"
    ],
    "document_class_names": [
      "from",
      "class"
    ],
    "content": "from dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional\n\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import BaseCallbackHandler, CallbackManager\nfrom llama_index.core.embeddings.utils import EmbedType, resolve_embed_model\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.llms.utils import LLMType, resolve_llm\nfrom llama_index.core.node_parser import NodeParser, SentenceSplitter\nfrom llama_index.core.schema import TransformComponent\nfrom llama_index.core.types import PydanticProgramMode\nfrom llama_index.core.utils import get_tokenizer, set_global_tokenizer\n\n\n@dataclass\nclass _Settings:\n    \"\"\"Settings for the Llama Index, lazily initialized.\"\"\"\n\n    # lazy initialization\n    _llm: Optional[LLM] = None\n    _embed_model: Optional[BaseEmbedding] = None\n    _callback_manager: Optional[CallbackManager] = None\n    _tokenizer: Optional[Callable[[str], List[Any]]] = None\n    _node_parser: Optional[NodeParser] = None\n    _prompt_helper: Optional[PromptHelper] = None\n    _transformations: Optional[List[TransformComponent]] = None\n\n    # ---- LLM ----\n\n    @property\n    def llm(self) -> LLM:\n        \"\"\"Get the LLM.\"\"\"\n        if self._llm is None:\n            self._llm = resolve_llm(\"default\")\n\n        if self._callback_manager is not None:\n            self._llm.callback_manager = self._callback_manager\n\n        return self._llm\n\n    @llm.setter\n    def llm(self, llm: LLMType) -> None:\n        \"\"\"Set the LLM.\"\"\"\n        self._llm = resolve_llm(llm)\n\n    @property\n    def pydantic_program_mode(self) -> PydanticProgramMode:\n        \"\"\"Get the pydantic program mode.\"\"\"\n        return self.llm.pydantic_program_mode\n\n    @pydantic_program_mode.setter\n    def pydantic_program_mode(self, pydantic_program_mode: PydanticProgramMode) -> None:\n        \"\"\"Set the pydantic program mode.\"\"\"\n        self.llm.pydantic_program_mode = pydantic_program_mode\n\n    # ---- Embedding ----\n\n    @property\n    def embed_model(self) -> BaseEmbedding:\n        \"\"\"Get the embedding model.\"\"\"\n        if self._embed_model is None:\n            self._embed_model = resolve_embed_model(\"default\")\n\n        if self._callback_manager is not None:\n            self._embed_model.callback_manager = self._callback_manager\n\n        return self._embed_model\n\n    @embed_model.setter\n    def embed_model(self, embed_model: EmbedType) -> None:\n        \"\"\"Set the embedding model.\"\"\"\n        self._embed_model = resolve_embed_model(embed_model)\n\n    # ---- Callbacks ----\n\n    @property\n    def global_handler(self) -> Optional[BaseCallbackHandler]:\n        \"\"\"Get the global handler.\"\"\"\n        import llama_index.core\n\n        # TODO: deprecated?\n        return llama_index.core.global_handler\n\n    @global_handler.setter\n    def global_handler(self, eval_mode: str, **eval_params: Any) -> None:\n        \"\"\"Set the global handler.\"\"\"\n        from llama_index.core import set_global_handler\n\n        # TODO: deprecated?\n        set_global_handler(eval_mode, **eval_params)\n\n    @property\n    def callback_manager(self) -> CallbackManager:\n        \"\"\"Get the callback manager.\"\"\"\n        if self._callback_manager is None:\n            self._callback_manager = CallbackManager()\n        return self._callback_manager\n\n    @callback_manager.setter\n    def callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set the callback manager.\"\"\"\n        self._callback_manager = callback_manager\n\n    # ---- Tokenizer ----\n\n    @property\n    def tokenizer(self) -> Callable[[str], List[Any]]:\n        \"\"\"Get the tokenizer.\"\"\"\n        import llama_index.core\n\n        if llama_index.core.global_tokenizer is None:\n            return get_tokenizer()\n\n        # TODO: deprecated?\n        return llama_index.core.global_tokenizer\n\n    @tokenizer.setter\n    def tokenizer(self, tokenizer: Callable[[str], List[Any]]) -> None:\n        \"\"\"Set the tokenizer.\"\"\"\n        try:\n            from transformers import PreTrainedTokenizerBase  # pants: no-infer-dep\n\n            if isinstance(tokenizer, PreTrainedTokenizerBase):\n                from functools import partial\n\n                tokenizer = partial(tokenizer.encode, add_special_tokens=False)\n        except ImportError:\n            pass\n\n        # TODO: deprecated?\n        set_global_tokenizer(tokenizer)\n\n    # ---- Node parser ----\n\n    @property\n    def node_parser(self) -> NodeParser:\n        \"\"\"Get the node parser.\"\"\"\n        if self._node_parser is None:\n            self._node_parser = SentenceSplitter()\n\n        if self._callback_manager is not None:\n            self._node_parser.callback_manager = self._callback_manager\n\n        return self._node_parser\n\n    @node_parser.setter\n    def node_parser(self, node_parser: NodeParser) -> None:\n        \"\"\"Set the node parser.\"\"\"\n        self._node_parser = node_parser\n\n    @property\n    def chunk_size(self) -> int:\n        \"\"\"Get the chunk size.\"\"\"\n        if hasattr(self.node_parser, \"chunk_size\"):\n            return self.node_parser.chunk_size\n        else:\n            raise ValueError(\"Configured node parser does not have chunk size.\")\n\n    @chunk_size.setter\n    def chunk_size(self, chunk_size: int) -> None:\n        \"\"\"Set the chunk size.\"\"\"\n        if hasattr(self.node_parser, \"chunk_size\"):\n            self.node_parser.chunk_size = chunk_size\n        else:\n            raise ValueError(\"Configured node parser does not have chunk size.\")\n\n    @property\n    def chunk_overlap(self) -> int:\n        \"\"\"Get the chunk overlap.\"\"\"\n        if hasattr(self.node_parser, \"chunk_overlap\"):\n            return self.node_parser.chunk_overlap\n        else:\n            raise ValueError(\"Configured node parser does not have chunk overlap.\")\n\n    @chunk_overlap.setter\n    def chunk_overlap(self, chunk_overlap: int) -> None:\n        \"\"\"Set the chunk overlap.\"\"\"\n        if hasattr(self.node_parser, \"chunk_overlap\"):\n            self.node_parser.chunk_overlap = chunk_overlap\n        else:\n            raise ValueError(\"Configured node parser does not have chunk overlap.\")\n\n    # ---- Node parser alias ----\n\n    @property\n    def text_splitter(self) -> NodeParser:\n        \"\"\"Get the text splitter.\"\"\"\n        return self.node_parser\n\n    @text_splitter.setter\n    def text_splitter(self, text_splitter: NodeParser) -> None:\n        \"\"\"Set the text splitter.\"\"\"\n        self.node_parser = text_splitter\n\n    @property\n    def prompt_helper(self) -> PromptHelper:\n        \"\"\"Get the prompt helper.\"\"\"\n        if self._llm is not None and self._prompt_helper is None:\n            self._prompt_helper = PromptHelper.from_llm_metadata(self._llm.metadata)\n        elif self._prompt_helper is None:\n            self._prompt_helper = PromptHelper()\n\n        return self._prompt_helper\n\n    @prompt_helper.setter\n    def prompt_helper(self, prompt_helper: PromptHelper) -> None:\n        \"\"\"Set the prompt helper.\"\"\"\n        self._prompt_helper = prompt_helper\n\n    @property\n    def num_output(self) -> int:\n        \"\"\"Get the number of outputs.\"\"\"\n        return self.prompt_helper.num_output\n\n    @num_output.setter\n    def num_output(self, num_output: int) -> None:\n        \"\"\"Set the number of outputs.\"\"\"\n        self.prompt_helper.num_output = num_output\n\n    @property\n    def context_window(self) -> int:\n        \"\"\"Get the context window.\"\"\"\n        return self.prompt_helper.context_window\n\n    @context_window.setter\n    def context_window(self, context_window: int) -> None:\n        \"\"\"Set the context window.\"\"\"\n        self.prompt_helper.context_window = context_window\n\n    # ---- Transformations ----\n\n    @property\n    def transformations(self) -> List[TransformComponent]:\n        \"\"\"Get the transformations.\"\"\"\n        if self._transformations is None:\n            self._transformations = [self.node_parser]\n        return self._transformations\n\n    @transformations.setter\n    def transformations(self, transformations: List[TransformComponent]) -> None:\n        \"\"\"Set the transformations.\"\"\"\n        self._transformations = transformations\n\n\n# Singleton\nSettings = _Settings()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/cogniswitch_query_engine.py",
    "filename": "cogniswitch_query_engine.py",
    "relpath": "query_engine/cogniswitch_query_engine.py",
    "start_line": 1,
    "end_line": 63,
    "length": 63,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "query_knowledge",
      "_query",
      "_aquery",
      "_get_prompt_modules"
    ],
    "chunk_class_names": [
      "CogniswitchQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "query_knowledge",
      "_query",
      "_aquery",
      "_get_prompt_modules"
    ],
    "document_class_names": [
      "CogniswitchQueryEngine"
    ],
    "content": "from typing import Any, Dict\n\nimport requests\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.schema import QueryBundle\n\n\nclass CogniswitchQueryEngine(BaseQueryEngine):\n    def __init__(self, cs_token: str, OAI_token: str, apiKey: str) -> None:\n        \"\"\"The required fields.\n\n        Args:\n            cs_token (str): Cogniswitch token.\n            OAI_token (str): OpenAI token.\n            apiKey (str): Oauth token.\n        \"\"\"\n        self.cs_token = cs_token\n        self.OAI_token = OAI_token\n        self.apiKey = apiKey\n        self.knowledge_request_endpoint = (\n            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeRequest\"\n        )\n        self.headers = {\n            \"apiKey\": self.apiKey,\n            \"platformToken\": self.cs_token,\n            \"openAIToken\": self.OAI_token,\n        }\n\n    def query_knowledge(self, query: str) -> Response:\n        \"\"\"\n        Send a query to the Cogniswitch service and retrieve the response.\n\n        Args:\n            query (str): Query to be answered.\n\n        Returns:\n            dict: Response JSON from the Cogniswitch service.\n        \"\"\"\n        data = {\"query\": query}\n        response = requests.post(\n            self.knowledge_request_endpoint,\n            headers=self.headers,\n            data=data,\n        )\n        if response.status_code == 200:\n            resp = response.json()\n            answer = resp[\"data\"][\"answer\"]\n\n            return Response(response=answer)\n        else:\n            error_message = response.json()[\"message\"]\n            return Response(response=error_message)\n\n    def _query(self, query_bundle: QueryBundle) -> Response:\n        return self.query_knowledge(query_bundle.query_str)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:\n        return self.query_knowledge(query_bundle.query_str)\n\n    def _get_prompt_modules(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/knowledge_graph_query_engine.py",
    "filename": "knowledge_graph_query_engine.py",
    "relpath": "query_engine/knowledge_graph_query_engine.py",
    "start_line": 1,
    "end_line": 267,
    "length": 267,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "generate_query",
      "agenerate_query",
      "_retrieve",
      "_query",
      "_aretrieve",
      "_aquery"
    ],
    "chunk_class_names": [
      "KnowledgeGraphQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "generate_query",
      "agenerate_query",
      "_retrieve",
      "_query",
      "_aretrieve",
      "_aquery"
    ],
    "document_class_names": [
      "KnowledgeGraphQueryEngine"
    ],
    "content": "\"\"\"Knowledge Graph Query Engine.\"\"\"\n\nimport deprecated\nimport logging\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.base import (\n    BasePromptTemplate,\n    PromptTemplate,\n    PromptType,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixinType\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.utils import print_text\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_KG_RESPONSE_ANSWER_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a Graph Database query.\nBoth the Graph query and the response are given below.\nGiven the Graph Query response, synthesise a response to the original question.\n\nOriginal question: {query_str}\nGraph query: {kg_query_str}\nGraph response: {kg_response_str}\nResponse:\n\"\"\"\n\nDEFAULT_KG_RESPONSE_ANSWER_PROMPT = PromptTemplate(\n    DEFAULT_KG_RESPONSE_ANSWER_PROMPT_TMPL,\n    prompt_type=PromptType.QUESTION_ANSWER,\n)\n\n\n@deprecated.deprecated(\n    version=\"0.10.53\",\n    reason=(\n        \"KnowledgeGraphQueryEngine is deprecated. It is recommended to use \"\n        \"the PropertyGraphIndex and associated retrievers instead.\"\n    ),\n)\nclass KnowledgeGraphQueryEngine(BaseQueryEngine):\n    \"\"\"Knowledge graph query engine.\n\n    Query engine to call a knowledge graph.\n\n    Args:\n        storage_context (Optional[StorageContext]): A storage context to use.\n        refresh_schema (bool): Whether to refresh the schema.\n        verbose (bool): Whether to print intermediate results.\n        response_synthesizer (Optional[BaseSynthesizer]):\n            A BaseSynthesizer object.\n        **kwargs: Additional keyword arguments.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        storage_context: Optional[StorageContext] = None,\n        graph_query_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        graph_response_answer_prompt: Optional[BasePromptTemplate] = None,\n        refresh_schema: bool = False,\n        verbose: bool = False,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        **kwargs: Any,\n    ):\n        # Ensure that we have a graph store\n        assert storage_context is not None, \"Must provide a storage context.\"\n        assert (\n            storage_context.graph_store is not None\n        ), \"Must provide a graph store in the storage context.\"\n        self._storage_context = storage_context\n        self.graph_store = storage_context.graph_store\n\n        self._llm = llm or Settings.llm\n\n        # Get Graph schema\n        self._graph_schema = self.graph_store.get_schema(refresh=refresh_schema)\n\n        # Get graph store query synthesis prompt\n        self._graph_query_synthesis_prompt = graph_query_synthesis_prompt\n\n        self._graph_response_answer_prompt = (\n            graph_response_answer_prompt or DEFAULT_KG_RESPONSE_ANSWER_PROMPT\n        )\n        self._verbose = verbose\n        callback_manager = Settings.callback_manager\n        self._response_synthesizer = response_synthesizer or get_response_synthesizer(\n            llm=self._llm, callback_manager=callback_manager\n        )\n\n        super().__init__(callback_manager=callback_manager)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"graph_query_synthesis_prompt\": self._graph_query_synthesis_prompt,\n            \"graph_response_answer_prompt\": self._graph_response_answer_prompt,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"graph_query_synthesis_prompt\" in prompts:\n            self._graph_query_synthesis_prompt = prompts[\"graph_query_synthesis_prompt\"]\n        if \"graph_response_answer_prompt\" in prompts:\n            self._graph_response_answer_prompt = prompts[\"graph_response_answer_prompt\"]\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\"response_synthesizer\": self._response_synthesizer}\n\n    def generate_query(self, query_str: str) -> str:\n        \"\"\"Generate a Graph Store Query from a query bundle.\"\"\"\n        # Get the query engine query string\n\n        graph_store_query: str = self._llm.predict(\n            self._graph_query_synthesis_prompt,\n            query_str=query_str,\n            schema=self._graph_schema,\n        )\n\n        return graph_store_query\n\n    async def agenerate_query(self, query_str: str) -> str:\n        \"\"\"Generate a Graph Store Query from a query bundle.\"\"\"\n        # Get the query engine query string\n\n        graph_store_query: str = await self._llm.apredict(\n            self._graph_query_synthesis_prompt,\n            query_str=query_str,\n            schema=self._graph_schema,\n        )\n\n        return graph_store_query\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        graph_store_query = self.generate_query(query_bundle.query_str)\n        if self._verbose:\n            print_text(f\"Graph Store Query:\\n{graph_store_query}\\n\", color=\"yellow\")\n        logger.debug(f\"Graph Store Query:\\n{graph_store_query}\")\n\n        with self.callback_manager.event(\n            CBEventType.RETRIEVE,\n            payload={EventPayload.QUERY_STR: graph_store_query},\n        ) as retrieve_event:\n            # Get the graph store response\n            graph_store_response = self.graph_store.query(query=graph_store_query)\n            if self._verbose:\n                print_text(\n                    f\"Graph Store Response:\\n{graph_store_response}\\n\",\n                    color=\"yellow\",\n                )\n            logger.debug(f\"Graph Store Response:\\n{graph_store_response}\")\n\n            retrieve_event.on_end(payload={EventPayload.RESPONSE: graph_store_response})\n\n        retrieved_graph_context: Sequence = self._graph_response_answer_prompt.format(\n            query_str=query_bundle.query_str,\n            kg_query_str=graph_store_query,\n            kg_response_str=graph_store_response,\n        )\n\n        node = NodeWithScore(\n            node=TextNode(\n                text=retrieved_graph_context,\n                metadata={\n                    \"query_str\": query_bundle.query_str,\n                    \"graph_store_query\": graph_store_query,\n                    \"graph_store_response\": graph_store_response,\n                    \"graph_schema\": self._graph_schema,\n                },\n            ),\n            score=1.0,\n        )\n        return [node]\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query the graph store.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            nodes: List[NodeWithScore] = self._retrieve(query_bundle)\n\n            response = self._response_synthesizer.synthesize(\n                query=query_bundle,\n                nodes=nodes,\n            )\n\n            if self._verbose:\n                print_text(f\"Final Response: {response}\\n\", color=\"green\")\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        graph_store_query = await self.agenerate_query(query_bundle.query_str)\n        if self._verbose:\n            print_text(f\"Graph Store Query:\\n{graph_store_query}\\n\", color=\"yellow\")\n        logger.debug(f\"Graph Store Query:\\n{graph_store_query}\")\n\n        with self.callback_manager.event(\n            CBEventType.RETRIEVE,\n            payload={EventPayload.QUERY_STR: graph_store_query},\n        ) as retrieve_event:\n            # Get the graph store response\n            # TBD: This is a blocking call. We need to make it async.\n            graph_store_response = self.graph_store.query(query=graph_store_query)\n            if self._verbose:\n                print_text(\n                    f\"Graph Store Response:\\n{graph_store_response}\\n\",\n                    color=\"yellow\",\n                )\n            logger.debug(f\"Graph Store Response:\\n{graph_store_response}\")\n\n            retrieve_event.on_end(payload={EventPayload.RESPONSE: graph_store_response})\n\n        retrieved_graph_context: Sequence = self._graph_response_answer_prompt.format(\n            query_str=query_bundle.query_str,\n            kg_query_str=graph_store_query,\n            kg_response_str=graph_store_response,\n        )\n\n        node = NodeWithScore(\n            node=TextNode(\n                text=retrieved_graph_context,\n                metadata={\n                    \"query_str\": query_bundle.query_str,\n                    \"graph_store_query\": graph_store_query,\n                    \"graph_store_response\": graph_store_response,\n                    \"graph_schema\": self._graph_schema,\n                },\n            ),\n            score=1.0,\n        )\n        return [node]\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query the graph store.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            nodes = await self._aretrieve(query_bundle)\n            response = await self._response_synthesizer.asynthesize(\n                query=query_bundle,\n                nodes=nodes,\n            )\n\n            if self._verbose:\n                print_text(f\"Final Response: {response}\\n\", color=\"green\")\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/citation_query_engine.py",
    "filename": "citation_query_engine.py",
    "relpath": "query_engine/citation_query_engine.py",
    "start_line": 1,
    "end_line": 324,
    "length": 324,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_args",
      "_get_prompt_modules",
      "_create_citation_nodes",
      "retrieve",
      "aretrieve",
      "retriever",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "CitationQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "from_args",
      "_get_prompt_modules",
      "_create_citation_nodes",
      "retrieve",
      "aretrieve",
      "retriever",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "CitationQueryEngine"
    ],
    "content": "from typing import Any, List, Optional, Sequence\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.indices.base import BaseGPTIndex\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.node_parser import SentenceSplitter, TextSplitter\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.prompts.base import BasePromptTemplate\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    ResponseMode,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import (\n    MetadataMode,\n    NodeWithScore,\n    QueryBundle,\n    TextNode,\n)\nfrom llama_index.core.settings import Settings\n\nCITATION_QA_TEMPLATE = PromptTemplate(\n    \"Please provide an answer based solely on the provided sources. \"\n    \"When referencing information from a source, \"\n    \"cite the appropriate source(s) using their corresponding numbers. \"\n    \"Every answer should include at least one source citation. \"\n    \"Only cite a source when you are explicitly referencing it. \"\n    \"If none of the sources are helpful, you should indicate that. \"\n    \"For example:\\n\"\n    \"Source 1:\\n\"\n    \"The sky is red in the evening and blue in the morning.\\n\"\n    \"Source 2:\\n\"\n    \"Water is wet when the sky is red.\\n\"\n    \"Query: When is water wet?\\n\"\n    \"Answer: Water will be wet when the sky is red [2], \"\n    \"which occurs in the evening [1].\\n\"\n    \"Now it's your turn. Below are several numbered sources of information:\"\n    \"\\n------\\n\"\n    \"{context_str}\"\n    \"\\n------\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\n\nCITATION_REFINE_TEMPLATE = PromptTemplate(\n    \"Please provide an answer based solely on the provided sources. \"\n    \"When referencing information from a source, \"\n    \"cite the appropriate source(s) using their corresponding numbers. \"\n    \"Every answer should include at least one source citation. \"\n    \"Only cite a source when you are explicitly referencing it. \"\n    \"If none of the sources are helpful, you should indicate that. \"\n    \"For example:\\n\"\n    \"Source 1:\\n\"\n    \"The sky is red in the evening and blue in the morning.\\n\"\n    \"Source 2:\\n\"\n    \"Water is wet when the sky is red.\\n\"\n    \"Query: When is water wet?\\n\"\n    \"Answer: Water will be wet when the sky is red [2], \"\n    \"which occurs in the evening [1].\\n\"\n    \"Now it's your turn. \"\n    \"We have provided an existing answer: {existing_answer}\"\n    \"Below are several numbered sources of information. \"\n    \"Use them to refine the existing answer. \"\n    \"If the provided sources are not helpful, you will repeat the existing answer.\"\n    \"\\nBegin refining!\"\n    \"\\n------\\n\"\n    \"{context_msg}\"\n    \"\\n------\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\n\nDEFAULT_CITATION_CHUNK_SIZE = 512\nDEFAULT_CITATION_CHUNK_OVERLAP = 20\n\n\nclass CitationQueryEngine(BaseQueryEngine):\n    \"\"\"Citation query engine.\n\n    Args:\n        retriever (BaseRetriever): A retriever object.\n        response_synthesizer (Optional[BaseSynthesizer]):\n            A BaseSynthesizer object.\n        citation_chunk_size (int):\n            Size of citation chunks, default=512. Useful for controlling\n            granularity of sources.\n        citation_chunk_overlap (int): Overlap of citation nodes, default=20.\n        text_splitter (Optional[TextSplitter]):\n            A text splitter for creating citation source nodes. Default is\n            a SentenceSplitter.\n        callback_manager (Optional[CallbackManager]): A callback manager.\n        metadata_mode (MetadataMode): A MetadataMode object that controls how\n            metadata is included in the citation prompt.\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        llm: Optional[LLM] = None,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        citation_chunk_size: int = DEFAULT_CITATION_CHUNK_SIZE,\n        citation_chunk_overlap: int = DEFAULT_CITATION_CHUNK_OVERLAP,\n        text_splitter: Optional[TextSplitter] = None,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        metadata_mode: MetadataMode = MetadataMode.NONE,\n    ) -> None:\n        self.text_splitter = text_splitter or SentenceSplitter(\n            chunk_size=citation_chunk_size, chunk_overlap=citation_chunk_overlap\n        )\n        self._retriever = retriever\n\n        callback_manager = callback_manager or Settings.callback_manager\n        llm = llm or Settings.llm\n\n        self._response_synthesizer = response_synthesizer or get_response_synthesizer(\n            llm=llm,\n            callback_manager=callback_manager,\n            text_qa_template=CITATION_QA_TEMPLATE,\n            refine_template=CITATION_REFINE_TEMPLATE,\n            response_mode=ResponseMode.COMPACT,\n            use_async=False,\n            streaming=False,\n        )\n\n        self._node_postprocessors = node_postprocessors or []\n        self._metadata_mode = metadata_mode\n\n        for node_postprocessor in self._node_postprocessors:\n            node_postprocessor.callback_manager = callback_manager\n\n        super().__init__(callback_manager=callback_manager)\n\n    @classmethod\n    def from_args(\n        cls,\n        index: BaseGPTIndex,\n        llm: Optional[LLM] = None,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        citation_chunk_size: int = DEFAULT_CITATION_CHUNK_SIZE,\n        citation_chunk_overlap: int = DEFAULT_CITATION_CHUNK_OVERLAP,\n        text_splitter: Optional[TextSplitter] = None,\n        citation_qa_template: BasePromptTemplate = CITATION_QA_TEMPLATE,\n        citation_refine_template: BasePromptTemplate = CITATION_REFINE_TEMPLATE,\n        retriever: Optional[BaseRetriever] = None,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        # response synthesizer args\n        response_mode: ResponseMode = ResponseMode.COMPACT,\n        use_async: bool = False,\n        streaming: bool = False,\n        # class-specific args\n        metadata_mode: MetadataMode = MetadataMode.NONE,\n        **kwargs: Any,\n    ) -> \"CitationQueryEngine\":\n        \"\"\"Initialize a CitationQueryEngine object.\".\n\n        Args:\n            index: (BastGPTIndex): index to use for querying\n            llm: (Optional[LLM]): LLM object to use for response generation.\n            citation_chunk_size (int):\n                Size of citation chunks, default=512. Useful for controlling\n                granularity of sources.\n            citation_chunk_overlap (int): Overlap of citation nodes, default=20.\n            text_splitter (Optional[TextSplitter]):\n                A text splitter for creating citation source nodes. Default is\n                a SentenceSplitter.\n            citation_qa_template (BasePromptTemplate): Template for initial citation QA\n            citation_refine_template (BasePromptTemplate):\n                Template for citation refinement.\n            retriever (BaseRetriever): A retriever object.\n            node_postprocessors (Optional[List[BaseNodePostprocessor]]): A list of\n                node postprocessors.\n            verbose (bool): Whether to print out debug info.\n            response_mode (ResponseMode): A ResponseMode object.\n            use_async (bool): Whether to use async.\n            streaming (bool): Whether to use streaming.\n            optimizer (Optional[BaseTokenUsageOptimizer]): A BaseTokenUsageOptimizer\n                object.\n\n        \"\"\"\n        retriever = retriever or index.as_retriever(**kwargs)\n\n        response_synthesizer = response_synthesizer or get_response_synthesizer(\n            llm=llm,\n            text_qa_template=citation_qa_template,\n            refine_template=citation_refine_template,\n            response_mode=response_mode,\n            use_async=use_async,\n            streaming=streaming,\n        )\n\n        return cls(\n            retriever=retriever,\n            llm=llm,\n            response_synthesizer=response_synthesizer,\n            callback_manager=Settings.callback_manager,\n            citation_chunk_size=citation_chunk_size,\n            citation_chunk_overlap=citation_chunk_overlap,\n            text_splitter=text_splitter,\n            node_postprocessors=node_postprocessors,\n            metadata_mode=metadata_mode,\n        )\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\"response_synthesizer\": self._response_synthesizer}\n\n    def _create_citation_nodes(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n        \"\"\"Modify retrieved nodes to be granular sources.\"\"\"\n        new_nodes: List[NodeWithScore] = []\n        for node in nodes:\n            text_chunks = self.text_splitter.split_text(\n                node.node.get_content(metadata_mode=self._metadata_mode)\n            )\n\n            for text_chunk in text_chunks:\n                text = f\"Source {len(new_nodes) + 1}:\\n{text_chunk}\\n\"\n\n                new_node = NodeWithScore(\n                    node=TextNode.model_validate(node.node.model_dump()),\n                    score=node.score,\n                )\n                new_node.node.set_content(text)\n                new_nodes.append(new_node)\n        return new_nodes\n\n    def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = self._retriever.retrieve(query_bundle)\n\n        for postprocessor in self._node_postprocessors:\n            nodes = postprocessor.postprocess_nodes(nodes, query_bundle=query_bundle)\n\n        return nodes\n\n    async def aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = await self._retriever.aretrieve(query_bundle)\n\n        for postprocessor in self._node_postprocessors:\n            nodes = postprocessor.postprocess_nodes(nodes, query_bundle=query_bundle)\n\n        return nodes\n\n    @property\n    def retriever(self) -> BaseRetriever:\n        \"\"\"Get the retriever object.\"\"\"\n        return self._retriever\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        nodes = self._create_citation_nodes(nodes)\n        return self._response_synthesizer.synthesize(\n            query=query_bundle,\n            nodes=nodes,\n            additional_source_nodes=additional_source_nodes,\n        )\n\n    async def asynthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        nodes = self._create_citation_nodes(nodes)\n        return await self._response_synthesizer.asynthesize(\n            query=query_bundle,\n            nodes=nodes,\n            additional_source_nodes=additional_source_nodes,\n        )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as retrieve_event:\n                nodes = self.retrieve(query_bundle)\n                nodes = self._create_citation_nodes(nodes)\n\n                retrieve_event.on_end(payload={EventPayload.NODES: nodes})\n\n            response = self._response_synthesizer.synthesize(\n                query=query_bundle,\n                nodes=nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as retrieve_event:\n                nodes = await self.aretrieve(query_bundle)\n                nodes = self._create_citation_nodes(nodes)\n\n                retrieve_event.on_end(payload={EventPayload.NODES: nodes})\n\n            response = await self._response_synthesizer.asynthesize(\n                query=query_bundle,\n                nodes=nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/multi_modal.py",
    "filename": "multi_modal.py",
    "relpath": "query_engine/multi_modal.py",
    "start_line": 1,
    "end_line": 263,
    "length": 263,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_image_and_text_nodes",
      "__init__",
      "_get_prompts",
      "_get_prompt_modules",
      "_apply_node_postprocessors",
      "retrieve",
      "aretrieve",
      "synthesize",
      "_get_response_with_images",
      "asynthesize",
      "_query",
      "image_query",
      "_aquery",
      "retriever"
    ],
    "chunk_class_names": [
      "SimpleMultiModalQueryEngine"
    ],
    "document_function_names": [
      "_get_image_and_text_nodes",
      "__init__",
      "_get_prompts",
      "_get_prompt_modules",
      "_apply_node_postprocessors",
      "retrieve",
      "aretrieve",
      "synthesize",
      "_get_response_with_images",
      "asynthesize",
      "_query",
      "image_query",
      "_aquery",
      "retriever"
    ],
    "document_class_names": [
      "SimpleMultiModalQueryEngine"
    ],
    "content": "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Tuple\n\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE, Response\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.indices.query.base import BaseQueryEngine\nfrom llama_index.core.indices.query.schema import QueryBundle, QueryType\nfrom llama_index.core.multi_modal_llms.base import MultiModalLLM\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.schema import ImageNode, NodeWithScore, MetadataMode\n\nif TYPE_CHECKING:\n    from llama_index.core.indices.multi_modal import MultiModalVectorIndexRetriever\n\n\ndef _get_image_and_text_nodes(\n    nodes: List[NodeWithScore],\n) -> Tuple[List[NodeWithScore], List[NodeWithScore]]:\n    image_nodes = []\n    text_nodes = []\n    for res_node in nodes:\n        if isinstance(res_node.node, ImageNode):\n            image_nodes.append(res_node)\n        else:\n            text_nodes.append(res_node)\n    return image_nodes, text_nodes\n\n\nclass SimpleMultiModalQueryEngine(BaseQueryEngine):\n    \"\"\"Simple Multi Modal Retriever query engine.\n\n    Assumes that retrieved text context fits within context window of LLM, along with images.\n\n    Args:\n        retriever (MultiModalVectorIndexRetriever): A retriever object.\n        multi_modal_llm (Optional[MultiModalLLM]): MultiModalLLM Models.\n        text_qa_template (Optional[BasePromptTemplate]): Text QA Prompt Template.\n        image_qa_template (Optional[BasePromptTemplate]): Image QA Prompt Template.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Node Postprocessors.\n        callback_manager (Optional[CallbackManager]): A callback manager.\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: \"MultiModalVectorIndexRetriever\",\n        multi_modal_llm: Optional[MultiModalLLM] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        image_qa_template: Optional[BasePromptTemplate] = None,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._retriever = retriever\n        if multi_modal_llm:\n            self._multi_modal_llm = multi_modal_llm\n        else:\n            try:\n                from llama_index.multi_modal_llms.openai import (\n                    OpenAIMultiModal,\n                )  # pants: no-infer-dep\n\n                self._multi_modal_llm = OpenAIMultiModal(\n                    model=\"gpt-4-vision-preview\", max_new_tokens=1000\n                )\n            except ImportError as e:\n                raise ImportError(\n                    \"`llama-index-multi-modal-llms-openai` package cannot be found. \"\n                    \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"\n                )\n        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self._image_qa_template = image_qa_template or DEFAULT_TEXT_QA_PROMPT\n\n        self._node_postprocessors = node_postprocessors or []\n        callback_manager = callback_manager or CallbackManager([])\n        for node_postprocessor in self._node_postprocessors:\n            node_postprocessor.callback_manager = callback_manager\n\n        super().__init__(callback_manager)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\"text_qa_template\": self._text_qa_template}\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {}\n\n    def _apply_node_postprocessors(\n        self, nodes: List[NodeWithScore], query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        for node_postprocessor in self._node_postprocessors:\n            nodes = node_postprocessor.postprocess_nodes(\n                nodes, query_bundle=query_bundle\n            )\n        return nodes\n\n    def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = self._retriever.retrieve(query_bundle)\n        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    async def aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = await self._retriever.aretrieve(query_bundle)\n        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)\n        context_str = \"\\n\\n\".join(\n            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]\n        )\n        fmt_prompt = self._text_qa_template.format(\n            context_str=context_str, query_str=query_bundle.query_str\n        )\n\n        llm_response = self._multi_modal_llm.complete(\n            prompt=fmt_prompt,\n            image_documents=[\n                image_node.node\n                for image_node in image_nodes\n                if isinstance(image_node.node, ImageNode)\n            ],\n        )\n        return Response(\n            response=str(llm_response),\n            source_nodes=nodes,\n            metadata={\"text_nodes\": text_nodes, \"image_nodes\": image_nodes},\n        )\n\n    def _get_response_with_images(\n        self,\n        prompt_str: str,\n        image_nodes: List[NodeWithScore],\n    ) -> RESPONSE_TYPE:\n        assert all(isinstance(node.node, ImageNode) for node in image_nodes)\n\n        fmt_prompt = self._image_qa_template.format(\n            query_str=prompt_str,\n        )\n\n        llm_response = self._multi_modal_llm.complete(\n            prompt=fmt_prompt,\n            image_documents=[\n                node.node for node in image_nodes if isinstance(node.node, ImageNode)\n            ],\n        )\n        return Response(\n            response=str(llm_response),\n            source_nodes=image_nodes,\n            metadata={\"image_nodes\": image_nodes},\n        )\n\n    async def asynthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        image_nodes, text_nodes = _get_image_and_text_nodes(nodes)\n        context_str = \"\\n\\n\".join(\n            [r.get_content(metadata_mode=MetadataMode.LLM) for r in text_nodes]\n        )\n        fmt_prompt = self._text_qa_template.format(\n            context_str=context_str, query_str=query_bundle.query_str\n        )\n\n        llm_response = await self._multi_modal_llm.acomplete(\n            prompt=fmt_prompt,\n            image_documents=[\n                image_node.node\n                for image_node in image_nodes\n                if isinstance(image_node.node, ImageNode)\n            ],\n        )\n        return Response(\n            response=str(llm_response),\n            source_nodes=nodes,\n            metadata={\"text_nodes\": text_nodes, \"image_nodes\": image_nodes},\n        )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as retrieve_event:\n                nodes = self.retrieve(query_bundle)\n\n                retrieve_event.on_end(\n                    payload={EventPayload.NODES: nodes},\n                )\n\n            response = self.synthesize(\n                query_bundle,\n                nodes=nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    def image_query(self, image_path: QueryType, prompt_str: str) -> RESPONSE_TYPE:\n        \"\"\"Answer a image query.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: str(image_path)}\n        ) as query_event:\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: str(image_path)},\n            ) as retrieve_event:\n                nodes = self._retriever.image_to_image_retrieve(image_path)\n\n                retrieve_event.on_end(\n                    payload={EventPayload.NODES: nodes},\n                )\n\n            image_nodes, _ = _get_image_and_text_nodes(nodes)\n            response = self._get_response_with_images(\n                prompt_str=prompt_str,\n                image_nodes=image_nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as retrieve_event:\n                nodes = await self.aretrieve(query_bundle)\n\n                retrieve_event.on_end(\n                    payload={EventPayload.NODES: nodes},\n                )\n\n            response = await self.asynthesize(\n                query_bundle,\n                nodes=nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    @property\n    def retriever(self) -> \"MultiModalVectorIndexRetriever\":\n        \"\"\"Get the retriever object.\"\"\"\n        return self._retriever"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/router_query_engine.py",
    "filename": "router_query_engine.py",
    "relpath": "query_engine/router_query_engine.py",
    "start_line": 1,
    "end_line": 393,
    "length": 393,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "combine_responses",
      "acombine_responses",
      "__init__",
      "_get_prompt_modules",
      "from_defaults",
      "_query",
      "_aquery",
      "default_node_to_metadata_fn",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "RouterQueryEngine",
      "RetrieverRouterQueryEngine",
      "ToolRetrieverRouterQueryEngine"
    ],
    "document_function_names": [
      "combine_responses",
      "acombine_responses",
      "__init__",
      "_get_prompt_modules",
      "from_defaults",
      "_query",
      "_aquery",
      "default_node_to_metadata_fn",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "RouterQueryEngine",
      "RetrieverRouterQueryEngine",
      "ToolRetrieverRouterQueryEngine"
    ],
    "content": "import logging\nfrom typing import Callable, Generator, List, Optional, Sequence, Any\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.base_selector import BaseSelector\nfrom llama_index.core.base.response.schema import (\n    RESPONSE_TYPE,\n    PydanticResponse,\n    Response,\n    StreamingResponse,\n    AsyncStreamingResponse,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.response_synthesizers import TreeSummarize\nfrom llama_index.core.schema import BaseNode, QueryBundle\nfrom llama_index.core.selectors.utils import get_selector_from_llm\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.query_engine import QueryEngineTool\nfrom llama_index.core.tools.types import ToolMetadata\nfrom llama_index.core.utils import print_text\n\nlogger = logging.getLogger(__name__)\n\n\ndef combine_responses(\n    summarizer: TreeSummarize, responses: List[RESPONSE_TYPE], query_bundle: QueryBundle\n) -> RESPONSE_TYPE:\n    \"\"\"Combine multiple response from sub-engines.\"\"\"\n    logger.info(\"Combining responses from multiple query engines.\")\n\n    response_strs = []\n    source_nodes = []\n    for response in responses:\n        if isinstance(response, (StreamingResponse, PydanticResponse)):\n            response_obj = response.get_response()\n        elif isinstance(response, AsyncStreamingResponse):\n            raise ValueError(\"AsyncStreamingResponse not supported in sync code.\")\n        else:\n            response_obj = response\n        source_nodes.extend(response_obj.source_nodes)\n        response_strs.append(str(response))\n\n    summary = summarizer.get_response(query_bundle.query_str, response_strs)\n\n    if isinstance(summary, str):\n        return Response(response=summary, source_nodes=source_nodes)\n    elif isinstance(summary, BaseModel):\n        return PydanticResponse(response=summary, source_nodes=source_nodes)\n    elif isinstance(summary, Generator):\n        return StreamingResponse(response_gen=summary, source_nodes=source_nodes)\n    else:\n        return AsyncStreamingResponse(response_gen=summary, source_nodes=source_nodes)\n\n\nasync def acombine_responses(\n    summarizer: TreeSummarize, responses: List[RESPONSE_TYPE], query_bundle: QueryBundle\n) -> RESPONSE_TYPE:\n    \"\"\"Async combine multiple response from sub-engines.\"\"\"\n    logger.info(\"Combining responses from multiple query engines.\")\n\n    response_strs = []\n    source_nodes = []\n    for response in responses:\n        if isinstance(response, (StreamingResponse, PydanticResponse)):\n            response_obj = response.get_response()\n        elif isinstance(response, AsyncStreamingResponse):\n            response_obj = await response.get_response()\n        else:\n            response_obj = response\n        source_nodes.extend(response_obj.source_nodes)\n        response_strs.append(str(response))\n\n    summary = await summarizer.aget_response(query_bundle.query_str, response_strs)\n\n    if isinstance(summary, str):\n        return Response(response=summary, source_nodes=source_nodes)\n    elif isinstance(summary, BaseModel):\n        return PydanticResponse(response=summary, source_nodes=source_nodes)\n    elif isinstance(summary, Generator):\n        return StreamingResponse(response_gen=summary, source_nodes=source_nodes)\n    else:\n        return AsyncStreamingResponse(response_gen=summary, source_nodes=source_nodes)\n\n\nclass RouterQueryEngine(BaseQueryEngine):\n    \"\"\"Router query engine.\n\n    Selects one out of several candidate query engines to execute a query.\n\n    Args:\n        selector (BaseSelector): A selector that chooses one out of many options based\n            on each candidate's metadata and query.\n        query_engine_tools (Sequence[QueryEngineTool]): A sequence of candidate\n            query engines. They must be wrapped as tools to expose metadata to\n            the selector.\n        summarizer (Optional[TreeSummarize]): Tree summarizer to summarize sub-results.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        selector: BaseSelector,\n        query_engine_tools: Sequence[QueryEngineTool],\n        llm: Optional[LLM] = None,\n        summarizer: Optional[TreeSummarize] = None,\n        verbose: bool = False,\n    ) -> None:\n        self._llm = llm or Settings.llm\n        self._selector = selector\n        self._query_engines = [x.query_engine for x in query_engine_tools]\n        self._metadatas = [x.metadata for x in query_engine_tools]\n        self._summarizer = summarizer or TreeSummarize(\n            llm=self._llm,\n            summary_template=DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n        )\n        self._verbose = verbose\n\n        super().__init__(callback_manager=Settings.callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        # NOTE: don't include tools for now\n        return {\"summarizer\": self._summarizer, \"selector\": self._selector}\n\n    @classmethod\n    def from_defaults(\n        cls,\n        query_engine_tools: Sequence[QueryEngineTool],\n        llm: Optional[LLM] = None,\n        selector: Optional[BaseSelector] = None,\n        summarizer: Optional[TreeSummarize] = None,\n        select_multi: bool = False,\n        **kwargs: Any,\n    ) -> \"RouterQueryEngine\":\n        llm = llm or Settings.llm\n\n        selector = selector or get_selector_from_llm(llm, is_multi=select_multi)\n\n        assert selector is not None\n\n        return cls(\n            selector,\n            query_engine_tools,\n            llm=llm,\n            summarizer=summarizer,\n            **kwargs,\n        )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            result = self._selector.select(self._metadatas, query_bundle)\n\n            if len(result.inds) > 1:\n                responses = []\n                for i, engine_ind in enumerate(result.inds):\n                    log_str = (\n                        f\"Selecting query engine {engine_ind}: \" f\"{result.reasons[i]}.\"\n                    )\n                    logger.info(log_str)\n                    if self._verbose:\n                        print_text(log_str + \"\\n\", color=\"pink\")\n\n                    selected_query_engine = self._query_engines[engine_ind]\n                    responses.append(selected_query_engine.query(query_bundle))\n\n                if len(responses) > 1:\n                    final_response = combine_responses(\n                        self._summarizer, responses, query_bundle\n                    )\n                else:\n                    final_response = responses[0]\n            else:\n                try:\n                    selected_query_engine = self._query_engines[result.ind]\n                    log_str = f\"Selecting query engine {result.ind}: {result.reason}.\"\n                    logger.info(log_str)\n                    if self._verbose:\n                        print_text(log_str + \"\\n\", color=\"pink\")\n                except ValueError as e:\n                    raise ValueError(\"Failed to select query engine\") from e\n\n                final_response = selected_query_engine.query(query_bundle)\n\n            # add selected result\n            final_response.metadata = final_response.metadata or {}\n            final_response.metadata[\"selector_result\"] = result\n\n            query_event.on_end(payload={EventPayload.RESPONSE: final_response})\n\n        return final_response\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            result = await self._selector.aselect(self._metadatas, query_bundle)\n\n            if len(result.inds) > 1:\n                tasks = []\n                for i, engine_ind in enumerate(result.inds):\n                    log_str = (\n                        f\"Selecting query engine {engine_ind}: \" f\"{result.reasons[i]}.\"\n                    )\n                    logger.info(log_str)\n                    if self._verbose:\n                        print_text(log_str + \"\\n\", color=\"pink\")\n                    selected_query_engine = self._query_engines[engine_ind]\n                    tasks.append(selected_query_engine.aquery(query_bundle))\n\n                responses = run_async_tasks(tasks)\n                if len(responses) > 1:\n                    final_response = await acombine_responses(\n                        self._summarizer, responses, query_bundle\n                    )\n                else:\n                    final_response = responses[0]\n            else:\n                try:\n                    selected_query_engine = self._query_engines[result.ind]\n                    log_str = f\"Selecting query engine {result.ind}: {result.reason}.\"\n                    logger.info(log_str)\n                    if self._verbose:\n                        print_text(log_str + \"\\n\", color=\"pink\")\n                except ValueError as e:\n                    raise ValueError(\"Failed to select query engine\") from e\n\n                final_response = await selected_query_engine.aquery(query_bundle)\n\n            # add selected result\n            final_response.metadata = final_response.metadata or {}\n            final_response.metadata[\"selector_result\"] = result\n\n            query_event.on_end(payload={EventPayload.RESPONSE: final_response})\n\n        return final_response\n\n\ndef default_node_to_metadata_fn(node: BaseNode) -> ToolMetadata:\n    \"\"\"Default node to metadata function.\n\n    We use the node's text as the Tool description.\n\n    \"\"\"\n    metadata = node.metadata or {}\n    if \"tool_name\" not in metadata:\n        raise ValueError(\"Node must have a tool_name in metadata.\")\n    return ToolMetadata(name=metadata[\"tool_name\"], description=node.get_content())\n\n\nclass RetrieverRouterQueryEngine(BaseQueryEngine):\n    \"\"\"Retriever-based router query engine.\n\n    NOTE: this is deprecated, please use our new ToolRetrieverRouterQueryEngine\n\n    Use a retriever to select a set of Nodes. Each node will be converted\n    into a ToolMetadata object, and also used to retrieve a query engine, to form\n    a QueryEngineTool.\n\n    NOTE: this is a beta feature. We are figuring out the right interface\n    between the retriever and query engine.\n\n    Args:\n        selector (BaseSelector): A selector that chooses one out of many options based\n            on each candidate's metadata and query.\n        query_engine_tools (Sequence[QueryEngineTool]): A sequence of candidate\n            query engines. They must be wrapped as tools to expose metadata to\n            the selector.\n        callback_manager (Optional[CallbackManager]): A callback manager.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        node_to_query_engine_fn: Callable,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        self._retriever = retriever\n        self._node_to_query_engine_fn = node_to_query_engine_fn\n        super().__init__(callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        # NOTE: don't include tools for now\n        return {\"retriever\": self._retriever}\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        nodes_with_score = self._retriever.retrieve(query_bundle)\n        # TODO: for now we only support retrieving one node\n        if len(nodes_with_score) > 1:\n            raise ValueError(\"Retrieved more than one node.\")\n\n        node = nodes_with_score[0].node\n        query_engine = self._node_to_query_engine_fn(node)\n        return query_engine.query(query_bundle)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        return self._query(query_bundle)\n\n\nclass ToolRetrieverRouterQueryEngine(BaseQueryEngine):\n    \"\"\"Tool Retriever router query engine.\n\n    Selects a set of candidate query engines to execute a query.\n\n    Args:\n        retriever (ObjectRetriever): A retriever that retrieves a set of\n            query engine tools.\n        summarizer (Optional[TreeSummarize]): Tree summarizer to summarize sub-results.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: ObjectRetriever[QueryEngineTool],\n        llm: Optional[LLM] = None,\n        summarizer: Optional[TreeSummarize] = None,\n    ) -> None:\n        llm = llm or Settings.llm\n        self._summarizer = summarizer or TreeSummarize(\n            llm=llm,\n            summary_template=DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n        )\n        self._retriever = retriever\n\n        super().__init__(Settings.callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        # NOTE: don't include tools for now\n        return {\"summarizer\": self._summarizer}\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            query_engine_tools = self._retriever.retrieve(query_bundle)\n            responses = []\n            for query_engine_tool in query_engine_tools:\n                query_engine = query_engine_tool.query_engine\n                responses.append(query_engine.query(query_bundle))\n\n            if len(responses) > 1:\n                final_response = combine_responses(\n                    self._summarizer, responses, query_bundle\n                )\n            else:\n                final_response = responses[0]\n\n            # add selected result\n            final_response.metadata = final_response.metadata or {}\n            final_response.metadata[\"retrieved_tools\"] = query_engine_tools\n\n            query_event.on_end(payload={EventPayload.RESPONSE: final_response})\n\n        return final_response\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            query_engine_tools = self._retriever.retrieve(query_bundle)\n            tasks = []\n            for query_engine_tool in query_engine_tools:\n                query_engine = query_engine_tool.query_engine\n                tasks.append(query_engine.aquery(query_bundle))\n            responses = run_async_tasks(tasks)\n            if len(responses) > 1:\n                final_response = await acombine_responses(\n                    self._summarizer, responses, query_bundle\n                )\n            else:\n                final_response = responses[0]\n\n            # add selected result\n            final_response.metadata = final_response.metadata or {}\n            final_response.metadata[\"retrieved_tools\"] = query_engine_tools\n\n            query_event.on_end(payload={EventPayload.RESPONSE: final_response})\n\n        return final_response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/__init__.py",
    "filename": "__init__.py",
    "relpath": "query_engine/__init__.py",
    "start_line": 1,
    "end_line": 87,
    "length": 87,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.base.base_query_engine import BaseQueryEngine\n\n# SQL\nfrom llama_index.core.indices.struct_store.sql_query import (\n    NLSQLTableQueryEngine,\n    PGVectorSQLQueryEngine,\n    SQLTableRetrieverQueryEngine,\n)\nfrom llama_index.core.query_engine.citation_query_engine import CitationQueryEngine\nfrom llama_index.core.query_engine.cogniswitch_query_engine import (\n    CogniswitchQueryEngine,\n)\nfrom llama_index.core.query_engine.custom import CustomQueryEngine\nfrom llama_index.core.query_engine.flare.base import FLAREInstructQueryEngine\nfrom llama_index.core.query_engine.graph_query_engine import (\n    ComposableGraphQueryEngine,\n)\nfrom llama_index.core.query_engine.jsonalyze import (\n    JSONalyzeQueryEngine,\n)\nfrom llama_index.core.query_engine.knowledge_graph_query_engine import (\n    KnowledgeGraphQueryEngine,\n)\nfrom llama_index.core.query_engine.multi_modal import SimpleMultiModalQueryEngine\nfrom llama_index.core.query_engine.multistep_query_engine import (\n    MultiStepQueryEngine,\n)\nfrom llama_index.core.query_engine.pandas.pandas_query_engine import (\n    PandasQueryEngine,\n)\nfrom llama_index.core.query_engine.retriever_query_engine import (\n    RetrieverQueryEngine,\n)\nfrom llama_index.core.query_engine.retry_query_engine import (\n    RetryGuidelineQueryEngine,\n    RetryQueryEngine,\n)\nfrom llama_index.core.query_engine.retry_source_query_engine import (\n    RetrySourceQueryEngine,\n)\nfrom llama_index.core.query_engine.router_query_engine import (\n    RetrieverRouterQueryEngine,\n    RouterQueryEngine,\n    ToolRetrieverRouterQueryEngine,\n)\nfrom llama_index.core.query_engine.sql_join_query_engine import SQLJoinQueryEngine\nfrom llama_index.core.query_engine.sql_vector_query_engine import (\n    SQLAutoVectorQueryEngine,\n)\nfrom llama_index.core.query_engine.sub_question_query_engine import (\n    SubQuestionAnswerPair,\n    SubQuestionQueryEngine,\n)\nfrom llama_index.core.query_engine.transform_query_engine import (\n    TransformQueryEngine,\n)\n\n__all__ = [\n    \"CitationQueryEngine\",\n    \"CogniswitchQueryEngine\",\n    \"ComposableGraphQueryEngine\",\n    \"RetrieverQueryEngine\",\n    \"TransformQueryEngine\",\n    \"MultiStepQueryEngine\",\n    \"RouterQueryEngine\",\n    \"RetrieverRouterQueryEngine\",\n    \"ToolRetrieverRouterQueryEngine\",\n    \"SubQuestionQueryEngine\",\n    \"SubQuestionAnswerPair\",\n    \"SQLJoinQueryEngine\",\n    \"SQLAutoVectorQueryEngine\",\n    \"RetryQueryEngine\",\n    \"RetrySourceQueryEngine\",\n    \"RetryGuidelineQueryEngine\",\n    \"FLAREInstructQueryEngine\",\n    \"PandasQueryEngine\",\n    \"JSONalyzeQueryEngine\",\n    \"KnowledgeGraphQueryEngine\",\n    \"BaseQueryEngine\",\n    \"CustomQueryEngine\",\n    # multimodal\n    \"SimpleMultiModalQueryEngine\",\n    # SQL\n    \"SQLTableRetrieverQueryEngine\",\n    \"NLSQLTableQueryEngine\",\n    \"PGVectorSQLQueryEngine\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/sql_vector_query_engine.py",
    "filename": "sql_vector_query_engine.py",
    "relpath": "query_engine/sql_vector_query_engine.py",
    "start_line": 1,
    "end_line": 173,
    "length": 173,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_get_prompts",
      "_update_prompts",
      "from_sql_and_vector_query_engines"
    ],
    "chunk_class_names": [
      "SQLAutoVectorQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_get_prompts",
      "_update_prompts",
      "from_sql_and_vector_query_engines"
    ],
    "document_class_names": [
      "SQLAutoVectorQueryEngine"
    ],
    "content": "\"\"\"SQL Vector query engine.\"\"\"\n\nimport logging\nfrom typing import Any, Optional, Union\n\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.struct_store.sql_query import (\n    BaseSQLTableQueryEngine,\n    NLSQLTableQueryEngine,\n)\nfrom llama_index.core.indices.vector_store.retrievers.auto_retriever import (\n    VectorIndexAutoRetriever,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixinType\nfrom llama_index.core.query_engine.retriever_query_engine import (\n    RetrieverQueryEngine,\n)\nfrom llama_index.core.query_engine.sql_join_query_engine import (\n    SQLAugmentQueryTransform,\n    SQLJoinQueryEngine,\n)\nfrom llama_index.core.selectors.llm_selectors import LLMSingleSelector\nfrom llama_index.core.selectors.pydantic_selectors import PydanticSingleSelector\nfrom llama_index.core.tools.query_engine import QueryEngineTool\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a SQL query. \\\nBoth the SQL query and the response are given below.\nGiven the SQL response, the question has also been translated into a vector store query.\nThe vector store query and response is given below.\nGiven SQL query, SQL response, transformed vector store query, and vector store \\\nresponse, please synthesize a response to the original question.\n\nOriginal question: {query_str}\nSQL query: {sql_query_str}\nSQL response: {sql_response_str}\nTransformed vector store query: {query_engine_query_str}\nVector store response: {query_engine_response_str}\nResponse:\n\"\"\"\nDEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT_TMPL\n)\n\n\n# NOTE: maintain for backwards compatibility\nclass SQLAutoVectorQueryEngine(SQLJoinQueryEngine):\n    \"\"\"SQL + Vector Index Auto Retriever Query Engine.\n\n    This query engine can query both a SQL database\n    as well as a vector database. It will first decide\n    whether it needs to query the SQL database or vector store.\n    If it decides to query the SQL database, it will also decide\n    whether to augment information with retrieved results from the vector store.\n    We use the VectorIndexAutoRetriever to retrieve results.\n\n    Args:\n        sql_query_tool (QueryEngineTool): Query engine tool for SQL database.\n        vector_query_tool (QueryEngineTool): Query engine tool for vector database.\n        selector (Optional[Union[LLMSingleSelector, PydanticSingleSelector]]):\n            Selector to use.\n        sql_vector_synthesis_prompt (Optional[BasePromptTemplate]):\n            Prompt to use for SQL vector synthesis.\n        sql_augment_query_transform (Optional[SQLAugmentQueryTransform]): Query\n            transform to use for SQL augmentation.\n        use_sql_vector_synthesis (bool): Whether to use SQL vector synthesis.\n        callback_manager (Optional[CallbackManager]): Callback manager to use.\n        verbose (bool): Whether to print intermediate results.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_query_tool: QueryEngineTool,\n        vector_query_tool: QueryEngineTool,\n        selector: Optional[Union[LLMSingleSelector, PydanticSingleSelector]] = None,\n        llm: Optional[LLM] = None,\n        sql_vector_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        sql_augment_query_transform: Optional[SQLAugmentQueryTransform] = None,\n        use_sql_vector_synthesis: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = True,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # validate that the query engines are of the right type\n        if not isinstance(\n            sql_query_tool.query_engine,\n            (BaseSQLTableQueryEngine, NLSQLTableQueryEngine),\n        ):\n            raise ValueError(\n                \"sql_query_tool.query_engine must be an instance of \"\n                \"BaseSQLTableQueryEngine or NLSQLTableQueryEngine\"\n            )\n        if not isinstance(vector_query_tool.query_engine, RetrieverQueryEngine):\n            raise ValueError(\n                \"vector_query_tool.query_engine must be an instance of \"\n                \"RetrieverQueryEngine\"\n            )\n        if not isinstance(\n            vector_query_tool.query_engine.retriever, VectorIndexAutoRetriever\n        ):\n            raise ValueError(\n                \"vector_query_tool.query_engine.retriever must be an instance \"\n                \"of VectorIndexAutoRetriever\"\n            )\n\n        sql_vector_synthesis_prompt = (\n            sql_vector_synthesis_prompt or DEFAULT_SQL_VECTOR_SYNTHESIS_PROMPT\n        )\n        super().__init__(\n            sql_query_tool,\n            vector_query_tool,\n            selector=selector,\n            llm=llm,\n            sql_join_synthesis_prompt=sql_vector_synthesis_prompt,\n            sql_augment_query_transform=sql_augment_query_transform,\n            use_sql_join_synthesis=use_sql_vector_synthesis,\n            callback_manager=callback_manager,\n            verbose=verbose,\n        )\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\n            \"selector\": self._selector,\n            \"sql_augment_query_transform\": self._sql_augment_query_transform,\n        }\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"sql_join_synthesis_prompt\": self._sql_join_synthesis_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"sql_join_synthesis_prompt\" in prompts:\n            self._sql_join_synthesis_prompt = prompts[\"sql_join_synthesis_prompt\"]\n\n    @classmethod\n    def from_sql_and_vector_query_engines(\n        cls,\n        sql_query_engine: Union[BaseSQLTableQueryEngine, NLSQLTableQueryEngine],\n        sql_tool_name: str,\n        sql_tool_description: str,\n        vector_auto_retriever: RetrieverQueryEngine,\n        vector_tool_name: str,\n        vector_tool_description: str,\n        selector: Optional[Union[LLMSingleSelector, PydanticSingleSelector]] = None,\n        **kwargs: Any,\n    ) -> \"SQLAutoVectorQueryEngine\":\n        \"\"\"From SQL and vector query engines.\n\n        Args:\n            sql_query_engine (BaseSQLTableQueryEngine): SQL query engine.\n            vector_query_engine (VectorIndexAutoRetriever): Vector retriever.\n            selector (Optional[Union[LLMSingleSelector, PydanticSingleSelector]]):\n                Selector to use.\n\n        \"\"\"\n        sql_query_tool = QueryEngineTool.from_defaults(\n            sql_query_engine, name=sql_tool_name, description=sql_tool_description\n        )\n        vector_query_tool = QueryEngineTool.from_defaults(\n            vector_auto_retriever,\n            name=vector_tool_name,\n            description=vector_tool_description,\n        )\n        return cls(sql_query_tool, vector_query_tool, selector, **kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/multistep_query_engine.py",
    "filename": "multistep_query_engine.py",
    "relpath": "query_engine/multistep_query_engine.py",
    "start_line": 1,
    "end_line": 177,
    "length": 177,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "default_stop_fn",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery",
      "_combine_queries",
      "_query_multistep"
    ],
    "chunk_class_names": [
      "MultiStepQueryEngine"
    ],
    "document_function_names": [
      "default_stop_fn",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery",
      "_combine_queries",
      "_query_multistep"
    ],
    "document_class_names": [
      "MultiStepQueryEngine"
    ],
    "content": "from typing import Any, Callable, Dict, List, Optional, Tuple, cast\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.indices.query.query_transform.base import (\n    StepDecomposeQueryTransform,\n)\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n\n\ndef default_stop_fn(stop_dict: Dict) -> bool:\n    \"\"\"Stop function for multi-step query combiner.\"\"\"\n    query_bundle = cast(QueryBundle, stop_dict.get(\"query_bundle\"))\n    if query_bundle is None:\n        raise ValueError(\"Response must be provided to stop function.\")\n\n    return \"none\" in query_bundle.query_str.lower()\n\n\nclass MultiStepQueryEngine(BaseQueryEngine):\n    \"\"\"Multi-step query engine.\n\n    This query engine can operate over an existing base query engine,\n    along with the multi-step query transform.\n\n    Args:\n        query_engine (BaseQueryEngine): A BaseQueryEngine object.\n        query_transform (StepDecomposeQueryTransform): A StepDecomposeQueryTransform\n            object.\n        response_synthesizer (Optional[BaseSynthesizer]): A BaseSynthesizer\n            object.\n        num_steps (Optional[int]): Number of steps to run the multi-step query.\n        early_stopping (bool): Whether to stop early if the stop function returns True.\n        index_summary (str): A string summary of the index.\n        stop_fn (Optional[Callable[[Dict], bool]]): A stop function that takes in a\n            dictionary of information and returns a boolean.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        query_transform: StepDecomposeQueryTransform,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        num_steps: Optional[int] = 3,\n        early_stopping: bool = True,\n        index_summary: str = \"None\",\n        stop_fn: Optional[Callable[[Dict], bool]] = None,\n    ) -> None:\n        self._query_engine = query_engine\n        self._query_transform = query_transform\n        self._response_synthesizer = response_synthesizer or get_response_synthesizer(\n            callback_manager=self._query_engine.callback_manager\n        )\n\n        self._index_summary = index_summary\n        self._num_steps = num_steps\n        self._early_stopping = early_stopping\n        # TODO: make interface to stop function better\n        self._stop_fn = stop_fn or default_stop_fn\n        # num_steps must be provided if early_stopping is False\n        if not self._early_stopping and self._num_steps is None:\n            raise ValueError(\"Must specify num_steps if early_stopping is False.\")\n\n        callback_manager = self._query_engine.callback_manager\n        super().__init__(callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\n            \"response_synthesizer\": self._response_synthesizer,\n            \"query_transform\": self._query_transform,\n        }\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            nodes, source_nodes, metadata = self._query_multistep(query_bundle)\n\n            final_response = self._response_synthesizer.synthesize(\n                query=query_bundle,\n                nodes=nodes,\n                additional_source_nodes=source_nodes,\n            )\n            final_response.metadata = metadata\n\n            query_event.on_end(payload={EventPayload.RESPONSE: final_response})\n\n        return final_response\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            nodes, source_nodes, metadata = self._query_multistep(query_bundle)\n\n            final_response = await self._response_synthesizer.asynthesize(\n                query=query_bundle,\n                nodes=nodes,\n                additional_source_nodes=source_nodes,\n            )\n            final_response.metadata = metadata\n\n            query_event.on_end(payload={EventPayload.RESPONSE: final_response})\n\n        return final_response\n\n    def _combine_queries(\n        self, query_bundle: QueryBundle, prev_reasoning: str\n    ) -> QueryBundle:\n        \"\"\"Combine queries.\"\"\"\n        transform_metadata = {\n            \"prev_reasoning\": prev_reasoning,\n            \"index_summary\": self._index_summary,\n        }\n        return self._query_transform(query_bundle, metadata=transform_metadata)\n\n    def _query_multistep(\n        self, query_bundle: QueryBundle\n    ) -> Tuple[List[NodeWithScore], List[NodeWithScore], Dict[str, Any]]:\n        \"\"\"Run query combiner.\"\"\"\n        prev_reasoning = \"\"\n        cur_response = None\n        should_stop = False\n        cur_steps = 0\n\n        # use response\n        final_response_metadata: Dict[str, Any] = {\"sub_qa\": []}\n\n        text_chunks = []\n        source_nodes = []\n        while not should_stop:\n            if self._num_steps is not None and cur_steps >= self._num_steps:\n                should_stop = True\n                break\n            elif should_stop:\n                break\n\n            updated_query_bundle = self._combine_queries(query_bundle, prev_reasoning)\n\n            # TODO: make stop logic better\n            stop_dict = {\"query_bundle\": updated_query_bundle}\n            if self._stop_fn(stop_dict):\n                should_stop = True\n                break\n\n            cur_response = self._query_engine.query(updated_query_bundle)\n\n            # append to response builder\n            cur_qa_text = (\n                f\"\\nQuestion: {updated_query_bundle.query_str}\\n\"\n                f\"Answer: {cur_response!s}\"\n            )\n            text_chunks.append(cur_qa_text)\n            for source_node in cur_response.source_nodes:\n                source_nodes.append(source_node)\n            # update metadata\n            final_response_metadata[\"sub_qa\"].append(\n                (updated_query_bundle.query_str, cur_response)\n            )\n\n            prev_reasoning += (\n                f\"- {updated_query_bundle.query_str}\\n\" f\"- {cur_response!s}\\n\"\n            )\n            cur_steps += 1\n\n        nodes = [\n            NodeWithScore(node=TextNode(text=text_chunk)) for text_chunk in text_chunks\n        ]\n        return nodes, source_nodes, final_response_metadata"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/retry_query_engine.py",
    "filename": "retry_query_engine.py",
    "relpath": "query_engine/retry_query_engine.py",
    "start_line": 1,
    "end_line": 142,
    "length": 142,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "RetryQueryEngine",
      "RetryGuidelineQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery",
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "RetryQueryEngine",
      "RetryGuidelineQueryEngine"
    ],
    "content": "import logging\nfrom typing import Optional\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import (\n    RESPONSE_TYPE,\n    Response,\n    AsyncStreamingResponse,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.evaluation.base import BaseEvaluator\nfrom llama_index.core.evaluation.guideline import GuidelineEvaluator\nfrom llama_index.core.indices.query.query_transform.feedback_transform import (\n    FeedbackQueryTransformation,\n)\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.schema import QueryBundle\n\nlogger = logging.getLogger(__name__)\n\n\nclass RetryQueryEngine(BaseQueryEngine):\n    \"\"\"Does retry on query engine if it fails evaluation.\n\n    Args:\n        query_engine (BaseQueryEngine): A query engine object\n        evaluator (BaseEvaluator): An evaluator object\n        max_retries (int): Maximum number of retries\n        callback_manager (Optional[CallbackManager]): A callback manager object\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        evaluator: BaseEvaluator,\n        max_retries: int = 3,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        self._query_engine = query_engine\n        self._evaluator = evaluator\n        self.max_retries = max_retries\n        super().__init__(callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\"query_engine\": self._query_engine, \"evaluator\": self._evaluator}\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query_engine._query(query_bundle)\n        assert not isinstance(response, AsyncStreamingResponse)\n        if self.max_retries <= 0:\n            return response\n        typed_response = (\n            response if isinstance(response, Response) else response.get_response()\n        )\n        query_str = query_bundle.query_str\n        eval = self._evaluator.evaluate_response(query_str, typed_response)\n        if eval.passing:\n            logger.debug(\"Evaluation returned True.\")\n            return response\n        else:\n            logger.debug(\"Evaluation returned False.\")\n            new_query_engine = RetryQueryEngine(\n                self._query_engine, self._evaluator, self.max_retries - 1\n            )\n            query_transformer = FeedbackQueryTransformation()\n            new_query = query_transformer.run(query_bundle, {\"evaluation\": eval})\n            return new_query_engine.query(new_query)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Not supported.\"\"\"\n        return self._query(query_bundle)\n\n\nclass RetryGuidelineQueryEngine(BaseQueryEngine):\n    \"\"\"Does retry with evaluator feedback\n    if query engine fails evaluation.\n\n    Args:\n        query_engine (BaseQueryEngine): A query engine object\n        guideline_evaluator (GuidelineEvaluator): A guideline evaluator object\n        resynthesize_query (bool): Whether to resynthesize query\n        max_retries (int): Maximum number of retries\n        callback_manager (Optional[CallbackManager]): A callback manager object\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        guideline_evaluator: GuidelineEvaluator,\n        resynthesize_query: bool = False,\n        max_retries: int = 3,\n        callback_manager: Optional[CallbackManager] = None,\n        query_transformer: Optional[FeedbackQueryTransformation] = None,\n    ) -> None:\n        self._query_engine = query_engine\n        self._guideline_evaluator = guideline_evaluator\n        self.max_retries = max_retries\n        self.resynthesize_query = resynthesize_query\n        self.query_transformer = query_transformer or FeedbackQueryTransformation(\n            resynthesize_query=self.resynthesize_query\n        )\n        super().__init__(callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\n            \"query_engine\": self._query_engine,\n            \"guideline_evalator\": self._guideline_evaluator,\n        }\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        response = self._query_engine._query(query_bundle)\n        assert not isinstance(response, AsyncStreamingResponse)\n        if self.max_retries <= 0:\n            return response\n        typed_response = (\n            response if isinstance(response, Response) else response.get_response()\n        )\n        query_str = query_bundle.query_str\n        eval = self._guideline_evaluator.evaluate_response(query_str, typed_response)\n        if eval.passing:\n            logger.debug(\"Evaluation returned True.\")\n            return response\n        else:\n            logger.debug(\"Evaluation returned False.\")\n            new_query_engine = RetryGuidelineQueryEngine(\n                self._query_engine,\n                self._guideline_evaluator,\n                self.resynthesize_query,\n                self.max_retries - 1,\n                self.callback_manager,\n            )\n            new_query = self.query_transformer.run(query_bundle, {\"evaluation\": eval})\n            logger.debug(\"New query: %s\", new_query.query_str)\n            return new_query_engine.query(new_query)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Not supported.\"\"\"\n        return self._query(query_bundle)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/sub_question_query_engine.py",
    "filename": "sub_question_query_engine.py",
    "relpath": "query_engine/sub_question_query_engine.py",
    "start_line": 1,
    "end_line": 276,
    "length": 276,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "from_defaults",
      "_query",
      "_aquery",
      "_construct_node",
      "_aquery_subq",
      "_query_subq"
    ],
    "chunk_class_names": [
      "SubQuestionAnswerPair",
      "SubQuestionQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "from_defaults",
      "_query",
      "_aquery",
      "_construct_node",
      "_aquery_subq",
      "_query_subq"
    ],
    "document_class_names": [
      "SubQuestionAnswerPair",
      "SubQuestionQueryEngine"
    ],
    "content": "import asyncio\nimport logging\nfrom typing import List, Optional, Sequence, cast\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.question_gen.llm_generators import LLMQuestionGenerator\nfrom llama_index.core.question_gen.types import BaseQuestionGenerator, SubQuestion\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.query_engine import QueryEngineTool\nfrom llama_index.core.utils import get_color_mapping, print_text\n\nlogger = logging.getLogger(__name__)\n\n\nclass SubQuestionAnswerPair(BaseModel):\n    \"\"\"\n    Pair of the sub question and optionally its answer (if its been answered yet).\n    \"\"\"\n\n    sub_q: SubQuestion\n    answer: Optional[str] = None\n    sources: List[NodeWithScore] = Field(default_factory=list)\n\n\nclass SubQuestionQueryEngine(BaseQueryEngine):\n    \"\"\"Sub question query engine.\n\n    A query engine that breaks down a complex query (e.g. compare and contrast) into\n        many sub questions and their target query engine for execution.\n        After executing all sub questions, all responses are gathered and sent to\n        response synthesizer to produce the final response.\n\n    Args:\n        question_gen (BaseQuestionGenerator): A module for generating sub questions\n            given a complex question and tools.\n        response_synthesizer (BaseSynthesizer): A response synthesizer for\n            generating the final response\n        query_engine_tools (Sequence[QueryEngineTool]): Tools to answer the\n            sub questions.\n        verbose (bool): whether to print intermediate questions and answers.\n            Defaults to True\n        use_async (bool): whether to execute the sub questions with asyncio.\n            Defaults to True\n    \"\"\"\n\n    def __init__(\n        self,\n        question_gen: BaseQuestionGenerator,\n        response_synthesizer: BaseSynthesizer,\n        query_engine_tools: Sequence[QueryEngineTool],\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = True,\n        use_async: bool = False,\n    ) -> None:\n        self._question_gen = question_gen\n        self._response_synthesizer = response_synthesizer\n        self._metadatas = [x.metadata for x in query_engine_tools]\n        self._query_engines = {\n            tool.metadata.name: tool.query_engine for tool in query_engine_tools\n        }\n        self._verbose = verbose\n        self._use_async = use_async\n        super().__init__(callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\n            \"question_gen\": self._question_gen,\n            \"response_synthesizer\": self._response_synthesizer,\n        }\n\n    @classmethod\n    def from_defaults(\n        cls,\n        query_engine_tools: Sequence[QueryEngineTool],\n        llm: Optional[LLM] = None,\n        question_gen: Optional[BaseQuestionGenerator] = None,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        verbose: bool = True,\n        use_async: bool = True,\n    ) -> \"SubQuestionQueryEngine\":\n        callback_manager = Settings.callback_manager\n        if len(query_engine_tools) > 0:\n            callback_manager = query_engine_tools[0].query_engine.callback_manager\n\n        llm = llm or Settings.llm\n        if question_gen is None:\n            try:\n                from llama_index.question_gen.openai import (\n                    OpenAIQuestionGenerator,\n                )  # pants: no-infer-dep\n\n                # try to use OpenAI function calling based question generator.\n                # if incompatible, use general LLM question generator\n                question_gen = OpenAIQuestionGenerator.from_defaults(llm=llm)\n\n            except ImportError as e:\n                raise ImportError(\n                    \"`llama-index-question-gen-openai` package cannot be found. \"\n                    \"Please install it by using `pip install `llama-index-question-gen-openai`\"\n                )\n            except ValueError:\n                question_gen = LLMQuestionGenerator.from_defaults(llm=llm)\n\n        synth = response_synthesizer or get_response_synthesizer(\n            llm=llm,\n            callback_manager=callback_manager,\n            use_async=use_async,\n        )\n\n        return cls(\n            question_gen,\n            synth,\n            query_engine_tools,\n            callback_manager=callback_manager,\n            verbose=verbose,\n            use_async=use_async,\n        )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            sub_questions = self._question_gen.generate(self._metadatas, query_bundle)\n\n            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])\n\n            if self._verbose:\n                print_text(f\"Generated {len(sub_questions)} sub questions.\\n\")\n\n            if self._use_async:\n                tasks = [\n                    self._aquery_subq(sub_q, color=colors[str(ind)])\n                    for ind, sub_q in enumerate(sub_questions)\n                ]\n\n                qa_pairs_all = run_async_tasks(tasks)\n                qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n            else:\n                qa_pairs_all = [\n                    self._query_subq(sub_q, color=colors[str(ind)])\n                    for ind, sub_q in enumerate(sub_questions)\n                ]\n\n            # filter out sub questions that failed\n            qa_pairs: List[SubQuestionAnswerPair] = list(filter(None, qa_pairs_all))\n\n            nodes = [self._construct_node(pair) for pair in qa_pairs]\n\n            source_nodes = [node for qa_pair in qa_pairs for node in qa_pair.sources]\n            response = self._response_synthesizer.synthesize(\n                query=query_bundle,\n                nodes=nodes,\n                additional_source_nodes=source_nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            sub_questions = await self._question_gen.agenerate(\n                self._metadatas, query_bundle\n            )\n\n            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])\n\n            if self._verbose:\n                print_text(f\"Generated {len(sub_questions)} sub questions.\\n\")\n\n            tasks = [\n                self._aquery_subq(sub_q, color=colors[str(ind)])\n                for ind, sub_q in enumerate(sub_questions)\n            ]\n\n            qa_pairs_all = await asyncio.gather(*tasks)\n            qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n\n            # filter out sub questions that failed\n            qa_pairs: List[SubQuestionAnswerPair] = list(filter(None, qa_pairs_all))\n\n            nodes = [self._construct_node(pair) for pair in qa_pairs]\n\n            source_nodes = [node for qa_pair in qa_pairs for node in qa_pair.sources]\n            response = await self._response_synthesizer.asynthesize(\n                query=query_bundle,\n                nodes=nodes,\n                additional_source_nodes=source_nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    def _construct_node(self, qa_pair: SubQuestionAnswerPair) -> NodeWithScore:\n        node_text = (\n            f\"Sub question: {qa_pair.sub_q.sub_question}\\nResponse: {qa_pair.answer}\"\n        )\n        return NodeWithScore(node=TextNode(text=node_text))\n\n    async def _aquery_subq(\n        self, sub_q: SubQuestion, color: Optional[str] = None\n    ) -> Optional[SubQuestionAnswerPair]:\n        try:\n            with self.callback_manager.event(\n                CBEventType.SUB_QUESTION,\n                payload={EventPayload.SUB_QUESTION: SubQuestionAnswerPair(sub_q=sub_q)},\n            ) as event:\n                question = sub_q.sub_question\n                query_engine = self._query_engines[sub_q.tool_name]\n\n                if self._verbose:\n                    print_text(f\"[{sub_q.tool_name}] Q: {question}\\n\", color=color)\n\n                response = await query_engine.aquery(question)\n                response_text = str(response)\n\n                if self._verbose:\n                    print_text(f\"[{sub_q.tool_name}] A: {response_text}\\n\", color=color)\n\n                qa_pair = SubQuestionAnswerPair(\n                    sub_q=sub_q, answer=response_text, sources=response.source_nodes\n                )\n\n                event.on_end(payload={EventPayload.SUB_QUESTION: qa_pair})\n\n            return qa_pair\n        except ValueError:\n            logger.warning(f\"[{sub_q.tool_name}] Failed to run {question}\")\n            return None\n\n    def _query_subq(\n        self, sub_q: SubQuestion, color: Optional[str] = None\n    ) -> Optional[SubQuestionAnswerPair]:\n        try:\n            with self.callback_manager.event(\n                CBEventType.SUB_QUESTION,\n                payload={EventPayload.SUB_QUESTION: SubQuestionAnswerPair(sub_q=sub_q)},\n            ) as event:\n                question = sub_q.sub_question\n                query_engine = self._query_engines[sub_q.tool_name]\n\n                if self._verbose:\n                    print_text(f\"[{sub_q.tool_name}] Q: {question}\\n\", color=color)\n\n                response = query_engine.query(question)\n                response_text = str(response)\n\n                if self._verbose:\n                    print_text(f\"[{sub_q.tool_name}] A: {response_text}\\n\", color=color)\n\n                qa_pair = SubQuestionAnswerPair(\n                    sub_q=sub_q, answer=response_text, sources=response.source_nodes\n                )\n\n                event.on_end(payload={EventPayload.SUB_QUESTION: qa_pair})\n\n            return qa_pair\n        except ValueError:\n            logger.warning(f\"[{sub_q.tool_name}] Failed to run {question}\")\n            return None"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/transform_query_engine.py",
    "filename": "transform_query_engine.py",
    "relpath": "query_engine/transform_query_engine.py",
    "start_line": 1,
    "end_line": 93,
    "length": 93,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "retrieve",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "TransformQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "retrieve",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "TransformQueryEngine"
    ],
    "content": "from typing import List, Optional, Sequence\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.query.query_transform.base import BaseQueryTransform\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\n\n\nclass TransformQueryEngine(BaseQueryEngine):\n    \"\"\"Transform query engine.\n\n    Applies a query transform to a query bundle before passing\n        it to a query engine.\n\n    Args:\n        query_engine (BaseQueryEngine): A query engine object.\n        query_transform (BaseQueryTransform): A query transform object.\n        transform_metadata (Optional[dict]): metadata to pass to the\n            query transform.\n        callback_manager (Optional[CallbackManager]): A callback manager.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        query_transform: BaseQueryTransform,\n        transform_metadata: Optional[dict] = None,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        self._query_engine = query_engine\n        self._query_transform = query_transform\n        self._transform_metadata = transform_metadata\n        super().__init__(callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\n            \"query_transform\": self._query_transform,\n            \"query_engine\": self._query_engine,\n        }\n\n    def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n            query_bundle, metadata=self._transform_metadata\n        )\n        return self._query_engine.retrieve(query_bundle)\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        query_bundle = self._query_transform.run(\n            query_bundle, metadata=self._transform_metadata\n        )\n        return self._query_engine.synthesize(\n            query_bundle=query_bundle,\n            nodes=nodes,\n            additional_source_nodes=additional_source_nodes,\n        )\n\n    async def asynthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        query_bundle = self._query_transform.run(\n            query_bundle, metadata=self._transform_metadata\n        )\n        return await self._query_engine.asynthesize(\n            query_bundle=query_bundle,\n            nodes=nodes,\n            additional_source_nodes=additional_source_nodes,\n        )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        query_bundle = self._query_transform.run(\n            query_bundle, metadata=self._transform_metadata\n        )\n        return self._query_engine.query(query_bundle)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        query_bundle = self._query_transform.run(\n            query_bundle, metadata=self._transform_metadata\n        )\n        return await self._query_engine.aquery(query_bundle)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/retriever_query_engine.py",
    "filename": "retriever_query_engine.py",
    "relpath": "query_engine/retriever_query_engine.py",
    "start_line": 1,
    "end_line": 207,
    "length": 207,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "from_args",
      "_apply_node_postprocessors",
      "retrieve",
      "aretrieve",
      "with_retriever",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery",
      "retriever"
    ],
    "chunk_class_names": [
      "RetrieverQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "from_args",
      "_apply_node_postprocessors",
      "retrieve",
      "aretrieve",
      "with_retriever",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery",
      "retriever"
    ],
    "document_class_names": [
      "RetrieverQueryEngine"
    ],
    "content": "from typing import Any, List, Optional, Sequence, Type\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    ResponseMode,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass RetrieverQueryEngine(BaseQueryEngine):\n    \"\"\"Retriever query engine.\n\n    Args:\n        retriever (BaseRetriever): A retriever object.\n        response_synthesizer (Optional[BaseSynthesizer]): A BaseSynthesizer\n            object.\n        callback_manager (Optional[CallbackManager]): A callback manager.\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        self._retriever = retriever\n        self._response_synthesizer = response_synthesizer or get_response_synthesizer(\n            llm=Settings.llm,\n            callback_manager=callback_manager or Settings.callback_manager,\n        )\n\n        self._node_postprocessors = node_postprocessors or []\n        callback_manager = (\n            callback_manager or self._response_synthesizer.callback_manager\n        )\n        for node_postprocessor in self._node_postprocessors:\n            node_postprocessor.callback_manager = callback_manager\n        super().__init__(callback_manager=callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\"response_synthesizer\": self._response_synthesizer}\n\n    @classmethod\n    def from_args(\n        cls,\n        retriever: BaseRetriever,\n        llm: Optional[LLM] = None,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        # response synthesizer args\n        response_mode: ResponseMode = ResponseMode.COMPACT,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        refine_template: Optional[BasePromptTemplate] = None,\n        summary_template: Optional[BasePromptTemplate] = None,\n        simple_template: Optional[BasePromptTemplate] = None,\n        output_cls: Optional[Type[BaseModel]] = None,\n        use_async: bool = False,\n        streaming: bool = False,\n        **kwargs: Any,\n    ) -> \"RetrieverQueryEngine\":\n        \"\"\"Initialize a RetrieverQueryEngine object.\".\n\n        Args:\n            retriever (BaseRetriever): A retriever object.\n            llm (Optional[LLM]): An instance of an LLM.\n            response_synthesizer (Optional[BaseSynthesizer]): An instance of a response\n                synthesizer.\n            node_postprocessors (Optional[List[BaseNodePostprocessor]]): A list of\n                node postprocessors.\n            callback_manager (Optional[CallbackManager]): A callback manager.\n            response_mode (ResponseMode): A ResponseMode object.\n            text_qa_template (Optional[BasePromptTemplate]): A BasePromptTemplate\n                object.\n            refine_template (Optional[BasePromptTemplate]): A BasePromptTemplate object.\n            summary_template (Optional[BasePromptTemplate]): A BasePromptTemplate object.\n            simple_template (Optional[BasePromptTemplate]): A BasePromptTemplate object.\n            output_cls (Optional[Type[BaseModel]]): The pydantic model to pass to the\n                response synthesizer.\n            use_async (bool): Whether to use async.\n            streaming (bool): Whether to use streaming.\n        \"\"\"\n        llm = llm or Settings.llm\n\n        response_synthesizer = response_synthesizer or get_response_synthesizer(\n            llm=llm,\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n            summary_template=summary_template,\n            simple_template=simple_template,\n            response_mode=response_mode,\n            output_cls=output_cls,\n            use_async=use_async,\n            streaming=streaming,\n        )\n\n        callback_manager = callback_manager or Settings.callback_manager\n\n        return cls(\n            retriever=retriever,\n            response_synthesizer=response_synthesizer,\n            callback_manager=callback_manager,\n            node_postprocessors=node_postprocessors,\n        )\n\n    def _apply_node_postprocessors(\n        self, nodes: List[NodeWithScore], query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        for node_postprocessor in self._node_postprocessors:\n            nodes = node_postprocessor.postprocess_nodes(\n                nodes, query_bundle=query_bundle\n            )\n        return nodes\n\n    def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = self._retriever.retrieve(query_bundle)\n        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    async def aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = await self._retriever.aretrieve(query_bundle)\n        return self._apply_node_postprocessors(nodes, query_bundle=query_bundle)\n\n    def with_retriever(self, retriever: BaseRetriever) -> \"RetrieverQueryEngine\":\n        return RetrieverQueryEngine(\n            retriever=retriever,\n            response_synthesizer=self._response_synthesizer,\n            callback_manager=self.callback_manager,\n            node_postprocessors=self._node_postprocessors,\n        )\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        return self._response_synthesizer.synthesize(\n            query=query_bundle,\n            nodes=nodes,\n            additional_source_nodes=additional_source_nodes,\n        )\n\n    async def asynthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        return await self._response_synthesizer.asynthesize(\n            query=query_bundle,\n            nodes=nodes,\n            additional_source_nodes=additional_source_nodes,\n        )\n\n    @dispatcher.span\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            nodes = self.retrieve(query_bundle)\n            response = self._response_synthesizer.synthesize(\n                query=query_bundle,\n                nodes=nodes,\n            )\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    @dispatcher.span\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            nodes = await self.aretrieve(query_bundle)\n\n            response = await self._response_synthesizer.asynthesize(\n                query=query_bundle,\n                nodes=nodes,\n            )\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    @property\n    def retriever(self) -> BaseRetriever:\n        \"\"\"Get the retriever object.\"\"\"\n        return self._retriever"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/graph_query_engine.py",
    "filename": "graph_query_engine.py",
    "relpath": "query_engine/graph_query_engine.py",
    "start_line": 1,
    "end_line": 129,
    "length": 129,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_aquery",
      "_query",
      "_query_index",
      "_fetch_recursive_nodes"
    ],
    "chunk_class_names": [
      "ComposableGraphQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_aquery",
      "_query",
      "_query_index",
      "_fetch_recursive_nodes"
    ],
    "document_class_names": [
      "ComposableGraphQueryEngine"
    ],
    "content": "from typing import Any, Dict, List, Optional, Tuple\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.indices.composability.graph import ComposableGraph\nfrom llama_index.core.schema import IndexNode, NodeWithScore, QueryBundle, TextNode\nfrom llama_index.core.settings import Settings\n\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass ComposableGraphQueryEngine(BaseQueryEngine):\n    \"\"\"Composable graph query engine.\n\n    This query engine can operate over a ComposableGraph.\n    It can take in custom query engines for its sub-indices.\n\n    Args:\n        graph (ComposableGraph): A ComposableGraph object.\n        custom_query_engines (Optional[Dict[str, BaseQueryEngine]]): A dictionary of\n            custom query engines.\n        recursive (bool): Whether to recursively query the graph.\n        **kwargs: additional arguments to be passed to the underlying index query\n            engine.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        graph: ComposableGraph,\n        custom_query_engines: Optional[Dict[str, BaseQueryEngine]] = None,\n        recursive: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._graph = graph\n        self._custom_query_engines = custom_query_engines or {}\n        self._kwargs = kwargs\n\n        # additional configs\n        self._recursive = recursive\n        callback_manager = Settings.callback_manager\n        super().__init__(callback_manager=callback_manager)\n\n    def _get_prompt_modules(self) -> Dict[str, Any]:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        return self._query_index(query_bundle, index_id=None, level=0)\n\n    @dispatcher.span\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        return self._query_index(query_bundle, index_id=None, level=0)\n\n    def _query_index(\n        self,\n        query_bundle: QueryBundle,\n        index_id: Optional[str] = None,\n        level: int = 0,\n    ) -> RESPONSE_TYPE:\n        \"\"\"Query a single index.\"\"\"\n        index_id = index_id or self._graph.root_id\n\n        with self.callback_manager.event(\n            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n        ) as query_event:\n            # get query engine\n            if index_id in self._custom_query_engines:\n                query_engine = self._custom_query_engines[index_id]\n            else:\n                query_engine = self._graph.get_index(index_id).as_query_engine(\n                    **self._kwargs\n                )\n\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as retrieve_event:\n                nodes = query_engine.retrieve(query_bundle)\n                retrieve_event.on_end(payload={EventPayload.NODES: nodes})\n\n            if self._recursive:\n                # do recursion here\n                nodes_for_synthesis = []\n                additional_source_nodes = []\n                for node_with_score in nodes:\n                    node_with_score, source_nodes = self._fetch_recursive_nodes(\n                        node_with_score, query_bundle, level\n                    )\n                    nodes_for_synthesis.append(node_with_score)\n                    additional_source_nodes.extend(source_nodes)\n                response = query_engine.synthesize(\n                    query_bundle, nodes_for_synthesis, additional_source_nodes\n                )\n            else:\n                response = query_engine.synthesize(query_bundle, nodes)\n\n            query_event.on_end(payload={EventPayload.RESPONSE: response})\n\n        return response\n\n    def _fetch_recursive_nodes(\n        self,\n        node_with_score: NodeWithScore,\n        query_bundle: QueryBundle,\n        level: int,\n    ) -> Tuple[NodeWithScore, List[NodeWithScore]]:\n        \"\"\"Fetch nodes.\n\n        Uses existing node if it's not an index node.\n        Otherwise fetch response from corresponding index.\n\n        \"\"\"\n        if isinstance(node_with_score.node, IndexNode):\n            index_node = node_with_score.node\n            # recursive call\n            response = self._query_index(query_bundle, index_node.index_id, level + 1)\n\n            new_node = TextNode(text=str(response))\n            new_node_with_score = NodeWithScore(\n                node=new_node, score=node_with_score.score\n            )\n            return new_node_with_score, response.source_nodes\n        else:\n            return node_with_score, []"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/sql_join_query_engine.py",
    "filename": "sql_join_query_engine.py",
    "relpath": "query_engine/sql_join_query_engine.py",
    "start_line": 1,
    "end_line": 349,
    "length": 349,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_default_check_stop",
      "_format_sql_query",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "check_stop",
      "__init__",
      "_get_prompt_modules",
      "_get_prompts",
      "_update_prompts",
      "_query_sql_other",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "SQLAugmentQueryTransform",
      "SQLJoinQueryEngine"
    ],
    "document_function_names": [
      "_default_check_stop",
      "_format_sql_query",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "check_stop",
      "__init__",
      "_get_prompt_modules",
      "_get_prompts",
      "_update_prompts",
      "_query_sql_other",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "SQLAugmentQueryTransform",
      "SQLJoinQueryEngine"
    ],
    "content": "\"\"\"SQL Join query engine.\"\"\"\n\nimport logging\nfrom typing import Callable, Dict, Optional, Union\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import (\n    RESPONSE_TYPE,\n    Response,\n    StreamingResponse,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.query.query_transform.base import BaseQueryTransform\nfrom llama_index.core.indices.struct_store.sql_query import (\n    BaseSQLTableQueryEngine,\n    NLSQLTableQueryEngine,\n)\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixinType\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.selectors.llm_selectors import LLMSingleSelector\nfrom llama_index.core.selectors.pydantic_selectors import PydanticSingleSelector\nfrom llama_index.core.selectors.utils import get_selector_from_llm\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.query_engine import QueryEngineTool\nfrom llama_index.core.utils import print_text\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_SQL_JOIN_SYNTHESIS_PROMPT_TMPL = \"\"\"\nThe original question is given below.\nThis question has been translated into a SQL query. Both the SQL query and \\\nthe response are given below.\nGiven the SQL response, the question has also been transformed into a more \\\ndetailed query,\nand executed against another query engine.\nThe transformed query and query engine response are also given below.\nGiven SQL query, SQL response, transformed query, and query engine response, \\\nplease synthesize a response to the original question.\n\nOriginal question: {query_str}\nSQL query: {sql_query_str}\nSQL response: {sql_response_str}\nTransformed query: {query_engine_query_str}\nQuery engine response: {query_engine_response_str}\nResponse:\n\"\"\"\nDEFAULT_SQL_JOIN_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_SQL_JOIN_SYNTHESIS_PROMPT_TMPL\n)\n\n\nDEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT_TMPL = \"\"\"\n\"The original question is given below.\nThis question has been translated into a SQL query. Both the SQL query and the \\\nresponse are given below.\nThe SQL response either answers the question, or should provide additional context \\\nthat can be used to make the question more specific.\nYour job is to come up with a more specific question that needs to be answered to \\\nfully answer the original question, or 'None' if the original question has already \\\nbeen fully answered from the SQL response. Do not create a new question that is \\\nirrelevant to the original question; in that case return None instead.\n\nExamples:\n\nOriginal question: Please give more details about the demographics of the city with \\\nthe highest population.\nSQL query: SELECT city, population FROM cities ORDER BY population DESC LIMIT 1\nSQL response: The city with the highest population is New York City.\nNew question: Can you tell me more about the demographics of New York City?\n\nOriginal question: Please compare the sports environment of cities in North America.\nSQL query: SELECT city_name FROM cities WHERE continent = 'North America' LIMIT 3\nSQL response: The cities in North America are New York, San Francisco, and Toronto.\nNew question: What sports are played in New York, San Francisco, and Toronto?\n\nOriginal question: What is the city with the highest population?\nSQL query: SELECT city, population FROM cities ORDER BY population DESC LIMIT 1\nSQL response: The city with the highest population is New York City.\nNew question: None\n\nOriginal question: What countries are the top 3 ATP players from?\nSQL query: SELECT country FROM players WHERE rank <= 3\nSQL response: The top 3 ATP players are from Serbia, Russia, and Spain.\nNew question: None\n\nOriginal question: {query_str}\nSQL query: {sql_query_str}\nSQL response: {sql_response_str}\nNew question: \"\n\"\"\"\nDEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT = PromptTemplate(\n    DEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT_TMPL\n)\n\n\ndef _default_check_stop(query_bundle: QueryBundle) -> bool:\n    \"\"\"Default check stop function.\"\"\"\n    return query_bundle.query_str.lower() == \"none\"\n\n\ndef _format_sql_query(sql_query: str) -> str:\n    \"\"\"Format SQL query.\"\"\"\n    return sql_query.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n\n\nclass SQLAugmentQueryTransform(BaseQueryTransform):\n    \"\"\"SQL Augment Query Transform.\n\n    This query transform will transform the query into a more specific query\n    after augmenting with SQL results.\n\n    Args:\n        llm (LLM): LLM to use for query transformation.\n        sql_augment_transform_prompt (BasePromptTemplate): PromptTemplate to use\n            for query transformation.\n        check_stop_parser (Optional[Callable[[str], bool]]): Check stop function.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        sql_augment_transform_prompt: Optional[BasePromptTemplate] = None,\n        check_stop_parser: Optional[Callable[[QueryBundle], bool]] = None,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or Settings.llm\n\n        self._sql_augment_transform_prompt = (\n            sql_augment_transform_prompt or DEFAULT_SQL_AUGMENT_TRANSFORM_PROMPT\n        )\n        self._check_stop_parser = check_stop_parser or _default_check_stop\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"sql_augment_transform_prompt\": self._sql_augment_transform_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"sql_augment_transform_prompt\" in prompts:\n            self._sql_augment_transform_prompt = prompts[\"sql_augment_transform_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        query_str = query_bundle.query_str\n        sql_query = metadata[\"sql_query\"]\n        sql_query_response = metadata[\"sql_query_response\"]\n        new_query_str = self._llm.predict(\n            self._sql_augment_transform_prompt,\n            query_str=query_str,\n            sql_query_str=sql_query,\n            sql_response_str=sql_query_response,\n        )\n        return QueryBundle(\n            new_query_str, custom_embedding_strs=query_bundle.custom_embedding_strs\n        )\n\n    def check_stop(self, query_bundle: QueryBundle) -> bool:\n        \"\"\"Check if query indicates stop.\"\"\"\n        return self._check_stop_parser(query_bundle)\n\n\nclass SQLJoinQueryEngine(BaseQueryEngine):\n    \"\"\"SQL Join Query Engine.\n\n    This query engine can \"Join\" a SQL database results\n    with another query engine.\n    It can decide it needs to query the SQL database or the other query engine.\n    If it decides to query the SQL database, it will first query the SQL database,\n    whether to augment information with retrieved results from the other query engine.\n\n    Args:\n        sql_query_tool (QueryEngineTool): Query engine tool for SQL database.\n            other_query_tool (QueryEngineTool): Other query engine tool.\n        selector (Optional[Union[LLMSingleSelector, PydanticSingleSelector]]):\n            Selector to use.\n        sql_join_synthesis_prompt (Optional[BasePromptTemplate]):\n            PromptTemplate to use for SQL join synthesis.\n        sql_augment_query_transform (Optional[SQLAugmentQueryTransform]): Query\n            transform to use for SQL augmentation.\n        use_sql_join_synthesis (bool): Whether to use SQL join synthesis.\n        callback_manager (Optional[CallbackManager]): Callback manager to use.\n        verbose (bool): Whether to print intermediate results.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_query_tool: QueryEngineTool,\n        other_query_tool: QueryEngineTool,\n        selector: Optional[Union[LLMSingleSelector, PydanticSingleSelector]] = None,\n        llm: Optional[LLM] = None,\n        sql_join_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        sql_augment_query_transform: Optional[SQLAugmentQueryTransform] = None,\n        use_sql_join_synthesis: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = True,\n        streaming: bool = False,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(callback_manager=callback_manager)\n        # validate that the query engines are of the right type\n        if not isinstance(\n            sql_query_tool.query_engine,\n            (BaseSQLTableQueryEngine, NLSQLTableQueryEngine),\n        ):\n            raise ValueError(\n                \"sql_query_tool.query_engine must be an instance of \"\n                \"BaseSQLTableQueryEngine or NLSQLTableQueryEngine\"\n            )\n        self._sql_query_tool = sql_query_tool\n        self._other_query_tool = other_query_tool\n\n        self._llm = llm or Settings.llm\n\n        self._selector = selector or get_selector_from_llm(self._llm, is_multi=False)  # type: ignore\n        assert isinstance(self._selector, (LLMSingleSelector, PydanticSingleSelector))\n\n        self._sql_join_synthesis_prompt = (\n            sql_join_synthesis_prompt or DEFAULT_SQL_JOIN_SYNTHESIS_PROMPT\n        )\n        self._sql_augment_query_transform = (\n            sql_augment_query_transform or SQLAugmentQueryTransform(llm=self._llm)\n        )\n        self._use_sql_join_synthesis = use_sql_join_synthesis\n        self._verbose = verbose\n        self._streaming = streaming\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\n            \"selector\": self._selector,\n            \"sql_augment_query_transform\": self._sql_augment_query_transform,\n        }\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"sql_join_synthesis_prompt\": self._sql_join_synthesis_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"sql_join_synthesis_prompt\" in prompts:\n            self._sql_join_synthesis_prompt = prompts[\"sql_join_synthesis_prompt\"]\n\n    def _query_sql_other(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query SQL database + other query engine in sequence.\"\"\"\n        # first query SQL database\n        sql_response = self._sql_query_tool.query_engine.query(query_bundle)\n        if not self._use_sql_join_synthesis:\n            return sql_response\n\n        sql_query = (\n            sql_response.metadata[\"sql_query\"] if sql_response.metadata else None\n        )\n        if self._verbose:\n            print_text(f\"SQL query: {sql_query}\\n\", color=\"yellow\")\n            print_text(f\"SQL response: {sql_response}\\n\", color=\"yellow\")\n\n        # given SQL db, transform query into new query\n        new_query = self._sql_augment_query_transform(\n            query_bundle.query_str,\n            metadata={\n                \"sql_query\": _format_sql_query(sql_query),\n                \"sql_query_response\": str(sql_response),\n            },\n        )\n\n        if self._verbose:\n            print_text(\n                f\"Transformed query given SQL response: {new_query.query_str}\\n\",\n                color=\"blue\",\n            )\n        logger.info(f\"> Transformed query given SQL response: {new_query.query_str}\")\n        if self._sql_augment_query_transform.check_stop(new_query):\n            return sql_response\n\n        other_response = self._other_query_tool.query_engine.query(new_query)\n        if self._verbose:\n            print_text(f\"query engine response: {other_response}\\n\", color=\"pink\")\n        logger.info(f\"> query engine response: {other_response}\")\n\n        if self._streaming:\n            response_gen = self._llm.stream(\n                self._sql_join_synthesis_prompt,\n                query_str=query_bundle.query_str,\n                sql_query_str=sql_query,\n                sql_response_str=str(sql_response),\n                query_engine_query_str=new_query.query_str,\n                query_engine_response_str=str(other_response),\n            )\n\n            response_metadata = {\n                **(sql_response.metadata or {}),\n                **(other_response.metadata or {}),\n            }\n            source_nodes = other_response.source_nodes\n            return StreamingResponse(\n                response_gen,\n                metadata=response_metadata,\n                source_nodes=source_nodes,\n            )\n        else:\n            response_str = self._llm.predict(\n                self._sql_join_synthesis_prompt,\n                query_str=query_bundle.query_str,\n                sql_query_str=sql_query,\n                sql_response_str=str(sql_response),\n                query_engine_query_str=new_query.query_str,\n                query_engine_response_str=str(other_response),\n            )\n\n            response_metadata = {\n                **(sql_response.metadata or {}),\n                **(other_response.metadata or {}),\n            }\n            source_nodes = other_response.source_nodes\n            return Response(\n                response_str,\n                metadata=response_metadata,\n                source_nodes=source_nodes,\n            )\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query and get response.\"\"\"\n        # TODO: see if this can be consolidated with logic in RouterQueryEngine\n        metadatas = [self._sql_query_tool.metadata, self._other_query_tool.metadata]\n        result = self._selector.select(metadatas, query_bundle)\n        # pick sql query\n        if result.ind == 0:\n            if self._verbose:\n                print_text(f\"Querying SQL database: {result.reason}\\n\", color=\"blue\")\n            logger.info(f\"> Querying SQL database: {result.reason}\")\n            return self._query_sql_other(query_bundle)\n        elif result.ind == 1:\n            if self._verbose:\n                print_text(\n                    f\"Querying other query engine: {result.reason}\\n\", color=\"blue\"\n                )\n            logger.info(f\"> Querying other query engine: {result.reason}\")\n            return self._other_query_tool.query_engine.query(query_bundle)\n        else:\n            raise ValueError(f\"Invalid result.ind: {result.ind}\")\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        # TODO: make async\n        return self._query(query_bundle)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/custom.py",
    "filename": "custom.py",
    "relpath": "query_engine/custom.py",
    "start_line": 1,
    "end_line": 76,
    "length": 76,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompt_modules",
      "query",
      "aquery",
      "custom_query",
      "acustom_query",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "CustomQueryEngine"
    ],
    "document_function_names": [
      "_get_prompt_modules",
      "query",
      "aquery",
      "custom_query",
      "acustom_query",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "CustomQueryEngine"
    ],
    "content": "\"\"\"Custom query engine.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Union\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE, Response\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.schema import QueryBundle, QueryType\n\nSTR_OR_RESPONSE_TYPE = Union[RESPONSE_TYPE, str]\n\n\nclass CustomQueryEngine(BaseModel, BaseQueryEngine):\n    \"\"\"Custom query engine.\n\n    Subclasses can define additional attributes as Pydantic fields.\n    Subclasses must implement the `custom_query` method, which takes a query string\n    and returns either a Response object or a string as output.\n\n    They can optionally implement the `acustom_query` method for async support.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {}\n\n    def query(self, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:\n        with self.callback_manager.as_trace(\"query\"):\n            # if query bundle, just run the query\n            if isinstance(str_or_query_bundle, QueryBundle):\n                query_str = str_or_query_bundle.query_str\n            else:\n                query_str = str_or_query_bundle\n            raw_response = self.custom_query(query_str)\n            return (\n                Response(raw_response)\n                if isinstance(raw_response, str)\n                else raw_response\n            )\n\n    async def aquery(self, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:\n        with self.callback_manager.as_trace(\"query\"):\n            if isinstance(str_or_query_bundle, QueryBundle):\n                query_str = str_or_query_bundle.query_str\n            else:\n                query_str = str_or_query_bundle\n            raw_response = await self.acustom_query(query_str)\n            return (\n                Response(raw_response)\n                if isinstance(raw_response, str)\n                else raw_response\n            )\n\n    @abstractmethod\n    def custom_query(self, query_str: str) -> STR_OR_RESPONSE_TYPE:\n        \"\"\"Run a custom query.\"\"\"\n\n    async def acustom_query(self, query_str: str) -> STR_OR_RESPONSE_TYPE:\n        \"\"\"Run a custom query asynchronously.\"\"\"\n        # by default, just run the synchronous version\n        return self.custom_query(query_str)\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        raise NotImplementedError(\"This query engine does not support _query.\")\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        raise NotImplementedError(\"This query engine does not support _aquery.\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/retry_source_query_engine.py",
    "filename": "retry_source_query_engine.py",
    "relpath": "query_engine/retry_source_query_engine.py",
    "start_line": 1,
    "end_line": 92,
    "length": 92,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "RetrySourceQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "RetrySourceQueryEngine"
    ],
    "content": "import logging\nfrom typing import Optional\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import (\n    AsyncStreamingResponse,\n    RESPONSE_TYPE,\n    Response,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.evaluation import BaseEvaluator\nfrom llama_index.core.indices.list.base import SummaryIndex\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.query_engine.retriever_query_engine import (\n    RetrieverQueryEngine,\n)\nfrom llama_index.core.schema import Document, QueryBundle\nfrom llama_index.core.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass RetrySourceQueryEngine(BaseQueryEngine):\n    \"\"\"Retry with different source nodes.\"\"\"\n\n    def __init__(\n        self,\n        query_engine: RetrieverQueryEngine,\n        evaluator: BaseEvaluator,\n        llm: Optional[LLM] = None,\n        max_retries: int = 3,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        \"\"\"Run a BaseQueryEngine with retries.\"\"\"\n        self._query_engine = query_engine\n        self._evaluator = evaluator\n        self._llm = llm or Settings.llm\n        self.max_retries = max_retries\n        super().__init__(callback_manager=callback_manager or Settings.callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\"query_engine\": self._query_engine, \"evaluator\": self._evaluator}\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        response = self._query_engine._query(query_bundle)\n        assert not isinstance(response, AsyncStreamingResponse)\n        if self.max_retries <= 0:\n            return response\n        typed_response = (\n            response if isinstance(response, Response) else response.get_response()\n        )\n        query_str = query_bundle.query_str\n        eval = self._evaluator.evaluate_response(query_str, typed_response)\n        if eval.passing:\n            logger.debug(\"Evaluation returned True.\")\n            return response\n        else:\n            logger.debug(\"Evaluation returned False.\")\n            # Test source nodes\n            source_evals = [\n                self._evaluator.evaluate(\n                    query=query_str,\n                    response=typed_response.response,\n                    contexts=[source_node.get_content()],\n                )\n                for source_node in typed_response.source_nodes\n            ]\n            orig_nodes = typed_response.source_nodes\n            assert len(source_evals) == len(orig_nodes)\n            new_docs = []\n            for node, eval_result in zip(orig_nodes, source_evals):\n                if eval_result:\n                    new_docs.append(Document(text=node.node.get_content()))\n            if len(new_docs) == 0:\n                raise ValueError(\"No source nodes passed evaluation.\")\n            new_index = SummaryIndex.from_documents(\n                new_docs,\n            )\n            new_retriever_engine = RetrieverQueryEngine(new_index.as_retriever())\n            new_query_engine = RetrySourceQueryEngine(\n                new_retriever_engine,\n                self._evaluator,\n                self._llm,\n                self.max_retries - 1,\n            )\n            return new_query_engine.query(query_bundle)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Not supported.\"\"\"\n        return self._query(query_bundle)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/flare/base.py",
    "filename": "base.py",
    "relpath": "query_engine/flare/base.py",
    "start_line": 1,
    "end_line": 276,
    "length": 276,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "_get_relevant_lookahead_response",
      "_query",
      "_aquery",
      "retrieve",
      "aretrieve"
    ],
    "chunk_class_names": [
      "FLAREInstructQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "_get_relevant_lookahead_response",
      "_query",
      "_aquery",
      "retrieve",
      "aretrieve"
    ],
    "document_class_names": [
      "FLAREInstructQueryEngine"
    ],
    "content": "\"\"\"Query engines based on the FLARE paper.\n\nActive Retrieval Augmented Generation.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, List\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE, Response\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixinType\nfrom llama_index.core.query_engine.flare.answer_inserter import (\n    BaseLookaheadAnswerInserter,\n    LLMLookaheadAnswerInserter,\n)\nfrom llama_index.core.query_engine.flare.output_parser import (\n    IsDoneOutputParser,\n    QueryTaskOutputParser,\n)\n\nfrom llama_index.core.query_engine.retriever_query_engine import RetrieverQueryEngine\nfrom llama_index.core.schema import QueryBundle, NodeWithScore\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utils import print_text\n\n# These prompts are taken from the FLARE repo:\n# https://github.com/jzbjyb/FLARE/blob/main/src/templates.py\n\nDEFAULT_EXAMPLES = \"\"\"\nQuery: But what are the risks during production of nanomaterials?\nAnswer: [Search(What are some nanomaterial production risks?)]\n\nQuery: The colors on the flag of Ghana have the following meanings.\nAnswer: Red is for [Search(What is the meaning of Ghana's flag being red?)], \\\n    green for forests, and gold for mineral wealth.\n\nQuery: What did the author do during his time in college?\nAnswer: The author took classes in [Search(What classes did the author take in \\\n    college?)].\n\n\"\"\"\n\nDEFAULT_FIRST_SKILL = f\"\"\"\\\nSkill 1. Use the Search API to look up relevant information by writing \\\n    \"[Search(query)]\" where \"query\" is the search query you want to look up. \\\n    For example:\n{DEFAULT_EXAMPLES}\n\n\"\"\"\n\nDEFAULT_SECOND_SKILL = \"\"\"\\\nSkill 2. Solve more complex generation tasks by thinking step by step. For example:\n\nQuery: Give a summary of the author's life and career.\nAnswer: The author was born in 1990. Growing up, he [Search(What did the \\\n    author do during his childhood?)].\n\nQuery: Can you write a summary of the Great Gatsby.\nAnswer: The Great Gatsby is a novel written by F. Scott Fitzgerald. It is about \\\n    [Search(What is the Great Gatsby about?)].\n\n\"\"\"\n\nDEFAULT_END = \"\"\"\nNow given the following task, and the stub of an existing answer, generate the \\\nnext portion of the answer. You may use the Search API \\\n\"[Search(query)]\" whenever possible.\nIf the answer is complete and no longer contains any \"[Search(query)]\" tags, write \\\n    \"done\" to finish the task.\nDo not write \"done\" if the answer still contains \"[Search(query)]\" tags.\nDo not make up answers. It is better to generate one \"[Search(query)]\" tag and stop \\\ngeneration\nthan to fill in the answer with made up information with no \"[Search(query)]\" tags\nor multiple \"[Search(query)]\" tags that assume a structure in the answer.\nTry to limit generation to one sentence if possible.\n\n\"\"\"\n\nDEFAULT_INSTRUCT_PROMPT_TMPL = (\n    DEFAULT_FIRST_SKILL\n    + DEFAULT_SECOND_SKILL\n    + DEFAULT_END\n    + (\n        \"\"\"\nQuery: {query_str}\nExisting Answer: {existing_answer}\nAnswer: \"\"\"\n    )\n)\n\nDEFAULT_INSTRUCT_PROMPT = PromptTemplate(DEFAULT_INSTRUCT_PROMPT_TMPL)\n\n\nclass FLAREInstructQueryEngine(BaseQueryEngine):\n    \"\"\"FLARE Instruct query engine.\n\n    This is the version of FLARE that uses retrieval-encouraging instructions.\n\n    NOTE: this is a beta feature. Interfaces might change, and it might not\n    always give correct answers.\n\n    Args:\n        query_engine (BaseQueryEngine): query engine to use\n        llm (Optional[LLM]): LLM model. Defaults to None.\n        instruct_prompt (Optional[PromptTemplate]): instruct prompt. Defaults to None.\n        lookahead_answer_inserter (Optional[BaseLookaheadAnswerInserter]):\n            lookahead answer inserter. Defaults to None.\n        done_output_parser (Optional[IsDoneOutputParser]): done output parser.\n            Defaults to None.\n        query_task_output_parser (Optional[QueryTaskOutputParser]):\n            query task output parser. Defaults to None.\n        max_iterations (int): max iterations. Defaults to 10.\n        max_lookahead_query_tasks (int): max lookahead query tasks. Defaults to 1.\n        callback_manager (Optional[CallbackManager]): callback manager.\n            Defaults to None.\n        verbose (bool): give verbose outputs. Defaults to False.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        llm: Optional[LLM] = None,\n        instruct_prompt: Optional[BasePromptTemplate] = None,\n        lookahead_answer_inserter: Optional[BaseLookaheadAnswerInserter] = None,\n        done_output_parser: Optional[IsDoneOutputParser] = None,\n        query_task_output_parser: Optional[QueryTaskOutputParser] = None,\n        max_iterations: int = 10,\n        max_lookahead_query_tasks: int = 1,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(callback_manager=callback_manager)\n        self._query_engine = query_engine\n        self._llm = llm or Settings.llm\n        self._instruct_prompt = instruct_prompt or DEFAULT_INSTRUCT_PROMPT\n        self._lookahead_answer_inserter = lookahead_answer_inserter or (\n            LLMLookaheadAnswerInserter(llm=self._llm)\n        )\n        self._done_output_parser = done_output_parser or IsDoneOutputParser()\n        self._query_task_output_parser = (\n            query_task_output_parser or QueryTaskOutputParser()\n        )\n        self._max_iterations = max_iterations\n        self._max_lookahead_query_tasks = max_lookahead_query_tasks\n        self._verbose = verbose\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"instruct_prompt\": self._instruct_prompt,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"instruct_prompt\" in prompts:\n            self._instruct_prompt = prompts[\"instruct_prompt\"]\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {\n            \"query_engine\": self._query_engine,\n            \"lookahead_answer_inserter\": self._lookahead_answer_inserter,\n        }\n\n    def _get_relevant_lookahead_response(self, updated_lookahead_resp: str) -> str:\n        \"\"\"Get relevant lookahead response.\"\"\"\n        # if there's remaining query tasks, then truncate the response\n        # until the start position of the first tag\n        # there may be remaining query tasks because the _max_lookahead_query_tasks\n        # is less than the total number of generated [Search(query)] tags\n        remaining_query_tasks = self._query_task_output_parser.parse(\n            updated_lookahead_resp\n        )\n        if len(remaining_query_tasks) == 0:\n            relevant_lookahead_resp = updated_lookahead_resp\n        else:\n            first_task = remaining_query_tasks[0]\n            relevant_lookahead_resp = updated_lookahead_resp[: first_task.start_idx]\n        return relevant_lookahead_resp\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Query and get response.\"\"\"\n        print_text(f\"Query: {query_bundle.query_str}\\n\", color=\"green\")\n        cur_response = \"\"\n        source_nodes = []\n        for iter in range(self._max_iterations):\n            if self._verbose:\n                print_text(f\"Current response: {cur_response}\\n\", color=\"blue\")\n            # generate \"lookahead response\" that contains \"[Search(query)]\" tags\n            # e.g.\n            # The colors on the flag of Ghana have the following meanings. Red is\n            # for [Search(Ghana flag meaning)],...\n            lookahead_resp = self._llm.predict(\n                self._instruct_prompt,\n                query_str=query_bundle.query_str,\n                existing_answer=cur_response,\n            )\n            lookahead_resp = lookahead_resp.strip()\n            if self._verbose:\n                print_text(f\"Lookahead response: {lookahead_resp}\\n\", color=\"pink\")\n\n            is_done, fmt_lookahead = self._done_output_parser.parse(lookahead_resp)\n            if is_done:\n                cur_response = cur_response.strip() + \" \" + fmt_lookahead.strip()\n                break\n\n            # parse lookahead response into query tasks\n            query_tasks = self._query_task_output_parser.parse(lookahead_resp)\n\n            # get answers for each query task\n            query_tasks = query_tasks[: self._max_lookahead_query_tasks]\n            query_answers = []\n            for _, query_task in enumerate(query_tasks):\n                answer_obj = self._query_engine.query(query_task.query_str)\n                if not isinstance(answer_obj, Response):\n                    raise ValueError(\n                        f\"Expected Response object, got {type(answer_obj)} instead.\"\n                    )\n                query_answer = str(answer_obj)\n                query_answers.append(query_answer)\n                source_nodes.extend(answer_obj.source_nodes)\n\n            # fill in the lookahead response template with the query answers\n            # from the query engine\n            updated_lookahead_resp = self._lookahead_answer_inserter.insert(\n                lookahead_resp, query_tasks, query_answers, prev_response=cur_response\n            )\n\n            # get \"relevant\" lookahead response by truncating the updated\n            # lookahead response until the start position of the first tag\n            # also remove the prefix from the lookahead response, so that\n            # we can concatenate it with the existing response\n            relevant_lookahead_resp_wo_prefix = self._get_relevant_lookahead_response(\n                updated_lookahead_resp\n            )\n\n            if self._verbose:\n                print_text(\n                    \"Updated lookahead response: \"\n                    + f\"{relevant_lookahead_resp_wo_prefix}\\n\",\n                    color=\"pink\",\n                )\n\n            # append the relevant lookahead response to the final response\n            cur_response = (\n                cur_response.strip() + \" \" + relevant_lookahead_resp_wo_prefix.strip()\n            )\n\n        # NOTE: at the moment, does not support streaming\n        return Response(response=cur_response, source_nodes=source_nodes)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        return self._query(query_bundle)\n\n    def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        # if the query engine is a retriever, then use the retrieve method\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            return self._query_engine.retrieve(query_bundle)\n        else:\n            raise NotImplementedError(\n                \"This query engine does not support retrieve, use query directly\"\n            )\n\n    async def aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        # if the query engine is a retriever, then use the retrieve method\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            return await self._query_engine.aretrieve(query_bundle)\n        else:\n            raise NotImplementedError(\n                \"This query engine does not support retrieve, use query directly\"\n            )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/flare/output_parser.py",
    "filename": "output_parser.py",
    "relpath": "query_engine/flare/output_parser.py",
    "start_line": 1,
    "end_line": 66,
    "length": 66,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "default_parse_is_done_fn",
      "default_format_done_answer",
      "__init__",
      "parse",
      "format",
      "parse",
      "format"
    ],
    "chunk_class_names": [
      "IsDoneOutputParser",
      "QueryTaskOutputParser"
    ],
    "document_function_names": [
      "default_parse_is_done_fn",
      "default_format_done_answer",
      "__init__",
      "parse",
      "format",
      "parse",
      "format"
    ],
    "document_class_names": [
      "IsDoneOutputParser",
      "QueryTaskOutputParser"
    ],
    "content": "\"\"\"FLARE output parsers.\"\"\"\n\nfrom typing import Any, Callable, Optional\n\nfrom llama_index.core.query_engine.flare.schema import QueryTask\nfrom llama_index.core.types import BaseOutputParser\n\n\ndef default_parse_is_done_fn(response: str) -> bool:\n    \"\"\"Default parse is done function.\"\"\"\n    return \"done\" in response.lower()\n\n\ndef default_format_done_answer(response: str) -> str:\n    \"\"\"Default format done answer.\"\"\"\n    return response.replace(\"done\", \"\").strip()\n\n\nclass IsDoneOutputParser(BaseOutputParser):\n    \"\"\"Is done output parser.\"\"\"\n\n    def __init__(\n        self,\n        is_done_fn: Optional[Callable[[str], bool]] = None,\n        fmt_answer_fn: Optional[Callable[[str], str]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._is_done_fn = is_done_fn or default_parse_is_done_fn\n        self._fmt_answer_fn = fmt_answer_fn or default_format_done_answer\n\n    def parse(self, output: str) -> Any:\n        \"\"\"Parse output.\"\"\"\n        is_done = default_parse_is_done_fn(output)\n        if is_done:\n            return True, self._fmt_answer_fn(output)\n        else:\n            return False, output\n\n    def format(self, output: str) -> str:\n        \"\"\"Format a query with structured output formatting instructions.\"\"\"\n        raise NotImplementedError\n\n\nclass QueryTaskOutputParser(BaseOutputParser):\n    \"\"\"QueryTask output parser.\n\n    By default, parses output that contains \"[Search(query)]\" tags.\n\n    \"\"\"\n\n    def parse(self, output: str) -> Any:\n        \"\"\"Parse output.\"\"\"\n        query_tasks = []\n        for idx, char in enumerate(output):\n            if char == \"[\":\n                start_idx = idx\n            elif char == \"]\":\n                end_idx = idx\n                raw_query_str = output[start_idx + 1 : end_idx]\n                query_str = raw_query_str.split(\"(\")[1].split(\")\")[0]\n                query_tasks.append(QueryTask(query_str, start_idx, end_idx))\n        return query_tasks\n\n    def format(self, output: str) -> str:\n        \"\"\"Format a query with structured output formatting instructions.\"\"\"\n        raise NotImplementedError"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/flare/schema.py",
    "filename": "schema.py",
    "relpath": "query_engine/flare/schema.py",
    "start_line": 1,
    "end_line": 12,
    "length": 12,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "class"
    ],
    "document_function_names": [],
    "document_class_names": [
      "class"
    ],
    "content": "\"\"\"FLARE schema.\"\"\"\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass QueryTask:\n    \"\"\"Query task.\"\"\"\n\n    query_str: str\n    start_idx: int\n    end_idx: int"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/flare/answer_inserter.py",
    "filename": "answer_inserter.py",
    "relpath": "query_engine/flare/answer_inserter.py",
    "start_line": 1,
    "end_line": 213,
    "length": 213,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompt_modules",
      "insert",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "insert",
      "_get_prompts",
      "_update_prompts",
      "insert"
    ],
    "chunk_class_names": [
      "BaseLookaheadAnswerInserter",
      "LLMLookaheadAnswerInserter",
      "DirectLookaheadAnswerInserter"
    ],
    "document_function_names": [
      "_get_prompt_modules",
      "insert",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "insert",
      "_get_prompts",
      "_update_prompts",
      "insert"
    ],
    "document_class_names": [
      "BaseLookaheadAnswerInserter",
      "LLMLookaheadAnswerInserter",
      "DirectLookaheadAnswerInserter"
    ],
    "content": "\"\"\"Answer inserter.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import (\n    PromptDictType,\n    PromptMixin,\n    PromptMixinType,\n)\nfrom llama_index.core.query_engine.flare.schema import QueryTask\nfrom llama_index.core.settings import Settings\n\n\nclass BaseLookaheadAnswerInserter(PromptMixin):\n    \"\"\"Lookahead answer inserter.\n\n    These are responsible for insert answers into a lookahead answer template.\n\n    E.g.\n    lookahead answer: Red is for [Search(What is the meaning of Ghana's\n        flag being red?)], green for forests, and gold for mineral wealth.\n    query: What is the meaning of Ghana's flag being red?\n    query answer: \"the blood of those who died in the country's struggle\n        for independence\"\n    final answer: Red is for the blood of those who died in the country's\n        struggle for independence, green for forests, and gold for mineral wealth.\n\n    \"\"\"\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {}\n\n    @abstractmethod\n    def insert(\n        self,\n        response: str,\n        query_tasks: List[QueryTask],\n        answers: List[str],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Insert answers into response.\"\"\"\n\n\nDEFAULT_ANSWER_INSERT_PROMPT_TMPL = \"\"\"\nAn existing 'lookahead response' is given below. The lookahead response\ncontains `[Search(query)]` tags. Some queries have been executed and the\nresponse retrieved. The queries and answers are also given below.\nAlso the previous response (the response before the lookahead response)\nis given below.\nGiven the lookahead template, previous response, and also queries and answers,\nplease 'fill in' the lookahead template with the appropriate answers.\n\nNOTE: Please make sure that the final response grammatically follows\nthe previous response + lookahead template. For example, if the previous\nresponse is \"New York City has a population of \" and the lookahead\ntemplate is \"[Search(What is the population of New York City?)]\", then\nthe final response should be \"8.4 million\".\n\nNOTE: the lookahead template may not be a complete sentence and may\ncontain trailing/leading commas, etc. Please preserve the original\nformatting of the lookahead template if possible.\n\nNOTE:\n\nNOTE: the exception to the above rule is if the answer to a query\nis equivalent to \"I don't know\" or \"I don't have an answer\". In this case,\nmodify the lookahead template to indicate that the answer is not known.\n\nNOTE: the lookahead template may contain multiple `[Search(query)]` tags\n    and only a subset of these queries have been executed.\n    Do not replace the `[Search(query)]` tags that have not been executed.\n\nPrevious Response:\n\n\nLookahead Template:\nRed is for [Search(What is the meaning of Ghana's \\\n    flag being red?)], green for forests, and gold for mineral wealth.\n\nQuery-Answer Pairs:\nQuery: What is the meaning of Ghana's flag being red?\nAnswer: The red represents the blood of those who died in the country's struggle \\\n    for independence\n\nFilled in Answers:\nRed is for the blood of those who died in the country's struggle for independence, \\\n    green for forests, and gold for mineral wealth.\n\nPrevious Response:\nOne of the largest cities in the world\n\nLookahead Template:\n, the city contains a population of [Search(What is the population \\\n    of New York City?)]\n\nQuery-Answer Pairs:\nQuery: What is the population of New York City?\nAnswer: The population of New York City is 8.4 million\n\nSynthesized Response:\n, the city contains a population of 8.4 million\n\nPrevious Response:\nthe city contains a population of\n\nLookahead Template:\n[Search(What is the population of New York City?)]\n\nQuery-Answer Pairs:\nQuery: What is the population of New York City?\nAnswer: The population of New York City is 8.4 million\n\nSynthesized Response:\n8.4 million\n\nPrevious Response:\n{prev_response}\n\nLookahead Template:\n{lookahead_response}\n\nQuery-Answer Pairs:\n{query_answer_pairs}\n\nSynthesized Response:\n\"\"\"\nDEFAULT_ANSWER_INSERT_PROMPT = PromptTemplate(DEFAULT_ANSWER_INSERT_PROMPT_TMPL)\n\n\nclass LLMLookaheadAnswerInserter(BaseLookaheadAnswerInserter):\n    \"\"\"LLM Lookahead answer inserter.\n\n    Takes in a lookahead response and a list of query tasks, and the\n        lookahead answers, and inserts the answers into the lookahead response.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        answer_insert_prompt: Optional[BasePromptTemplate] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n        self._answer_insert_prompt = (\n            answer_insert_prompt or DEFAULT_ANSWER_INSERT_PROMPT\n        )\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"answer_insert_prompt\": self._answer_insert_prompt,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"answer_insert_prompt\" in prompts:\n            self._answer_insert_prompt = prompts[\"answer_insert_prompt\"]\n\n    def insert(\n        self,\n        response: str,\n        query_tasks: List[QueryTask],\n        answers: List[str],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Insert answers into response.\"\"\"\n        prev_response = prev_response or \"\"\n\n        query_answer_pairs = \"\"\n        for query_task, answer in zip(query_tasks, answers):\n            query_answer_pairs += f\"Query: {query_task.query_str}\\nAnswer: {answer}\\n\"\n\n        return self._llm.predict(\n            self._answer_insert_prompt,\n            lookahead_response=response,\n            query_answer_pairs=query_answer_pairs,\n            prev_response=prev_response,\n        )\n\n\nclass DirectLookaheadAnswerInserter(BaseLookaheadAnswerInserter):\n    \"\"\"Direct lookahead answer inserter.\n\n    Simple inserter module that directly inserts answers into\n        the [Search(query)] tags in the lookahead response.\n    \"\"\"\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def insert(\n        self,\n        response: str,\n        query_tasks: List[QueryTask],\n        answers: List[str],\n        prev_response: Optional[str] = None,\n    ) -> str:\n        \"\"\"Insert answers into response.\"\"\"\n        for query_task, answer in zip(query_tasks, answers):\n            response = (\n                response[: query_task.start_idx]\n                + answer\n                + response[query_task.end_idx + 1 :]\n            )\n        return response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/flare/__init__.py",
    "filename": "__init__.py",
    "relpath": "query_engine/flare/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/pandas/output_parser.py",
    "filename": "output_parser.py",
    "relpath": "query_engine/pandas/output_parser.py",
    "start_line": 1,
    "end_line": 23,
    "length": 23,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__"
    ],
    "chunk_class_names": [
      "has",
      "PandasInstructionParser",
      "has"
    ],
    "document_function_names": [
      "__init__"
    ],
    "document_class_names": [
      "has",
      "PandasInstructionParser",
      "has"
    ],
    "content": "\"\"\"Pandas output parser.\n\nDEPRECATED: This class has been moved to `llama-index-experimental`.\n\"\"\"\n\nfrom typing import Any\n\n\nclass PandasInstructionParser:\n    \"\"\"Pandas instruction parser.\n\n    DEPRECATED: This class has been moved to `llama-index-experimental`.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        raise DeprecationWarning(\n            \"PandasInstructionParser has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine.pandas import PandasInstructionParser`\\n\"\n            \"Note that the PandasInstructionParser allows for arbitrary code execution, \\n\"\n            \"and should be used in a secure environment.\"\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/pandas/__init__.py",
    "filename": "__init__.py",
    "relpath": "query_engine/pandas/__init__.py",
    "start_line": 1,
    "end_line": 10,
    "length": 10,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file.\"\"\"\n\nfrom llama_index.core.query_engine.pandas.output_parser import (\n    PandasInstructionParser,\n)\nfrom llama_index.core.query_engine.pandas.pandas_query_engine import (\n    PandasQueryEngine,\n)\n\n__all__ = [\"PandasInstructionParser\", \"PandasQueryEngine\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/pandas/pandas_query_engine.py",
    "filename": "pandas_query_engine.py",
    "relpath": "query_engine/pandas/pandas_query_engine.py",
    "start_line": 1,
    "end_line": 33,
    "length": 33,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__"
    ],
    "chunk_class_names": [
      "PandasQueryEngine"
    ],
    "document_function_names": [
      "__init__"
    ],
    "document_class_names": [
      "PandasQueryEngine"
    ],
    "content": "\"\"\"Default query for PandasIndex.\n\nWARNING: This tool provides the LLM access to the `eval` function.\nArbitrary code execution is possible on the machine running this tool.\nThis tool is not recommended to be used in a production setting, and would\nrequire heavy sandboxing or virtual machines.\n\nDEPRECATED: Use `PandasQueryEngine` from `llama-index-experimental` instead.\n\n\"\"\"\n\nfrom typing import Any\n\n\nclass PandasQueryEngine:\n    \"\"\"Pandas query engine.\n\n    DEPRECATED: Use `PandasQueryEngine` from `llama-index-experimental` instead.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        raise DeprecationWarning(\n            \"PandasQueryEngine has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine import PandasQueryEngine`\\n\"\n            \"Note that the PandasQueryEngine allows for arbitrary code execution, \\n\"\n            \"and should be used in a secure environment.\"\n        )\n\n\n# legacy\nNLPandasQueryEngine = PandasQueryEngine\nGPTNLPandasQueryEngine = PandasQueryEngine"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/jsonalyze/__init__.py",
    "filename": "__init__.py",
    "relpath": "query_engine/jsonalyze/__init__.py",
    "start_line": 1,
    "end_line": 7,
    "length": 7,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file.\"\"\"\n\nfrom llama_index.core.query_engine.jsonalyze.jsonalyze_query_engine import (\n    JSONalyzeQueryEngine,\n)\n\n__all__ = [\"JSONalyzeQueryEngine\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_engine/jsonalyze/jsonalyze_query_engine.py",
    "filename": "jsonalyze_query_engine.py",
    "relpath": "query_engine/jsonalyze/jsonalyze_query_engine.py",
    "start_line": 1,
    "end_line": 28,
    "length": 28,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__"
    ],
    "chunk_class_names": [
      "JSONalyzeQueryEngine"
    ],
    "document_function_names": [
      "__init__"
    ],
    "document_class_names": [
      "JSONalyzeQueryEngine"
    ],
    "content": "\"\"\"JSONalyze Query Engine.\n\nWARNING: This tool executes a SQL prompt generated by the LLM with SQL Lite and\nmay lead to arbitrary file creation on the machine running this tool.\nThis tool is not recommended to be used in a production setting, and would\nrequire heavy sandboxing or virtual machines.\n\nDEPRECATED: Use `JSONalyzeQueryEngine` from `llama-index-experimental` instead.\n\n\"\"\"\n\nfrom typing import Any\n\n\nclass JSONalyzeQueryEngine:\n    \"\"\"JSONalyze query engine.\n\n    DEPRECATED: Use `JSONalyzeQueryEngine` from `llama-index-experimental` instead.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        raise DeprecationWarning(\n            \"JSONalyzeQueryEngine has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine import JSONalyzeQueryEngine`\\n\"\n            \"Note that the JSONalyzeQueryEngine allows for arbitrary file creation, \\n\"\n            \"and should be used in a secure environment.\"\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/json.py",
    "filename": "json.py",
    "relpath": "readers/json.py",
    "start_line": 1,
    "end_line": 142,
    "length": 142,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_depth_first_yield",
      "__init__",
      "load_data"
    ],
    "chunk_class_names": [
      "JSONReader"
    ],
    "document_function_names": [
      "_depth_first_yield",
      "__init__",
      "load_data"
    ],
    "document_class_names": [
      "JSONReader"
    ],
    "content": "\"\"\"JSON Reader.\"\"\"\n\nimport json\nimport re\nfrom typing import Any, Dict, Generator, List, Optional\n\nfrom llama_index.core.readers.base import BaseReader\nfrom llama_index.core.schema import Document\n\n\ndef _depth_first_yield(\n    json_data: Any,\n    levels_back: int,\n    collapse_length: Optional[int],\n    path: List[str],\n    ensure_ascii: bool = False,\n) -> Generator[str, None, None]:\n    \"\"\"Do depth first yield of all of the leaf nodes of a JSON.\n\n    Combines keys in the JSON tree using spaces.\n\n    If levels_back is set to 0, prints all levels.\n    If collapse_length is not None and the json_data is <= that number\n      of characters, then we collapse it into one line.\n\n    \"\"\"\n    if isinstance(json_data, (dict, list)):\n        # only try to collapse if we're not at a leaf node\n        json_str = json.dumps(json_data, ensure_ascii=ensure_ascii)\n        if collapse_length is not None and len(json_str) <= collapse_length:\n            new_path = path[-levels_back:]\n            new_path.append(json_str)\n            yield \" \".join(new_path)\n            return\n        elif isinstance(json_data, dict):\n            for key, value in json_data.items():\n                new_path = path[:]\n                new_path.append(key)\n                yield from _depth_first_yield(\n                    value, levels_back, collapse_length, new_path\n                )\n        elif isinstance(json_data, list):\n            for _, value in enumerate(json_data):\n                yield from _depth_first_yield(value, levels_back, collapse_length, path)\n    else:\n        new_path = path[-levels_back:]\n        new_path.append(str(json_data))\n        yield \" \".join(new_path)\n\n\nclass JSONReader(BaseReader):\n    \"\"\"JSON reader.\n\n    Reads JSON documents with options to help suss out relationships between nodes.\n\n    Args:\n        levels_back (int): the number of levels to go back in the JSON tree, 0\n          if you want all levels. If levels_back is None, then we just format the\n          JSON and make each line an embedding\n\n        collapse_length (int): the maximum number of characters a JSON fragment\n          would be collapsed in the output (levels_back needs to be not None)\n          ex: if collapse_length = 10, and\n          input is {a: [1, 2, 3], b: {\"hello\": \"world\", \"foo\": \"bar\"}}\n          then a would be collapsed into one line, while b would not.\n          Recommend starting around 100 and then adjusting from there.\n\n        is_jsonl (Optional[bool]): If True, indicates that the file is in JSONL format.\n        Defaults to False.\n\n        clean_json (Optional[bool]): If True, lines containing only JSON structure are removed.\n        This removes lines that are not as useful. If False, no lines are removed and the document maintains a valid JSON object structure.\n        If levels_back is set the json is not cleaned and this option is ignored.\n        Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        levels_back: Optional[int] = None,\n        collapse_length: Optional[int] = None,\n        ensure_ascii: bool = False,\n        is_jsonl: Optional[bool] = False,\n        clean_json: Optional[bool] = True,\n    ) -> None:\n        \"\"\"Initialize with arguments.\"\"\"\n        super().__init__()\n        self.levels_back = levels_back\n        self.collapse_length = collapse_length\n        self.ensure_ascii = ensure_ascii\n        self.is_jsonl = is_jsonl\n        self.clean_json = clean_json\n\n    def load_data(\n        self, input_file: str, extra_info: Optional[Dict] = {}\n    ) -> List[Document]:\n        \"\"\"Load data from the input file.\"\"\"\n        with open(input_file, encoding=\"utf-8\") as f:\n            load_data = []\n            if self.is_jsonl:\n                for line in f:\n                    load_data.append(json.loads(line.strip()))\n            else:\n                load_data = [json.load(f)]\n\n            documents = []\n            for data in load_data:\n                if self.levels_back is None and self.clean_json is True:\n                    # If levels_back isn't set and clean json is set,\n                    # remove lines containing only formatting, we just format and make each\n                    # line an embedding\n                    json_output = json.dumps(\n                        data, indent=0, ensure_ascii=self.ensure_ascii\n                    )\n                    lines = json_output.split(\"\\n\")\n                    useful_lines = [\n                        line for line in lines if not re.match(r\"^[{}\\[\\],]*$\", line)\n                    ]\n                    documents.append(\n                        Document(text=\"\\n\".join(useful_lines), metadata=extra_info)\n                    )\n\n                elif self.levels_back is None and self.clean_json is False:\n                    # If levels_back isn't set  and clean json is False, create documents without cleaning\n                    json_output = json.dumps(data, ensure_ascii=self.ensure_ascii)\n                    documents.append(Document(text=json_output, metadata=extra_info))\n\n                elif self.levels_back is not None:\n                    # If levels_back is set, we make the embeddings contain the labels\n                    # from further up the JSON tree\n                    lines = [\n                        *_depth_first_yield(\n                            data,\n                            self.levels_back,\n                            self.collapse_length,\n                            [],\n                            self.ensure_ascii,\n                        )\n                    ]\n                    documents.append(\n                        Document(text=\"\\n\".join(lines), metadata=extra_info)\n                    )\n            return documents"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/base.py",
    "filename": "base.py",
    "relpath": "readers/base.py",
    "start_line": 1,
    "end_line": 235,
    "length": 235,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "lazy_load_data",
      "alazy_load_data",
      "load_data",
      "aload_data",
      "load_langchain_documents",
      "list_resources",
      "alist_resources",
      "get_permission_info",
      "aget_permission_info",
      "get_resource_info",
      "aget_resource_info",
      "list_resources_with_info",
      "alist_resources_with_info",
      "load_resource",
      "aload_resource",
      "load_resources",
      "aload_resources",
      "class_name",
      "to_dict",
      "read"
    ],
    "chunk_class_names": [
      "BaseReader",
      "BasePydanticReader",
      "ResourcesReaderMixin",
      "ReaderConfig",
      "to"
    ],
    "document_function_names": [
      "lazy_load_data",
      "alazy_load_data",
      "load_data",
      "aload_data",
      "load_langchain_documents",
      "list_resources",
      "alist_resources",
      "get_permission_info",
      "aget_permission_info",
      "get_resource_info",
      "aget_resource_info",
      "list_resources_with_info",
      "alist_resources_with_info",
      "load_resource",
      "aload_resource",
      "load_resources",
      "aload_resources",
      "class_name",
      "to_dict",
      "read"
    ],
    "document_class_names": [
      "BaseReader",
      "BasePydanticReader",
      "ResourcesReaderMixin",
      "ReaderConfig",
      "to"
    ],
    "content": "\"\"\"Base reader class.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Iterable,\n    List,\n)\n\nif TYPE_CHECKING:  # pragma: no cover\n    from llama_index.core.bridge.langchain import Document as LCDocument  # type: ignore\nfrom llama_index.core.bridge.pydantic import ConfigDict, Field\nfrom llama_index.core.schema import BaseComponent, Document\n\n\nclass BaseReader(ABC):  # pragma: no cover\n    \"\"\"Utilities for loading data from a directory.\"\"\"\n\n    def lazy_load_data(self, *args: Any, **load_kwargs: Any) -> Iterable[Document]:\n        \"\"\"Load data from the input directory lazily.\"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not provide lazy_load_data method currently\"\n        )\n\n    async def alazy_load_data(\n        self, *args: Any, **load_kwargs: Any\n    ) -> Iterable[Document]:\n        \"\"\"Load data from the input directory lazily.\"\"\"\n        # Fake async - just calls the sync method. Override in subclasses for real async implementations.\n        return self.lazy_load_data(*args, **load_kwargs)\n\n    def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n        return list(self.lazy_load_data(*args, **load_kwargs))\n\n    async def aload_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:\n        \"\"\"Load data from the input directory.\"\"\"\n        return self.load_data(*args, **load_kwargs)\n\n    def load_langchain_documents(self, **load_kwargs: Any) -> List[\"LCDocument\"]:\n        \"\"\"Load data in LangChain document format.\"\"\"\n        docs = self.load_data(**load_kwargs)\n        return [d.to_langchain_format() for d in docs]\n\n\nclass BasePydanticReader(BaseReader, BaseComponent):\n    \"\"\"Serialiable Data Loader with Pydantic.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    is_remote: bool = Field(\n        default=False,\n        description=\"Whether the data is loaded from a remote API or a local file.\",\n    )\n\n\nclass ResourcesReaderMixin(ABC):  # pragma: no cover\n    \"\"\"\n    Mixin for readers that provide access to different types of resources.\n\n    Resources refer to specific data entities that can be accessed by the reader.\n    Examples of resources include files for a filesystem reader, channel IDs for a Slack reader, or pages for a Notion reader.\n    \"\"\"\n\n    @abstractmethod\n    def list_resources(self, *args: Any, **kwargs: Any) -> List[str]:\n        \"\"\"\n        List of identifiers for the specific type of resources available in the reader.\n\n        Returns:\n            List[str]: List of identifiers for the specific type of resources available in the reader.\n        \"\"\"\n\n    async def alist_resources(self, *args: Any, **kwargs: Any) -> List[str]:\n        \"\"\"\n        List of identifiers for the specific type of resources available in the reader asynchronously.\n\n        Returns:\n            List[str]: A list of resources based on the reader type, such as files for a filesystem reader,\n            channel IDs for a Slack reader, or pages for a Notion reader.\n        \"\"\"\n        return self.list_resources(*args, **kwargs)\n\n    def get_permission_info(self, resource_id: str, *args: Any, **kwargs: Any) -> Dict:\n        \"\"\"\n        Get a dictionary of information about the permissions of a specific resource.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not provide get_permission_info method currently\"\n        )\n\n    async def aget_permission_info(\n        self, resource_id: str, *args: Any, **kwargs: Any\n    ) -> Dict:\n        \"\"\"\n        Get a dictionary of information about the permissions of a specific resource asynchronously.\n        \"\"\"\n        return self.get_permission_info(resource_id, *args, **kwargs)\n\n    @abstractmethod\n    def get_resource_info(self, resource_id: str, *args: Any, **kwargs: Any) -> Dict:\n        \"\"\"\n        Get a dictionary of information about a specific resource.\n\n        Args:\n            resource (str): The resource identifier.\n\n        Returns:\n            Dict: A dictionary of information about the resource.\n        \"\"\"\n\n    async def aget_resource_info(\n        self, resource_id: str, *args: Any, **kwargs: Any\n    ) -> Dict:\n        \"\"\"\n        Get a dictionary of information about a specific resource asynchronously.\n\n        Args:\n            resource (str): The resource identifier.\n\n        Returns:\n            Dict: A dictionary of information about the resource.\n        \"\"\"\n        return self.get_resource_info(resource_id, *args, **kwargs)\n\n    def list_resources_with_info(self, *args: Any, **kwargs: Any) -> Dict[str, Dict]:\n        \"\"\"\n        Get a dictionary of information about all resources.\n\n        Returns:\n            Dict[str, Dict]: A dictionary of information about all resources.\n        \"\"\"\n        return {\n            resource: self.get_resource_info(resource, *args, **kwargs)\n            for resource in self.list_resources(*args, **kwargs)\n        }\n\n    async def alist_resources_with_info(\n        self, *args: Any, **kwargs: Any\n    ) -> Dict[str, Dict]:\n        \"\"\"\n        Get a dictionary of information about all resources asynchronously.\n\n        Returns:\n            Dict[str, Dict]: A dictionary of information about all resources.\n        \"\"\"\n        return {\n            resource: await self.aget_resource_info(resource, *args, **kwargs)\n            for resource in await self.alist_resources(*args, **kwargs)\n        }\n\n    @abstractmethod\n    def load_resource(\n        self, resource_id: str, *args: Any, **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"\n        Load data from a specific resource.\n\n        Args:\n            resource (str): The resource identifier.\n\n        Returns:\n            List[Document]: A list of documents loaded from the resource.\n        \"\"\"\n\n    async def aload_resource(\n        self, resource_id: str, *args: Any, **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Read file from filesystem and return documents asynchronously.\"\"\"\n        return self.load_resource(resource_id, *args, **kwargs)\n\n    def load_resources(\n        self, resource_ids: List[str], *args: Any, **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"\n        Similar to load_data, but only for specific resources.\n\n        Args:\n            resource_ids (List[str]): List of resource identifiers.\n\n        Returns:\n            List[Document]: A list of documents loaded from the resources.\n        \"\"\"\n        return [\n            doc\n            for resource in resource_ids\n            for doc in self.load_resource(resource, *args, **kwargs)\n        ]\n\n    async def aload_resources(\n        self, resource_ids: List[str], *args: Any, **kwargs: Any\n    ) -> Dict[str, List[Document]]:\n        \"\"\"\n        Similar ato load_data, but only for specific resources.\n\n        Args:\n            resource_ids (List[str]): List of resource identifiers.\n\n        Returns:\n            Dict[str, List[Document]]: A dictionary of documents loaded from the resources.\n        \"\"\"\n        return {\n            resource: await self.aload_resource(resource, *args, **kwargs)\n            for resource in resource_ids\n        }\n\n\nclass ReaderConfig(BaseComponent):  # pragma: no cover\n    \"\"\"Represents a reader and it's input arguments.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    reader: BasePydanticReader = Field(..., description=\"Reader to use.\")\n    reader_args: List[Any] = Field(default_factory=list, description=\"Reader args.\")\n    reader_kwargs: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Reader kwargs.\"\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get the name identifier of the class.\"\"\"\n        return \"ReaderConfig\"\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Convert the class to a dictionary.\"\"\"\n        return {\n            \"loader\": self.reader.to_dict(**kwargs),\n            \"reader_args\": self.reader_args,\n            \"reader_kwargs\": self.reader_kwargs,\n            \"class_name\": self.class_name(),\n        }\n\n    def read(self) -> List[Document]:\n        \"\"\"Call the loader with the given arguments.\"\"\"\n        return self.reader.load_data(*self.reader_args, **self.reader_kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/loading.py",
    "filename": "loading.py",
    "relpath": "readers/loading.py",
    "start_line": 1,
    "end_line": 26,
    "length": 26,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_reader"
    ],
    "chunk_class_names": [
      "name"
    ],
    "document_function_names": [
      "load_reader"
    ],
    "document_class_names": [
      "name"
    ],
    "content": "from typing import Any, Dict, Type\n\nfrom llama_index.core.readers.base import BasePydanticReader\nfrom llama_index.core.readers.string_iterable import StringIterableReader\n\nALL_READERS: Dict[str, Type[BasePydanticReader]] = {\n    StringIterableReader.class_name(): StringIterableReader,\n}\n\n\ndef load_reader(data: Dict[str, Any]) -> BasePydanticReader:\n    if isinstance(data, BasePydanticReader):\n        return data\n\n    class_name = data.get(\"class_name\")\n\n    if class_name is None:\n        raise ValueError(\"Must specify `class_name` in reader data.\")\n\n    if class_name not in ALL_READERS:\n        raise ValueError(f\"Reader class name {class_name} not found.\")\n\n    # remove static attribute\n    data.pop(\"is_remote\", None)\n\n    return ALL_READERS[class_name].from_dict(data)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/string_iterable.py",
    "filename": "string_iterable.py",
    "relpath": "readers/string_iterable.py",
    "start_line": 1,
    "end_line": 40,
    "length": 40,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "load_data"
    ],
    "chunk_class_names": [
      "StringIterableReader"
    ],
    "document_function_names": [
      "class_name",
      "load_data"
    ],
    "document_class_names": [
      "StringIterableReader"
    ],
    "content": "\"\"\"Simple reader that turns an iterable of strings into a list of Documents.\"\"\"\nfrom typing import List\n\nfrom llama_index.core.readers.base import BasePydanticReader\nfrom llama_index.core.schema import Document\n\n\nclass StringIterableReader(BasePydanticReader):\n    \"\"\"String Iterable Reader.\n\n    Gets a list of documents, given an iterable (e.g. list) of strings.\n\n    Example:\n        .. code-block:: python\n\n            from llama_index.core.legacy import StringIterableReader, TreeIndex\n\n            documents = StringIterableReader().load_data(\n                texts=[\"I went to the store\", \"I bought an apple\"]\n            )\n            index = TreeIndex.from_documents(documents)\n            query_engine = index.as_query_engine()\n            query_engine.query(\"what did I buy?\")\n\n            # response should be something like \"You bought an apple.\"\n    \"\"\"\n\n    is_remote: bool = False\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"StringIterableReader\"\n\n    def load_data(self, texts: List[str]) -> List[Document]:\n        \"\"\"Load the data.\"\"\"\n        results = []\n        for text in texts:\n            results.append(Document(text=text))\n\n        return results"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/__init__.py",
    "filename": "__init__.py",
    "relpath": "readers/__init__.py",
    "start_line": 1,
    "end_line": 32,
    "length": 32,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"\nData Connectors for LlamaIndex.\n\nThis module contains the data connectors for LlamaIndex. Each connector inherits\nfrom a `BaseReader` class, connects to a data source, and loads Document objects\nfrom that data source.\n\nYou may also choose to construct Document objects manually, for instance\nin our `Insert How-To Guide <../how_to/insert.html>`_. See below for the API\ndefinition of a Document - the bare minimum is a `text` property.\n\n\"\"\"\n\nfrom llama_index.core.readers.base import ReaderConfig\nfrom llama_index.core.readers.download import download_loader\n\n# readers\nfrom llama_index.core.readers.file.base import (\n    SimpleDirectoryReader,\n    FileSystemReaderMixin,\n)\nfrom llama_index.core.readers.string_iterable import StringIterableReader\nfrom llama_index.core.schema import Document\n\n__all__ = [\n    \"SimpleDirectoryReader\",\n    \"FileSystemReaderMixin\",\n    \"ReaderConfig\",\n    \"Document\",\n    \"StringIterableReader\",\n    \"download_loader\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/download.py",
    "filename": "download.py",
    "relpath": "readers/download.py",
    "start_line": 1,
    "end_line": 77,
    "length": 77,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "download_loader"
    ],
    "chunk_class_names": [
      "you",
      "in",
      "of"
    ],
    "document_function_names": [
      "download_loader"
    ],
    "document_class_names": [
      "you",
      "in",
      "of"
    ],
    "content": "\"\"\"Download loader from Llama Hub.\n\nNOTE: using `download_loader` is now deprecated.\nPlease do `pip install llama-index-reader-<reader_name>` instead.\n\n\"\"\"\n\nimport json\nimport os\nfrom typing import Optional, Type\n\nfrom deprecated import deprecated\n\nfrom llama_index.core.download.integration import download_integration\nfrom llama_index.core.readers.base import BaseReader\n\n\n@deprecated(\n    \"`download_loader()` is deprecated. \"\n    \"Please install tool using pip install directly instead.\"\n)\ndef download_loader(\n    loader_class: str,\n    loader_hub_url: str = \"\",\n    refresh_cache: bool = False,\n    use_gpt_index_import: bool = False,\n    custom_path: Optional[str] = None,\n) -> Type[BaseReader]:  # pragma: no cover\n    \"\"\"Download a single loader from the Loader Hub.\n\n    Args:\n        loader_class: The name of the loader class you want to download,\n            such as `SimpleWebPageReader`.\n        refresh_cache: If true, the local cache will be skipped and the\n            loader will be fetched directly from the remote repo.\n        use_gpt_index_import: If true, the loader files will use\n            llama_index as the base dependency. By default (False),\n            the loader files use llama_index as the base dependency.\n            NOTE: this is a temporary workaround while we fully migrate all usages\n            to llama_index.\n        custom_path: Custom dirpath to download loader into.\n\n    Returns:\n        A Loader.\n    \"\"\"\n    # maintain during deprecation period\n    del loader_hub_url\n    del refresh_cache\n    del use_gpt_index_import\n    del custom_path\n\n    mappings_path = os.path.join(\n        os.path.abspath(\n            os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir)\n        ),\n        \"command_line/mappings.json\",\n    )\n    with open(mappings_path) as f:\n        mappings = json.load(f)\n\n    if loader_class in mappings:\n        new_import_parent = mappings[loader_class]\n        new_install_parent = new_import_parent.replace(\".\", \"-\").replace(\"_\", \"-\")\n    else:\n        raise ValueError(f\"Failed to find python package for class {loader_class}\")\n\n    reader_cls = download_integration(\n        module_str=new_install_parent,\n        module_import_str=new_import_parent,\n        cls_name=loader_class,\n    )\n    if not issubclass(reader_cls, BaseReader):\n        raise ValueError(\n            f\"Loader class {loader_class} must be a subclass of BaseReader.\"\n        )\n\n    return reader_cls"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/file/base.py",
    "filename": "base.py",
    "relpath": "readers/file/base.py",
    "start_line": 1,
    "end_line": 189,
    "length": 189,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "read_file_content",
      "aread_file_content",
      "_try_loading_included_file_formats",
      "_format_file_timestamp",
      "default_file_metadata_func",
      "__init__",
      "__call__",
      "get_default_fs",
      "is_default_fs"
    ],
    "chunk_class_names": [
      "FileSystemReaderMixin",
      "_DefaultFileMetadataFunc"
    ],
    "document_function_names": [
      "read_file_content",
      "aread_file_content",
      "_try_loading_included_file_formats",
      "_format_file_timestamp",
      "default_file_metadata_func",
      "__init__",
      "__call__",
      "get_default_fs",
      "is_default_fs",
      "__init__",
      "is_hidden",
      "is_empty_file",
      "_add_files",
      "_exclude_metadata",
      "list_resources",
      "get_resource_info",
      "load_resource",
      "aload_resource",
      "read_file_content",
      "load_file",
      "aload_file",
      "load_data",
      "aload_data",
      "iter_data"
    ],
    "document_class_names": [
      "FileSystemReaderMixin",
      "_DefaultFileMetadataFunc",
      "SimpleDirectoryReader",
      "that",
      "that"
    ],
    "content": "\"\"\"Simple reader that reads files of different formats from a directory.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport mimetypes\nimport multiprocessing\nimport os\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timezone\nfrom functools import reduce\nfrom itertools import repeat\nfrom pathlib import Path, PurePosixPath\nfrom typing import (\n    Any,\n    Callable,\n    Generator,\n    Type,\n    cast,\n)\n\nimport fsspec\nfrom fsspec.implementations.local import LocalFileSystem\nfrom tqdm import tqdm\n\nfrom llama_index.core.async_utils import get_asyncio_module, run_jobs\nfrom llama_index.core.readers.base import BaseReader, ResourcesReaderMixin\nfrom llama_index.core.schema import Document\n\n\nclass FileSystemReaderMixin(ABC):\n    @abstractmethod\n    def read_file_content(self, input_file: Path, **kwargs: Any) -> bytes:\n        \"\"\"\n        Read the bytes content of a file.\n\n        Args:\n            input_file (Path): Path to the file.\n\n        Returns:\n            bytes: File content.\n        \"\"\"\n\n    async def aread_file_content(\n        self, input_file: Path, **kwargs: Any\n    ) -> bytes:  # pragma: no cover\n        \"\"\"\n        A thin wrapper around read_file_content.\n\n        Args:\n            input_file (Path): Path to the file.\n\n        Returns:\n            bytes: File content.\n        \"\"\"\n        return self.read_file_content(input_file, **kwargs)\n\n\ndef _try_loading_included_file_formats() -> (\n    dict[str, Type[BaseReader]]\n):  # pragma: no cover\n    try:\n        from llama_index.readers.file import (\n            DocxReader,\n            EpubReader,\n            HWPReader,\n            ImageReader,\n            IPYNBReader,\n            MboxReader,\n            PandasCSVReader,\n            PandasExcelReader,\n            PDFReader,\n            PptxReader,\n            VideoAudioReader,\n        )  # pants: no-infer-dep\n    except ImportError:\n        raise ImportError(\"`llama-index-readers-file` package not found\")\n\n    default_file_reader_cls: dict[str, Type[BaseReader]] = {\n        \".hwp\": HWPReader,\n        \".pdf\": PDFReader,\n        \".docx\": DocxReader,\n        \".pptx\": PptxReader,\n        \".ppt\": PptxReader,\n        \".pptm\": PptxReader,\n        \".gif\": ImageReader,\n        \".jpg\": ImageReader,\n        \".png\": ImageReader,\n        \".jpeg\": ImageReader,\n        \".webp\": ImageReader,\n        \".mp3\": VideoAudioReader,\n        \".mp4\": VideoAudioReader,\n        \".csv\": PandasCSVReader,\n        \".epub\": EpubReader,\n        \".mbox\": MboxReader,\n        \".ipynb\": IPYNBReader,\n        \".xls\": PandasExcelReader,\n        \".xlsx\": PandasExcelReader,\n    }\n    return default_file_reader_cls\n\n\ndef _format_file_timestamp(\n    timestamp: float | None, include_time: bool = False\n) -> str | None:\n    \"\"\"\n    Format file timestamp to a string.\n    The format will be %Y-%m-%d if include_time is False or missing,\n    %Y-%m-%dT%H:%M:%SZ if include_time is True.\n\n    Args:\n        timestamp (float): timestamp in float\n        include_time (bool): whether to include time in the formatted string\n\n    Returns:\n        str: formatted timestamp\n        None: if the timestamp passed was None\n    \"\"\"\n    if timestamp is None:\n        return None\n\n    timestamp_dt = datetime.fromtimestamp(timestamp, tz=timezone.utc)\n    if include_time:\n        return timestamp_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    return timestamp_dt.strftime(\"%Y-%m-%d\")\n\n\ndef default_file_metadata_func(\n    file_path: str, fs: fsspec.AbstractFileSystem | None = None\n) -> dict:\n    \"\"\"\n    Get some handy metadata from filesystem.\n\n    Args:\n        file_path: str: file path in str\n    \"\"\"\n    fs = fs or get_default_fs()\n    stat_result = fs.stat(file_path)\n\n    try:\n        file_name = os.path.basename(str(stat_result[\"name\"]))\n    except Exception as e:\n        file_name = os.path.basename(file_path)\n\n    creation_date = _format_file_timestamp(stat_result.get(\"created\"))\n    last_modified_date = _format_file_timestamp(stat_result.get(\"mtime\"))\n    last_accessed_date = _format_file_timestamp(stat_result.get(\"atime\"))\n    default_meta = {\n        \"file_path\": file_path,\n        \"file_name\": file_name,\n        \"file_type\": mimetypes.guess_type(file_path)[0],\n        \"file_size\": stat_result.get(\"size\"),\n        \"creation_date\": creation_date,\n        \"last_modified_date\": last_modified_date,\n        \"last_accessed_date\": last_accessed_date,\n    }\n\n    # Return not null value\n    return {\n        meta_key: meta_value\n        for meta_key, meta_value in default_meta.items()\n        if meta_value is not None\n    }\n\n\nclass _DefaultFileMetadataFunc:\n    \"\"\"\n    Default file metadata function wrapper which stores the fs.\n    Allows for pickling of the function.\n    \"\"\"\n\n    def __init__(self, fs: fsspec.AbstractFileSystem | None = None):\n        self.fs = fs or get_default_fs()\n\n    def __call__(self, file_path: str) -> dict:\n        return default_file_metadata_func(file_path, self.fs)\n\n\ndef get_default_fs() -> fsspec.AbstractFileSystem:\n    return LocalFileSystem()\n\n\ndef is_default_fs(fs: fsspec.AbstractFileSystem) -> bool:\n    return isinstance(fs, LocalFileSystem) and not fs.auto_mkdir\n\n\nlogger = logging.getLogger(__name__)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/file/base.py",
    "filename": "base.py",
    "relpath": "readers/file/base.py",
    "start_line": 189,
    "end_line": 192,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "SimpleDirectoryReader"
    ],
    "document_function_names": [
      "read_file_content",
      "aread_file_content",
      "_try_loading_included_file_formats",
      "_format_file_timestamp",
      "default_file_metadata_func",
      "__init__",
      "__call__",
      "get_default_fs",
      "is_default_fs",
      "__init__",
      "is_hidden",
      "is_empty_file",
      "_add_files",
      "_exclude_metadata",
      "list_resources",
      "get_resource_info",
      "load_resource",
      "aload_resource",
      "read_file_content",
      "load_file",
      "aload_file",
      "load_data",
      "aload_data",
      "iter_data"
    ],
    "document_class_names": [
      "FileSystemReaderMixin",
      "_DefaultFileMetadataFunc",
      "SimpleDirectoryReader",
      "that",
      "that"
    ],
    "content": "class SimpleDirectoryReader(BaseReader, ResourcesReaderMixin, FileSystemReaderMixin):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/file/base.py",
    "filename": "base.py",
    "relpath": "readers/file/base.py",
    "start_line": 192,
    "end_line": 494,
    "length": 303,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "is_hidden",
      "is_empty_file",
      "_add_files",
      "_exclude_metadata",
      "list_resources",
      "get_resource_info",
      "load_resource",
      "aload_resource",
      "read_file_content"
    ],
    "chunk_class_names": [
      "that"
    ],
    "document_function_names": [
      "read_file_content",
      "aread_file_content",
      "_try_loading_included_file_formats",
      "_format_file_timestamp",
      "default_file_metadata_func",
      "__init__",
      "__call__",
      "get_default_fs",
      "is_default_fs",
      "__init__",
      "is_hidden",
      "is_empty_file",
      "_add_files",
      "_exclude_metadata",
      "list_resources",
      "get_resource_info",
      "load_resource",
      "aload_resource",
      "read_file_content",
      "load_file",
      "aload_file",
      "load_data",
      "aload_data",
      "iter_data"
    ],
    "document_class_names": [
      "FileSystemReaderMixin",
      "_DefaultFileMetadataFunc",
      "SimpleDirectoryReader",
      "that",
      "that"
    ],
    "content": "\"\"\"\n    Simple directory reader.\n\n    Load files from file directory.\n    Automatically select the best file reader given file extensions.\n\n    Args:\n        input_dir (Union[Path, str]): Path to the directory.\n        input_files (List): List of file paths to read\n            (Optional; overrides input_dir, exclude)\n        exclude (List): glob of python file paths to exclude (Optional)\n        exclude_hidden (bool): Whether to exclude hidden files (dotfiles).\n        exclude_empty (bool): Whether to exclude empty files (Optional).\n        encoding (str): Encoding of the files.\n            Default is utf-8.\n        errors (str): how encoding and decoding errors are to be handled,\n              see https://docs.python.org/3/library/functions.html#open\n        recursive (bool): Whether to recursively search in subdirectories.\n            False by default.\n        filename_as_id (bool): Whether to use the filename as the document id.\n            False by default.\n        required_exts (Optional[List[str]]): List of required extensions.\n            Default is None.\n        file_extractor (Optional[Dict[str, BaseReader]]): A mapping of file\n            extension to a BaseReader class that specifies how to convert that file\n            to text. If not specified, use default from DEFAULT_FILE_READER_CLS.\n        num_files_limit (Optional[int]): Maximum number of files to read.\n            Default is None.\n        file_metadata (Optional[Callable[str, Dict]]): A function that takes\n            in a filename and returns a Dict of metadata for the Document.\n            Default is None.\n        raise_on_error (bool): Whether to raise an error if a file cannot be read.\n        fs (Optional[fsspec.AbstractFileSystem]): File system to use. Defaults\n        to using the local file system. Can be changed to use any remote file system\n        exposed via the fsspec interface.\n    \"\"\"\n\n    supported_suffix_fn: Callable = _try_loading_included_file_formats\n\n    def __init__(\n        self,\n        input_dir: Path | str | None = None,\n        input_files: list | None = None,\n        exclude: list | None = None,\n        exclude_hidden: bool = True,\n        exclude_empty: bool = False,\n        errors: str = \"ignore\",\n        recursive: bool = False,\n        encoding: str = \"utf-8\",\n        filename_as_id: bool = False,\n        required_exts: list[str] | None = None,\n        file_extractor: dict[str, BaseReader] | None = None,\n        num_files_limit: int | None = None,\n        file_metadata: Callable[[str], dict] | None = None,\n        raise_on_error: bool = False,\n        fs: fsspec.AbstractFileSystem | None = None,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        super().__init__()\n\n        if not input_dir and not input_files:\n            raise ValueError(\"Must provide either `input_dir` or `input_files`.\")\n\n        self.fs = fs or get_default_fs()\n        self.errors = errors\n        self.encoding = encoding\n\n        self.exclude = exclude\n        self.recursive = recursive\n        self.exclude_hidden = exclude_hidden\n        self.exclude_empty = exclude_empty\n        self.required_exts = required_exts\n        self.num_files_limit = num_files_limit\n        self.raise_on_error = raise_on_error\n        _Path = Path if is_default_fs(self.fs) else PurePosixPath\n\n        if input_files:\n            self.input_files = []\n            for path in input_files:\n                if not self.fs.isfile(path):\n                    raise ValueError(f\"File {path} does not exist.\")\n                input_file = _Path(path)\n                self.input_files.append(input_file)\n        elif input_dir:\n            if not self.fs.isdir(input_dir):\n                raise ValueError(f\"Directory {input_dir} does not exist.\")\n            self.input_dir = _Path(input_dir)\n            self.exclude = exclude\n            self.input_files = self._add_files(self.input_dir)\n\n        self.file_extractor = file_extractor or {}\n        self.file_metadata = file_metadata or _DefaultFileMetadataFunc(self.fs)\n        self.filename_as_id = filename_as_id\n\n    def is_hidden(self, path: Path | PurePosixPath) -> bool:\n        return any(\n            part.startswith(\".\") and part not in [\".\", \"..\"] for part in path.parts\n        )\n\n    def is_empty_file(self, path: Path | PurePosixPath) -> bool:\n        if isinstance(path, PurePosixPath):\n            path = Path(path)\n        return path.is_file() and len(path.read_bytes()) == 0\n\n    def _add_files(self, input_dir: Path | PurePosixPath) -> list[Path | PurePosixPath]:\n        \"\"\"Add files.\"\"\"\n        all_files: set[Path | PurePosixPath] = set()\n        rejected_files: set[Path | PurePosixPath] = set()\n        rejected_dirs: set[Path | PurePosixPath] = set()\n        # Default to POSIX paths for non-default file systems (e.g. S3)\n        _Path = Path if is_default_fs(self.fs) else PurePosixPath\n\n        if self.exclude is not None:\n            for excluded_pattern in self.exclude:\n                if self.recursive:\n                    # Recursive glob\n                    excluded_glob = _Path(input_dir) / _Path(\"**\") / excluded_pattern\n                else:\n                    # Non-recursive glob\n                    excluded_glob = _Path(input_dir) / excluded_pattern\n                for file in self.fs.glob(str(excluded_glob)):\n                    if self.fs.isdir(file):\n                        rejected_dirs.add(_Path(str(file)))\n                    else:\n                        rejected_files.add(_Path(str(file)))\n\n        file_refs: list[str] = []\n        if self.recursive:\n            file_refs = cast(list[str], self.fs.glob(str(input_dir) + \"/**/*\"))\n        else:\n            file_refs = cast(list[str], self.fs.glob(str(input_dir) + \"/*\"))\n\n        for _ref in file_refs:\n            # Manually check if file is hidden or directory instead of\n            # in glob for backwards compatibility.\n            ref = _Path(_ref)\n            is_dir = self.fs.isdir(ref)\n            skip_because_hidden = self.exclude_hidden and self.is_hidden(ref)\n            skip_because_empty = self.exclude_empty and self.is_empty_file(ref)\n            skip_because_bad_ext = (\n                self.required_exts is not None and ref.suffix not in self.required_exts\n            )\n            skip_because_excluded = ref in rejected_files\n            if not skip_because_excluded:\n                if is_dir:\n                    ref_parent_dir = ref\n                else:\n                    ref_parent_dir = self.fs._parent(ref)\n                for rejected_dir in rejected_dirs:\n                    if str(ref_parent_dir).startswith(str(rejected_dir)):\n                        skip_because_excluded = True\n                        logger.debug(\n                            \"Skipping %s because it in parent dir %s which is in %s\",\n                            ref,\n                            ref_parent_dir,\n                            rejected_dir,\n                        )\n                        break\n\n            if (\n                is_dir\n                or skip_because_hidden\n                or skip_because_bad_ext\n                or skip_because_excluded\n                or skip_because_empty\n            ):\n                continue\n            else:\n                all_files.add(ref)\n\n        new_input_files = sorted(all_files)\n\n        if len(new_input_files) == 0:\n            raise ValueError(f\"No files found in {input_dir}.\")\n\n        if self.num_files_limit is not None and self.num_files_limit > 0:\n            new_input_files = new_input_files[0 : self.num_files_limit]\n\n        # print total number of files added\n        logger.debug(\n            f\"> [SimpleDirectoryReader] Total files added: {len(new_input_files)}\"\n        )\n\n        return new_input_files\n\n    def _exclude_metadata(self, documents: list[Document]) -> list[Document]:\n        \"\"\"\n        Exclude metadata from documents.\n\n        Args:\n            documents (List[Document]): List of documents.\n        \"\"\"\n        for doc in documents:\n            # Keep only metadata['file_path'] in both embedding and llm content\n            # str, which contain extreme important context that about the chunks.\n            # Dates is provided for convenience of postprocessor such as\n            # TimeWeightedPostprocessor, but excluded for embedding and LLMprompts\n            doc.excluded_embed_metadata_keys.extend(\n                [\n                    \"file_name\",\n                    \"file_type\",\n                    \"file_size\",\n                    \"creation_date\",\n                    \"last_modified_date\",\n                    \"last_accessed_date\",\n                ]\n            )\n            doc.excluded_llm_metadata_keys.extend(\n                [\n                    \"file_name\",\n                    \"file_type\",\n                    \"file_size\",\n                    \"creation_date\",\n                    \"last_modified_date\",\n                    \"last_accessed_date\",\n                ]\n            )\n\n        return documents\n\n    def list_resources(self, *args: Any, **kwargs: Any) -> list[str]:\n        \"\"\"List files in the given filesystem.\"\"\"\n        return [str(x) for x in self.input_files]\n\n    def get_resource_info(self, resource_id: str, *args: Any, **kwargs: Any) -> dict:\n        info_result = self.fs.info(resource_id)\n\n        creation_date = _format_file_timestamp(\n            info_result.get(\"created\"), include_time=True\n        )\n        last_modified_date = _format_file_timestamp(\n            info_result.get(\"mtime\"), include_time=True\n        )\n\n        info_dict = {\n            \"file_path\": resource_id,\n            \"file_size\": info_result.get(\"size\"),\n            \"creation_date\": creation_date,\n            \"last_modified_date\": last_modified_date,\n        }\n\n        # Ignore None values\n        return {\n            meta_key: meta_value\n            for meta_key, meta_value in info_dict.items()\n            if meta_value is not None\n        }\n\n    def load_resource(\n        self, resource_id: str, *args: Any, **kwargs: Any\n    ) -> list[Document]:\n        file_metadata = kwargs.get(\"file_metadata\", self.file_metadata)\n        file_extractor = kwargs.get(\"file_extractor\", self.file_extractor)\n        filename_as_id = kwargs.get(\"filename_as_id\", self.filename_as_id)\n        encoding = kwargs.get(\"encoding\", self.encoding)\n        errors = kwargs.get(\"errors\", self.errors)\n        raise_on_error = kwargs.get(\"raise_on_error\", self.raise_on_error)\n        fs = kwargs.get(\"fs\", self.fs)\n\n        path_func = Path if is_default_fs(fs) else PurePosixPath\n\n        return SimpleDirectoryReader.load_file(\n            input_file=path_func(resource_id),\n            file_metadata=file_metadata,\n            file_extractor=file_extractor,\n            filename_as_id=filename_as_id,\n            encoding=encoding,\n            errors=errors,\n            raise_on_error=raise_on_error,\n            fs=fs,\n            **kwargs,\n        )\n\n    async def aload_resource(\n        self, resource_id: str, *args: Any, **kwargs: Any\n    ) -> list[Document]:\n        file_metadata = kwargs.get(\"file_metadata\", self.file_metadata)\n        file_extractor = kwargs.get(\"file_extractor\", self.file_extractor)\n        filename_as_id = kwargs.get(\"filename_as_id\", self.filename_as_id)\n        encoding = kwargs.get(\"encoding\", self.encoding)\n        errors = kwargs.get(\"errors\", self.errors)\n        raise_on_error = kwargs.get(\"raise_on_error\", self.raise_on_error)\n        fs = kwargs.get(\"fs\", self.fs)\n\n        return await SimpleDirectoryReader.aload_file(\n            input_file=Path(resource_id),\n            file_metadata=file_metadata,\n            file_extractor=file_extractor,\n            filename_as_id=filename_as_id,\n            encoding=encoding,\n            errors=errors,\n            raise_on_error=raise_on_error,\n            fs=fs,\n            **kwargs,\n        )\n\n    def read_file_content(self, input_file: Path, **kwargs: Any) -> bytes:\n        \"\"\"Read file content.\"\"\"\n        fs: fsspec.AbstractFileSystem = kwargs.get(\"fs\", self.fs)\n        with fs.open(input_file, errors=self.errors, encoding=self.encoding) as f:\n            # default mode is 'rb', we can cast the return value of f.read()\n            return cast(bytes, f.read())"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/readers/file/base.py",
    "filename": "base.py",
    "relpath": "readers/file/base.py",
    "start_line": 494,
    "end_line": 816,
    "length": 323,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_file",
      "aload_file",
      "load_data",
      "aload_data",
      "iter_data"
    ],
    "chunk_class_names": [
      "that"
    ],
    "document_function_names": [
      "read_file_content",
      "aread_file_content",
      "_try_loading_included_file_formats",
      "_format_file_timestamp",
      "default_file_metadata_func",
      "__init__",
      "__call__",
      "get_default_fs",
      "is_default_fs",
      "__init__",
      "is_hidden",
      "is_empty_file",
      "_add_files",
      "_exclude_metadata",
      "list_resources",
      "get_resource_info",
      "load_resource",
      "aload_resource",
      "read_file_content",
      "load_file",
      "aload_file",
      "load_data",
      "aload_data",
      "iter_data"
    ],
    "document_class_names": [
      "FileSystemReaderMixin",
      "_DefaultFileMetadataFunc",
      "SimpleDirectoryReader",
      "that",
      "that"
    ],
    "content": "@staticmethod\n    def load_file(\n        input_file: Path | PurePosixPath,\n        file_metadata: Callable[[str], dict],\n        file_extractor: dict[str, BaseReader],\n        filename_as_id: bool = False,\n        encoding: str = \"utf-8\",\n        errors: str = \"ignore\",\n        raise_on_error: bool = False,\n        fs: fsspec.AbstractFileSystem | None = None,\n    ) -> list[Document]:\n        \"\"\"\n        Static method for loading file.\n\n        NOTE: necessarily as a static method for parallel processing.\n\n        Args:\n            input_file (Path): _description_\n            file_metadata (Callable[[str], Dict]): _description_\n            file_extractor (Dict[str, BaseReader]): _description_\n            filename_as_id (bool, optional): _description_. Defaults to False.\n            encoding (str, optional): _description_. Defaults to \"utf-8\".\n            errors (str, optional): _description_. Defaults to \"ignore\".\n            fs (Optional[fsspec.AbstractFileSystem], optional): _description_. Defaults to None.\n\n        input_file (Path): File path to read\n        file_metadata ([Callable[str, Dict]]): A function that takes\n            in a filename and returns a Dict of metadata for the Document.\n        file_extractor (Dict[str, BaseReader]): A mapping of file\n            extension to a BaseReader class that specifies how to convert that file\n            to text.\n        filename_as_id (bool): Whether to use the filename as the document id.\n        encoding (str): Encoding of the files.\n            Default is utf-8.\n        errors (str): how encoding and decoding errors are to be handled,\n              see https://docs.python.org/3/library/functions.html#open\n        raise_on_error (bool): Whether to raise an error if a file cannot be read.\n        fs (Optional[fsspec.AbstractFileSystem]): File system to use. Defaults\n            to using the local file system. Can be changed to use any remote file system\n\n        Returns:\n            List[Document]: loaded documents\n        \"\"\"\n        # TODO: make this less redundant\n        default_file_reader_cls = SimpleDirectoryReader.supported_suffix_fn()\n        default_file_reader_suffix = list(default_file_reader_cls.keys())\n        metadata: dict | None = None\n        documents: list[Document] = []\n\n        if file_metadata is not None:\n            metadata = file_metadata(str(input_file))\n\n        file_suffix = input_file.suffix.lower()\n        if file_suffix in default_file_reader_suffix or file_suffix in file_extractor:\n            # use file readers\n            if file_suffix not in file_extractor:\n                # instantiate file reader if not already\n                reader_cls = default_file_reader_cls[file_suffix]\n                file_extractor[file_suffix] = reader_cls()\n            reader = file_extractor[file_suffix]\n\n            # load data -- catch all errors except for ImportError\n            try:\n                kwargs: dict[str, Any] = {\"extra_info\": metadata}\n                if fs and not is_default_fs(fs):\n                    kwargs[\"fs\"] = fs\n                docs = reader.load_data(input_file, **kwargs)\n            except ImportError as e:\n                # ensure that ImportError is raised so user knows\n                # about missing dependencies\n                raise ImportError(str(e))\n            except Exception as e:\n                if raise_on_error:\n                    raise Exception(\"Error loading file\") from e\n                # otherwise, just skip the file and report the error\n                print(\n                    f\"Failed to load file {input_file} with error: {e}. Skipping...\",\n                    flush=True,\n                )\n                return []\n\n            # iterate over docs if needed\n            if filename_as_id:\n                for i, doc in enumerate(docs):\n                    doc.id_ = f\"{input_file!s}_part_{i}\"\n\n            documents.extend(docs)\n        else:\n            # do standard read\n            fs = fs or get_default_fs()\n            with fs.open(input_file, errors=errors, encoding=encoding) as f:\n                data = cast(bytes, f.read()).decode(encoding, errors=errors)\n\n            doc = Document(text=data, metadata=metadata or {})  # type: ignore\n            if filename_as_id:\n                doc.id_ = str(input_file)\n\n            documents.append(doc)\n\n        return documents\n\n    @staticmethod\n    async def aload_file(\n        input_file: Path | PurePosixPath,\n        file_metadata: Callable[[str], dict],\n        file_extractor: dict[str, BaseReader],\n        filename_as_id: bool = False,\n        encoding: str = \"utf-8\",\n        errors: str = \"ignore\",\n        raise_on_error: bool = False,\n        fs: fsspec.AbstractFileSystem | None = None,\n    ) -> list[Document]:\n        \"\"\"Load file asynchronously.\"\"\"\n        # TODO: make this less redundant\n        default_file_reader_cls = SimpleDirectoryReader.supported_suffix_fn()\n        default_file_reader_suffix = list(default_file_reader_cls.keys())\n        metadata: dict | None = None\n        documents: list[Document] = []\n\n        if file_metadata is not None:\n            metadata = file_metadata(str(input_file))\n\n        file_suffix = input_file.suffix.lower()\n        if file_suffix in default_file_reader_suffix or file_suffix in file_extractor:\n            # use file readers\n            if file_suffix not in file_extractor:\n                # instantiate file reader if not already\n                reader_cls = default_file_reader_cls[file_suffix]\n                file_extractor[file_suffix] = reader_cls()\n            reader = file_extractor[file_suffix]\n\n            # load data -- catch all errors except for ImportError\n            try:\n                kwargs: dict[str, Any] = {\"extra_info\": metadata}\n                if fs and not is_default_fs(fs):\n                    kwargs[\"fs\"] = fs\n                docs = await reader.aload_data(input_file, **kwargs)\n            except ImportError as e:\n                # ensure that ImportError is raised so user knows\n                # about missing dependencies\n                raise ImportError(str(e))\n            except Exception as e:\n                if raise_on_error:\n                    raise\n                # otherwise, just skip the file and report the error\n                print(\n                    f\"Failed to load file {input_file} with error: {e}. Skipping...\",\n                    flush=True,\n                )\n                return []\n\n            # iterate over docs if needed\n            if filename_as_id:\n                for i, doc in enumerate(docs):\n                    doc.id_ = f\"{input_file!s}_part_{i}\"\n\n            documents.extend(docs)\n        else:\n            # do standard read\n            fs = fs or get_default_fs()\n            with fs.open(input_file, errors=errors, encoding=encoding) as f:\n                data = cast(bytes, f.read()).decode(encoding, errors=errors)\n\n            doc = Document(text=data, metadata=metadata or {})  # type: ignore\n            if filename_as_id:\n                doc.id_ = str(input_file)\n\n            documents.append(doc)\n\n        return documents\n\n    def load_data(\n        self,\n        show_progress: bool = False,\n        num_workers: int | None = None,\n        fs: fsspec.AbstractFileSystem | None = None,\n    ) -> list[Document]:\n        \"\"\"\n        Load data from the input directory.\n\n        Args:\n            show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n            num_workers  (Optional[int]): Number of workers to parallelize data-loading over.\n            fs (Optional[fsspec.AbstractFileSystem]): File system to use. If fs was specified\n                in the constructor, it will override the fs parameter here.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        documents = []\n\n        files_to_process = self.input_files\n        fs = fs or self.fs\n\n        if num_workers and num_workers > 1:\n            num_cpus = multiprocessing.cpu_count()\n            if num_workers > num_cpus:\n                warnings.warn(\n                    \"Specified num_workers exceed number of CPUs in the system. \"\n                    \"Setting `num_workers` down to the maximum CPU count.\"\n                )\n                num_workers = num_cpus\n\n            with multiprocessing.get_context(\"spawn\").Pool(num_workers) as p:\n                results = p.starmap(\n                    SimpleDirectoryReader.load_file,\n                    zip(\n                        files_to_process,\n                        repeat(self.file_metadata),\n                        repeat(self.file_extractor),\n                        repeat(self.filename_as_id),\n                        repeat(self.encoding),\n                        repeat(self.errors),\n                        repeat(self.raise_on_error),\n                        repeat(fs),\n                    ),\n                )\n                documents = reduce(lambda x, y: x + y, results)\n\n        else:\n            if show_progress:\n                files_to_process = tqdm(\n                    self.input_files, desc=\"Loading files\", unit=\"file\"\n                )\n            for input_file in files_to_process:\n                documents.extend(\n                    SimpleDirectoryReader.load_file(\n                        input_file=input_file,\n                        file_metadata=self.file_metadata,\n                        file_extractor=self.file_extractor,\n                        filename_as_id=self.filename_as_id,\n                        encoding=self.encoding,\n                        errors=self.errors,\n                        raise_on_error=self.raise_on_error,\n                        fs=fs,\n                    )\n                )\n\n        return self._exclude_metadata(documents)\n\n    async def aload_data(\n        self,\n        show_progress: bool = False,\n        num_workers: int | None = None,\n        fs: fsspec.AbstractFileSystem | None = None,\n    ) -> list[Document]:\n        \"\"\"\n        Load data from the input directory.\n\n        Args:\n            show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n            num_workers  (Optional[int]): Number of workers to parallelize data-loading over.\n            fs (Optional[fsspec.AbstractFileSystem]): File system to use. If fs was specified\n                in the constructor, it will override the fs parameter here.\n\n        Returns:\n            List[Document]: A list of documents.\n        \"\"\"\n        files_to_process = self.input_files\n        fs = fs or self.fs\n\n        coroutines = [\n            SimpleDirectoryReader.aload_file(\n                input_file,\n                self.file_metadata,\n                self.file_extractor,\n                self.filename_as_id,\n                self.encoding,\n                self.errors,\n                self.raise_on_error,\n                fs,\n            )\n            for input_file in files_to_process\n        ]\n\n        if num_workers:\n            document_lists = await run_jobs(\n                coroutines, show_progress=show_progress, workers=num_workers\n            )\n        elif show_progress:\n            _asyncio = get_asyncio_module(show_progress=show_progress)\n            document_lists = await _asyncio.gather(*coroutines)\n        else:\n            document_lists = await asyncio.gather(*coroutines)\n        documents = [doc for doc_list in document_lists for doc in doc_list]\n\n        return self._exclude_metadata(documents)\n\n    def iter_data(\n        self, show_progress: bool = False\n    ) -> Generator[list[Document], Any, Any]:\n        \"\"\"\n        Load data iteratively from the input directory.\n\n        Args:\n            show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n\n        Returns:\n            Generator[List[Document]]: A list of documents.\n        \"\"\"\n        files_to_process = self.input_files\n\n        if show_progress:\n            files_to_process = tqdm(self.input_files, desc=\"Loading files\", unit=\"file\")\n\n        for input_file in files_to_process:\n            documents = SimpleDirectoryReader.load_file(\n                input_file=input_file,\n                file_metadata=self.file_metadata,\n                file_extractor=self.file_extractor,\n                filename_as_id=self.filename_as_id,\n                encoding=self.encoding,\n                errors=self.errors,\n                raise_on_error=self.raise_on_error,\n                fs=self.fs,\n            )\n\n            documents = self._exclude_metadata(documents)\n\n            if len(documents) > 0:\n                yield documents"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/embeddings/loading.py",
    "filename": "loading.py",
    "relpath": "embeddings/loading.py",
    "start_line": 1,
    "end_line": 49,
    "length": 49,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_embed_model"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "load_embed_model"
    ],
    "document_class_names": [],
    "content": "from typing import Dict, Type\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.embeddings.mock_embed_model import MockEmbedding\n\nRECOGNIZED_EMBEDDINGS: Dict[str, Type[BaseEmbedding]] = {\n    MockEmbedding.class_name(): MockEmbedding,\n}\n\n# conditionals for llama-cloud support\ntry:\n    from llama_index.embeddings.openai import OpenAIEmbedding  # pants: no-infer-dep\n\n    RECOGNIZED_EMBEDDINGS[OpenAIEmbedding.class_name()] = OpenAIEmbedding\nexcept ImportError:\n    pass\n\ntry:\n    from llama_index.embeddings.azure_openai import (\n        AzureOpenAIEmbedding,\n    )  # pants: no-infer-dep\n\n    RECOGNIZED_EMBEDDINGS[AzureOpenAIEmbedding.class_name()] = AzureOpenAIEmbedding\nexcept ImportError:\n    pass\n\ntry:\n    from llama_index.embeddings.huggingface_api import (\n        HuggingFaceInferenceAPIEmbedding,\n    )  # pants: no-infer-dep\n\n    RECOGNIZED_EMBEDDINGS[\n        HuggingFaceInferenceAPIEmbedding.class_name()\n    ] = HuggingFaceInferenceAPIEmbedding\nexcept ImportError:\n    pass\n\n\ndef load_embed_model(data: dict) -> BaseEmbedding:\n    \"\"\"Load Embedding by name.\"\"\"\n    if isinstance(data, BaseEmbedding):\n        return data\n    name = data.get(\"class_name\", None)\n    if name is None:\n        raise ValueError(\"Embedding loading requires a class_name\")\n    if name not in RECOGNIZED_EMBEDDINGS:\n        raise ValueError(f\"Invalid Embedding name: {name}\")\n\n    return RECOGNIZED_EMBEDDINGS[name].from_dict(data)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/embeddings/__init__.py",
    "filename": "__init__.py",
    "relpath": "embeddings/__init__.py",
    "start_line": 1,
    "end_line": 13,
    "length": 13,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.embeddings.mock_embed_model import MockEmbedding\nfrom llama_index.core.embeddings.multi_modal_base import MultiModalEmbedding\nfrom llama_index.core.embeddings.pooling import Pooling\nfrom llama_index.core.embeddings.utils import resolve_embed_model\n\n__all__ = [\n    \"BaseEmbedding\",\n    \"MockEmbedding\",\n    \"MultiModalEmbedding\",\n    \"Pooling\",\n    \"resolve_embed_model\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/embeddings/utils.py",
    "filename": "utils.py",
    "relpath": "embeddings/utils.py",
    "start_line": 1,
    "end_line": 140,
    "length": 140,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "save_embedding",
      "load_embedding",
      "resolve_embed_model"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "save_embedding",
      "load_embedding",
      "resolve_embed_model"
    ],
    "document_class_names": [],
    "content": "\"\"\"Embedding utils for LlamaIndex.\"\"\"\n\nimport os\nfrom typing import TYPE_CHECKING, List, Optional, Union\n\nif TYPE_CHECKING:\n    from llama_index.core.bridge.langchain import Embeddings as LCEmbeddings\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.embeddings.mock_embed_model import MockEmbedding\nfrom llama_index.core.utils import get_cache_dir\n\nEmbedType = Union[BaseEmbedding, \"LCEmbeddings\", str]\n\n\ndef save_embedding(embedding: List[float], file_path: str) -> None:\n    \"\"\"Save embedding to file.\"\"\"\n    with open(file_path, \"w\") as f:\n        f.write(\",\".join([str(x) for x in embedding]))\n\n\ndef load_embedding(file_path: str) -> List[float]:\n    \"\"\"Load embedding from file. Will only return first embedding in file.\"\"\"\n    with open(file_path) as f:\n        for line in f:\n            embedding = [float(x) for x in line.strip().split(\",\")]\n            break\n        return embedding\n\n\ndef resolve_embed_model(\n    embed_model: Optional[EmbedType] = None,\n    callback_manager: Optional[CallbackManager] = None,\n) -> BaseEmbedding:\n    \"\"\"Resolve embed model.\"\"\"\n    from llama_index.core.settings import Settings\n\n    try:\n        from llama_index.core.bridge.langchain import Embeddings as LCEmbeddings\n    except ImportError:\n        LCEmbeddings = None  # type: ignore\n\n    if embed_model == \"default\":\n        if os.getenv(\"IS_TESTING\"):\n            embed_model = MockEmbedding(embed_dim=8)\n            embed_model.callback_manager = callback_manager or Settings.callback_manager\n            return embed_model\n\n        try:\n            from llama_index.embeddings.openai import (\n                OpenAIEmbedding,\n            )  # pants: no-infer-dep\n\n            from llama_index.embeddings.openai.utils import (\n                validate_openai_api_key,\n            )  # pants: no-infer-dep\n\n            embed_model = OpenAIEmbedding()\n            validate_openai_api_key(embed_model.api_key)  # type: ignore\n        except ImportError:\n            raise ImportError(\n                \"`llama-index-embeddings-openai` package not found, \"\n                \"please run `pip install llama-index-embeddings-openai`\"\n            )\n        except ValueError as e:\n            raise ValueError(\n                \"\\n******\\n\"\n                \"Could not load OpenAI embedding model. \"\n                \"If you intended to use OpenAI, please check your OPENAI_API_KEY.\\n\"\n                \"Original error:\\n\"\n                f\"{e!s}\"\n                \"\\nConsider using embed_model='local'.\\n\"\n                \"Visit our documentation for more embedding options: \"\n                \"https://docs.llamaindex.ai/en/stable/module_guides/models/\"\n                \"embeddings.html#modules\"\n                \"\\n******\"\n            )\n    # for image multi-modal embeddings\n    elif isinstance(embed_model, str) and embed_model.startswith(\"clip\"):\n        try:\n            from llama_index.embeddings.clip import ClipEmbedding  # pants: no-infer-dep\n\n            clip_model_name = (\n                embed_model.split(\":\")[1] if \":\" in embed_model else \"ViT-B/32\"\n            )\n            embed_model = ClipEmbedding(model_name=clip_model_name)\n        except ImportError as e:\n            raise ImportError(\n                \"`llama-index-embeddings-clip` package not found, \"\n                \"please run `pip install llama-index-embeddings-clip` and `pip install git+https://github.com/openai/CLIP.git`\"\n            )\n\n    if isinstance(embed_model, str):\n        try:\n            from llama_index.embeddings.huggingface import (\n                HuggingFaceEmbedding,\n            )  # pants: no-infer-dep\n\n            splits = embed_model.split(\":\", 1)\n            is_local = splits[0]\n            model_name = splits[1] if len(splits) > 1 else None\n            if is_local != \"local\":\n                raise ValueError(\n                    \"embed_model must start with str 'local' or of type BaseEmbedding\"\n                )\n\n            cache_folder = os.path.join(get_cache_dir(), \"models\")\n            os.makedirs(cache_folder, exist_ok=True)\n\n            embed_model = HuggingFaceEmbedding(\n                model_name=model_name, cache_folder=cache_folder\n            )\n        except ImportError:\n            raise ImportError(\n                \"`llama-index-embeddings-huggingface` package not found, \"\n                \"please run `pip install llama-index-embeddings-huggingface`\"\n            )\n\n    if LCEmbeddings is not None and isinstance(embed_model, LCEmbeddings):\n        try:\n            from llama_index.embeddings.langchain import (\n                LangchainEmbedding,\n            )  # pants: no-infer-dep\n\n            embed_model = LangchainEmbedding(embed_model)\n        except ImportError as e:\n            raise ImportError(\n                \"`llama-index-embeddings-langchain` package not found, \"\n                \"please run `pip install llama-index-embeddings-langchain`\"\n            )\n\n    if embed_model is None:\n        print(\"Embeddings have been explicitly disabled. Using MockEmbedding.\")\n        embed_model = MockEmbedding(embed_dim=1)\n\n    assert isinstance(embed_model, BaseEmbedding)\n\n    embed_model.callback_manager = callback_manager or Settings.callback_manager\n\n    return embed_model"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/embeddings/mock_embed_model.py",
    "filename": "mock_embed_model.py",
    "relpath": "embeddings/mock_embed_model.py",
    "start_line": 1,
    "end_line": 43,
    "length": 43,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "_get_vector",
      "_aget_text_embedding",
      "_aget_query_embedding",
      "_get_query_embedding",
      "_get_text_embedding"
    ],
    "chunk_class_names": [
      "MockEmbedding"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "_get_vector",
      "_aget_text_embedding",
      "_aget_query_embedding",
      "_get_query_embedding",
      "_get_text_embedding"
    ],
    "document_class_names": [
      "MockEmbedding"
    ],
    "content": "\"\"\"Mock embedding model.\"\"\"\n\nfrom typing import Any, List\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\n\n\nclass MockEmbedding(BaseEmbedding):\n    \"\"\"Mock embedding.\n\n    Used for token prediction.\n\n    Args:\n        embed_dim (int): embedding dimension\n\n    \"\"\"\n\n    embed_dim: int\n\n    def __init__(self, embed_dim: int, **kwargs: Any) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(embed_dim=embed_dim, **kwargs)\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockEmbedding\"\n\n    def _get_vector(self) -> List[float]:\n        return [0.5] * self.embed_dim\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        return self._get_vector()\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        return self._get_vector()\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return self._get_vector()\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return self._get_vector()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/embeddings/pooling.py",
    "filename": "pooling.py",
    "relpath": "embeddings/pooling.py",
    "start_line": 1,
    "end_line": 49,
    "length": 49,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__call__",
      "cls_pooling",
      "cls_pooling",
      "cls_pooling",
      "mean_pooling"
    ],
    "chunk_class_names": [
      "Pooling"
    ],
    "document_function_names": [
      "__call__",
      "cls_pooling",
      "cls_pooling",
      "cls_pooling",
      "mean_pooling"
    ],
    "document_class_names": [
      "Pooling"
    ],
    "content": "from enum import Enum\nfrom typing import TYPE_CHECKING, Union, overload\n\nimport numpy as np\n\nif TYPE_CHECKING:\n    import torch  # pants: no-infer-dep\n\n\nclass Pooling(str, Enum):\n    \"\"\"Enum of possible pooling choices with pooling behaviors.\"\"\"\n\n    CLS = \"cls\"\n    MEAN = \"mean\"\n\n    def __call__(self, array: np.ndarray) -> np.ndarray:\n        if self == self.CLS:\n            return Pooling.cls_pooling(array)\n        return Pooling.mean_pooling(array)\n\n    @classmethod\n    @overload\n    def cls_pooling(cls, array: np.ndarray) -> np.ndarray:\n        ...\n\n    @classmethod\n    @overload\n    # TODO: Remove this `type: ignore` after the false positive problem\n    #  is addressed in mypy: https://github.com/python/mypy/issues/15683 .\n    def cls_pooling(cls, array: \"torch.Tensor\") -> \"torch.Tensor\":  # type: ignore\n        ...\n\n    @classmethod\n    def cls_pooling(\n        cls, array: \"Union[np.ndarray, torch.Tensor]\"\n    ) -> \"Union[np.ndarray, torch.Tensor]\":\n        if len(array.shape) == 3:\n            return array[:, 0]\n        if len(array.shape) == 2:\n            return array[0]\n        raise NotImplementedError(f\"Unhandled shape {array.shape}.\")\n\n    @classmethod\n    def mean_pooling(cls, array: np.ndarray) -> np.ndarray:\n        if len(array.shape) == 3:\n            return array.mean(axis=1)\n        if len(array.shape) == 2:\n            return array.mean(axis=0)\n        raise NotImplementedError(f\"Unhandled shape {array.shape}.\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/embeddings/multi_modal_base.py",
    "filename": "multi_modal_base.py",
    "relpath": "embeddings/multi_modal_base.py",
    "start_line": 1,
    "end_line": 186,
    "length": 186,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_image_embedding",
      "_aget_image_embedding",
      "get_image_embedding",
      "aget_image_embedding",
      "_get_image_embeddings",
      "_aget_image_embeddings",
      "get_image_embedding_batch",
      "aget_image_embedding_batch"
    ],
    "chunk_class_names": [
      "MultiModalEmbedding",
      "for"
    ],
    "document_function_names": [
      "_get_image_embedding",
      "_aget_image_embedding",
      "get_image_embedding",
      "aget_image_embedding",
      "_get_image_embeddings",
      "_aget_image_embeddings",
      "get_image_embedding_batch",
      "aget_image_embedding_batch"
    ],
    "document_class_names": [
      "MultiModalEmbedding",
      "for"
    ],
    "content": "\"\"\"Base embeddings file.\"\"\"\n\nimport asyncio\nfrom abc import abstractmethod\nfrom typing import Coroutine, List, Tuple\n\nfrom llama_index.core.base.embeddings.base import (\n    BaseEmbedding,\n    Embedding,\n)\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.schema import ImageType\nfrom llama_index.core.utils import get_tqdm_iterable\n\n\nclass MultiModalEmbedding(BaseEmbedding):\n    \"\"\"Base class for Multi Modal embeddings.\"\"\"\n\n    @abstractmethod\n    def _get_image_embedding(self, img_file_path: ImageType) -> Embedding:\n        \"\"\"\n        Embed the input image synchronously.\n\n        Subclasses should implement this method. Reference get_image_embedding's\n        docstring for more information.\n        \"\"\"\n\n    @abstractmethod\n    async def _aget_image_embedding(self, img_file_path: ImageType) -> Embedding:\n        \"\"\"\n        Embed the input image asynchronously.\n\n        Subclasses should implement this method. Reference get_image_embedding's\n        docstring for more information.\n        \"\"\"\n\n    def get_image_embedding(self, img_file_path: ImageType) -> Embedding:\n        \"\"\"\n        Embed the input image.\n        \"\"\"\n        with self.callback_manager.event(\n            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}\n        ) as event:\n            image_embedding = self._get_image_embedding(img_file_path)\n\n            event.on_end(\n                payload={\n                    EventPayload.CHUNKS: [img_file_path],\n                    EventPayload.EMBEDDINGS: [image_embedding],\n                },\n            )\n        return image_embedding\n\n    async def aget_image_embedding(self, img_file_path: ImageType) -> Embedding:\n        \"\"\"Get image embedding.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}\n        ) as event:\n            image_embedding = await self._aget_image_embedding(img_file_path)\n\n            event.on_end(\n                payload={\n                    EventPayload.CHUNKS: [img_file_path],\n                    EventPayload.EMBEDDINGS: [image_embedding],\n                },\n            )\n        return image_embedding\n\n    def _get_image_embeddings(self, img_file_paths: List[ImageType]) -> List[Embedding]:\n        \"\"\"\n        Embed the input sequence of image synchronously.\n\n        Subclasses can implement this method if batch queries are supported.\n        \"\"\"\n        # Default implementation just loops over _get_image_embedding\n        return [\n            self._get_image_embedding(img_file_path) for img_file_path in img_file_paths\n        ]\n\n    async def _aget_image_embeddings(\n        self, img_file_paths: List[ImageType]\n    ) -> List[Embedding]:\n        \"\"\"\n        Embed the input sequence of image asynchronously.\n\n        Subclasses can implement this method if batch queries are supported.\n        \"\"\"\n        return await asyncio.gather(\n            *[\n                self._aget_image_embedding(img_file_path)\n                for img_file_path in img_file_paths\n            ]\n        )\n\n    def get_image_embedding_batch(\n        self, img_file_paths: List[ImageType], show_progress: bool = False\n    ) -> List[Embedding]:\n        \"\"\"Get a list of image embeddings, with batching.\"\"\"\n        cur_batch: List[ImageType] = []\n        result_embeddings: List[Embedding] = []\n\n        queue_with_progress = enumerate(\n            get_tqdm_iterable(\n                img_file_paths, show_progress, \"Generating image embeddings\"\n            )\n        )\n\n        for idx, img_file_path in queue_with_progress:\n            cur_batch.append(img_file_path)\n            if (\n                idx == len(img_file_paths) - 1\n                or len(cur_batch) == self.embed_batch_size\n            ):\n                # flush\n                with self.callback_manager.event(\n                    CBEventType.EMBEDDING,\n                    payload={EventPayload.SERIALIZED: self.to_dict()},\n                ) as event:\n                    embeddings = self._get_image_embeddings(cur_batch)\n                    result_embeddings.extend(embeddings)\n                    event.on_end(\n                        payload={\n                            EventPayload.CHUNKS: cur_batch,\n                            EventPayload.EMBEDDINGS: embeddings,\n                        },\n                    )\n                cur_batch = []\n\n        return result_embeddings\n\n    async def aget_image_embedding_batch(\n        self, img_file_paths: List[ImageType], show_progress: bool = False\n    ) -> List[Embedding]:\n        \"\"\"Asynchronously get a list of image embeddings, with batching.\"\"\"\n        cur_batch: List[ImageType] = []\n        callback_payloads: List[Tuple[str, List[ImageType]]] = []\n        result_embeddings: List[Embedding] = []\n        embeddings_coroutines: List[Coroutine] = []\n        for idx, img_file_path in enumerate(img_file_paths):\n            cur_batch.append(img_file_path)\n            if (\n                idx == len(img_file_paths) - 1\n                or len(cur_batch) == self.embed_batch_size\n            ):\n                # flush\n                event_id = self.callback_manager.on_event_start(\n                    CBEventType.EMBEDDING,\n                    payload={EventPayload.SERIALIZED: self.to_dict()},\n                )\n                callback_payloads.append((event_id, cur_batch))\n                embeddings_coroutines.append(self._aget_image_embeddings(cur_batch))\n                cur_batch = []\n\n        # flatten the results of asyncio.gather, which is a list of embeddings lists\n        nested_embeddings = []\n        if show_progress:\n            try:\n                from tqdm.asyncio import tqdm_asyncio\n\n                nested_embeddings = await tqdm_asyncio.gather(\n                    *embeddings_coroutines,\n                    total=len(embeddings_coroutines),\n                    desc=\"Generating embeddings\",\n                )\n            except ImportError:\n                nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n        else:\n            nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n\n        result_embeddings = [\n            embedding for embeddings in nested_embeddings for embedding in embeddings\n        ]\n\n        for (event_id, image_batch), embeddings in zip(\n            callback_payloads, nested_embeddings\n        ):\n            self.callback_manager.on_event_end(\n                CBEventType.EMBEDDING,\n                payload={\n                    EventPayload.CHUNKS: image_batch,\n                    EventPayload.EMBEDDINGS: embeddings,\n                },\n                event_id=event_id,\n            )\n\n        return result_embeddings"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/objects/base.py",
    "filename": "base.py",
    "relpath": "objects/base.py",
    "start_line": 1,
    "end_line": 260,
    "length": 260,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "retriever",
      "object_node_mapping",
      "node_postprocessors",
      "retrieve",
      "aretrieve",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "__init__",
      "index",
      "object_node_mapping",
      "from_objects",
      "from_objects_and_index",
      "insert_object",
      "as_retriever",
      "as_node_retriever",
      "persist",
      "from_persist_dir"
    ],
    "chunk_class_names": [
      "ObjectRetriever",
      "ObjectRetrieverComponent",
      "ObjectIndex"
    ],
    "document_function_names": [
      "__init__",
      "retriever",
      "object_node_mapping",
      "node_postprocessors",
      "retrieve",
      "aretrieve",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "__init__",
      "index",
      "object_node_mapping",
      "from_objects",
      "from_objects_and_index",
      "insert_object",
      "as_retriever",
      "as_node_retriever",
      "persist",
      "from_persist_dir"
    ],
    "document_class_names": [
      "ObjectRetriever",
      "ObjectRetrieverComponent",
      "ObjectIndex"
    ],
    "content": "\"\"\"Base object types.\"\"\"\n\nimport pickle\nimport warnings\nfrom typing import Any, Callable, Dict, Generic, List, Optional, Sequence, Type, TypeVar\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.objects.base_node_mapping import (\n    DEFAULT_PERSIST_FNAME,\n    BaseObjectNodeMapping,\n    SimpleObjectNodeMapping,\n)\nfrom llama_index.core.schema import BaseNode, QueryBundle, QueryType\nfrom llama_index.core.storage.storage_context import (\n    DEFAULT_PERSIST_DIR,\n    StorageContext,\n)\n\nOT = TypeVar(\"OT\")\n\n\nclass ObjectRetriever(ChainableMixin, Generic[OT]):\n    \"\"\"Object retriever.\"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        object_node_mapping: BaseObjectNodeMapping[OT],\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n    ):\n        self._retriever = retriever\n        self._object_node_mapping = object_node_mapping\n        self._node_postprocessors = node_postprocessors or []\n\n    @property\n    def retriever(self) -> BaseRetriever:\n        \"\"\"Retriever.\"\"\"\n        return self._retriever\n\n    @property\n    def object_node_mapping(self) -> BaseObjectNodeMapping[OT]:\n        \"\"\"Object node mapping.\"\"\"\n        return self._object_node_mapping\n\n    @property\n    def node_postprocessors(self) -> List[BaseNodePostprocessor]:\n        \"\"\"Node postprocessors.\"\"\"\n        return self._node_postprocessors\n\n    def retrieve(self, str_or_query_bundle: QueryType) -> List[OT]:\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(query_str=str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n\n        nodes = self._retriever.retrieve(query_bundle)\n        for node_postprocessor in self._node_postprocessors:\n            nodes = node_postprocessor.postprocess_nodes(\n                nodes, query_bundle=query_bundle\n            )\n\n        return [self._object_node_mapping.from_node(node.node) for node in nodes]\n\n    async def aretrieve(self, str_or_query_bundle: QueryType) -> List[OT]:\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(query_str=str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n\n        nodes = await self._retriever.aretrieve(query_bundle)\n        for node_postprocessor in self._node_postprocessors:\n            nodes = node_postprocessor.postprocess_nodes(\n                nodes, query_bundle=query_bundle\n            )\n\n        return [self._object_node_mapping.from_node(node.node) for node in nodes]\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"As query component.\"\"\"\n        return ObjectRetrieverComponent(retriever=self)\n\n\nclass ObjectRetrieverComponent(QueryComponent):\n    \"\"\"Object retriever component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    retriever: ObjectRetriever = Field(..., description=\"Retriever.\")\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.retriever.retriever.callback_manager = callback_manager\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # make sure input is a string\n        input[\"input\"] = validate_and_convert_stringable(input[\"input\"])\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = self.retriever.retrieve(kwargs[\"input\"])\n        return {\"output\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        output = await self.retriever.aretrieve(kwargs[\"input\"])\n        return {\"output\": output}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"input\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})\n\n\nclass ObjectIndex(Generic[OT]):\n    \"\"\"Object index.\"\"\"\n\n    def __init__(\n        self, index: BaseIndex, object_node_mapping: BaseObjectNodeMapping\n    ) -> None:\n        self._index = index\n        self._object_node_mapping = object_node_mapping\n\n    @property\n    def index(self) -> BaseIndex:\n        \"\"\"Index.\"\"\"\n        return self._index\n\n    @property\n    def object_node_mapping(self) -> BaseObjectNodeMapping:\n        \"\"\"Object node mapping.\"\"\"\n        return self._object_node_mapping\n\n    @classmethod\n    def from_objects(\n        cls,\n        objects: Sequence[OT],\n        object_mapping: Optional[BaseObjectNodeMapping] = None,\n        from_node_fn: Optional[Callable[[BaseNode], OT]] = None,\n        to_node_fn: Optional[Callable[[OT], BaseNode]] = None,\n        index_cls: Type[BaseIndex] = VectorStoreIndex,\n        **index_kwargs: Any,\n    ) -> \"ObjectIndex\":\n        from llama_index.core.objects.utils import get_object_mapping\n\n        # pick the best mapping if not provided\n        if object_mapping is None:\n            object_mapping = get_object_mapping(\n                objects,\n                from_node_fn=from_node_fn,\n                to_node_fn=to_node_fn,\n            )\n\n        nodes = object_mapping.to_nodes(objects)\n        index = index_cls(nodes, **index_kwargs)\n        return cls(index, object_mapping)\n\n    @classmethod\n    def from_objects_and_index(\n        cls,\n        objects: Sequence[OT],\n        index: BaseIndex,\n        object_mapping: Optional[BaseObjectNodeMapping] = None,\n        from_node_fn: Optional[Callable[[BaseNode], OT]] = None,\n        to_node_fn: Optional[Callable[[OT], BaseNode]] = None,\n    ) -> \"ObjectIndex\":\n        from llama_index.core.objects.utils import get_object_mapping\n\n        # pick the best mapping if not provided\n        if object_mapping is None:\n            object_mapping = get_object_mapping(\n                objects,\n                from_node_fn=from_node_fn,\n                to_node_fn=to_node_fn,\n            )\n\n        return cls(index, object_mapping)\n\n    def insert_object(self, obj: Any) -> None:\n        self._object_node_mapping.add_object(obj)\n        node = self._object_node_mapping.to_node(obj)\n        self._index.insert_nodes([node])\n\n    def as_retriever(\n        self,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        **kwargs: Any,\n    ) -> ObjectRetriever:\n        return ObjectRetriever(\n            retriever=self._index.as_retriever(**kwargs),\n            object_node_mapping=self._object_node_mapping,\n            node_postprocessors=node_postprocessors,\n        )\n\n    def as_node_retriever(self, **kwargs: Any) -> BaseRetriever:\n        return self._index.as_retriever(**kwargs)\n\n    def persist(\n        self,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> None:\n        # try to persist object node mapping\n        try:\n            self._object_node_mapping.persist(\n                persist_dir=persist_dir, obj_node_mapping_fname=obj_node_mapping_fname\n            )\n        except (NotImplementedError, pickle.PickleError) as err:\n            warnings.warn(\n                (\n                    \"Unable to persist ObjectNodeMapping. You will need to \"\n                    \"reconstruct the same object node mapping to build this ObjectIndex\"\n                ),\n                stacklevel=2,\n            )\n        self._index._storage_context.persist(persist_dir=persist_dir)\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        object_node_mapping: Optional[BaseObjectNodeMapping] = None,\n    ) -> \"ObjectIndex\":\n        from llama_index.core.indices import load_index_from_storage\n\n        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n        index = load_index_from_storage(storage_context)\n        if object_node_mapping:\n            return cls(index=index, object_node_mapping=object_node_mapping)\n        else:\n            # try to load object_node_mapping\n            # assume SimpleObjectNodeMapping for simplicity as its only subclass\n            # that supports this method\n            try:\n                object_node_mapping = SimpleObjectNodeMapping.from_persist_dir(\n                    persist_dir=persist_dir\n                )\n            except Exception as err:\n                raise Exception(\n                    \"Unable to load from persist dir. The object_node_mapping cannot be loaded.\"\n                ) from err\n            else:\n                return cls(index=index, object_node_mapping=object_node_mapping)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/objects/tool_node_mapping.py",
    "filename": "tool_node_mapping.py",
    "relpath": "objects/tool_node_mapping.py",
    "start_line": 1,
    "end_line": 153,
    "length": 153,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "convert_tool_to_node",
      "validate_object",
      "obj_node_mapping",
      "persist",
      "from_persist_dir",
      "__init__",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node",
      "from_persist_dir",
      "obj_node_mapping",
      "persist",
      "__init__",
      "validate_object",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node"
    ],
    "chunk_class_names": [
      "BaseToolNodeMapping",
      "SimpleToolNodeMapping",
      "BaseQueryToolNodeMapping",
      "SimpleQueryToolNodeMapping"
    ],
    "document_function_names": [
      "convert_tool_to_node",
      "validate_object",
      "obj_node_mapping",
      "persist",
      "from_persist_dir",
      "__init__",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node",
      "from_persist_dir",
      "obj_node_mapping",
      "persist",
      "__init__",
      "validate_object",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node"
    ],
    "document_class_names": [
      "BaseToolNodeMapping",
      "SimpleToolNodeMapping",
      "BaseQueryToolNodeMapping",
      "SimpleQueryToolNodeMapping"
    ],
    "content": "\"\"\"Tool mapping.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence\n\nfrom llama_index.core.objects.base_node_mapping import (\n    DEFAULT_PERSIST_DIR,\n    DEFAULT_PERSIST_FNAME,\n    BaseObjectNodeMapping,\n)\nfrom llama_index.core.schema import BaseNode, TextNode\nfrom llama_index.core.tools.query_engine import QueryEngineTool\nfrom llama_index.core.tools.types import BaseTool\n\n\ndef convert_tool_to_node(tool: BaseTool) -> TextNode:\n    \"\"\"Function convert Tool to node.\"\"\"\n    node_text = (\n        f\"Tool name: {tool.metadata.name}\\n\"\n        f\"Tool description: {tool.metadata.description}\\n\"\n    )\n    if tool.metadata.fn_schema is not None:\n        node_text += f\"Tool schema: {tool.metadata.fn_schema.model_json_schema()}\\n\"\n\n    tool_identity = (\n        f\"{tool.metadata.name}{tool.metadata.description}{tool.metadata.fn_schema}\"\n    )\n\n    return TextNode(\n        id_=str(hash(tool_identity)),\n        text=node_text,\n        metadata={\"name\": tool.metadata.name},\n        excluded_embed_metadata_keys=[\"name\"],\n        excluded_llm_metadata_keys=[\"name\"],\n    )\n\n\nclass BaseToolNodeMapping(BaseObjectNodeMapping[BaseTool]):\n    \"\"\"Base Tool node mapping.\"\"\"\n\n    def validate_object(self, obj: BaseTool) -> None:\n        if not isinstance(obj, BaseTool):\n            raise ValueError(f\"Object must be of type {BaseTool}\")\n\n    @property\n    def obj_node_mapping(self) -> Dict[int, Any]:\n        \"\"\"The mapping data structure between node and object.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    def persist(\n        self, persist_dir: str = ..., obj_node_mapping_fname: str = ...\n    ) -> None:\n        \"\"\"Persist objs.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> \"BaseToolNodeMapping\":\n        raise NotImplementedError(\n            \"This object node mapping does not support persist method.\"\n        )\n\n\nclass SimpleToolNodeMapping(BaseToolNodeMapping):\n    \"\"\"Simple Tool mapping.\n\n    In this setup, we assume that the tool name is unique, and\n    that the list of all tools are stored in memory.\n\n    \"\"\"\n\n    def __init__(self, objs: Optional[Sequence[BaseTool]] = None) -> None:\n        objs = objs or []\n        self._tools = {tool.metadata.name: tool for tool in objs}\n\n    @classmethod\n    def from_objects(\n        cls, objs: Sequence[BaseTool], *args: Any, **kwargs: Any\n    ) -> \"BaseObjectNodeMapping\":\n        return cls(objs)\n\n    def _add_object(self, tool: BaseTool) -> None:\n        self._tools[tool.metadata.name] = tool\n\n    def to_node(self, tool: BaseTool) -> TextNode:\n        \"\"\"To node.\"\"\"\n        return convert_tool_to_node(tool)\n\n    def _from_node(self, node: BaseNode) -> BaseTool:\n        \"\"\"From node.\"\"\"\n        if node.metadata is None:\n            raise ValueError(\"Metadata must be set\")\n        return self._tools[node.metadata[\"name\"]]\n\n\nclass BaseQueryToolNodeMapping(BaseObjectNodeMapping[QueryEngineTool]):\n    \"\"\"Base query tool node mapping.\"\"\"\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> \"BaseQueryToolNodeMapping\":\n        raise NotImplementedError(\n            \"This object node mapping does not support persist method.\"\n        )\n\n    @property\n    def obj_node_mapping(self) -> Dict[int, Any]:\n        \"\"\"The mapping data structure between node and object.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    def persist(\n        self, persist_dir: str = ..., obj_node_mapping_fname: str = ...\n    ) -> None:\n        \"\"\"Persist objs.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n\nclass SimpleQueryToolNodeMapping(BaseQueryToolNodeMapping):\n    \"\"\"Simple query tool mapping.\"\"\"\n\n    def __init__(self, objs: Optional[Sequence[QueryEngineTool]] = None) -> None:\n        objs = objs or []\n        self._tools = {tool.metadata.name: tool for tool in objs}\n\n    def validate_object(self, obj: QueryEngineTool) -> None:\n        if not isinstance(obj, QueryEngineTool):\n            raise ValueError(f\"Object must be of type {QueryEngineTool}\")\n\n    @classmethod\n    def from_objects(\n        cls, objs: Sequence[QueryEngineTool], *args: Any, **kwargs: Any\n    ) -> \"BaseObjectNodeMapping\":\n        return cls(objs)\n\n    def _add_object(self, tool: QueryEngineTool) -> None:\n        if tool.metadata.name is None:\n            raise ValueError(\"Tool name must be set\")\n        self._tools[tool.metadata.name] = tool\n\n    def to_node(self, obj: QueryEngineTool) -> TextNode:\n        \"\"\"To node.\"\"\"\n        return convert_tool_to_node(obj)\n\n    def _from_node(self, node: BaseNode) -> QueryEngineTool:\n        \"\"\"From node.\"\"\"\n        if node.metadata is None:\n            raise ValueError(\"Metadata must be set\")\n        return self._tools[node.metadata[\"name\"]]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/objects/fn_node_mapping.py",
    "filename": "fn_node_mapping.py",
    "relpath": "objects/fn_node_mapping.py",
    "start_line": 1,
    "end_line": 64,
    "length": 64,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node",
      "obj_node_mapping",
      "persist",
      "from_persist_dir"
    ],
    "chunk_class_names": [
      "FnNodeMapping"
    ],
    "document_function_names": [
      "__init__",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node",
      "obj_node_mapping",
      "persist",
      "from_persist_dir"
    ],
    "document_class_names": [
      "FnNodeMapping"
    ],
    "content": "\"\"\"Table node mapping.\"\"\"\n\nfrom typing import Any, Callable, Dict, Sequence\n\nfrom llama_index.core.objects.base_node_mapping import (\n    DEFAULT_PERSIST_DIR,\n    DEFAULT_PERSIST_FNAME,\n    BaseObjectNodeMapping,\n)\nfrom llama_index.core.schema import BaseNode\n\n\nclass FnNodeMapping(BaseObjectNodeMapping[Any]):\n    \"\"\"Fn node mapping.\"\"\"\n\n    def __init__(\n        self,\n        from_node_fn: Callable[[BaseNode], Any],\n        to_node_fn: Callable[[Any], BaseNode],\n    ) -> None:\n        self._to_node_fn = to_node_fn\n        self._from_node_fn = from_node_fn\n\n    @classmethod\n    def from_objects(  # type: ignore\n        cls,\n        objs: Sequence[Any],\n        from_node_fn: Callable[[BaseNode], Any],\n        to_node_fn: Callable[[Any], BaseNode],\n        *args: Any,\n        **kwargs: Any,\n    ) -> \"BaseObjectNodeMapping\":\n        \"\"\"Initialize node mapping.\"\"\"\n        return cls(from_node_fn, to_node_fn)\n\n    def _add_object(self, obj: Any) -> None:\n        \"\"\"Add object. NOTE: unused.\"\"\"\n\n    def to_node(self, obj: Any) -> BaseNode:\n        \"\"\"To node.\"\"\"\n        return self._to_node_fn(obj)\n\n    def _from_node(self, node: BaseNode) -> Any:\n        \"\"\"From node.\"\"\"\n        return self._from_node_fn(node)\n\n    @property\n    def obj_node_mapping(self) -> Dict[int, Any]:\n        \"\"\"The mapping data structure between node and object.\"\"\"\n        raise NotImplementedError(\"FnNodeMapping does not support obj_node_mapping\")\n\n    def persist(\n        self, persist_dir: str = ..., obj_node_mapping_fname: str = ...\n    ) -> None:\n        \"\"\"Persist objs.\"\"\"\n        raise NotImplementedError(\"FnNodeMapping does not support persist method.\")\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> \"FnNodeMapping\":\n        raise NotImplementedError(\"FnNodeMapping does not support persist method.\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/objects/table_node_mapping.py",
    "filename": "table_node_mapping.py",
    "relpath": "objects/table_node_mapping.py",
    "start_line": 1,
    "end_line": 97,
    "length": 97,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node",
      "obj_node_mapping",
      "persist",
      "from_persist_dir"
    ],
    "chunk_class_names": [
      "SQLTableSchema",
      "SQLTableNodeMapping"
    ],
    "document_function_names": [
      "__init__",
      "from_objects",
      "_add_object",
      "to_node",
      "_from_node",
      "obj_node_mapping",
      "persist",
      "from_persist_dir"
    ],
    "document_class_names": [
      "SQLTableSchema",
      "SQLTableNodeMapping"
    ],
    "content": "\"\"\"Table node mapping.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.objects.base_node_mapping import (\n    DEFAULT_PERSIST_DIR,\n    DEFAULT_PERSIST_FNAME,\n    BaseObjectNodeMapping,\n)\nfrom llama_index.core.schema import BaseNode, TextNode\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\n\n\nclass SQLTableSchema(BaseModel):\n    \"\"\"Lightweight representation of a SQL table.\"\"\"\n\n    table_name: str\n    context_str: Optional[str] = None\n\n\nclass SQLTableNodeMapping(BaseObjectNodeMapping[SQLTableSchema]):\n    \"\"\"SQL Table node mapping.\"\"\"\n\n    def __init__(self, sql_database: SQLDatabase) -> None:\n        self._sql_database = sql_database\n\n    @classmethod\n    def from_objects(\n        cls,\n        objs: Sequence[SQLTableSchema],\n        *args: Any,\n        sql_database: Optional[SQLDatabase] = None,\n        **kwargs: Any,\n    ) -> \"BaseObjectNodeMapping\":\n        \"\"\"Initialize node mapping.\"\"\"\n        if sql_database is None:\n            raise ValueError(\"Must provide sql_database\")\n        # ignore objs, since we are building from sql_database\n        return cls(sql_database)\n\n    def _add_object(self, obj: SQLTableSchema) -> None:\n        raise NotImplementedError\n\n    def to_node(self, obj: SQLTableSchema) -> TextNode:\n        \"\"\"To node.\"\"\"\n        # taken from existing schema logic\n        table_text = (\n            f\"Schema of table {obj.table_name}:\\n\"\n            f\"{self._sql_database.get_single_table_info(obj.table_name)}\\n\"\n        )\n\n        metadata = {\"name\": obj.table_name}\n\n        if obj.context_str is not None:\n            table_text += f\"Context of table {obj.table_name}:\\n\"\n            table_text += obj.context_str\n            metadata[\"context\"] = obj.context_str\n\n        table_identity = f\"{obj.table_name}{obj.context_str}\"\n\n        return TextNode(\n            id_=str(hash(table_identity)),\n            text=table_text,\n            metadata=metadata,\n            excluded_embed_metadata_keys=[\"name\", \"context\"],\n            excluded_llm_metadata_keys=[\"name\", \"context\"],\n        )\n\n    def _from_node(self, node: BaseNode) -> SQLTableSchema:\n        \"\"\"From node.\"\"\"\n        if node.metadata is None:\n            raise ValueError(\"Metadata must be set\")\n        return SQLTableSchema(\n            table_name=node.metadata[\"name\"], context_str=node.metadata.get(\"context\")\n        )\n\n    @property\n    def obj_node_mapping(self) -> Dict[int, Any]:\n        \"\"\"The mapping data structure between node and object.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    def persist(\n        self, persist_dir: str = ..., obj_node_mapping_fname: str = ...\n    ) -> None:\n        \"\"\"Persist objs.\"\"\"\n        raise NotImplementedError(\"Subclasses should implement this!\")\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> \"SQLTableNodeMapping\":\n        raise NotImplementedError(\n            \"This object node mapping does not support persist method.\"\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/objects/__init__.py",
    "filename": "__init__.py",
    "relpath": "objects/__init__.py",
    "start_line": 1,
    "end_line": 22,
    "length": 22,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"LlamaIndex objects.\"\"\"\n\nfrom llama_index.core.objects.base import ObjectIndex, ObjectRetriever\nfrom llama_index.core.objects.base_node_mapping import SimpleObjectNodeMapping\nfrom llama_index.core.objects.table_node_mapping import (\n    SQLTableNodeMapping,\n    SQLTableSchema,\n)\nfrom llama_index.core.objects.tool_node_mapping import (\n    SimpleQueryToolNodeMapping,\n    SimpleToolNodeMapping,\n)\n\n__all__ = [\n    \"ObjectRetriever\",\n    \"ObjectIndex\",\n    \"SimpleObjectNodeMapping\",\n    \"SimpleToolNodeMapping\",\n    \"SimpleQueryToolNodeMapping\",\n    \"SQLTableNodeMapping\",\n    \"SQLTableSchema\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/objects/utils.py",
    "filename": "utils.py",
    "relpath": "objects/utils.py",
    "start_line": 1,
    "end_line": 22,
    "length": 22,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_object_mapping"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_object_mapping"
    ],
    "document_class_names": [],
    "content": "from typing import Any, Callable, Optional, Sequence\n\nfrom llama_index.core.tools import BaseTool\nfrom llama_index.core.objects.base import SimpleObjectNodeMapping\nfrom llama_index.core.objects.base_node_mapping import BaseObjectNodeMapping\nfrom llama_index.core.objects.fn_node_mapping import FnNodeMapping\nfrom llama_index.core.objects.tool_node_mapping import SimpleToolNodeMapping\nfrom llama_index.core.schema import BaseNode\n\n\ndef get_object_mapping(\n    objects: Sequence[Any],\n    from_node_fn: Optional[Callable[[BaseNode], Any]] = None,\n    to_node_fn: Optional[Callable[[Any], BaseNode]] = None,\n) -> BaseObjectNodeMapping:\n    \"\"\"Get object mapping according to object.\"\"\"\n    if from_node_fn is not None and to_node_fn is not None:\n        return FnNodeMapping.from_objects(objects, from_node_fn, to_node_fn)\n    elif all(isinstance(obj, BaseTool) for obj in objects):\n        return SimpleToolNodeMapping.from_objects(objects)\n    else:\n        return SimpleObjectNodeMapping.from_objects(objects)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/objects/base_node_mapping.py",
    "filename": "base_node_mapping.py",
    "relpath": "objects/base_node_mapping.py",
    "start_line": 1,
    "end_line": 176,
    "length": 176,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_objects",
      "validate_object",
      "add_object",
      "obj_node_mapping",
      "_add_object",
      "to_node",
      "to_nodes",
      "from_node",
      "_from_node",
      "persist",
      "from_persist_dir",
      "__init__",
      "from_objects",
      "obj_node_mapping",
      "obj_node_mapping",
      "_add_object",
      "to_node",
      "_from_node",
      "persist",
      "from_persist_dir"
    ],
    "chunk_class_names": [
      "BaseObjectNodeMapping",
      "SimpleObjectNodeMapping"
    ],
    "document_function_names": [
      "from_objects",
      "validate_object",
      "add_object",
      "obj_node_mapping",
      "_add_object",
      "to_node",
      "to_nodes",
      "from_node",
      "_from_node",
      "persist",
      "from_persist_dir",
      "__init__",
      "from_objects",
      "obj_node_mapping",
      "obj_node_mapping",
      "_add_object",
      "to_node",
      "_from_node",
      "persist",
      "from_persist_dir"
    ],
    "document_class_names": [
      "BaseObjectNodeMapping",
      "SimpleObjectNodeMapping"
    ],
    "content": "\"\"\"Base object types.\"\"\"\n\nimport os\nimport pickle\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\nfrom llama_index.core.storage.storage_context import DEFAULT_PERSIST_DIR\nfrom llama_index.core.utils import concat_dirs\n\nDEFAULT_PERSIST_FNAME = \"object_node_mapping.pickle\"\n\nOT = TypeVar(\"OT\")\n\n\nclass BaseObjectNodeMapping(Generic[OT]):\n    \"\"\"Base object node mapping.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def from_objects(\n        cls, objs: Sequence[OT], *args: Any, **kwargs: Any\n    ) -> \"BaseObjectNodeMapping\":\n        \"\"\"Initialize node mapping from a list of objects.\n\n        Only needs to be specified if the node mapping\n        needs to be initialized with a list of objects.\n\n        \"\"\"\n\n    def validate_object(self, obj: OT) -> None:\n        \"\"\"Validate object.\"\"\"\n\n    def add_object(self, obj: OT) -> None:\n        \"\"\"Add object.\n\n        Only needs to be specified if the node mapping\n        needs to be initialized with a list of objects.\n\n        \"\"\"\n        self.validate_object(obj)\n        self._add_object(obj)\n\n    @property\n    @abstractmethod\n    def obj_node_mapping(self) -> Dict[Any, Any]:\n        \"\"\"The mapping data structure between node and object.\"\"\"\n\n    @abstractmethod\n    def _add_object(self, obj: OT) -> None:\n        \"\"\"Add object.\n\n        Only needs to be specified if the node mapping\n        needs to be initialized with a list of objects.\n\n        \"\"\"\n\n    @abstractmethod\n    def to_node(self, obj: OT) -> BaseNode:\n        \"\"\"To node.\"\"\"\n\n    def to_nodes(self, objs: Sequence[OT]) -> Sequence[BaseNode]:\n        return [self.to_node(obj) for obj in objs]\n\n    def from_node(self, node: BaseNode) -> OT:\n        \"\"\"From node.\"\"\"\n        obj = self._from_node(node)\n        self.validate_object(obj)\n        return obj\n\n    @abstractmethod\n    def _from_node(self, node: BaseNode) -> OT:\n        \"\"\"From node.\"\"\"\n\n    @abstractmethod\n    def persist(\n        self,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> None:\n        \"\"\"Persist objs.\"\"\"\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> \"BaseObjectNodeMapping[OT]\":\n        \"\"\"Load from serialization.\"\"\"\n        obj_node_mapping = None\n        errors = []\n        for cls in BaseObjectNodeMapping.__subclasses__():  # type: ignore\n            try:\n                obj_node_mapping = cls.from_persist_dir(\n                    persist_dir=persist_dir,\n                    obj_node_mapping_fname=obj_node_mapping_fname,\n                )\n                break\n            except (NotImplementedError, pickle.PickleError) as err:\n                # raise unhandled exception otherwise\n                errors.append(err)\n        if obj_node_mapping:\n            return obj_node_mapping\n        else:\n            raise Exception(errors)\n\n\nclass SimpleObjectNodeMapping(BaseObjectNodeMapping[Any]):\n    \"\"\"General node mapping that works for any obj.\n\n    More specifically, any object with a meaningful string representation.\n\n    \"\"\"\n\n    def __init__(self, objs: Optional[Sequence[Any]] = None) -> None:\n        objs = objs or []\n        for obj in objs:\n            self.validate_object(obj)\n        self._objs = {hash(str(obj)): obj for obj in objs}\n\n    @classmethod\n    def from_objects(\n        cls, objs: Sequence[Any], *args: Any, **kwargs: Any\n    ) -> \"SimpleObjectNodeMapping\":\n        return cls(objs)\n\n    @property\n    def obj_node_mapping(self) -> Dict[int, Any]:\n        return self._objs\n\n    @obj_node_mapping.setter\n    def obj_node_mapping(self, mapping: Dict[int, Any]) -> None:\n        self._objs = mapping\n\n    def _add_object(self, obj: Any) -> None:\n        self._objs[hash(str(obj))] = obj\n\n    def to_node(self, obj: Any) -> TextNode:\n        return TextNode(id_=str(hash(str(obj))), text=str(obj))\n\n    def _from_node(self, node: BaseNode) -> Any:\n        return self._objs[hash(node.get_content(metadata_mode=MetadataMode.NONE))]\n\n    def persist(\n        self,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> None:\n        \"\"\"Persist object node mapping.\n\n        NOTE: This may fail depending on whether the object types are\n        pickle-able.\n        \"\"\"\n        if not os.path.exists(persist_dir):\n            os.makedirs(persist_dir)\n        obj_node_mapping_path = concat_dirs(persist_dir, obj_node_mapping_fname)\n        try:\n            with open(obj_node_mapping_path, \"wb\") as f:\n                pickle.dump(self, f)\n        except pickle.PickleError as err:\n            raise ValueError(\"Objs is not pickleable\") from err\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        obj_node_mapping_fname: str = DEFAULT_PERSIST_FNAME,\n    ) -> \"SimpleObjectNodeMapping\":\n        obj_node_mapping_path = concat_dirs(persist_dir, obj_node_mapping_fname)\n        try:\n            with open(obj_node_mapping_path, \"rb\") as f:\n                simple_object_node_mapping = pickle.load(f)\n        except pickle.PickleError as err:\n            raise ValueError(\"Objs cannot be loaded.\") from err\n        return simple_object_node_mapping"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/base.py",
    "filename": "base.py",
    "relpath": "indices/base.py",
    "start_line": 1,
    "end_line": 22,
    "length": 22,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_documents",
      "index_struct",
      "index_id",
      "set_index_id",
      "docstore",
      "storage_context",
      "summary",
      "summary",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "insert",
      "_delete_node",
      "delete_nodes",
      "delete",
      "delete_ref_doc",
      "update",
      "update_ref_doc",
      "refresh",
      "refresh_ref_docs",
      "ref_doc_info",
      "as_retriever",
      "as_query_engine",
      "as_chat_engine"
    ],
    "document_class_names": [
      "BaseIndex"
    ],
    "content": "\"\"\"Base index classes.\"\"\"\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, Generic, List, Optional, Sequence, Type, TypeVar\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.chat_engine.types import BaseChatEngine, ChatMode\nfrom llama_index.core.data_structs.data_structs import IndexStruct\nfrom llama_index.core.ingestion import run_transformations\nfrom llama_index.core.llms.utils import LLMType, resolve_llm\nfrom llama_index.core.schema import BaseNode, Document, IndexNode, TransformComponent\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore.types import BaseDocumentStore, RefDocInfo\nfrom llama_index.core.storage.storage_context import StorageContext\n\nIS = TypeVar(\"IS\", bound=IndexStruct)\nIndexType = TypeVar(\"IndexType\", bound=\"BaseIndex\")\n\nlogger = logging.getLogger(__name__)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/base.py",
    "filename": "base.py",
    "relpath": "indices/base.py",
    "start_line": 22,
    "end_line": 25,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "BaseIndex"
    ],
    "document_function_names": [
      "__init__",
      "from_documents",
      "index_struct",
      "index_id",
      "set_index_id",
      "docstore",
      "storage_context",
      "summary",
      "summary",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "insert",
      "_delete_node",
      "delete_nodes",
      "delete",
      "delete_ref_doc",
      "update",
      "update_ref_doc",
      "refresh",
      "refresh_ref_docs",
      "ref_doc_info",
      "as_retriever",
      "as_query_engine",
      "as_chat_engine"
    ],
    "document_class_names": [
      "BaseIndex"
    ],
    "content": "class BaseIndex(Generic[IS], ABC):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/base.py",
    "filename": "base.py",
    "relpath": "indices/base.py",
    "start_line": 25,
    "end_line": 385,
    "length": 361,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_documents",
      "index_struct",
      "index_id",
      "set_index_id",
      "docstore",
      "storage_context",
      "summary",
      "summary",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "insert",
      "_delete_node",
      "delete_nodes",
      "delete",
      "delete_ref_doc",
      "update",
      "update_ref_doc",
      "refresh",
      "refresh_ref_docs",
      "ref_doc_info",
      "as_retriever",
      "as_query_engine"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_documents",
      "index_struct",
      "index_id",
      "set_index_id",
      "docstore",
      "storage_context",
      "summary",
      "summary",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "insert",
      "_delete_node",
      "delete_nodes",
      "delete",
      "delete_ref_doc",
      "update",
      "update_ref_doc",
      "refresh",
      "refresh_ref_docs",
      "ref_doc_info",
      "as_retriever",
      "as_query_engine",
      "as_chat_engine"
    ],
    "document_class_names": [
      "BaseIndex"
    ],
    "content": "\"\"\"Base LlamaIndex.\n\n    Args:\n        nodes (List[Node]): List of nodes to index\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n    \"\"\"\n\n    index_struct_cls: Type[IS]\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        objects: Optional[Sequence[IndexNode]] = None,\n        index_struct: Optional[IS] = None,\n        storage_context: Optional[StorageContext] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize with parameters.\"\"\"\n        if index_struct is None and nodes is None and objects is None:\n            raise ValueError(\"One of nodes, objects, or index_struct must be provided.\")\n        if index_struct is not None and nodes is not None and len(nodes) >= 1:\n            raise ValueError(\"Only one of nodes or index_struct can be provided.\")\n        # This is to explicitly make sure that the old UX is not used\n        if nodes is not None and len(nodes) >= 1 and not isinstance(nodes[0], BaseNode):\n            if isinstance(nodes[0], Document):\n                raise ValueError(\n                    \"The constructor now takes in a list of Node objects. \"\n                    \"Since you are passing in a list of Document objects, \"\n                    \"please use `from_documents` instead.\"\n                )\n            else:\n                raise ValueError(\"nodes must be a list of Node objects.\")\n\n        self._storage_context = storage_context or StorageContext.from_defaults()\n        self._docstore = self._storage_context.docstore\n        self._show_progress = show_progress\n        self._vector_store = self._storage_context.vector_store\n        self._graph_store = self._storage_context.graph_store\n        self._callback_manager = callback_manager or Settings.callback_manager\n\n        objects = objects or []\n        self._object_map = {obj.index_id: obj.obj for obj in objects}\n        for obj in objects:\n            obj.obj = None  # clear the object to avoid serialization issues\n\n        with self._callback_manager.as_trace(\"index_construction\"):\n            if index_struct is None:\n                nodes = nodes or []\n                index_struct = self.build_index_from_nodes(\n                    nodes + objects,  # type: ignore\n                    **kwargs,  # type: ignore\n                )\n            self._index_struct = index_struct\n            self._storage_context.index_store.add_index_struct(self._index_struct)\n\n        self._transformations = transformations or Settings.transformations\n\n    @classmethod\n    def from_documents(\n        cls: Type[IndexType],\n        documents: Sequence[Document],\n        storage_context: Optional[StorageContext] = None,\n        show_progress: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        **kwargs: Any,\n    ) -> IndexType:\n        \"\"\"Create index from documents.\n\n        Args:\n            documents (Optional[Sequence[BaseDocument]]): List of documents to\n                build the index from.\n\n        \"\"\"\n        storage_context = storage_context or StorageContext.from_defaults()\n        docstore = storage_context.docstore\n        callback_manager = callback_manager or Settings.callback_manager\n        transformations = transformations or Settings.transformations\n\n        with callback_manager.as_trace(\"index_construction\"):\n            for doc in documents:\n                docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n\n            nodes = run_transformations(\n                documents,  # type: ignore\n                transformations,\n                show_progress=show_progress,\n                **kwargs,\n            )\n\n            return cls(\n                nodes=nodes,\n                storage_context=storage_context,\n                callback_manager=callback_manager,\n                show_progress=show_progress,\n                transformations=transformations,\n                **kwargs,\n            )\n\n    @property\n    def index_struct(self) -> IS:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct\n\n    @property\n    def index_id(self) -> str:\n        \"\"\"Get the index struct.\"\"\"\n        return self._index_struct.index_id\n\n    def set_index_id(self, index_id: str) -> None:\n        \"\"\"Set the index id.\n\n        NOTE: if you decide to set the index_id on the index_struct manually,\n        you will need to explicitly call `add_index_struct` on the `index_store`\n        to update the index store.\n\n        Args:\n            index_id (str): Index id to set.\n\n        \"\"\"\n        # delete the old index struct\n        old_id = self._index_struct.index_id\n        self._storage_context.index_store.delete_index_struct(old_id)\n        # add the new index struct\n        self._index_struct.index_id = index_id\n        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    @property\n    def docstore(self) -> BaseDocumentStore:\n        \"\"\"Get the docstore corresponding to the index.\"\"\"\n        return self._docstore\n\n    @property\n    def storage_context(self) -> StorageContext:\n        return self._storage_context\n\n    @property\n    def summary(self) -> str:\n        return str(self._index_struct.summary)\n\n    @summary.setter\n    def summary(self, new_summary: str) -> None:\n        self._index_struct.summary = new_summary\n        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    @abstractmethod\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> IS:\n        \"\"\"Build the index from nodes.\"\"\"\n\n    def build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> IS:\n        \"\"\"Build the index from nodes.\"\"\"\n        self._docstore.add_documents(nodes, allow_update=True)\n        return self._build_index_from_nodes(nodes, **build_kwargs)\n\n    @abstractmethod\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Index-specific logic for inserting nodes to the index struct.\"\"\"\n\n    def insert_nodes(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert nodes.\"\"\"\n        for node in nodes:\n            if isinstance(node, IndexNode):\n                try:\n                    node.dict()\n                except ValueError:\n                    self._object_map[node.index_id] = node.obj\n                    node.obj = None\n\n        with self._callback_manager.as_trace(\"insert_nodes\"):\n            self.docstore.add_documents(nodes, allow_update=True)\n            self._insert(nodes, **insert_kwargs)\n            self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    def insert(self, document: Document, **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        with self._callback_manager.as_trace(\"insert\"):\n            nodes = run_transformations(\n                [document],\n                self._transformations,\n                show_progress=self._show_progress,\n                **insert_kwargs,\n            )\n\n            self.insert_nodes(nodes, **insert_kwargs)\n            self.docstore.set_document_hash(document.get_doc_id(), document.hash)\n\n    @abstractmethod\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n\n    def delete_nodes(\n        self,\n        node_ids: List[str],\n        delete_from_docstore: bool = False,\n        **delete_kwargs: Any,\n    ) -> None:\n        \"\"\"Delete a list of nodes from the index.\n\n        Args:\n            doc_ids (List[str]): A list of doc_ids from the nodes to delete\n\n        \"\"\"\n        for node_id in node_ids:\n            self._delete_node(node_id, **delete_kwargs)\n            if delete_from_docstore:\n                self.docstore.delete_document(node_id, raise_error=False)\n\n        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a document from the index.\n        All nodes in the index related to the index will be deleted.\n\n        Args:\n            doc_id (str): A doc_id of the ingested document\n\n        \"\"\"\n        logger.warning(\n            \"delete() is now deprecated, please refer to delete_ref_doc() to delete \"\n            \"ingested documents+nodes or delete_nodes to delete a list of nodes.\"\n        )\n        self.delete_ref_doc(doc_id)\n\n    def delete_ref_doc(\n        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any\n    ) -> None:\n        \"\"\"Delete a document and it's nodes by using ref_doc_id.\"\"\"\n        ref_doc_info = self.docstore.get_ref_doc_info(ref_doc_id)\n        if ref_doc_info is None:\n            logger.warning(f\"ref_doc_id {ref_doc_id} not found, nothing deleted.\")\n            return\n\n        self.delete_nodes(\n            ref_doc_info.node_ids,\n            delete_from_docstore=False,\n            **delete_kwargs,\n        )\n\n        if delete_from_docstore:\n            self.docstore.delete_ref_doc(ref_doc_id, raise_error=False)\n\n    def update(self, document: Document, **update_kwargs: Any) -> None:\n        \"\"\"Update a document and it's corresponding nodes.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document (Union[BaseDocument, BaseIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        logger.warning(\n            \"update() is now deprecated, please refer to update_ref_doc() to update \"\n            \"ingested documents+nodes.\"\n        )\n        self.update_ref_doc(document, **update_kwargs)\n\n    def update_ref_doc(self, document: Document, **update_kwargs: Any) -> None:\n        \"\"\"Update a document and it's corresponding nodes.\n\n        This is equivalent to deleting the document and then inserting it again.\n\n        Args:\n            document (Union[BaseDocument, BaseIndex]): document to update\n            insert_kwargs (Dict): kwargs to pass to insert\n            delete_kwargs (Dict): kwargs to pass to delete\n\n        \"\"\"\n        with self._callback_manager.as_trace(\"update\"):\n            self.delete_ref_doc(\n                document.get_doc_id(),\n                delete_from_docstore=True,\n                **update_kwargs.pop(\"delete_kwargs\", {}),\n            )\n            self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n\n    def refresh(\n        self, documents: Sequence[Document], **update_kwargs: Any\n    ) -> List[bool]:\n        \"\"\"Refresh an index with documents that have changed.\n\n        This allows users to save LLM and Embedding model calls, while only\n        updating documents that have any changes in text or metadata. It\n        will also insert any documents that previously were not stored.\n        \"\"\"\n        logger.warning(\n            \"refresh() is now deprecated, please refer to refresh_ref_docs() to \"\n            \"refresh ingested documents+nodes with an updated list of documents.\"\n        )\n        return self.refresh_ref_docs(documents, **update_kwargs)\n\n    def refresh_ref_docs(\n        self, documents: Sequence[Document], **update_kwargs: Any\n    ) -> List[bool]:\n        \"\"\"Refresh an index with documents that have changed.\n\n        This allows users to save LLM and Embedding model calls, while only\n        updating documents that have any changes in text or metadata. It\n        will also insert any documents that previously were not stored.\n        \"\"\"\n        with self._callback_manager.as_trace(\"refresh\"):\n            refreshed_documents = [False] * len(documents)\n            for i, document in enumerate(documents):\n                existing_doc_hash = self._docstore.get_document_hash(\n                    document.get_doc_id()\n                )\n                if existing_doc_hash is None:\n                    self.insert(document, **update_kwargs.pop(\"insert_kwargs\", {}))\n                    refreshed_documents[i] = True\n                elif existing_doc_hash != document.hash:\n                    self.update_ref_doc(\n                        document, **update_kwargs.pop(\"update_kwargs\", {})\n                    )\n                    refreshed_documents[i] = True\n\n            return refreshed_documents\n\n    @property\n    @abstractmethod\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        ...\n\n    @abstractmethod\n    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\n        ...\n\n    def as_query_engine(\n        self, llm: Optional[LLMType] = None, **kwargs: Any\n    ) -> BaseQueryEngine:\n        \"\"\"Convert the index to a query engine.\n\n        Calls `index.as_retriever(**kwargs)` to get the retriever and then wraps it in a\n        `RetrieverQueryEngine.from_args(retriever, **kwrags)` call.\n        \"\"\"\n        # NOTE: lazy import\n        from llama_index.core.query_engine.retriever_query_engine import (\n            RetrieverQueryEngine,\n        )\n\n        retriever = self.as_retriever(**kwargs)\n        llm = (\n            resolve_llm(llm, callback_manager=self._callback_manager)\n            if llm\n            else Settings.llm\n        )\n\n        return RetrieverQueryEngine.from_args(\n            retriever,\n            llm=llm,\n            **kwargs,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/base.py",
    "filename": "base.py",
    "relpath": "indices/base.py",
    "start_line": 385,
    "end_line": 466,
    "length": 82,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "as_chat_engine"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_documents",
      "index_struct",
      "index_id",
      "set_index_id",
      "docstore",
      "storage_context",
      "summary",
      "summary",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "insert",
      "_delete_node",
      "delete_nodes",
      "delete",
      "delete_ref_doc",
      "update",
      "update_ref_doc",
      "refresh",
      "refresh_ref_docs",
      "ref_doc_info",
      "as_retriever",
      "as_query_engine",
      "as_chat_engine"
    ],
    "document_class_names": [
      "BaseIndex"
    ],
    "content": "def as_chat_engine(\n        self,\n        chat_mode: ChatMode = ChatMode.BEST,\n        llm: Optional[LLMType] = None,\n        **kwargs: Any,\n    ) -> BaseChatEngine:\n        \"\"\"Convert the index to a chat engine.\n\n        Calls `index.as_query_engine(llm=llm, **kwargs)` to get the query engine and then\n        wraps it in a chat engine based on the chat mode.\n\n        Chat modes:\n            - `ChatMode.BEST` (default): Chat engine that uses an agent (react or openai) with a query engine tool\n            - `ChatMode.CONTEXT`: Chat engine that uses a retriever to get context\n            - `ChatMode.CONDENSE_QUESTION`: Chat engine that condenses questions\n            - `ChatMode.CONDENSE_PLUS_CONTEXT`: Chat engine that condenses questions and uses a retriever to get context\n            - `ChatMode.SIMPLE`: Simple chat engine that uses the LLM directly\n            - `ChatMode.REACT`: Chat engine that uses a react agent with a query engine tool\n            - `ChatMode.OPENAI`: Chat engine that uses an openai agent with a query engine tool\n        \"\"\"\n        llm = (\n            resolve_llm(llm, callback_manager=self._callback_manager)\n            if llm\n            else Settings.llm\n        )\n\n        query_engine = self.as_query_engine(llm=llm, **kwargs)\n\n        # resolve chat mode\n        if chat_mode in [ChatMode.REACT, ChatMode.OPENAI, ChatMode.BEST]:\n            # use an agent with query engine tool in these chat modes\n            # NOTE: lazy import\n            from llama_index.core.agent import AgentRunner\n            from llama_index.core.tools.query_engine import QueryEngineTool\n\n            # convert query engine to tool\n            query_engine_tool = QueryEngineTool.from_defaults(query_engine=query_engine)\n\n            return AgentRunner.from_llm(\n                tools=[query_engine_tool],\n                llm=llm,\n                **kwargs,\n            )\n\n        if chat_mode == ChatMode.CONDENSE_QUESTION:\n            # NOTE: lazy import\n            from llama_index.core.chat_engine import CondenseQuestionChatEngine\n\n            return CondenseQuestionChatEngine.from_defaults(\n                query_engine=query_engine,\n                llm=llm,\n                **kwargs,\n            )\n        elif chat_mode == ChatMode.CONTEXT:\n            from llama_index.core.chat_engine import ContextChatEngine\n\n            return ContextChatEngine.from_defaults(\n                retriever=self.as_retriever(**kwargs),\n                llm=llm,\n                **kwargs,\n            )\n\n        elif chat_mode == ChatMode.CONDENSE_PLUS_CONTEXT:\n            from llama_index.core.chat_engine import CondensePlusContextChatEngine\n\n            return CondensePlusContextChatEngine.from_defaults(\n                retriever=self.as_retriever(**kwargs),\n                llm=llm,\n                **kwargs,\n            )\n\n        elif chat_mode == ChatMode.SIMPLE:\n            from llama_index.core.chat_engine import SimpleChatEngine\n\n            return SimpleChatEngine.from_defaults(\n                llm=llm,\n                **kwargs,\n            )\n        else:\n            raise ValueError(f\"Unknown chat mode: {chat_mode}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/base.py",
    "filename": "base.py",
    "relpath": "indices/base.py",
    "start_line": 22,
    "end_line": 470,
    "length": 449,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_documents",
      "index_struct",
      "index_id",
      "set_index_id",
      "docstore",
      "storage_context",
      "summary",
      "summary",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "insert",
      "_delete_node",
      "delete_nodes",
      "delete",
      "delete_ref_doc",
      "update",
      "update_ref_doc",
      "refresh",
      "refresh_ref_docs",
      "ref_doc_info",
      "as_retriever",
      "as_query_engine",
      "as_chat_engine"
    ],
    "document_class_names": [
      "BaseIndex"
    ],
    "content": "# legacy\nBaseGPTIndex = BaseIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/loading.py",
    "filename": "loading.py",
    "relpath": "indices/loading.py",
    "start_line": 1,
    "end_line": 100,
    "length": 100,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_index_from_storage",
      "load_indices_from_storage",
      "load_graph_from_storage"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "load_index_from_storage",
      "load_indices_from_storage",
      "load_graph_from_storage"
    ],
    "document_class_names": [],
    "content": "import logging\nfrom typing import Any, List, Optional, Sequence\n\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.composability.graph import ComposableGraph\nfrom llama_index.core.indices.registry import INDEX_STRUCT_TYPE_TO_INDEX_CLASS\nfrom llama_index.core.storage.storage_context import StorageContext\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_index_from_storage(\n    storage_context: StorageContext,\n    index_id: Optional[str] = None,\n    **kwargs: Any,\n) -> BaseIndex:\n    \"\"\"Load index from storage context.\n\n    Args:\n        storage_context (StorageContext): storage context containing\n            docstore, index store and vector store.\n        index_id (Optional[str]): ID of the index to load.\n            Defaults to None, which assumes there's only a single index\n            in the index store and load it.\n        **kwargs: Additional keyword args to pass to the index constructors.\n    \"\"\"\n    index_ids: Optional[Sequence[str]]\n    if index_id is None:\n        index_ids = None\n    else:\n        index_ids = [index_id]\n\n    indices = load_indices_from_storage(storage_context, index_ids=index_ids, **kwargs)\n\n    if len(indices) == 0:\n        raise ValueError(\n            \"No index in storage context, check if you specified the right persist_dir.\"\n        )\n    elif len(indices) > 1:\n        raise ValueError(\n            f\"Expected to load a single index, but got {len(indices)} instead. \"\n            \"Please specify index_id.\"\n        )\n\n    return indices[0]\n\n\ndef load_indices_from_storage(\n    storage_context: StorageContext,\n    index_ids: Optional[Sequence[str]] = None,\n    **kwargs: Any,\n) -> List[BaseIndex]:\n    \"\"\"Load multiple indices from storage context.\n\n    Args:\n        storage_context (StorageContext): storage context containing\n            docstore, index store and vector store.\n        index_id (Optional[Sequence[str]]): IDs of the indices to load.\n            Defaults to None, which loads all indices in the index store.\n        **kwargs: Additional keyword args to pass to the index constructors.\n    \"\"\"\n    if index_ids is None:\n        logger.info(\"Loading all indices.\")\n        index_structs = storage_context.index_store.index_structs()\n    else:\n        logger.info(f\"Loading indices with ids: {index_ids}\")\n        index_structs = []\n        for index_id in index_ids:\n            index_struct = storage_context.index_store.get_index_struct(index_id)\n            if index_struct is None:\n                raise ValueError(f\"Failed to load index with ID {index_id}\")\n            index_structs.append(index_struct)\n\n    indices = []\n    for index_struct in index_structs:\n        type_ = index_struct.get_type()\n        index_cls = INDEX_STRUCT_TYPE_TO_INDEX_CLASS[type_]\n        index = index_cls(\n            index_struct=index_struct, storage_context=storage_context, **kwargs\n        )\n        indices.append(index)\n    return indices\n\n\ndef load_graph_from_storage(\n    storage_context: StorageContext,\n    root_id: str,\n    **kwargs: Any,\n) -> ComposableGraph:\n    \"\"\"Load composable graph from storage context.\n\n    Args:\n        storage_context (StorageContext): storage context containing\n            docstore, index store and vector store.\n        root_id (str): ID of the root index of the graph.\n        **kwargs: Additional keyword args to pass to the index constructors.\n    \"\"\"\n    indices = load_indices_from_storage(storage_context, index_ids=None, **kwargs)\n    all_indices = {index.index_id: index for index in indices}\n    return ComposableGraph(all_indices=all_indices, root_id=root_id)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/base_retriever.py",
    "filename": "base_retriever.py",
    "relpath": "indices/base_retriever.py",
    "start_line": 1,
    "end_line": 6,
    "length": 6,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# for backwards compatibility\nfrom llama_index.core.base.base_retriever import BaseRetriever\n\n__all__ = [\n    \"BaseRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/__init__.py",
    "start_line": 1,
    "end_line": 87,
    "length": 87,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"LlamaIndex data structures.\"\"\"\n\n# indices\nfrom llama_index.core.indices.composability.graph import ComposableGraph\nfrom llama_index.core.indices.document_summary import (\n    DocumentSummaryIndex,\n    GPTDocumentSummaryIndex,\n)\nfrom llama_index.core.indices.document_summary.base import DocumentSummaryIndex\nfrom llama_index.core.indices.empty.base import EmptyIndex, GPTEmptyIndex\nfrom llama_index.core.indices.keyword_table.base import (\n    GPTKeywordTableIndex,\n    KeywordTableIndex,\n)\nfrom llama_index.core.indices.keyword_table.rake_base import (\n    GPTRAKEKeywordTableIndex,\n    RAKEKeywordTableIndex,\n)\nfrom llama_index.core.indices.keyword_table.simple_base import (\n    GPTSimpleKeywordTableIndex,\n    SimpleKeywordTableIndex,\n)\nfrom llama_index.core.indices.knowledge_graph import (\n    KnowledgeGraphIndex,\n)\nfrom llama_index.core.indices.list import GPTListIndex, ListIndex, SummaryIndex\nfrom llama_index.core.indices.list.base import (\n    GPTListIndex,\n    ListIndex,\n    SummaryIndex,\n)\nfrom llama_index.core.indices.loading import (\n    load_graph_from_storage,\n    load_index_from_storage,\n    load_indices_from_storage,\n)\nfrom llama_index.core.indices.multi_modal import MultiModalVectorStoreIndex\nfrom llama_index.core.indices.struct_store.pandas import (\n    GPTPandasIndex,\n    PandasIndex,\n)\nfrom llama_index.core.indices.struct_store.sql import (\n    GPTSQLStructStoreIndex,\n    SQLStructStoreIndex,\n)\nfrom llama_index.core.indices.tree.base import GPTTreeIndex, TreeIndex\nfrom llama_index.core.indices.vector_store import (\n    GPTVectorStoreIndex,\n    VectorStoreIndex,\n)\n\nfrom llama_index.core.indices.property_graph.base import (\n    PropertyGraphIndex,\n)\n\n__all__ = [\n    \"load_graph_from_storage\",\n    \"load_index_from_storage\",\n    \"load_indices_from_storage\",\n    \"KeywordTableIndex\",\n    \"SimpleKeywordTableIndex\",\n    \"RAKEKeywordTableIndex\",\n    \"SummaryIndex\",\n    \"TreeIndex\",\n    \"DocumentSummaryIndex\",\n    \"KnowledgeGraphIndex\",\n    \"PandasIndex\",\n    \"VectorStoreIndex\",\n    \"SQLStructStoreIndex\",\n    \"MultiModalVectorStoreIndex\",\n    \"EmptyIndex\",\n    \"ComposableGraph\",\n    \"PropertyGraphIndex\",\n    # legacy\n    \"GPTKnowledgeGraphIndex\",\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n    \"GPTDocumentSummaryIndex\",\n    \"GPTListIndex\",\n    \"GPTTreeIndex\",\n    \"GPTPandasIndex\",\n    \"ListIndex\",\n    \"GPTVectorStoreIndex\",\n    \"GPTSQLStructStoreIndex\",\n    \"GPTEmptyIndex\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/utils.py",
    "filename": "utils.py",
    "relpath": "indices/utils.py",
    "start_line": 1,
    "end_line": 273,
    "length": 273,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_sorted_node_list",
      "extract_numbers_given_response",
      "expand_tokens_with_subtokens",
      "log_vector_store_query_result",
      "default_format_node_batch_fn",
      "default_parse_choice_select_answer_fn",
      "embed_nodes",
      "embed_image_nodes",
      "async_embed_nodes",
      "async_embed_image_nodes"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_sorted_node_list",
      "extract_numbers_given_response",
      "expand_tokens_with_subtokens",
      "log_vector_store_query_result",
      "default_format_node_batch_fn",
      "default_parse_choice_select_answer_fn",
      "embed_nodes",
      "embed_image_nodes",
      "async_embed_nodes",
      "async_embed_image_nodes"
    ],
    "document_class_names": [],
    "content": "\"\"\"Utilities for GPT indices.\"\"\"\nimport logging\nimport re\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.embeddings.multi_modal_base import MultiModalEmbedding\nfrom llama_index.core.schema import BaseNode, ImageNode, MetadataMode\nfrom llama_index.core.utils import globals_helper, truncate_text\nfrom llama_index.core.vector_stores.types import VectorStoreQueryResult\nfrom typing import Dict, List, Optional, Sequence, Set, Tuple\n\n_logger = logging.getLogger(__name__)\n\n\ndef get_sorted_node_list(node_dict: Dict[int, BaseNode]) -> List[BaseNode]:\n    \"\"\"Get sorted node list. Used by tree-strutured indices.\"\"\"\n    sorted_indices = sorted(node_dict.keys())\n    return [node_dict[index] for index in sorted_indices]\n\n\ndef extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:\n    \"\"\"Extract number given the GPT-generated response.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    numbers = re.findall(r\"\\d+\", response)\n    if len(numbers) == 0:\n        return None\n    else:\n        return numbers[:n]\n\n\ndef expand_tokens_with_subtokens(tokens: Set[str]) -> Set[str]:\n    \"\"\"Get subtokens from a list of tokens., filtering for stopwords.\"\"\"\n    results = set()\n    for token in tokens:\n        results.add(token)\n        sub_tokens = re.findall(r\"\\w+\", token)\n        if len(sub_tokens) > 1:\n            results.update({w for w in sub_tokens if w not in globals_helper.stopwords})\n\n    return results\n\n\ndef log_vector_store_query_result(\n    result: VectorStoreQueryResult, logger: Optional[logging.Logger] = None\n) -> None:\n    \"\"\"Log vector store query result.\"\"\"\n    logger = logger or _logger\n\n    assert result.ids is not None\n    assert result.nodes is not None\n    similarities = (\n        result.similarities\n        if result.similarities is not None and len(result.similarities) > 0\n        else [1.0 for _ in result.ids]\n    )\n\n    fmt_txts = []\n    for node_idx, node_similarity, node in zip(result.ids, similarities, result.nodes):\n        fmt_txt = f\"> [Node {node_idx}] [Similarity score: \\\n            {float(node_similarity):.6}] {truncate_text(node.get_content(), 100)}\"\n        fmt_txts.append(fmt_txt)\n    top_k_node_text = \"\\n\".join(fmt_txts)\n    logger.debug(f\"> Top {len(result.nodes)} nodes:\\n{top_k_node_text}\")\n\n\ndef default_format_node_batch_fn(\n    summary_nodes: List[BaseNode],\n) -> str:\n    \"\"\"Default format node batch function.\n\n    Assign each summary node a number, and format the batch of nodes.\n\n    \"\"\"\n    fmt_node_txts = []\n    for idx in range(len(summary_nodes)):\n        number = idx + 1\n        fmt_node_txts.append(\n            f\"Document {number}:\\n\"\n            f\"{summary_nodes[idx].get_content(metadata_mode=MetadataMode.LLM)}\"\n        )\n    return \"\\n\\n\".join(fmt_node_txts)\n\n\ndef default_parse_choice_select_answer_fn(\n    answer: str, num_choices: int, raise_error: bool = False\n) -> Tuple[List[int], List[float]]:\n    \"\"\"Default parse choice select answer function.\"\"\"\n    answer_lines = answer.split(\"\\n\")\n    answer_nums = []\n    answer_relevances = []\n    for answer_line in answer_lines:\n        line_tokens = answer_line.split(\",\")\n        if len(line_tokens) != 2:\n            if not raise_error:\n                continue\n            else:\n                raise ValueError(\n                    f\"Invalid answer line: {answer_line}. \"\n                    \"Answer line must be of the form: \"\n                    \"answer_num: <int>, answer_relevance: <float>\"\n                )\n        try:\n            answer_num = int(line_tokens[0].split(\":\")[1].strip())\n        except (IndexError, ValueError) as e:\n            if not raise_error:\n                continue\n            else:\n                raise ValueError(\n                    f\"Invalid answer line: {answer_line}. \"\n                    \"Answer line must be of the form: \"\n                    \"answer_num: <int>, answer_relevance: <float>\"\n                )\n        if answer_num > num_choices:\n            continue\n        answer_nums.append(answer_num)\n        # extract just the first digits after the colon.\n        try:\n            _answer_relevance = re.findall(\n                r\"\\d+\", line_tokens[1].split(\":\")[1].strip()\n            )[0]\n            answer_relevances.append(float(_answer_relevance))\n        except (IndexError, ValueError) as e:\n            if not raise_error:\n                continue\n            else:\n                raise ValueError(\n                    f\"Invalid answer line: {answer_line}. \"\n                    \"Answer line must be of the form: \"\n                    \"answer_num: <int>, answer_relevance: <float>\"\n                )\n    return answer_nums, answer_relevances\n\n\ndef embed_nodes(\n    nodes: Sequence[BaseNode], embed_model: BaseEmbedding, show_progress: bool = False\n) -> Dict[str, List[float]]:\n    \"\"\"Get embeddings of the given nodes, run embedding model if necessary.\n\n    Args:\n        nodes (Sequence[BaseNode]): The nodes to embed.\n        embed_model (BaseEmbedding): The embedding model to use.\n        show_progress (bool): Whether to show progress bar.\n\n    Returns:\n        Dict[str, List[float]]: A map from node id to embedding.\n    \"\"\"\n    id_to_embed_map: Dict[str, List[float]] = {}\n\n    texts_to_embed = []\n    ids_to_embed = []\n    for node in nodes:\n        if node.embedding is None:\n            ids_to_embed.append(node.node_id)\n            texts_to_embed.append(node.get_content(metadata_mode=MetadataMode.EMBED))\n        else:\n            id_to_embed_map[node.node_id] = node.embedding\n\n    new_embeddings = embed_model.get_text_embedding_batch(\n        texts_to_embed, show_progress=show_progress\n    )\n\n    for new_id, text_embedding in zip(ids_to_embed, new_embeddings):\n        id_to_embed_map[new_id] = text_embedding\n\n    return id_to_embed_map\n\n\ndef embed_image_nodes(\n    nodes: Sequence[ImageNode],\n    embed_model: MultiModalEmbedding,\n    show_progress: bool = False,\n) -> Dict[str, List[float]]:\n    \"\"\"Get image embeddings of the given nodes, run image embedding model if necessary.\n\n    Args:\n        nodes (Sequence[ImageNode]): The nodes to embed.\n        embed_model (MultiModalEmbedding): The embedding model to use.\n        show_progress (bool): Whether to show progress bar.\n\n    Returns:\n        Dict[str, List[float]]: A map from node id to embedding.\n    \"\"\"\n    id_to_embed_map: Dict[str, List[float]] = {}\n\n    images_to_embed = []\n    ids_to_embed = []\n    for node in nodes:\n        if node.embedding is None:\n            ids_to_embed.append(node.node_id)\n            images_to_embed.append(node.resolve_image())\n        else:\n            id_to_embed_map[node.node_id] = node.embedding\n\n    new_embeddings = embed_model.get_image_embedding_batch(\n        images_to_embed, show_progress=show_progress\n    )\n\n    for new_id, img_embedding in zip(ids_to_embed, new_embeddings):\n        id_to_embed_map[new_id] = img_embedding\n\n    return id_to_embed_map\n\n\nasync def async_embed_nodes(\n    nodes: Sequence[BaseNode], embed_model: BaseEmbedding, show_progress: bool = False\n) -> Dict[str, List[float]]:\n    \"\"\"Async get embeddings of the given nodes, run embedding model if necessary.\n\n    Args:\n        nodes (Sequence[BaseNode]): The nodes to embed.\n        embed_model (BaseEmbedding): The embedding model to use.\n        show_progress (bool): Whether to show progress bar.\n\n    Returns:\n        Dict[str, List[float]]: A map from node id to embedding.\n    \"\"\"\n    id_to_embed_map: Dict[str, List[float]] = {}\n\n    texts_to_embed = []\n    ids_to_embed = []\n    for node in nodes:\n        if node.embedding is None:\n            ids_to_embed.append(node.node_id)\n            texts_to_embed.append(node.get_content(metadata_mode=MetadataMode.EMBED))\n        else:\n            id_to_embed_map[node.node_id] = node.embedding\n\n    new_embeddings = await embed_model.aget_text_embedding_batch(\n        texts_to_embed, show_progress=show_progress\n    )\n\n    for new_id, text_embedding in zip(ids_to_embed, new_embeddings):\n        id_to_embed_map[new_id] = text_embedding\n\n    return id_to_embed_map\n\n\nasync def async_embed_image_nodes(\n    nodes: Sequence[ImageNode],\n    embed_model: MultiModalEmbedding,\n    show_progress: bool = False,\n) -> Dict[str, List[float]]:\n    \"\"\"Get image embeddings of the given nodes, run image embedding model if necessary.\n\n    Args:\n        nodes (Sequence[ImageNode]): The nodes to embed.\n        embed_model (MultiModalEmbedding): The embedding model to use.\n        show_progress (bool): Whether to show progress bar.\n\n    Returns:\n        Dict[str, List[float]]: A map from node id to embedding.\n    \"\"\"\n    id_to_embed_map: Dict[str, List[float]] = {}\n\n    images_to_embed = []\n    ids_to_embed = []\n    for node in nodes:\n        if node.embedding is None:\n            ids_to_embed.append(node.node_id)\n            images_to_embed.append(node.resolve_image())\n        else:\n            id_to_embed_map[node.node_id] = node.embedding\n\n    new_embeddings = await embed_model.aget_image_embedding_batch(\n        images_to_embed, show_progress=show_progress\n    )\n\n    for new_id, img_embedding in zip(ids_to_embed, new_embeddings):\n        id_to_embed_map[new_id] = img_embedding\n\n    return id_to_embed_map"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/postprocessor.py",
    "filename": "postprocessor.py",
    "relpath": "indices/postprocessor.py",
    "start_line": 1,
    "end_line": 40,
    "length": 40,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# for backward compatibility\nfrom llama_index.core.postprocessor import (\n    AutoPrevNextNodePostprocessor,\n    EmbeddingRecencyPostprocessor,\n    FixedRecencyPostprocessor,\n    KeywordNodePostprocessor,\n    LLMRerank,\n    LongContextReorder,\n    MetadataReplacementPostProcessor,\n    NERPIINodePostprocessor,\n    PIINodePostprocessor,\n    PrevNextNodePostprocessor,\n    SentenceEmbeddingOptimizer,\n    SentenceTransformerRerank,\n    SimilarityPostprocessor,\n    TimeWeightedPostprocessor,\n)\nfrom llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\nfrom llama_index.core.postprocessor.sbert_rerank import SentenceTransformerRerank\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\n\n__all__ = [\n    \"SimilarityPostprocessor\",\n    \"KeywordNodePostprocessor\",\n    \"PrevNextNodePostprocessor\",\n    \"AutoPrevNextNodePostprocessor\",\n    \"FixedRecencyPostprocessor\",\n    \"EmbeddingRecencyPostprocessor\",\n    \"TimeWeightedPostprocessor\",\n    \"PIINodePostprocessor\",\n    \"NERPIINodePostprocessor\",\n    \"LLMRerank\",\n    \"SentenceEmbeddingOptimizer\",\n    \"SentenceTransformerRerank\",\n    \"MetadataReplacementPostProcessor\",\n    \"LongContextReorder\",\n    \"FlagEmbeddingReranker\",\n    \"RankGPTRerank\",\n    \"BaseNodePostprocessor\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/registry.py",
    "filename": "registry.py",
    "relpath": "indices/registry.py",
    "start_line": 1,
    "end_line": 31,
    "length": 31,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Index registry.\"\"\"\n\nfrom typing import Dict, Type\n\nfrom llama_index.core.data_structs.struct_type import IndexStructType\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.document_summary.base import DocumentSummaryIndex\nfrom llama_index.core.indices.empty.base import EmptyIndex\nfrom llama_index.core.indices.keyword_table.base import KeywordTableIndex\nfrom llama_index.core.indices.knowledge_graph.base import KnowledgeGraphIndex\nfrom llama_index.core.indices.list.base import SummaryIndex\nfrom llama_index.core.indices.multi_modal import MultiModalVectorStoreIndex\nfrom llama_index.core.indices.property_graph import PropertyGraphIndex\nfrom llama_index.core.indices.struct_store.pandas import PandasIndex\nfrom llama_index.core.indices.struct_store.sql import SQLStructStoreIndex\nfrom llama_index.core.indices.tree.base import TreeIndex\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\n\nINDEX_STRUCT_TYPE_TO_INDEX_CLASS: Dict[IndexStructType, Type[BaseIndex]] = {\n    IndexStructType.TREE: TreeIndex,\n    IndexStructType.LIST: SummaryIndex,\n    IndexStructType.KEYWORD_TABLE: KeywordTableIndex,\n    IndexStructType.VECTOR_STORE: VectorStoreIndex,\n    IndexStructType.SQL: SQLStructStoreIndex,\n    IndexStructType.PANDAS: PandasIndex,  # type: ignore\n    IndexStructType.KG: KnowledgeGraphIndex,\n    IndexStructType.SIMPLE_LPG: PropertyGraphIndex,\n    IndexStructType.EMPTY: EmptyIndex,\n    IndexStructType.DOCUMENT_SUMMARY: DocumentSummaryIndex,\n    IndexStructType.MULTIMODAL_VECTOR_STORE: MultiModalVectorStoreIndex,\n}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/prompt_helper.py",
    "filename": "prompt_helper.py",
    "relpath": "indices/prompt_helper.py",
    "start_line": 1,
    "end_line": 297,
    "length": 297,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_llm_metadata",
      "class_name",
      "_get_available_context_size",
      "_get_tools_from_llm",
      "_get_available_chunk_size",
      "get_text_splitter_given_prompt",
      "truncate",
      "repack"
    ],
    "chunk_class_names": [
      "PromptHelper"
    ],
    "document_function_names": [
      "__init__",
      "from_llm_metadata",
      "class_name",
      "_get_available_context_size",
      "_get_tools_from_llm",
      "_get_available_chunk_size",
      "get_text_splitter_given_prompt",
      "truncate",
      "repack"
    ],
    "document_class_names": [
      "PromptHelper"
    ],
    "content": "\"\"\"General prompt helper that can help deal with LLM context window token limitations.\n\nAt its core, it calculates available context size by starting with the context window\nsize of an LLM and reserve token space for the prompt template, and the output.\n\nIt provides utility for \"repacking\" text chunks (retrieved from index) to maximally\nmake use of the available context window (and thereby reducing the number of LLM calls\nneeded), or truncating them so that they fit in a single LLM call.\n\"\"\"\n\nimport logging\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Callable, List, Optional, Sequence\n\nif TYPE_CHECKING:\n    from llama_index.core.tools import BaseTool\n\nfrom llama_index.core.base.llms.types import ChatMessage, LLMMetadata\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.constants import DEFAULT_CONTEXT_WINDOW, DEFAULT_NUM_OUTPUTS\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.llms.structured_llm import StructuredLLM\nfrom llama_index.core.node_parser.text.token import TokenTextSplitter\nfrom llama_index.core.node_parser.text.utils import truncate_text\nfrom llama_index.core.prompts import (\n    BasePromptTemplate,\n    ChatPromptTemplate,\n    SelectorPromptTemplate,\n)\nfrom llama_index.core.prompts.prompt_utils import get_empty_prompt_txt\nfrom llama_index.core.prompts.utils import format_content_blocks\nfrom llama_index.core.schema import BaseComponent\nfrom llama_index.core.utilities.token_counting import TokenCounter\n\nDEFAULT_PADDING = 5\nDEFAULT_CHUNK_OVERLAP_RATIO = 0.1\n\nlogger = logging.getLogger(__name__)\n\n\nclass PromptHelper(BaseComponent):\n    \"\"\"Prompt helper.\n\n    General prompt helper that can help deal with LLM context window token limitations.\n\n    At its core, it calculates available context size by starting with the context\n    window size of an LLM and reserve token space for the prompt template, and the\n    output.\n\n    It provides utility for \"repacking\" text chunks (retrieved from index) to maximally\n    make use of the available context window (and thereby reducing the number of LLM\n    calls needed), or truncating them so that they fit in a single LLM call.\n\n    Args:\n        context_window (int):                   Context window for the LLM.\n        num_output (int):                       Number of outputs for the LLM.\n        chunk_overlap_ratio (float):            Chunk overlap as a ratio of chunk size\n        chunk_size_limit (Optional[int]):         Maximum chunk size to use.\n        tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.\n        separator (str):                        Separator for text splitter\n\n    \"\"\"\n\n    context_window: int = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=\"The maximum context size that will get sent to the LLM.\",\n    )\n    num_output: int = Field(\n        default=DEFAULT_NUM_OUTPUTS,\n        description=\"The amount of token-space to leave in input for generation.\",\n    )\n    chunk_overlap_ratio: float = Field(\n        default=DEFAULT_CHUNK_OVERLAP_RATIO,\n        description=\"The percentage token amount that each chunk should overlap.\",\n    )\n    chunk_size_limit: Optional[int] = Field(description=\"The maximum size of a chunk.\")\n    separator: str = Field(\n        default=\" \", description=\"The separator when chunking tokens.\"\n    )\n\n    _token_counter: TokenCounter = PrivateAttr()\n\n    def __init__(\n        self,\n        context_window: int = DEFAULT_CONTEXT_WINDOW,\n        num_output: int = DEFAULT_NUM_OUTPUTS,\n        chunk_overlap_ratio: float = DEFAULT_CHUNK_OVERLAP_RATIO,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if chunk_overlap_ratio > 1.0 or chunk_overlap_ratio < 0.0:\n            raise ValueError(\"chunk_overlap_ratio must be a float between 0. and 1.\")\n        super().__init__(\n            context_window=context_window,\n            num_output=num_output,\n            chunk_overlap_ratio=chunk_overlap_ratio,\n            chunk_size_limit=chunk_size_limit,\n            separator=separator,\n        )\n\n        # TODO: make configurable\n        self._token_counter = TokenCounter(tokenizer=tokenizer)\n\n    @classmethod\n    def from_llm_metadata(\n        cls,\n        llm_metadata: LLMMetadata,\n        chunk_overlap_ratio: float = DEFAULT_CHUNK_OVERLAP_RATIO,\n        chunk_size_limit: Optional[int] = None,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        separator: str = \" \",\n    ) -> \"PromptHelper\":\n        \"\"\"Create from llm predictor.\n\n        This will autofill values like context_window and num_output.\n\n        \"\"\"\n        context_window = llm_metadata.context_window\n\n        if llm_metadata.num_output == -1:\n            num_output = DEFAULT_NUM_OUTPUTS\n        else:\n            num_output = llm_metadata.num_output\n\n        return cls(\n            context_window=context_window,\n            num_output=num_output,\n            chunk_overlap_ratio=chunk_overlap_ratio,\n            chunk_size_limit=chunk_size_limit,\n            tokenizer=tokenizer,\n            separator=separator,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"PromptHelper\"\n\n    def _get_available_context_size(self, num_prompt_tokens: int) -> int:\n        \"\"\"Get available context size.\n\n        This is calculated as:\n            available context window = total context window\n                - input (partially filled prompt)\n                - output (room reserved for response)\n\n        Notes:\n        - Available context size is further clamped to be non-negative.\n        \"\"\"\n        context_size_tokens = self.context_window - num_prompt_tokens - self.num_output\n        if context_size_tokens < 0:\n            raise ValueError(\n                f\"Calculated available context size {context_size_tokens} was\"\n                \" not non-negative.\"\n            )\n        return context_size_tokens\n\n    def _get_tools_from_llm(\n        self, llm: Optional[LLM] = None, tools: Optional[List[\"BaseTool\"]] = None\n    ) -> List[\"BaseTool\"]:\n        from llama_index.core.program.function_program import get_function_tool\n\n        tools = tools or []\n        if isinstance(llm, StructuredLLM):\n            tools.append(get_function_tool(llm.output_cls))\n\n        return tools\n\n    def _get_available_chunk_size(\n        self,\n        prompt: BasePromptTemplate,\n        num_chunks: int = 1,\n        padding: int = 5,\n        llm: Optional[LLM] = None,\n        tools: Optional[List[\"BaseTool\"]] = None,\n    ) -> int:\n        \"\"\"Get available chunk size.\n\n        This is calculated as:\n            available chunk size = available context window  // number_chunks\n                - padding\n\n        Notes:\n        - By default, we use padding of 5 (to save space for formatting needs).\n        - Available chunk size is further clamped to chunk_size_limit if specified.\n        \"\"\"\n        tools = self._get_tools_from_llm(llm=llm, tools=tools)\n\n        if isinstance(prompt, SelectorPromptTemplate):\n            prompt = prompt.select(llm=llm)\n\n        if isinstance(prompt, ChatPromptTemplate):\n            messages: List[ChatMessage] = prompt.message_templates\n\n            # account for partial formatting\n            partial_messages = []\n            for message in messages:\n                partial_message = deepcopy(message)\n\n                # TODO: This does not count tokens in non-text blocks\n                prompt_kwargs = prompt.kwargs or {}\n                partial_message.blocks = format_content_blocks(\n                    partial_message.blocks, **prompt_kwargs\n                )\n\n                # add to list of partial messages\n                partial_messages.append(partial_message)\n\n            num_prompt_tokens = self._token_counter.estimate_tokens_in_messages(\n                partial_messages\n            )\n        else:\n            prompt_str = get_empty_prompt_txt(prompt)\n            num_prompt_tokens = self._token_counter.get_string_tokens(prompt_str)\n\n        num_prompt_tokens += self._token_counter.estimate_tokens_in_tools(\n            [x.metadata.to_openai_tool() for x in tools]\n        )\n\n        # structured llms cannot have system prompts currently -- check the underlying llm\n        if isinstance(llm, StructuredLLM):\n            num_prompt_tokens += self._token_counter.get_string_tokens(\n                llm.llm.system_prompt or \"\"\n            )\n        elif llm is not None:\n            num_prompt_tokens += self._token_counter.get_string_tokens(\n                llm.system_prompt or \"\"\n            )\n\n        available_context_size = self._get_available_context_size(num_prompt_tokens)\n        result = available_context_size // num_chunks - padding\n        if self.chunk_size_limit is not None:\n            result = min(result, self.chunk_size_limit)\n        return result\n\n    def get_text_splitter_given_prompt(\n        self,\n        prompt: BasePromptTemplate,\n        num_chunks: int = 1,\n        padding: int = DEFAULT_PADDING,\n        llm: Optional[LLM] = None,\n        tools: Optional[List[\"BaseTool\"]] = None,\n    ) -> TokenTextSplitter:\n        \"\"\"Get text splitter configured to maximally pack available context window,\n        taking into account of given prompt, and desired number of chunks.\n        \"\"\"\n        chunk_size = self._get_available_chunk_size(\n            prompt, num_chunks, padding=padding, llm=llm, tools=tools\n        )\n        if chunk_size <= 0:\n            raise ValueError(f\"Chunk size {chunk_size} is not positive.\")\n        chunk_overlap = int(self.chunk_overlap_ratio * chunk_size)\n        return TokenTextSplitter(\n            separator=self.separator,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            tokenizer=self._token_counter.tokenizer,\n        )\n\n    def truncate(\n        self,\n        prompt: BasePromptTemplate,\n        text_chunks: Sequence[str],\n        padding: int = DEFAULT_PADDING,\n        llm: Optional[LLM] = None,\n        tools: Optional[List[\"BaseTool\"]] = None,\n    ) -> List[str]:\n        \"\"\"Truncate text chunks to fit available context window.\"\"\"\n        text_splitter = self.get_text_splitter_given_prompt(\n            prompt,\n            num_chunks=len(text_chunks),\n            padding=padding,\n            llm=llm,\n            tools=tools,\n        )\n        return [truncate_text(chunk, text_splitter) for chunk in text_chunks]\n\n    def repack(\n        self,\n        prompt: BasePromptTemplate,\n        text_chunks: Sequence[str],\n        padding: int = DEFAULT_PADDING,\n        llm: Optional[LLM] = None,\n        tools: Optional[List[\"BaseTool\"]] = None,\n    ) -> List[str]:\n        \"\"\"Repack text chunks to fit available context window.\n\n        This will combine text chunks into consolidated chunks\n        that more fully \"pack\" the prompt template given the context_window.\n\n        \"\"\"\n        text_splitter = self.get_text_splitter_given_prompt(\n            prompt, padding=padding, llm=llm, tools=tools\n        )\n        combined_str = \"\\n\\n\".join([c.strip() for c in text_chunks if c.strip()])\n        return text_splitter.split_text(combined_str)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/common/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/common/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/base.py",
    "filename": "base.py",
    "relpath": "indices/common/struct_store/base.py",
    "start_line": 1,
    "end_line": 215,
    "length": 215,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "build_all_context_from_documents",
      "build_table_context_from_documents",
      "__init__",
      "_clean_and_validate_fields",
      "_insert_datapoint",
      "_get_col_types_map",
      "_get_schema_text",
      "insert_datapoint_from_nodes"
    ],
    "chunk_class_names": [
      "SQLDocumentContextBuilder",
      "BaseStructDatapointExtractor"
    ],
    "document_function_names": [
      "__init__",
      "build_all_context_from_documents",
      "build_table_context_from_documents",
      "__init__",
      "_clean_and_validate_fields",
      "_insert_datapoint",
      "_get_col_types_map",
      "_get_schema_text",
      "insert_datapoint_from_nodes"
    ],
    "document_class_names": [
      "SQLDocumentContextBuilder",
      "BaseStructDatapointExtractor"
    ],
    "content": "\"\"\"Common classes for structured operations.\"\"\"\n\nimport logging\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, cast\n\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.data_structs.table import StructDatapoint\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.node_parser.interface import TextSplitter\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_TABLE_CONTEXT_PROMPT,\n    DEFAULT_TABLE_CONTEXT_QUERY,\n)\nfrom llama_index.core.prompts.prompt_type import PromptType\nfrom llama_index.core.response_synthesizers import get_response_synthesizer\nfrom llama_index.core.schema import BaseNode, MetadataMode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\nfrom llama_index.core.utils import truncate_text\n\nlogger = logging.getLogger(__name__)\n\n\nclass SQLDocumentContextBuilder:\n    \"\"\"Builder that builds context for a given set of SQL tables.\n\n    Args:\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n        text_splitter (Optional[TextSplitter]): Text Splitter to use.\n        table_context_prompt (Optional[BasePromptTemplate]): A\n            Table Context Prompt (see :ref:`Prompt-Templates`).\n        refine_table_context_prompt (Optional[BasePromptTemplate]):\n            A Refine Table Context Prompt (see :ref:`Prompt-Templates`).\n        table_context_task (Optional[str]): The query to perform\n            on the table context. A default query string is used\n            if none is provided by the user.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        llm: Optional[LLM] = None,\n        text_splitter: Optional[TextSplitter] = None,\n        table_context_prompt: Optional[BasePromptTemplate] = None,\n        refine_table_context_prompt: Optional[BasePromptTemplate] = None,\n        table_context_task: Optional[str] = None,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # TODO: take in an entire index instead of forming a response builder\n        if sql_database is None:\n            raise ValueError(\"sql_database must be provided.\")\n        self._sql_database = sql_database\n        self._text_splitter = text_splitter\n        self._llm = llm or Settings.llm\n        self._prompt_helper = Settings._prompt_helper or PromptHelper.from_llm_metadata(\n            self._llm.metadata,\n        )\n        self._callback_manager = Settings.callback_manager\n        self._table_context_prompt = (\n            table_context_prompt or DEFAULT_TABLE_CONTEXT_PROMPT\n        )\n        self._refine_table_context_prompt = (\n            refine_table_context_prompt or DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL\n        )\n        self._table_context_task = table_context_task or DEFAULT_TABLE_CONTEXT_QUERY\n\n    def build_all_context_from_documents(\n        self,\n        documents_dict: Dict[str, List[BaseNode]],\n    ) -> Dict[str, str]:\n        \"\"\"Build context for all tables in the database.\"\"\"\n        context_dict = {}\n        for table_name in self._sql_database.get_usable_table_names():\n            context_dict[table_name] = self.build_table_context_from_documents(\n                documents_dict[table_name], table_name\n            )\n        return context_dict\n\n    def build_table_context_from_documents(\n        self,\n        documents: Sequence[BaseNode],\n        table_name: str,\n    ) -> str:\n        \"\"\"Build context from documents for a single table.\"\"\"\n        schema = self._sql_database.get_single_table_info(table_name)\n        prompt_with_schema = self._table_context_prompt.partial_format(schema=schema)\n        prompt_with_schema.metadata[\"prompt_type\"] = PromptType.QUESTION_ANSWER\n        refine_prompt_with_schema = self._refine_table_context_prompt.partial_format(\n            schema=schema\n        )\n        refine_prompt_with_schema.metadata[\"prompt_type\"] = PromptType.REFINE\n\n        text_splitter = (\n            self._text_splitter\n            or self._prompt_helper.get_text_splitter_given_prompt(\n                prompt_with_schema, llm=self._llm\n            )\n        )\n        # we use the ResponseBuilder to iteratively go through all texts\n        response_builder = get_response_synthesizer(\n            llm=self._llm,\n            text_qa_template=prompt_with_schema,\n            refine_template=refine_prompt_with_schema,\n        )\n        with self._callback_manager.event(\n            CBEventType.CHUNKING,\n            payload={EventPayload.DOCUMENTS: documents},\n        ) as event:\n            text_chunks = []\n            for doc in documents:\n                chunks = text_splitter.split_text(\n                    doc.get_content(metadata_mode=MetadataMode.LLM)\n                )\n                text_chunks.extend(chunks)\n\n            event.on_end(\n                payload={EventPayload.CHUNKS: text_chunks},\n            )\n\n        # feed in the \"query_str\" or the task\n        table_context = response_builder.get_response(\n            text_chunks=text_chunks, query_str=self._table_context_task\n        )\n        return cast(str, table_context)\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseStructDatapointExtractor:\n    \"\"\"Extracts datapoints from a structured document.\"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        schema_extract_prompt: BasePromptTemplate,\n        output_parser: OUTPUT_PARSER_TYPE,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm\n        self._schema_extract_prompt = schema_extract_prompt\n        self._output_parser = output_parser\n\n    def _clean_and_validate_fields(self, fields: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate fields with col_types_map.\"\"\"\n        new_fields = {}\n        col_types_map = self._get_col_types_map()\n        for field, value in fields.items():\n            clean_value = value\n            if field not in col_types_map:\n                continue\n            # if expected type is int or float, try to convert value to int or float\n            expected_type = col_types_map[field]\n            if expected_type == int:\n                try:\n                    clean_value = int(value)\n                except ValueError:\n                    continue\n            elif expected_type == float:\n                try:\n                    clean_value = float(value)\n                except ValueError:\n                    continue\n            else:\n                if len(value) == 0:\n                    continue\n                if not isinstance(value, col_types_map[field]):\n                    continue\n            new_fields[field] = clean_value\n        return new_fields\n\n    @abstractmethod\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n\n    @abstractmethod\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n\n    @abstractmethod\n    def _get_schema_text(self) -> str:\n        \"\"\"Get schema text for extracting relevant info from unstructured text.\"\"\"\n\n    def insert_datapoint_from_nodes(self, nodes: Sequence[BaseNode]) -> None:\n        \"\"\"Extract datapoint from a document and insert it.\"\"\"\n        text_chunks = [\n            node.get_content(metadata_mode=MetadataMode.LLM) for node in nodes\n        ]\n        fields = {}\n        for i, text_chunk in enumerate(text_chunks):\n            fmt_text_chunk = truncate_text(text_chunk, 50)\n            logger.info(f\"> Adding chunk {i}: {fmt_text_chunk}\")\n            # if embedding specified in document, pass it to the Node\n            schema_text = self._get_schema_text()\n            response_str = self._llm.predict(\n                self._schema_extract_prompt,\n                text=text_chunk,\n                schema=schema_text,\n            )\n            cur_fields = self._output_parser(response_str)\n            if cur_fields is None:\n                continue\n            # validate fields with col_types_map\n            new_cur_fields = self._clean_and_validate_fields(cur_fields)\n            fields.update(new_cur_fields)\n        struct_datapoint = StructDatapoint(fields)\n        if struct_datapoint is not None:\n            self._insert_datapoint(struct_datapoint)\n            logger.debug(f\"> Added datapoint: {fields}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/schema.py",
    "filename": "schema.py",
    "relpath": "indices/common/struct_store/schema.py",
    "start_line": 1,
    "end_line": 24,
    "length": 24,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "from",
      "class"
    ],
    "document_function_names": [],
    "document_class_names": [
      "from",
      "class"
    ],
    "content": "\"\"\"Common structures for structured indices.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n# TODO: migrate this to be a data_struct\n@dataclass\nclass SQLContextContainer(DataClassJsonMixin):\n    \"\"\"SQLContextContainer.\n\n    A container interface to store context for a given table.\n    Context can be built from unstructured documents (e.g. using SQLContextBuilder).\n    Context can also be dumped to an underlying LlamaIndex data structure.\n\n    Contains both the raw context_dict as well as any index_structure.\n\n    Should be not be used directly - build one from SQLContextContainerBuilder instead.\n\n    \"\"\"\n\n    context_dict: Optional[Dict[str, str]] = None\n    context_str: Optional[str] = None"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/common/struct_store/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/common/struct_store/sql.py",
    "filename": "sql.py",
    "relpath": "indices/common/struct_store/sql.py",
    "start_line": 1,
    "end_line": 65,
    "length": 65,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_col_types_map",
      "_get_schema_text",
      "_insert_datapoint"
    ],
    "chunk_class_names": [
      "SQLStructDatapointExtractor"
    ],
    "document_function_names": [
      "__init__",
      "_get_col_types_map",
      "_get_schema_text",
      "_insert_datapoint"
    ],
    "document_class_names": [
      "SQLStructDatapointExtractor"
    ],
    "content": "\"\"\"SQL StructDatapointExtractor.\"\"\"\n\nfrom typing import Any, Dict, Optional, cast\n\nfrom llama_index.core.data_structs.table import StructDatapoint\nfrom llama_index.core.indices.common.struct_store.base import (\n    OUTPUT_PARSER_TYPE,\n    BaseStructDatapointExtractor,\n)\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\nfrom sqlalchemy import Table\n\n\nclass SQLStructDatapointExtractor(BaseStructDatapointExtractor):\n    \"\"\"Extracts datapoints from a structured document for a SQL db.\"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        schema_extract_prompt: BasePromptTemplate,\n        output_parser: OUTPUT_PARSER_TYPE,\n        sql_database: SQLDatabase,\n        table_name: Optional[str] = None,\n        table: Optional[Table] = None,\n        ref_doc_id_column: Optional[str] = None,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(llm, schema_extract_prompt, output_parser)\n        self._sql_database = sql_database\n        # currently the user must specify a table info\n        if table_name is None and table is None:\n            raise ValueError(\"table_name must be specified\")\n        self._table_name = table_name or cast(Table, table).name\n        if table is None:\n            table_name = cast(str, table_name)\n            table = self._sql_database.metadata_obj.tables[table_name]\n        # if ref_doc_id_column is specified, then we need to check that\n        # it is a valid column in the table\n        col_names = [c.name for c in table.c]\n        if ref_doc_id_column is not None and ref_doc_id_column not in col_names:\n            raise ValueError(\n                f\"ref_doc_id_column {ref_doc_id_column} not in table {table_name}\"\n            )\n        self.ref_doc_id_column = ref_doc_id_column\n        # then store python types of each column\n        self._col_types_map: Dict[str, type] = {\n            c.name: table.c[c.name].type.python_type for c in table.c\n        }\n\n    def _get_col_types_map(self) -> Dict[str, type]:\n        \"\"\"Get col types map for schema.\"\"\"\n        return self._col_types_map\n\n    def _get_schema_text(self) -> str:\n        \"\"\"Insert datapoint into index.\"\"\"\n        return self._sql_database.get_single_table_info(self._table_name)\n\n    def _insert_datapoint(self, datapoint: StructDatapoint) -> None:\n        \"\"\"Insert datapoint into index.\"\"\"\n        datapoint_dict = datapoint.to_dict()[\"fields\"]\n        self._sql_database.insert_into_table(\n            self._table_name, cast(Dict[Any, Any], datapoint_dict)\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/multi_modal/base.py",
    "filename": "base.py",
    "relpath": "indices/multi_modal/base.py",
    "start_line": 1,
    "end_line": 431,
    "length": 431,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "image_vector_store",
      "image_embed_model",
      "is_image_vector_store_empty",
      "is_text_vector_store_empty",
      "as_retriever",
      "as_query_engine",
      "from_vector_store",
      "_get_node_with_embedding",
      "_aget_node_with_embedding",
      "_async_add_nodes_to_index",
      "_add_nodes_to_index",
      "delete_ref_doc"
    ],
    "chunk_class_names": [
      "MultiModalVectorStoreIndex"
    ],
    "document_function_names": [
      "__init__",
      "image_vector_store",
      "image_embed_model",
      "is_image_vector_store_empty",
      "is_text_vector_store_empty",
      "as_retriever",
      "as_query_engine",
      "from_vector_store",
      "_get_node_with_embedding",
      "_aget_node_with_embedding",
      "_async_add_nodes_to_index",
      "_add_nodes_to_index",
      "delete_ref_doc"
    ],
    "document_class_names": [
      "MultiModalVectorStoreIndex"
    ],
    "content": "\"\"\"Multi Modal Vector Store Index.\n\nAn index that is built on top of multiple vector stores for different modalities.\n\n\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, Sequence, cast\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.data_structs.data_structs import (\n    IndexDict,\n    MultiModelIndexDict,\n)\nfrom llama_index.core.embeddings.multi_modal_base import MultiModalEmbedding\nfrom llama_index.core.embeddings.utils import EmbedType, resolve_embed_model\nfrom llama_index.core.indices.utils import (\n    async_embed_image_nodes,\n    async_embed_nodes,\n    embed_image_nodes,\n    embed_nodes,\n)\nfrom llama_index.core.indices.multi_modal.retriever import (\n    MultiModalVectorIndexRetriever,\n)\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.core.llms.utils import LLMType\nfrom llama_index.core.multi_modal_llms import MultiModalLLM\nfrom llama_index.core.query_engine.multi_modal import SimpleMultiModalQueryEngine\nfrom llama_index.core.schema import BaseNode, ImageNode, TextNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.vector_stores.simple import (\n    DEFAULT_VECTOR_STORE,\n    SimpleVectorStore,\n)\nfrom llama_index.core.vector_stores.types import BasePydanticVectorStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass MultiModalVectorStoreIndex(VectorStoreIndex):\n    \"\"\"Multi-Modal Vector Store Index.\n\n    Args:\n        use_async (bool): Whether to use asynchronous calls. Defaults to False.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n        store_nodes_override (bool): set to True to always store Node objects in index\n            store and document store even if vector store keeps text. Defaults to False\n    \"\"\"\n\n    image_namespace = \"image\"\n    index_struct_cls = MultiModelIndexDict\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        index_struct: Optional[MultiModelIndexDict] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        storage_context: Optional[StorageContext] = None,\n        use_async: bool = False,\n        store_nodes_override: bool = False,\n        show_progress: bool = False,\n        # Image-related kwargs\n        # image_vector_store going to be deprecated. image_store can be passed from storage_context\n        # keep image_vector_store here for backward compatibility\n        image_vector_store: Optional[BasePydanticVectorStore] = None,\n        image_embed_model: EmbedType = \"clip:ViT-B/32\",\n        is_image_to_text: bool = False,\n        # is_image_vector_store_empty is used to indicate whether image_vector_store is empty\n        # those flags are used for cases when only one vector store is used\n        is_image_vector_store_empty: bool = False,\n        is_text_vector_store_empty: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        image_embed_model = resolve_embed_model(\n            image_embed_model, callback_manager=kwargs.get(\"callback_manager\", None)\n        )\n        assert isinstance(image_embed_model, MultiModalEmbedding)\n        self._image_embed_model = image_embed_model\n        self._is_image_to_text = is_image_to_text\n        self._is_image_vector_store_empty = is_image_vector_store_empty\n        self._is_text_vector_store_empty = is_text_vector_store_empty\n        storage_context = storage_context or StorageContext.from_defaults()\n\n        if image_vector_store is not None:\n            if self.image_namespace not in storage_context.vector_stores:\n                storage_context.add_vector_store(\n                    image_vector_store, self.image_namespace\n                )\n            else:\n                # overwrite image_store from storage_context\n                storage_context.vector_stores[self.image_namespace] = image_vector_store\n\n        if self.image_namespace not in storage_context.vector_stores:\n            storage_context.add_vector_store(SimpleVectorStore(), self.image_namespace)\n\n        self._image_vector_store = storage_context.vector_stores[self.image_namespace]\n\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            embed_model=embed_model,\n            storage_context=storage_context,\n            show_progress=show_progress,\n            use_async=use_async,\n            store_nodes_override=store_nodes_override,\n            **kwargs,\n        )\n\n    @property\n    def image_vector_store(self) -> BasePydanticVectorStore:\n        return self._image_vector_store\n\n    @property\n    def image_embed_model(self) -> MultiModalEmbedding:\n        return self._image_embed_model\n\n    @property\n    def is_image_vector_store_empty(self) -> bool:\n        return self._is_image_vector_store_empty\n\n    @property\n    def is_text_vector_store_empty(self) -> bool:\n        return self._is_text_vector_store_empty\n\n    def as_retriever(self, **kwargs: Any) -> MultiModalVectorIndexRetriever:\n        return MultiModalVectorIndexRetriever(\n            self,\n            node_ids=list(self.index_struct.nodes_dict.values()),\n            **kwargs,\n        )\n\n    def as_query_engine(\n        self,\n        llm: Optional[LLMType] = None,\n        **kwargs: Any,\n    ) -> SimpleMultiModalQueryEngine:\n        retriever = cast(MultiModalVectorIndexRetriever, self.as_retriever(**kwargs))\n\n        llm = llm or Settings.llm\n        assert isinstance(llm, MultiModalLLM)\n\n        return SimpleMultiModalQueryEngine(\n            retriever,\n            multi_modal_llm=llm,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_vector_store(\n        cls,\n        vector_store: BasePydanticVectorStore,\n        embed_model: Optional[EmbedType] = None,\n        # Image-related kwargs\n        image_vector_store: Optional[BasePydanticVectorStore] = None,\n        image_embed_model: EmbedType = \"clip\",\n        **kwargs: Any,\n    ) -> \"MultiModalVectorStoreIndex\":\n        if not vector_store.stores_text:\n            raise ValueError(\n                \"Cannot initialize from a vector store that does not store text.\"\n            )\n\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        return cls(\n            nodes=[],\n            storage_context=storage_context,\n            image_vector_store=image_vector_store,\n            image_embed_model=image_embed_model,\n            embed_model=(\n                resolve_embed_model(\n                    embed_model, callback_manager=kwargs.get(\"callback_manager\", None)\n                )\n                if embed_model\n                else Settings.embed_model\n            ),\n            **kwargs,\n        )\n\n    def _get_node_with_embedding(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        is_image: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"Get tuples of id, node, and embedding.\n\n        Allows us to store these nodes in a vector store.\n        Embeddings are called in batches.\n\n        \"\"\"\n        id_to_text_embed_map = None\n\n        if is_image:\n            assert all(isinstance(node, ImageNode) for node in nodes)\n            id_to_embed_map = embed_image_nodes(\n                nodes,  # type: ignore\n                embed_model=self._image_embed_model,\n                show_progress=show_progress,\n            )\n\n            # text field is populate, so embed them\n            if self._is_image_to_text:\n                id_to_text_embed_map = embed_nodes(\n                    nodes,\n                    embed_model=self._embed_model,\n                    show_progress=show_progress,\n                )\n                # TODO: refactor this change of image embed model to same as text\n                self._image_embed_model = self._embed_model  # type: ignore\n\n        else:\n            id_to_embed_map = embed_nodes(\n                nodes,\n                embed_model=self._embed_model,\n                show_progress=show_progress,\n            )\n\n        results = []\n        for node in nodes:\n            embedding = id_to_embed_map[node.node_id]\n            result = node.model_copy()\n            result.embedding = embedding\n            if is_image and id_to_text_embed_map:\n                assert isinstance(result, ImageNode)\n                text_embedding = id_to_text_embed_map[node.node_id]\n                result.text_embedding = text_embedding\n                result.embedding = (\n                    text_embedding  # TODO: re-factor to make use of both embeddings\n                )\n            results.append(result)\n        return results\n\n    async def _aget_node_with_embedding(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        is_image: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"Asynchronously get tuples of id, node, and embedding.\n\n        Allows us to store these nodes in a vector store.\n        Embeddings are called in batches.\n\n        \"\"\"\n        id_to_text_embed_map = None\n\n        if is_image:\n            assert all(isinstance(node, ImageNode) for node in nodes)\n            id_to_embed_map = await async_embed_image_nodes(\n                nodes,  # type: ignore\n                embed_model=self._image_embed_model,\n                show_progress=show_progress,\n            )\n\n            if self._is_image_to_text:\n                id_to_text_embed_map = await async_embed_nodes(\n                    nodes,\n                    embed_model=self._embed_model,\n                    show_progress=show_progress,\n                )\n                # TODO: refactor this change of image embed model to same as text\n                self._image_embed_model = self._embed_model  # type: ignore\n\n        else:\n            id_to_embed_map = await async_embed_nodes(\n                nodes,\n                embed_model=self._embed_model,\n                show_progress=show_progress,\n            )\n\n        results = []\n        for node in nodes:\n            embedding = id_to_embed_map[node.node_id]\n            result = node.model_copy()\n            result.embedding = embedding\n            if is_image and id_to_text_embed_map:\n                assert isinstance(result, ImageNode)\n                text_embedding = id_to_text_embed_map[node.node_id]\n                result.text_embedding = text_embedding\n                result.embedding = (\n                    text_embedding  # TODO: re-factor to make use of both embeddings\n                )\n            results.append(result)\n        return results\n\n    async def _async_add_nodes_to_index(\n        self,\n        index_struct: IndexDict,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **insert_kwargs: Any,\n    ) -> None:\n        \"\"\"Asynchronously add nodes to index.\"\"\"\n        if not nodes:\n            return\n\n        image_nodes: List[ImageNode] = []\n        text_nodes: List[BaseNode] = []\n        new_text_ids: List[str] = []\n        new_img_ids: List[str] = []\n\n        for node in nodes:\n            if isinstance(node, ImageNode):\n                image_nodes.append(node)\n            if isinstance(node, TextNode) and node.text:\n                text_nodes.append(node)\n\n        if len(text_nodes) > 0:\n            # embed all nodes as text - include image nodes that have text attached\n            text_nodes = await self._aget_node_with_embedding(\n                text_nodes, show_progress, is_image=False\n            )\n            new_text_ids = await self.storage_context.vector_stores[\n                DEFAULT_VECTOR_STORE\n            ].async_add(text_nodes, **insert_kwargs)\n        else:\n            self._is_text_vector_store_empty = True\n\n        if len(image_nodes) > 0:\n            # embed image nodes as images directly\n            image_nodes = await self._aget_node_with_embedding(  # type: ignore\n                image_nodes,\n                show_progress,\n                is_image=True,\n            )\n            new_img_ids = await self.storage_context.vector_stores[\n                self.image_namespace\n            ].async_add(image_nodes, **insert_kwargs)\n        else:\n            self._is_image_vector_store_empty = True\n\n        # if the vector store doesn't store text, we need to add the nodes to the\n        # index struct and document store\n        all_nodes = text_nodes + image_nodes\n        all_new_ids = new_text_ids + new_img_ids\n        if not self._vector_store.stores_text or self._store_nodes_override:\n            for node, new_id in zip(all_nodes, all_new_ids):\n                # NOTE: remove embedding from node to avoid duplication\n                node_without_embedding = node.model_copy()\n                node_without_embedding.embedding = None\n\n                index_struct.add_node(node_without_embedding, text_id=new_id)\n                self._docstore.add_documents(\n                    [node_without_embedding], allow_update=True\n                )\n\n    def _add_nodes_to_index(\n        self,\n        index_struct: IndexDict,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **insert_kwargs: Any,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        if not nodes:\n            return\n\n        image_nodes: List[ImageNode] = []\n        text_nodes: List[TextNode] = []\n        new_text_ids: List[str] = []\n        new_img_ids: List[str] = []\n\n        for node in nodes:\n            if isinstance(node, ImageNode):\n                image_nodes.append(node)\n            if isinstance(node, TextNode) and node.text:\n                text_nodes.append(node)\n\n        if len(text_nodes) > 0:\n            # embed all nodes as text - include image nodes that have text attached\n            text_nodes = self._get_node_with_embedding(  # type: ignore\n                text_nodes, show_progress, is_image=False\n            )\n            new_text_ids = self.storage_context.vector_stores[DEFAULT_VECTOR_STORE].add(\n                text_nodes, **insert_kwargs\n            )\n        else:\n            self._is_text_vector_store_empty = True\n\n        if len(image_nodes) > 0:\n            # embed image nodes as images directly\n            # check if we should use text embedding for images instead of default\n            image_nodes = self._get_node_with_embedding(  # type: ignore\n                image_nodes,\n                show_progress,\n                is_image=True,\n            )\n            new_img_ids = self.storage_context.vector_stores[self.image_namespace].add(\n                image_nodes, **insert_kwargs\n            )\n        else:\n            self._is_image_vector_store_empty = True\n\n        # if the vector store doesn't store text, we need to add the nodes to the\n        # index struct and document store\n        all_nodes = text_nodes + image_nodes\n        all_new_ids = new_text_ids + new_img_ids\n        if not self._vector_store.stores_text or self._store_nodes_override:\n            for node, new_id in zip(all_nodes, all_new_ids):\n                # NOTE: remove embedding from node to avoid duplication\n                node_without_embedding = node.model_copy()\n                node_without_embedding.embedding = None\n\n                index_struct.add_node(node_without_embedding, text_id=new_id)\n                self._docstore.add_documents(\n                    [node_without_embedding], allow_update=True\n                )\n\n    def delete_ref_doc(\n        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any\n    ) -> None:\n        \"\"\"Delete a document and it's nodes by using ref_doc_id.\"\"\"\n        # delete from all vector stores\n\n        for vector_store in self._storage_context.vector_stores.values():\n            vector_store.delete(ref_doc_id)\n\n            if self._store_nodes_override or self._vector_store.stores_text:\n                ref_doc_info = self._docstore.get_ref_doc_info(ref_doc_id)\n                if ref_doc_info is not None:\n                    for node_id in ref_doc_info.node_ids:\n                        self._index_struct.delete(node_id)\n                        self._vector_store.delete(node_id)\n\n        if delete_from_docstore:\n            self._docstore.delete_ref_doc(ref_doc_id, raise_error=False)\n\n        self._storage_context.index_store.add_index_struct(self._index_struct)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/multi_modal/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/multi_modal/__init__.py",
    "start_line": 1,
    "end_line": 11,
    "length": 11,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Vector-store based data structures.\"\"\"\n\nfrom llama_index.core.indices.multi_modal.base import MultiModalVectorStoreIndex\nfrom llama_index.core.indices.multi_modal.retriever import (\n    MultiModalVectorIndexRetriever,\n)\n\n__all__ = [\n    \"MultiModalVectorStoreIndex\",\n    \"MultiModalVectorIndexRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/multi_modal/retriever.py",
    "filename": "retriever.py",
    "relpath": "indices/multi_modal/retriever.py",
    "start_line": 1,
    "end_line": 376,
    "length": 376,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "similarity_top_k",
      "similarity_top_k",
      "image_similarity_top_k",
      "image_similarity_top_k",
      "_build_vector_store_query",
      "_retrieve",
      "_text_retrieve",
      "text_retrieve",
      "_text_to_image_retrieve",
      "text_to_image_retrieve",
      "_image_to_image_retrieve",
      "image_to_image_retrieve",
      "_get_nodes_with_embeddings",
      "_build_node_list_from_query_result",
      "_aretrieve",
      "_atext_retrieve",
      "atext_retrieve",
      "_atext_to_image_retrieve",
      "atext_to_image_retrieve",
      "_aget_nodes_with_embeddings",
      "_aimage_to_image_retrieve",
      "aimage_to_image_retrieve"
    ],
    "chunk_class_names": [
      "MultiModalVectorIndexRetriever"
    ],
    "document_function_names": [
      "__init__",
      "similarity_top_k",
      "similarity_top_k",
      "image_similarity_top_k",
      "image_similarity_top_k",
      "_build_vector_store_query",
      "_retrieve",
      "_text_retrieve",
      "text_retrieve",
      "_text_to_image_retrieve",
      "text_to_image_retrieve",
      "_image_to_image_retrieve",
      "image_to_image_retrieve",
      "_get_nodes_with_embeddings",
      "_build_node_list_from_query_result",
      "_aretrieve",
      "_atext_retrieve",
      "atext_retrieve",
      "_atext_to_image_retrieve",
      "atext_to_image_retrieve",
      "_aget_nodes_with_embeddings",
      "_aimage_to_image_retrieve",
      "aimage_to_image_retrieve"
    ],
    "document_class_names": [
      "MultiModalVectorIndexRetriever"
    ],
    "content": "\"\"\"Base vector store index query.\"\"\"\n\nimport asyncio\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional\n\nfrom llama_index.core.base.base_multi_modal_retriever import (\n    MultiModalRetriever,\n)\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\nfrom llama_index.core.data_structs.data_structs import IndexDict\nfrom llama_index.core.embeddings.multi_modal_base import MultiModalEmbedding\nfrom llama_index.core.indices.utils import log_vector_store_query_result\nfrom llama_index.core.schema import (\n    NodeWithScore,\n    ObjectType,\n    QueryBundle,\n    QueryType,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.vector_stores.types import (\n    MetadataFilters,\n    BasePydanticVectorStore,\n    VectorStoreQuery,\n    VectorStoreQueryMode,\n    VectorStoreQueryResult,\n)\n\nif TYPE_CHECKING:\n    from llama_index.core.indices.multi_modal.base import MultiModalVectorStoreIndex\n\n\nclass MultiModalVectorIndexRetriever(MultiModalRetriever):\n    \"\"\"Multi Modal Vector index retriever.\n\n    Args:\n        index (MultiModalVectorStoreIndex): Multi Modal vector store index for images and texts.\n        similarity_top_k (int): number of top k results to return.\n        vector_store_query_mode (str): vector store query mode\n            See reference for VectorStoreQueryMode for full list of supported modes.\n        filters (Optional[MetadataFilters]): metadata filters, defaults to None\n        alpha (float): weight for sparse/dense retrieval, only used for\n            hybrid query mode.\n        doc_ids (Optional[List[str]]): list of documents to constrain search.\n        vector_store_kwargs (dict): Additional vector store specific kwargs to pass\n            through to the vector store at query time.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: \"MultiModalVectorStoreIndex\",\n        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n        image_similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n        vector_store_query_mode: VectorStoreQueryMode = VectorStoreQueryMode.DEFAULT,\n        filters: Optional[MetadataFilters] = None,\n        alpha: Optional[float] = None,\n        node_ids: Optional[List[str]] = None,\n        doc_ids: Optional[List[str]] = None,\n        sparse_top_k: Optional[int] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._index = index\n        self._vector_store = self._index.vector_store\n        # separate image vector store for image retrieval\n        self._image_vector_store = self._index.image_vector_store\n\n        assert isinstance(self._index.image_embed_model, BaseEmbedding)\n        self._image_embed_model = index._image_embed_model\n        self._embed_model = index._embed_model\n        self._docstore = self._index.docstore\n\n        self._similarity_top_k = similarity_top_k\n        self._image_similarity_top_k = image_similarity_top_k\n        self._vector_store_query_mode = VectorStoreQueryMode(vector_store_query_mode)\n        self._alpha = alpha\n        self._node_ids = node_ids\n        self._doc_ids = doc_ids\n        self._filters = filters\n        self._sparse_top_k = sparse_top_k\n\n        self._kwargs: Dict[str, Any] = kwargs.get(\"vector_store_kwargs\", {})\n        self.callback_manager = callback_manager or Settings.callback_manager\n\n    @property\n    def similarity_top_k(self) -> int:\n        \"\"\"Return similarity top k.\"\"\"\n        return self._similarity_top_k\n\n    @similarity_top_k.setter\n    def similarity_top_k(self, similarity_top_k: int) -> None:\n        \"\"\"Set similarity top k.\"\"\"\n        self._similarity_top_k = similarity_top_k\n\n    @property\n    def image_similarity_top_k(self) -> int:\n        \"\"\"Return image similarity top k.\"\"\"\n        return self._image_similarity_top_k\n\n    @image_similarity_top_k.setter\n    def image_similarity_top_k(self, image_similarity_top_k: int) -> None:\n        \"\"\"Set image similarity top k.\"\"\"\n        self._image_similarity_top_k = image_similarity_top_k\n\n    def _build_vector_store_query(\n        self, query_bundle_with_embeddings: QueryBundle, similarity_top_k: int\n    ) -> VectorStoreQuery:\n        return VectorStoreQuery(\n            query_embedding=query_bundle_with_embeddings.embedding,\n            similarity_top_k=similarity_top_k,\n            node_ids=self._node_ids,\n            doc_ids=self._doc_ids,\n            query_str=query_bundle_with_embeddings.query_str,\n            mode=self._vector_store_query_mode,\n            alpha=self._alpha,\n            filters=self._filters,\n            sparse_top_k=self._sparse_top_k,\n        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        res = []\n        # If text vector store is not empty, retrieve text nodes\n        # If text vector store is empty, please create index without text vector store\n        if self._vector_store is not None:\n            res.extend(self._text_retrieve(query_bundle))\n\n        # If image vector store is not empty, retrieve text nodes\n        # If image vector store is empty, please create index without image vector store\n        if self._image_vector_store is not None:\n            res.extend(self._text_to_image_retrieve(query_bundle))\n        return res\n\n    def _text_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_text_vector_store_empty:\n            if self._vector_store.is_embedding_query:\n                if (\n                    query_bundle.embedding is None\n                    and len(query_bundle.embedding_strs) > 0\n                ):\n                    query_bundle.embedding = (\n                        self._embed_model.get_agg_embedding_from_queries(\n                            query_bundle.embedding_strs\n                        )\n                    )\n            return self._get_nodes_with_embeddings(\n                query_bundle, self._similarity_top_k, self._vector_store\n            )\n        else:\n            return []\n\n    def text_retrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        return self._text_retrieve(str_or_query_bundle)\n\n    def _text_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_image_vector_store_empty:\n            if self._image_vector_store.is_embedding_query:\n                # change the embedding for query bundle to Multi Modal Text encoder\n                query_bundle.embedding = (\n                    self._image_embed_model.get_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )\n                )\n            return self._get_nodes_with_embeddings(\n                query_bundle, self._image_similarity_top_k, self._image_vector_store\n            )\n        else:\n            return []\n\n    def text_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        return self._text_to_image_retrieve(str_or_query_bundle)\n\n    def _image_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_image_vector_store_empty:\n            if self._image_vector_store.is_embedding_query:\n                # change the embedding for query bundle to Multi Modal Image encoder for image input\n                assert isinstance(self._index.image_embed_model, MultiModalEmbedding)\n                query_bundle.embedding = self._image_embed_model.get_image_embedding(\n                    query_bundle.embedding_image[0]\n                )\n            return self._get_nodes_with_embeddings(\n                query_bundle, self._image_similarity_top_k, self._image_vector_store\n            )\n        else:\n            return []\n\n    def image_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(\n                query_str=\"\", image_path=str_or_query_bundle\n            )\n        return self._image_to_image_retrieve(str_or_query_bundle)\n\n    def _get_nodes_with_embeddings(\n        self,\n        query_bundle_with_embeddings: QueryBundle,\n        similarity_top_k: int,\n        vector_store: BasePydanticVectorStore,\n    ) -> List[NodeWithScore]:\n        query = self._build_vector_store_query(\n            query_bundle_with_embeddings, similarity_top_k\n        )\n        query_result = vector_store.query(query, **self._kwargs)\n        return self._build_node_list_from_query_result(query_result)\n\n    def _build_node_list_from_query_result(\n        self, query_result: VectorStoreQueryResult\n    ) -> List[NodeWithScore]:\n        if query_result.nodes is None:\n            # NOTE: vector store does not keep text and returns node indices.\n            # Need to recover all nodes from docstore\n            if query_result.ids is None:\n                raise ValueError(\n                    \"Vector store query result should return at \"\n                    \"least one of nodes or ids.\"\n                )\n            assert isinstance(self._index.index_struct, IndexDict)\n            node_ids = [\n                self._index.index_struct.nodes_dict[idx] for idx in query_result.ids\n            ]\n            nodes = self._docstore.get_nodes(node_ids)\n            query_result.nodes = nodes\n        else:\n            # NOTE: vector store keeps text, returns nodes.\n            # Only need to recover image or index nodes from docstore\n            for i in range(len(query_result.nodes)):\n                source_node = query_result.nodes[i].source_node\n                if (not self._vector_store.stores_text) or (\n                    source_node is not None and source_node.node_type != ObjectType.TEXT\n                ):\n                    node_id = query_result.nodes[i].node_id\n                    if self._docstore.document_exists(node_id):\n                        query_result.nodes[i] = self._docstore.get_node(  # type: ignore[index]\n                            node_id\n                        )\n\n        log_vector_store_query_result(query_result)\n\n        node_with_scores: List[NodeWithScore] = []\n        for ind, node in enumerate(query_result.nodes):\n            score: Optional[float] = None\n            if query_result.similarities is not None:\n                score = query_result.similarities[ind]\n            node_with_scores.append(NodeWithScore(node=node, score=score))\n\n        return node_with_scores\n\n    # Async Retrieval Methods\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        # Run the two retrievals in async, and return their results as a concatenated list\n        results: List[NodeWithScore] = []\n        tasks = [\n            self._atext_retrieve(query_bundle),\n            self._atext_to_image_retrieve(query_bundle),\n        ]\n\n        task_results = await asyncio.gather(*tasks)\n\n        for task_result in task_results:\n            results.extend(task_result)\n        return results\n\n    async def _atext_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_text_vector_store_empty:\n            if self._vector_store.is_embedding_query:\n                # change the embedding for query bundle to Multi Modal Text encoder\n                query_bundle.embedding = (\n                    await self._embed_model.aget_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )\n                )\n            return await self._aget_nodes_with_embeddings(\n                query_bundle, self._similarity_top_k, self._vector_store\n            )\n        else:\n            return []\n\n    async def atext_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        return await self._atext_retrieve(str_or_query_bundle)\n\n    async def _atext_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_image_vector_store_empty:\n            if self._image_vector_store.is_embedding_query:\n                # change the embedding for query bundle to Multi Modal Text encoder\n                query_bundle.embedding = (\n                    await self._image_embed_model.aget_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )\n                )\n            return await self._aget_nodes_with_embeddings(\n                query_bundle, self._image_similarity_top_k, self._image_vector_store\n            )\n        else:\n            return []\n\n    async def atext_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        return await self._atext_to_image_retrieve(str_or_query_bundle)\n\n    async def _aget_nodes_with_embeddings(\n        self,\n        query_bundle_with_embeddings: QueryBundle,\n        similarity_top_k: int,\n        vector_store: BasePydanticVectorStore,\n    ) -> List[NodeWithScore]:\n        query = self._build_vector_store_query(\n            query_bundle_with_embeddings, similarity_top_k\n        )\n        query_result = await vector_store.aquery(query, **self._kwargs)\n        return self._build_node_list_from_query_result(query_result)\n\n    async def _aimage_to_image_retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if not self._index.is_image_vector_store_empty:\n            if self._image_vector_store.is_embedding_query:\n                # change the embedding for query bundle to Multi Modal Image encoder for image input\n                assert isinstance(self._index.image_embed_model, MultiModalEmbedding)\n                # Using the first imaage in the list for image retrieval\n                query_bundle.embedding = (\n                    await self._image_embed_model.aget_image_embedding(\n                        query_bundle.embedding_image[0]\n                    )\n                )\n            return await self._aget_nodes_with_embeddings(\n                query_bundle, self._image_similarity_top_k, self._image_vector_store\n            )\n        else:\n            return []\n\n    async def aimage_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        if isinstance(str_or_query_bundle, str):\n            # leave query_str as empty since we are using image_path for image retrieval\n            str_or_query_bundle = QueryBundle(\n                query_str=\"\", image_path=str_or_query_bundle\n            )\n        return await self._aimage_to_image_retrieve(str_or_query_bundle)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/base.py",
    "filename": "base.py",
    "relpath": "indices/knowledge_graph/base.py",
    "start_line": 1,
    "end_line": 381,
    "length": 381,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "graph_store",
      "as_retriever",
      "_extract_triplets",
      "_llm_extract_triplets",
      "_parse_triplet_response",
      "_build_index_from_nodes",
      "_insert",
      "upsert_triplet",
      "add_node",
      "upsert_triplet_and_node",
      "_delete_node",
      "ref_doc_info",
      "get_networkx_graph",
      "query_context"
    ],
    "chunk_class_names": [
      "has",
      "instead",
      "KnowledgeGraphIndex"
    ],
    "document_function_names": [
      "__init__",
      "graph_store",
      "as_retriever",
      "_extract_triplets",
      "_llm_extract_triplets",
      "_parse_triplet_response",
      "_build_index_from_nodes",
      "_insert",
      "upsert_triplet",
      "add_node",
      "upsert_triplet_and_node",
      "_delete_node",
      "ref_doc_info",
      "get_networkx_graph",
      "query_context"
    ],
    "document_class_names": [
      "has",
      "instead",
      "KnowledgeGraphIndex"
    ],
    "content": "\"\"\"Knowledge Graph Index.\n\nBuild a KG by extracting triplets, and leveraging the KG during query-time.\n\n\"\"\"\n\nimport logging\nimport deprecated\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.constants import GRAPH_STORE_KEY\nfrom llama_index.core.data_structs.data_structs import KG\nfrom llama_index.core.graph_stores.simple import SimpleGraphStore\nfrom llama_index.core.graph_stores.types import GraphStore\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n)\nfrom llama_index.core.schema import BaseNode, IndexNode, MetadataMode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore.types import RefDocInfo\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.utils import get_tqdm_iterable\n\nlogger = logging.getLogger(__name__)\n\n\n@deprecated.deprecated(\n    version=\"0.10.53\",\n    reason=(\n        \"The KnowledgeGraphIndex class has been deprecated. \"\n        \"Please use the new PropertyGraphIndex class instead. \"\n        \"If a certain graph store integration is missing in the new class, \"\n        \"please open an issue on the GitHub repository or contribute it!\"\n    ),\n)\nclass KnowledgeGraphIndex(BaseIndex[KG]):\n    \"\"\"Knowledge Graph Index.\n\n    Build a KG by extracting triplets, and leveraging the KG during query-time.\n\n    Args:\n        kg_triplet_extract_template (BasePromptTemplate): The prompt to use for\n            extracting triplets.\n        max_triplets_per_chunk (int): The maximum number of triplets to extract.\n        storage_context (Optional[StorageContext]): The storage context to use.\n        graph_store (Optional[GraphStore]): The graph store to use.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n        include_embeddings (bool): Whether to include embeddings in the index.\n            Defaults to False.\n        max_object_length (int): The maximum length of the object in a triplet.\n            Defaults to 128.\n        kg_triplet_extract_fn (Optional[Callable]): The function to use for\n            extracting triplets. Defaults to None.\n\n    \"\"\"\n\n    index_struct_cls = KG\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        objects: Optional[Sequence[IndexNode]] = None,\n        index_struct: Optional[KG] = None,\n        llm: Optional[LLM] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        storage_context: Optional[StorageContext] = None,\n        kg_triplet_extract_template: Optional[BasePromptTemplate] = None,\n        max_triplets_per_chunk: int = 10,\n        include_embeddings: bool = False,\n        show_progress: bool = False,\n        max_object_length: int = 128,\n        kg_triplet_extract_fn: Optional[Callable] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self.include_embeddings = include_embeddings\n        self.max_triplets_per_chunk = max_triplets_per_chunk\n        self.kg_triplet_extract_template = (\n            kg_triplet_extract_template or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT\n        )\n        # NOTE: Partially format keyword extract template here.\n        self.kg_triplet_extract_template = (\n            self.kg_triplet_extract_template.partial_format(\n                max_knowledge_triplets=self.max_triplets_per_chunk\n            )\n        )\n        self._max_object_length = max_object_length\n        self._kg_triplet_extract_fn = kg_triplet_extract_fn\n\n        self._llm = llm or Settings.llm\n        self._embed_model = embed_model or Settings.embed_model\n\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            storage_context=storage_context,\n            show_progress=show_progress,\n            objects=objects,\n            **kwargs,\n        )\n\n        # TODO: legacy conversion - remove in next release\n        if (\n            len(self.index_struct.table) > 0\n            and isinstance(self.graph_store, SimpleGraphStore)\n            and len(self.graph_store._data.graph_dict) == 0\n        ):\n            logger.warning(\"Upgrading previously saved KG index to new storage format.\")\n            self.graph_store._data.graph_dict = self.index_struct.rel_map\n\n    @property\n    def graph_store(self) -> GraphStore:\n        return self._graph_store\n\n    def as_retriever(\n        self,\n        retriever_mode: Optional[str] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        from llama_index.core.indices.knowledge_graph.retrievers import (\n            KGRetrieverMode,\n            KGTableRetriever,\n        )\n\n        if len(self.index_struct.embedding_dict) > 0 and retriever_mode is None:\n            retriever_mode = KGRetrieverMode.HYBRID\n        elif retriever_mode is None:\n            retriever_mode = KGRetrieverMode.KEYWORD\n        elif isinstance(retriever_mode, str):\n            retriever_mode = KGRetrieverMode(retriever_mode)\n        else:\n            retriever_mode = retriever_mode\n\n        return KGTableRetriever(\n            self,\n            object_map=self._object_map,\n            llm=self._llm,\n            embed_model=embed_model or self._embed_model,\n            retriever_mode=retriever_mode,\n            **kwargs,\n        )\n\n    def _extract_triplets(self, text: str) -> List[Tuple[str, str, str]]:\n        if self._kg_triplet_extract_fn is not None:\n            return self._kg_triplet_extract_fn(text)\n        else:\n            return self._llm_extract_triplets(text)\n\n    def _llm_extract_triplets(self, text: str) -> List[Tuple[str, str, str]]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.kg_triplet_extract_template,\n            text=text,\n        )\n        return self._parse_triplet_response(\n            response, max_length=self._max_object_length\n        )\n\n    @staticmethod\n    def _parse_triplet_response(\n        response: str, max_length: int = 128\n    ) -> List[Tuple[str, str, str]]:\n        knowledge_strs = response.strip().split(\"\\n\")\n        results = []\n        for text in knowledge_strs:\n            if \"(\" not in text or \")\" not in text or text.index(\")\") < text.index(\"(\"):\n                # skip empty lines and non-triplets\n                continue\n            triplet_part = text[text.index(\"(\") + 1 : text.index(\")\")]\n            tokens = triplet_part.split(\",\")\n            if len(tokens) != 3:\n                continue\n\n            if any(len(s.encode(\"utf-8\")) > max_length for s in tokens):\n                # We count byte-length instead of len() for UTF-8 chars,\n                # will skip if any of the tokens are too long.\n                # This is normally due to a poorly formatted triplet\n                # extraction, in more serious KG building cases\n                # we'll need NLP models to better extract triplets.\n                continue\n\n            subj, pred, obj = map(str.strip, tokens)\n            if not subj or not pred or not obj:\n                # skip partial triplets\n                continue\n\n            # Strip double quotes and Capitalize triplets for disambiguation\n            subj, pred, obj = (\n                entity.strip('\"').capitalize() for entity in [subj, pred, obj]\n            )\n\n            results.append((subj, pred, obj))\n        return results\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> KG:\n        \"\"\"Build the index from nodes.\"\"\"\n        # do simple concatenation\n        index_struct = self.index_struct_cls()\n        nodes_with_progress = get_tqdm_iterable(\n            nodes, self._show_progress, \"Processing nodes\"\n        )\n        for n in nodes_with_progress:\n            triplets = self._extract_triplets(\n                n.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            logger.debug(f\"> Extracted triplets: {triplets}\")\n            for triplet in triplets:\n                subj, _, obj = triplet\n                self.upsert_triplet(triplet)\n                index_struct.add_node([subj, obj], n)\n\n            if self.include_embeddings:\n                triplet_texts = [str(t) for t in triplets]\n\n                embed_outputs = self._embed_model.get_text_embedding_batch(\n                    triplet_texts, show_progress=self._show_progress\n                )\n                for rel_text, rel_embed in zip(triplet_texts, embed_outputs):\n                    index_struct.add_to_embedding_dict(rel_text, rel_embed)\n\n        return index_struct\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        for n in nodes:\n            triplets = self._extract_triplets(\n                n.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            logger.debug(f\"Extracted triplets: {triplets}\")\n            for triplet in triplets:\n                subj, _, obj = triplet\n                triplet_str = str(triplet)\n                self.upsert_triplet(triplet)\n                self._index_struct.add_node([subj, obj], n)\n                if (\n                    self.include_embeddings\n                    and triplet_str not in self._index_struct.embedding_dict\n                ):\n                    rel_embedding = self._embed_model.get_text_embedding(triplet_str)\n                    self._index_struct.add_to_embedding_dict(triplet_str, rel_embedding)\n\n        # Update the storage context's index_store\n        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    def upsert_triplet(\n        self, triplet: Tuple[str, str, str], include_embeddings: bool = False\n    ) -> None:\n        \"\"\"Insert triplets and optionally embeddings.\n\n        Used for manual insertion of KG triplets (in the form\n        of (subject, relationship, object)).\n\n        Args:\n            triplet (tuple): Knowledge triplet\n            embedding (Any, optional): Embedding option for the triplet. Defaults to None.\n        \"\"\"\n        self._graph_store.upsert_triplet(*triplet)\n        triplet_str = str(triplet)\n        if include_embeddings:\n            set_embedding = self._embed_model.get_text_embedding(triplet_str)\n            self._index_struct.add_to_embedding_dict(str(triplet), set_embedding)\n            self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    def add_node(self, keywords: List[str], node: BaseNode) -> None:\n        \"\"\"Add node.\n\n        Used for manual insertion of nodes (keyed by keywords).\n\n        Args:\n            keywords (List[str]): Keywords to index the node.\n            node (Node): Node to be indexed.\n\n        \"\"\"\n        self._index_struct.add_node(keywords, node)\n        self._docstore.add_documents([node], allow_update=True)\n\n    def upsert_triplet_and_node(\n        self,\n        triplet: Tuple[str, str, str],\n        node: BaseNode,\n        include_embeddings: bool = False,\n    ) -> None:\n        \"\"\"Upsert KG triplet and node.\n\n        Calls both upsert_triplet and add_node.\n        Behavior is idempotent; if Node already exists,\n        only triplet will be added.\n\n        Args:\n            keywords (List[str]): Keywords to index the node.\n            node (Node): Node to be indexed.\n            include_embeddings (bool): Option to add embeddings for triplets. Defaults to False\n\n        \"\"\"\n        subj, _, obj = triplet\n        self.upsert_triplet(triplet)\n        self.add_node([subj, obj], node)\n        triplet_str = str(triplet)\n        if include_embeddings:\n            set_embedding = self._embed_model.get_text_embedding(triplet_str)\n            self._index_struct.add_to_embedding_dict(str(triplet), set_embedding)\n            self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        raise NotImplementedError(\"Delete is not supported for KG index yet.\")\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        node_doc_ids_sets = list(self._index_struct.table.values())\n        node_doc_ids = list(set().union(*node_doc_ids_sets))\n        nodes = self.docstore.get_nodes(node_doc_ids)\n\n        all_ref_doc_info = {}\n        for node in nodes:\n            ref_node = node.source_node\n            if not ref_node:\n                continue\n\n            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)\n            if not ref_doc_info:\n                continue\n\n            all_ref_doc_info[ref_node.node_id] = ref_doc_info\n        return all_ref_doc_info\n\n    def get_networkx_graph(self, limit: int = 100) -> Any:\n        \"\"\"Get networkx representation of the graph structure.\n\n        Args:\n            limit (int): Number of starting nodes to be included in the graph.\n\n        NOTE: This function requires networkx to be installed.\n        NOTE: This is a beta feature.\n\n        \"\"\"\n        try:\n            import networkx as nx\n        except ImportError:\n            raise ImportError(\n                \"Please install networkx to visualize the graph: `pip install networkx`\"\n            )\n\n        g = nx.Graph()\n        subjs = list(self.index_struct.table.keys())\n\n        # add edges\n        rel_map = self._graph_store.get_rel_map(subjs=subjs, depth=1, limit=limit)\n\n        added_nodes = set()\n        for keyword in rel_map:\n            for path in rel_map[keyword]:\n                subj = keyword\n                for i in range(0, len(path), 2):\n                    if i + 2 >= len(path):\n                        break\n\n                    if subj not in added_nodes:\n                        g.add_node(subj)\n                        added_nodes.add(subj)\n\n                    rel = path[i + 1]\n                    obj = path[i + 2]\n\n                    g.add_edge(subj, obj, label=rel, title=rel)\n                    subj = obj\n        return g\n\n    @property\n    def query_context(self) -> Dict[str, Any]:\n        return {GRAPH_STORE_KEY: self._graph_store}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/knowledge_graph/__init__.py",
    "start_line": 1,
    "end_line": 15,
    "length": 15,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"KG-based data structures.\"\"\"\n\nfrom llama_index.core.indices.knowledge_graph.base import (\n    KnowledgeGraphIndex,\n)\nfrom llama_index.core.indices.knowledge_graph.retrievers import (\n    KGTableRetriever,\n    KnowledgeGraphRAGRetriever,\n)\n\n__all__ = [\n    \"KnowledgeGraphIndex\",\n    \"KGTableRetriever\",\n    \"KnowledgeGraphRAGRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/knowledge_graph/retrievers.py",
    "start_line": 1,
    "end_line": 398,
    "length": 398,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_keywords",
      "_extract_rel_text_keywords",
      "_retrieve",
      "_get_metadata_for_response"
    ],
    "chunk_class_names": [
      "KGRetrieverMode",
      "KGTableRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_keywords",
      "_extract_rel_text_keywords",
      "_retrieve",
      "_get_metadata_for_response",
      "__init__",
      "_process_entities",
      "_aprocess_entities",
      "_get_entities",
      "_aget_entities",
      "_expand_synonyms",
      "_aexpand_synonyms",
      "_get_knowledge_sequence",
      "_aget_knowledge_sequence",
      "_build_nodes",
      "_retrieve_keyword",
      "_aretrieve_keyword",
      "_retrieve_embedding",
      "_aretrieve_embedding",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "KGRetrieverMode",
      "KGTableRetriever",
      "KnowledgeGraphRAGRetriever"
    ],
    "content": "\"\"\"KG Retrievers.\"\"\"\n\nimport deprecated\nimport logging\nfrom collections import defaultdict\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional, Set, Tuple\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.keyword_table.utils import (\n    extract_keywords_given_response,\n)\nfrom llama_index.core.indices.knowledge_graph.base import KnowledgeGraphIndex\nfrom llama_index.core.indices.query.embedding_utils import get_top_k_embeddings\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate, PromptType\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    MetadataMode,\n    NodeWithScore,\n    QueryBundle,\n    TextNode,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.utils import print_text, truncate_text\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\nDEFAULT_NODE_SCORE = 1000.0\nGLOBAL_EXPLORE_NODE_LIMIT = 3\nREL_TEXT_LIMIT = 30\n\nlogger = logging.getLogger(__name__)\n\n\nclass KGRetrieverMode(str, Enum):\n    \"\"\"Query mode enum for Knowledge Graphs.\n\n    Can be passed as the enum struct, or as the underlying string.\n\n    Attributes:\n        KEYWORD (\"keyword\"): Default query mode, using keywords to find triplets.\n        EMBEDDING (\"embedding\"): Embedding mode, using embeddings to find\n            similar triplets.\n        HYBRID (\"hybrid\"): Hybrid mode, combining both keywords and embeddings\n            to find relevant triplets.\n    \"\"\"\n\n    KEYWORD = \"keyword\"\n    EMBEDDING = \"embedding\"\n    HYBRID = \"hybrid\"\n\n\n@deprecated.deprecated(\n    version=\"0.10.53\",\n    reason=(\n        \"KGTableRetriever is deprecated, it is recommended to use \"\n        \"PropertyGraphIndex and associated retrievers instead.\"\n    ),\n)\nclass KGTableRetriever(BaseRetriever):\n    \"\"\"KG Table Retriever.\n\n    Arguments are shared among subclasses.\n\n    Args:\n        query_keyword_extract_template (Optional[QueryKGExtractPrompt]): A Query\n            KG Extraction\n            Prompt (see :ref:`Prompt-Templates`).\n        refine_template (Optional[BasePromptTemplate]): A Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[BasePromptTemplate]): A Question Answering Prompt\n            (see :ref:`Prompt-Templates`).\n        max_keywords_per_query (int): Maximum number of keywords to extract from query.\n        num_chunks_per_query (int): Maximum number of text chunks to query.\n        include_text (bool): Use the document text source from each relevant triplet\n            during queries.\n        retriever_mode (KGRetrieverMode): Specifies whether to use keywords,\n            embeddings, or both to find relevant triplets. Should be one of \"keyword\",\n            \"embedding\", or \"hybrid\".\n        similarity_top_k (int): The number of top embeddings to use\n            (if embeddings are used).\n        graph_store_query_depth (int): The depth of the graph store query.\n        use_global_node_triplets (bool): Whether to get more keywords(entities) from\n            text chunks matched by keywords. This helps introduce more global knowledge.\n            While it's more expensive, thus to be turned off by default.\n        max_knowledge_sequence (int): The maximum number of knowledge sequence to\n            include in the response. By default, it's 30.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: KnowledgeGraphIndex,\n        llm: Optional[LLM] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        query_keyword_extract_template: Optional[BasePromptTemplate] = None,\n        max_keywords_per_query: int = 10,\n        num_chunks_per_query: int = 10,\n        include_text: bool = True,\n        retriever_mode: Optional[KGRetrieverMode] = KGRetrieverMode.KEYWORD,\n        similarity_top_k: int = 2,\n        graph_store_query_depth: int = 2,\n        use_global_node_triplets: bool = False,\n        max_knowledge_sequence: int = REL_TEXT_LIMIT,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        assert isinstance(index, KnowledgeGraphIndex)\n        self._index = index\n        self._index_struct = self._index.index_struct\n        self._docstore = self._index.docstore\n\n        self.max_keywords_per_query = max_keywords_per_query\n        self.num_chunks_per_query = num_chunks_per_query\n        self.query_keyword_extract_template = query_keyword_extract_template or DQKET\n        self.similarity_top_k = similarity_top_k\n        self._include_text = include_text\n        self._retriever_mode = (\n            KGRetrieverMode(retriever_mode)\n            if retriever_mode\n            else KGRetrieverMode.KEYWORD\n        )\n\n        self._llm = llm or Settings.llm\n        self._embed_model = embed_model or Settings.embed_model\n        self._graph_store = index.graph_store\n        self.graph_store_query_depth = graph_store_query_depth\n        self.use_global_node_triplets = use_global_node_triplets\n        self.max_knowledge_sequence = max_knowledge_sequence\n        self._verbose = kwargs.get(\"verbose\", False)\n        refresh_schema = kwargs.get(\"refresh_schema\", False)\n        try:\n            self._graph_schema = self._graph_store.get_schema(refresh=refresh_schema)\n        except NotImplementedError:\n            self._graph_schema = \"\"\n        except Exception as e:\n            logger.warning(f\"Failed to get graph schema: {e}\")\n            self._graph_schema = \"\"\n        super().__init__(\n            callback_manager=callback_manager or Settings.callback_manager,\n            object_map=object_map,\n            verbose=verbose,\n        )\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,\n            max_keywords=self.max_keywords_per_query,\n            question=query_str,\n        )\n        keywords = extract_keywords_given_response(\n            response, start_token=\"KEYWORDS:\", lowercase=False\n        )\n        return list(keywords)\n\n    def _extract_rel_text_keywords(self, rel_texts: List[str]) -> List[str]:\n        \"\"\"Find the keywords for given rel text triplets.\"\"\"\n        keywords = []\n\n        for rel_text in rel_texts:\n            splited_texts = rel_text.split(\",\")\n\n            if len(splited_texts) <= 0:\n                continue\n            keyword = splited_texts[0]\n            if keyword:\n                keywords.append(keyword.strip(\"(\\\"'\"))\n\n            # Return the Object as well\n            if len(splited_texts) <= 2:\n                continue\n            keyword = splited_texts[2]\n            if keyword:\n                keywords.append(keyword.strip(\" ()\\\"'\"))\n        return keywords\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        node_visited = set()\n        keywords = self._get_keywords(query_bundle.query_str)\n        if self._verbose:\n            print_text(f\"Extracted keywords: {keywords}\\n\", color=\"green\")\n        rel_texts = []\n        cur_rel_map = {}\n        chunk_indices_count: Dict[str, int] = defaultdict(int)\n        if self._retriever_mode != KGRetrieverMode.EMBEDDING:\n            for keyword in keywords:\n                subjs = {keyword}\n                node_ids = self._index_struct.search_node_by_keyword(keyword)\n                for node_id in node_ids[:GLOBAL_EXPLORE_NODE_LIMIT]:\n                    if node_id in node_visited:\n                        continue\n\n                    if self._include_text:\n                        chunk_indices_count[node_id] += 1\n\n                    node_visited.add(node_id)\n                    if self.use_global_node_triplets:\n                        # Get nodes from keyword search, and add them to the subjs\n                        # set. This helps introduce more global knowledge into the\n                        # query. While it's more expensive, thus to be turned off\n                        # by default, it can be useful for some applications.\n\n                        # TODO: we should a keyword-node_id map in IndexStruct, so that\n                        # node-keywords extraction with LLM will be called only once\n                        # during indexing.\n                        extended_subjs = self._get_keywords(\n                            self._docstore.get_node(node_id).get_content(\n                                metadata_mode=MetadataMode.LLM\n                            )\n                        )\n                        subjs.update(extended_subjs)\n\n                rel_map = self._graph_store.get_rel_map(\n                    list(subjs), self.graph_store_query_depth\n                )\n\n                logger.debug(f\"rel_map: {rel_map}\")\n\n                if not rel_map:\n                    continue\n                rel_texts.extend(\n                    [\n                        str(rel_obj)\n                        for rel_objs in rel_map.values()\n                        for rel_obj in rel_objs\n                    ]\n                )\n                cur_rel_map.update(rel_map)\n\n        if (\n            self._retriever_mode != KGRetrieverMode.KEYWORD\n            and len(self._index_struct.embedding_dict) > 0\n        ):\n            query_embedding = self._embed_model.get_text_embedding(\n                query_bundle.query_str\n            )\n            all_rel_texts = list(self._index_struct.embedding_dict.keys())\n\n            rel_text_embeddings = [\n                self._index_struct.embedding_dict[_id] for _id in all_rel_texts\n            ]\n            similarities, top_rel_texts = get_top_k_embeddings(\n                query_embedding,\n                rel_text_embeddings,\n                similarity_top_k=self.similarity_top_k,\n                embedding_ids=all_rel_texts,\n            )\n            logger.debug(\n                f\"Found the following rel_texts+query similarites: {similarities!s}\"\n            )\n            logger.debug(f\"Found the following top_k rel_texts: {rel_texts!s}\")\n            rel_texts.extend(top_rel_texts)\n\n        elif len(self._index_struct.embedding_dict) == 0:\n            logger.warning(\n                \"Index was not constructed with embeddings, skipping embedding usage...\"\n            )\n\n        # remove any duplicates from keyword + embedding queries\n        if self._retriever_mode == KGRetrieverMode.HYBRID:\n            rel_texts = list(set(rel_texts))\n\n            # remove shorter rel_texts that are substrings of longer rel_texts\n            rel_texts.sort(key=len, reverse=True)\n            for i in range(len(rel_texts)):\n                for j in range(i + 1, len(rel_texts)):\n                    if rel_texts[j] in rel_texts[i]:\n                        rel_texts[j] = \"\"\n            rel_texts = [rel_text for rel_text in rel_texts if rel_text != \"\"]\n\n            # truncate rel_texts\n            rel_texts = rel_texts[: self.max_knowledge_sequence]\n\n        # When include_text = True just get the actual content of all the nodes\n        # (Nodes with actual keyword match, Nodes which are found from the depth search and Nodes founnd from top_k similarity)\n        if self._include_text:\n            keywords = self._extract_rel_text_keywords(\n                rel_texts\n            )  # rel_texts will have all the Triplets retrieved with respect to the Query\n            nested_node_ids = [\n                self._index_struct.search_node_by_keyword(keyword)\n                for keyword in keywords\n            ]\n            node_ids = [_id for ids in nested_node_ids for _id in ids]\n            for node_id in node_ids:\n                chunk_indices_count[node_id] += 1\n\n        sorted_chunk_indices = sorted(\n            chunk_indices_count.keys(),\n            key=lambda x: chunk_indices_count[x],\n            reverse=True,\n        )\n        sorted_chunk_indices = sorted_chunk_indices[: self.num_chunks_per_query]\n        sorted_nodes = self._docstore.get_nodes(sorted_chunk_indices)\n\n        # TMP/TODO: also filter rel_texts as nodes until we figure out better\n        # abstraction\n        # TODO(suo): figure out what this does\n        # rel_text_nodes = [Node(text=rel_text) for rel_text in rel_texts]\n        # for node_processor in self._node_postprocessors:\n        #     rel_text_nodes = node_processor.postprocess_nodes(rel_text_nodes)\n        # rel_texts = [node.get_content() for node in rel_text_nodes]\n\n        sorted_nodes_with_scores = []\n        for chunk_idx, node in zip(sorted_chunk_indices, sorted_nodes):\n            # nodes are found with keyword mapping, give high conf to avoid cutoff\n            sorted_nodes_with_scores.append(\n                NodeWithScore(node=node, score=DEFAULT_NODE_SCORE)\n            )\n            logger.info(\n                f\"> Querying with idx: {chunk_idx}: \"\n                f\"{truncate_text(node.get_content(), 80)}\"\n            )\n        # if no relationship is found, return the nodes found by keywords\n        if not rel_texts:\n            logger.info(\"> No relationships found, returning nodes found by keywords.\")\n            if len(sorted_nodes_with_scores) == 0:\n                logger.info(\"> No nodes found by keywords, returning empty response.\")\n                return [\n                    NodeWithScore(\n                        node=TextNode(text=\"No relationships found.\"), score=1.0\n                    )\n                ]\n            # In else case the sorted_nodes_with_scores is not empty\n            # thus returning the nodes found by keywords\n            return sorted_nodes_with_scores\n\n        # add relationships as Node\n        # TODO: make initial text customizable\n        rel_initial_text = (\n            f\"The following are knowledge sequence in max depth\"\n            f\" {self.graph_store_query_depth} \"\n            f\"in the form of directed graph like:\\n\"\n            f\"`subject -[predicate]->, object, <-[predicate_next_hop]-,\"\n            f\" object_next_hop ...`\"\n        )\n        rel_info = [rel_initial_text, *rel_texts]\n        rel_node_info = {\n            \"kg_rel_texts\": rel_texts,\n            \"kg_rel_map\": cur_rel_map,\n        }\n        if self._graph_schema != \"\":\n            rel_node_info[\"kg_schema\"] = {\"schema\": self._graph_schema}\n        rel_info_text = \"\\n\".join(\n            [\n                str(item)\n                for sublist in rel_info\n                for item in (sublist if isinstance(sublist, list) else [sublist])\n            ]\n        )\n        if self._verbose:\n            print_text(f\"KG context:\\n{rel_info_text}\\n\", color=\"blue\")\n        rel_text_node = TextNode(\n            text=rel_info_text,\n            metadata=rel_node_info,\n            excluded_embed_metadata_keys=[\"kg_rel_map\", \"kg_rel_texts\"],\n            excluded_llm_metadata_keys=[\"kg_rel_map\", \"kg_rel_texts\"],\n        )\n        # this node is constructed from rel_texts, give high confidence to avoid cutoff\n        sorted_nodes_with_scores.append(\n            NodeWithScore(node=rel_text_node, score=DEFAULT_NODE_SCORE)\n        )\n\n        return sorted_nodes_with_scores\n\n    def _get_metadata_for_response(\n        self, nodes: List[BaseNode]\n    ) -> Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for response.\"\"\"\n        for node in nodes:\n            if node.metadata is None or \"kg_rel_map\" not in node.metadata:\n                continue\n            return node.metadata\n        raise ValueError(\"kg_rel_map must be found in at least one Node.\")\n\n\nDEFAULT_SYNONYM_EXPAND_TEMPLATE = \"\"\"\nGenerate synonyms or possible form of keywords up to {max_keywords} in total,\nconsidering possible cases of capitalization, pluralization, common expressions, etc.\nProvide all synonyms of keywords in comma-separated format: 'SYNONYMS: <keywords>'\nNote, result should be in one-line with only one 'SYNONYMS: ' prefix\n----\nKEYWORDS: {question}\n----\n\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/knowledge_graph/retrievers.py",
    "start_line": 398,
    "end_line": 403,
    "length": 6,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "_get_keywords",
      "_extract_rel_text_keywords",
      "_retrieve",
      "_get_metadata_for_response",
      "__init__",
      "_process_entities",
      "_aprocess_entities",
      "_get_entities",
      "_aget_entities",
      "_expand_synonyms",
      "_aexpand_synonyms",
      "_get_knowledge_sequence",
      "_aget_knowledge_sequence",
      "_build_nodes",
      "_retrieve_keyword",
      "_aretrieve_keyword",
      "_retrieve_embedding",
      "_aretrieve_embedding",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "KGRetrieverMode",
      "KGTableRetriever",
      "KnowledgeGraphRAGRetriever"
    ],
    "content": "DEFAULT_SYNONYM_EXPAND_PROMPT = PromptTemplate(\n    DEFAULT_SYNONYM_EXPAND_TEMPLATE,\n    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/knowledge_graph/retrievers.py",
    "start_line": 403,
    "end_line": 412,
    "length": 10,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "_get_keywords",
      "_extract_rel_text_keywords",
      "_retrieve",
      "_get_metadata_for_response",
      "__init__",
      "_process_entities",
      "_aprocess_entities",
      "_get_entities",
      "_aget_entities",
      "_expand_synonyms",
      "_aexpand_synonyms",
      "_get_knowledge_sequence",
      "_aget_knowledge_sequence",
      "_build_nodes",
      "_retrieve_keyword",
      "_aretrieve_keyword",
      "_retrieve_embedding",
      "_aretrieve_embedding",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "KGRetrieverMode",
      "KGTableRetriever",
      "KnowledgeGraphRAGRetriever"
    ],
    "content": "@deprecated.deprecated(\n    version=\"0.10.53\",\n    reason=(\n        \"KnowledgeGraphRAGRetriever is deprecated, it is recommended to use \"\n        \"PropertyGraphIndex and associated retrievers instead.\"\n    ),\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/knowledge_graph/retrievers.py",
    "start_line": 412,
    "end_line": 413,
    "length": 2,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "KnowledgeGraphRAGRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_keywords",
      "_extract_rel_text_keywords",
      "_retrieve",
      "_get_metadata_for_response",
      "__init__",
      "_process_entities",
      "_aprocess_entities",
      "_get_entities",
      "_aget_entities",
      "_expand_synonyms",
      "_aexpand_synonyms",
      "_get_knowledge_sequence",
      "_aget_knowledge_sequence",
      "_build_nodes",
      "_retrieve_keyword",
      "_aretrieve_keyword",
      "_retrieve_embedding",
      "_aretrieve_embedding",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "KGRetrieverMode",
      "KGTableRetriever",
      "KnowledgeGraphRAGRetriever"
    ],
    "content": "class KnowledgeGraphRAGRetriever(BaseRetriever):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/knowledge_graph/retrievers.py",
    "start_line": 413,
    "end_line": 812,
    "length": 400,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_process_entities",
      "_aprocess_entities",
      "_get_entities",
      "_aget_entities",
      "_expand_synonyms",
      "_aexpand_synonyms",
      "_get_knowledge_sequence",
      "_aget_knowledge_sequence",
      "_build_nodes",
      "_retrieve_keyword",
      "_aretrieve_keyword"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "_get_keywords",
      "_extract_rel_text_keywords",
      "_retrieve",
      "_get_metadata_for_response",
      "__init__",
      "_process_entities",
      "_aprocess_entities",
      "_get_entities",
      "_aget_entities",
      "_expand_synonyms",
      "_aexpand_synonyms",
      "_get_knowledge_sequence",
      "_aget_knowledge_sequence",
      "_build_nodes",
      "_retrieve_keyword",
      "_aretrieve_keyword",
      "_retrieve_embedding",
      "_aretrieve_embedding",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "KGRetrieverMode",
      "KGTableRetriever",
      "KnowledgeGraphRAGRetriever"
    ],
    "content": "\"\"\"\n    Knowledge Graph RAG retriever.\n\n    Retriever that perform SubGraph RAG towards knowledge graph.\n\n    Args:\n        storage_context (Optional[StorageContext]): A storage context to use.\n        entity_extract_fn (Optional[Callable]): A function to extract entities.\n        entity_extract_template Optional[BasePromptTemplate]): A Query Key Entity\n            Extraction Prompt (see :ref:`Prompt-Templates`).\n        entity_extract_policy (Optional[str]): The entity extraction policy to use.\n            default: \"union\"\n            possible values: \"union\", \"intersection\"\n        synonym_expand_fn (Optional[Callable]): A function to expand synonyms.\n        synonym_expand_template (Optional[QueryKeywordExpandPrompt]): A Query Key Entity\n            Expansion Prompt (see :ref:`Prompt-Templates`).\n        synonym_expand_policy (Optional[str]): The synonym expansion policy to use.\n            default: \"union\"\n            possible values: \"union\", \"intersection\"\n        max_entities (int): The maximum number of entities to extract.\n            default: 5\n        max_synonyms (int): The maximum number of synonyms to expand per entity.\n            default: 5\n        retriever_mode (Optional[str]): The retriever mode to use.\n            default: \"keyword\"\n            possible values: \"keyword\", \"embedding\", \"keyword_embedding\"\n        with_nl2graphquery (bool): Whether to combine NL2GraphQuery in context.\n            default: False\n        graph_traversal_depth (int): The depth of graph traversal.\n            default: 2\n        max_knowledge_sequence (int): The maximum number of knowledge sequence to\n            include in the response. By default, it's 30.\n        verbose (bool): Whether to print out debug info.\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_context: Optional[StorageContext] = None,\n        llm: Optional[LLM] = None,\n        entity_extract_fn: Optional[Callable] = None,\n        entity_extract_template: Optional[BasePromptTemplate] = None,\n        entity_extract_policy: Optional[str] = \"union\",\n        synonym_expand_fn: Optional[Callable] = None,\n        synonym_expand_template: Optional[BasePromptTemplate] = None,\n        synonym_expand_policy: Optional[str] = \"union\",\n        max_entities: int = 5,\n        max_synonyms: int = 5,\n        retriever_mode: Optional[str] = \"keyword\",\n        with_nl2graphquery: bool = False,\n        graph_traversal_depth: int = 2,\n        max_knowledge_sequence: int = REL_TEXT_LIMIT,\n        verbose: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize the retriever.\"\"\"\n        # Ensure that we have a graph store\n        assert storage_context is not None, \"Must provide a storage context.\"\n        assert (\n            storage_context.graph_store is not None\n        ), \"Must provide a graph store in the storage context.\"\n        self._storage_context = storage_context\n        self._graph_store = storage_context.graph_store\n\n        self._llm = llm or Settings.llm\n\n        self._entity_extract_fn = entity_extract_fn\n        self._entity_extract_template = (\n            entity_extract_template or DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n        )\n        self._entity_extract_policy = entity_extract_policy\n\n        self._synonym_expand_fn = synonym_expand_fn\n        self._synonym_expand_template = (\n            synonym_expand_template or DEFAULT_SYNONYM_EXPAND_PROMPT\n        )\n        self._synonym_expand_policy = synonym_expand_policy\n\n        self._max_entities = max_entities\n        self._max_synonyms = max_synonyms\n        self._retriever_mode = retriever_mode\n        self._with_nl2graphquery = with_nl2graphquery\n        if self._with_nl2graphquery:\n            from llama_index.core.query_engine.knowledge_graph_query_engine import (\n                KnowledgeGraphQueryEngine,\n            )\n\n            graph_query_synthesis_prompt = kwargs.get(\n                \"graph_query_synthesis_prompt\",\n                None,\n            )\n            if graph_query_synthesis_prompt is not None:\n                del kwargs[\"graph_query_synthesis_prompt\"]\n\n            graph_response_answer_prompt = kwargs.get(\n                \"graph_response_answer_prompt\",\n                None,\n            )\n            if graph_response_answer_prompt is not None:\n                del kwargs[\"graph_response_answer_prompt\"]\n\n            refresh_schema = kwargs.get(\"refresh_schema\", False)\n            response_synthesizer = kwargs.get(\"response_synthesizer\", None)\n            self._kg_query_engine = KnowledgeGraphQueryEngine(\n                llm=self._llm,\n                storage_context=self._storage_context,\n                graph_query_synthesis_prompt=graph_query_synthesis_prompt,\n                graph_response_answer_prompt=graph_response_answer_prompt,\n                refresh_schema=refresh_schema,\n                verbose=verbose,\n                response_synthesizer=response_synthesizer,\n                **kwargs,\n            )\n\n        self._graph_traversal_depth = graph_traversal_depth\n        self._max_knowledge_sequence = max_knowledge_sequence\n        self._verbose = verbose\n        refresh_schema = kwargs.get(\"refresh_schema\", False)\n        try:\n            self._graph_schema = self._graph_store.get_schema(refresh=refresh_schema)\n        except NotImplementedError:\n            self._graph_schema = \"\"\n        except Exception as e:\n            logger.warning(f\"Failed to get graph schema: {e}\")\n            self._graph_schema = \"\"\n\n        super().__init__(callback_manager=callback_manager or Settings.callback_manager)\n\n    def _process_entities(\n        self,\n        query_str: str,\n        handle_fn: Optional[Callable],\n        handle_llm_prompt_template: Optional[BasePromptTemplate],\n        cross_handle_policy: Optional[str] = \"union\",\n        max_items: Optional[int] = 5,\n        result_start_token: str = \"KEYWORDS:\",\n    ) -> List[str]:\n        \"\"\"Get entities from query string.\"\"\"\n        assert cross_handle_policy in [\n            \"union\",\n            \"intersection\",\n        ], \"Invalid entity extraction policy.\"\n        if cross_handle_policy == \"intersection\":\n            assert all(\n                [\n                    handle_fn is not None,\n                    handle_llm_prompt_template is not None,\n                ]\n            ), \"Must provide entity extract function and template.\"\n        assert any(\n            [\n                handle_fn is not None,\n                handle_llm_prompt_template is not None,\n            ]\n        ), \"Must provide either entity extract function or template.\"\n        enitities_fn: List[str] = []\n        enitities_llm: Set[str] = set()\n\n        if handle_fn is not None:\n            enitities_fn = handle_fn(query_str)\n        if handle_llm_prompt_template is not None:\n            response = self._llm.predict(\n                handle_llm_prompt_template,\n                max_keywords=max_items,\n                question=query_str,\n            )\n            enitities_llm = extract_keywords_given_response(\n                response, start_token=result_start_token, lowercase=False\n            )\n        if cross_handle_policy == \"union\":\n            entities = list(set(enitities_fn) | enitities_llm)\n        elif cross_handle_policy == \"intersection\":\n            entities = list(set(enitities_fn).intersection(set(enitities_llm)))\n        if self._verbose:\n            print_text(f\"Entities processed: {entities}\\n\", color=\"green\")\n\n        return entities\n\n    async def _aprocess_entities(\n        self,\n        query_str: str,\n        handle_fn: Optional[Callable],\n        handle_llm_prompt_template: Optional[BasePromptTemplate],\n        cross_handle_policy: Optional[str] = \"union\",\n        max_items: Optional[int] = 5,\n        result_start_token: str = \"KEYWORDS:\",\n    ) -> List[str]:\n        \"\"\"Get entities from query string.\"\"\"\n        assert cross_handle_policy in [\n            \"union\",\n            \"intersection\",\n        ], \"Invalid entity extraction policy.\"\n        if cross_handle_policy == \"intersection\":\n            assert all(\n                [\n                    handle_fn is not None,\n                    handle_llm_prompt_template is not None,\n                ]\n            ), \"Must provide entity extract function and template.\"\n        assert any(\n            [\n                handle_fn is not None,\n                handle_llm_prompt_template is not None,\n            ]\n        ), \"Must provide either entity extract function or template.\"\n        enitities_fn: List[str] = []\n        enitities_llm: Set[str] = set()\n\n        if handle_fn is not None:\n            enitities_fn = handle_fn(query_str)\n        if handle_llm_prompt_template is not None:\n            response = await self._llm.apredict(\n                handle_llm_prompt_template,\n                max_keywords=max_items,\n                question=query_str,\n            )\n            enitities_llm = extract_keywords_given_response(\n                response, start_token=result_start_token, lowercase=False\n            )\n        if cross_handle_policy == \"union\":\n            entities = list(set(enitities_fn) | enitities_llm)\n        elif cross_handle_policy == \"intersection\":\n            entities = list(set(enitities_fn).intersection(set(enitities_llm)))\n        if self._verbose:\n            print_text(f\"Entities processed: {entities}\\n\", color=\"green\")\n\n        return entities\n\n    def _get_entities(self, query_str: str) -> List[str]:\n        \"\"\"Get entities from query string.\"\"\"\n        entities = self._process_entities(\n            query_str,\n            self._entity_extract_fn,\n            self._entity_extract_template,\n            self._entity_extract_policy,\n            self._max_entities,\n            \"KEYWORDS:\",\n        )\n        expanded_entities = self._expand_synonyms(entities)\n        return list(set(entities) | set(expanded_entities))\n\n    async def _aget_entities(self, query_str: str) -> List[str]:\n        \"\"\"Get entities from query string.\"\"\"\n        entities = await self._aprocess_entities(\n            query_str,\n            self._entity_extract_fn,\n            self._entity_extract_template,\n            self._entity_extract_policy,\n            self._max_entities,\n            \"KEYWORDS:\",\n        )\n        expanded_entities = await self._aexpand_synonyms(entities)\n        return list(set(entities) | set(expanded_entities))\n\n    def _expand_synonyms(self, keywords: List[str]) -> List[str]:\n        \"\"\"Expand synonyms or similar expressions for keywords.\"\"\"\n        return self._process_entities(\n            str(keywords),\n            self._synonym_expand_fn,\n            self._synonym_expand_template,\n            self._synonym_expand_policy,\n            self._max_synonyms,\n            \"SYNONYMS:\",\n        )\n\n    async def _aexpand_synonyms(self, keywords: List[str]) -> List[str]:\n        \"\"\"Expand synonyms or similar expressions for keywords.\"\"\"\n        return await self._aprocess_entities(\n            str(keywords),\n            self._synonym_expand_fn,\n            self._synonym_expand_template,\n            self._synonym_expand_policy,\n            self._max_synonyms,\n            \"SYNONYMS:\",\n        )\n\n    def _get_knowledge_sequence(\n        self, entities: List[str]\n    ) -> Tuple[List[str], Optional[Dict[Any, Any]]]:\n        \"\"\"Get knowledge sequence from entities.\"\"\"\n        # Get SubGraph from Graph Store as Knowledge Sequence\n        rel_map: Optional[Dict] = self._graph_store.get_rel_map(\n            entities, self._graph_traversal_depth, limit=self._max_knowledge_sequence\n        )\n        logger.debug(f\"rel_map: {rel_map}\")\n\n        # Build Knowledge Sequence\n        knowledge_sequence = []\n        if rel_map:\n            knowledge_sequence.extend(\n                [str(rel_obj) for rel_objs in rel_map.values() for rel_obj in rel_objs]\n            )\n        else:\n            logger.info(\"> No knowledge sequence extracted from entities.\")\n            return [], None\n\n        return knowledge_sequence, rel_map\n\n    async def _aget_knowledge_sequence(\n        self, entities: List[str]\n    ) -> Tuple[List[str], Optional[Dict[Any, Any]]]:\n        \"\"\"Get knowledge sequence from entities.\"\"\"\n        # Get SubGraph from Graph Store as Knowledge Sequence\n        # TBD: async in graph store\n        rel_map: Optional[Dict] = self._graph_store.get_rel_map(\n            entities, self._graph_traversal_depth, limit=self._max_knowledge_sequence\n        )\n        logger.debug(f\"rel_map from GraphStore:\\n{rel_map}\")\n\n        # Build Knowledge Sequence\n        knowledge_sequence = []\n        if rel_map:\n            knowledge_sequence.extend(\n                [str(rel_obj) for rel_objs in rel_map.values() for rel_obj in rel_objs]\n            )\n        else:\n            logger.info(\"> No knowledge sequence extracted from entities.\")\n            return [], None\n\n        return knowledge_sequence, rel_map\n\n    def _build_nodes(\n        self, knowledge_sequence: List[str], rel_map: Optional[Dict[Any, Any]] = None\n    ) -> List[NodeWithScore]:\n        \"\"\"Build nodes from knowledge sequence.\"\"\"\n        if len(knowledge_sequence) == 0:\n            logger.info(\"> No knowledge sequence extracted from entities.\")\n            return []\n        _new_line_char = \"\\n\"\n        context_string = (\n            f\"The following are knowledge sequence in max depth\"\n            f\" {self._graph_traversal_depth} \"\n            f\"in the form of directed graph like:\\n\"\n            f\"`subject -[predicate]->, object, <-[predicate_next_hop]-,\"\n            f\" object_next_hop ...`\"\n            f\" extracted based on key entities as subject:\\n\"\n            f\"{_new_line_char.join(knowledge_sequence)}\"\n        )\n        if self._verbose:\n            print_text(f\"Graph RAG context:\\n{context_string}\\n\", color=\"blue\")\n\n        rel_node_info = {\n            \"kg_rel_map\": rel_map,\n            \"kg_rel_text\": knowledge_sequence,\n        }\n        metadata_keys = [\"kg_rel_map\", \"kg_rel_text\"]\n        if self._graph_schema != \"\":\n            rel_node_info[\"kg_schema\"] = {\"schema\": self._graph_schema}\n            metadata_keys.append(\"kg_schema\")\n        node = NodeWithScore(\n            node=TextNode(\n                text=context_string,\n                score=1.0,\n                metadata=rel_node_info,\n                excluded_embed_metadata_keys=metadata_keys,\n                excluded_llm_metadata_keys=metadata_keys,\n            )\n        )\n        return [node]\n\n    def _retrieve_keyword(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve in keyword mode.\"\"\"\n        if self._retriever_mode not in [\"keyword\", \"keyword_embedding\"]:\n            return []\n        # Get entities\n        entities = self._get_entities(query_bundle.query_str)\n        # Before we enable embedding/semantic search, we need to make sure\n        # we don't miss any entities that's synoynm of the entities we extracted\n        # in string matching based retrieval in following steps, thus we expand\n        # synonyms here.\n        if len(entities) == 0:\n            logger.info(\"> No entities extracted from query string.\")\n            return []\n\n        # Get SubGraph from Graph Store as Knowledge Sequence\n        knowledge_sequence, rel_map = self._get_knowledge_sequence(entities)\n\n        return self._build_nodes(knowledge_sequence, rel_map)\n\n    async def _aretrieve_keyword(\n        self, query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve in keyword mode.\"\"\"\n        if self._retriever_mode not in [\"keyword\", \"keyword_embedding\"]:\n            return []\n        # Get entities\n        entities = await self._aget_entities(query_bundle.query_str)\n        # Before we enable embedding/semantic search, we need to make sure\n        # we don't miss any entities that's synoynm of the entities we extracted\n        # in string matching based retrieval in following steps, thus we expand\n        # synonyms here.\n        if len(entities) == 0:\n            logger.info(\"> No entities extracted from query string.\")\n            return []\n\n        # Get SubGraph from Graph Store as Knowledge Sequence\n        knowledge_sequence, rel_map = await self._aget_knowledge_sequence(entities)\n\n        return self._build_nodes(knowledge_sequence, rel_map)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/knowledge_graph/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/knowledge_graph/retrievers.py",
    "start_line": 812,
    "end_line": 860,
    "length": 49,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_retrieve_embedding",
      "_aretrieve_embedding",
      "_retrieve",
      "_aretrieve"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "_get_keywords",
      "_extract_rel_text_keywords",
      "_retrieve",
      "_get_metadata_for_response",
      "__init__",
      "_process_entities",
      "_aprocess_entities",
      "_get_entities",
      "_aget_entities",
      "_expand_synonyms",
      "_aexpand_synonyms",
      "_get_knowledge_sequence",
      "_aget_knowledge_sequence",
      "_build_nodes",
      "_retrieve_keyword",
      "_aretrieve_keyword",
      "_retrieve_embedding",
      "_aretrieve_embedding",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "KGRetrieverMode",
      "KGTableRetriever",
      "KnowledgeGraphRAGRetriever"
    ],
    "content": "def _retrieve_embedding(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve in embedding mode.\"\"\"\n        if self._retriever_mode not in [\"embedding\", \"keyword_embedding\"]:\n            return []\n        # TBD: will implement this later with vector store.\n        raise NotImplementedError\n\n    async def _aretrieve_embedding(\n        self, query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve in embedding mode.\"\"\"\n        if self._retriever_mode not in [\"embedding\", \"keyword_embedding\"]:\n            return []\n        # TBD: will implement this later with vector store.\n        raise NotImplementedError\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Build nodes for response.\"\"\"\n        nodes: List[NodeWithScore] = []\n        if self._with_nl2graphquery:\n            try:\n                nodes_nl2graphquery = self._kg_query_engine._retrieve(query_bundle)\n                nodes.extend(nodes_nl2graphquery)\n            except Exception as e:\n                logger.warning(f\"Error in retrieving from nl2graphquery: {e}\")\n\n        nodes.extend(self._retrieve_keyword(query_bundle))\n        nodes.extend(self._retrieve_embedding(query_bundle))\n\n        return nodes\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Build nodes for response.\"\"\"\n        nodes: List[NodeWithScore] = []\n        if self._with_nl2graphquery:\n            try:\n                nodes_nl2graphquery = await self._kg_query_engine._aretrieve(\n                    query_bundle\n                )\n                nodes.extend(nodes_nl2graphquery)\n            except Exception as e:\n                logger.warning(f\"Error in retrieving from nl2graphquery: {e}\")\n\n        nodes.extend(await self._aretrieve_keyword(query_bundle))\n        nodes.extend(await self._aretrieve_embedding(query_bundle))\n\n        return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/inserter.py",
    "filename": "inserter.py",
    "relpath": "indices/tree/inserter.py",
    "start_line": 1,
    "end_line": 179,
    "length": 179,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_insert_under_parent_and_consolidate",
      "_insert_node",
      "insert"
    ],
    "chunk_class_names": [
      "TreeIndexInserter"
    ],
    "document_function_names": [
      "__init__",
      "_insert_under_parent_and_consolidate",
      "_insert_node",
      "insert"
    ],
    "document_class_names": [
      "TreeIndexInserter"
    ],
    "content": "\"\"\"Tree Index inserter.\"\"\"\n\nfrom typing import Optional, Sequence\n\nfrom llama_index.core.data_structs.data_structs import IndexGraph\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.indices.tree.utils import get_numbered_text_from_nodes\nfrom llama_index.core.indices.utils import (\n    extract_numbers_given_response,\n    get_sorted_node_list,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_INSERT_PROMPT,\n    DEFAULT_SUMMARY_PROMPT,\n)\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore import BaseDocumentStore\nfrom llama_index.core.storage.docstore.registry import get_default_docstore\n\n\nclass TreeIndexInserter:\n    \"\"\"LlamaIndex inserter.\"\"\"\n\n    def __init__(\n        self,\n        index_graph: IndexGraph,\n        llm: Optional[LLM] = None,\n        num_children: int = 10,\n        insert_prompt: BasePromptTemplate = DEFAULT_INSERT_PROMPT,\n        summary_prompt: BasePromptTemplate = DEFAULT_SUMMARY_PROMPT,\n        docstore: Optional[BaseDocumentStore] = None,\n    ) -> None:\n        \"\"\"Initialize with params.\"\"\"\n        if num_children < 2:\n            raise ValueError(\"Invalid number of children.\")\n        self.num_children = num_children\n        self.summary_prompt = summary_prompt\n        self.insert_prompt = insert_prompt\n        self.index_graph = index_graph\n        self._llm = llm or Settings.llm\n        self._prompt_helper = Settings._prompt_helper or PromptHelper.from_llm_metadata(\n            self._llm.metadata,\n        )\n        self._docstore = docstore or get_default_docstore()\n\n    def _insert_under_parent_and_consolidate(\n        self, text_node: BaseNode, parent_node: Optional[BaseNode]\n    ) -> None:\n        \"\"\"Insert node under parent and consolidate.\n\n        Consolidation will happen by dividing up child nodes, and creating a new\n        intermediate layer of nodes.\n\n        \"\"\"\n        # perform insertion\n        self.index_graph.insert_under_parent(text_node, parent_node)\n\n        # if under num_children limit, then we're fine\n        if len(self.index_graph.get_children(parent_node)) <= self.num_children:\n            return\n        else:\n            # perform consolidation\n            cur_graph_node_ids = self.index_graph.get_children(parent_node)\n            cur_graph_nodes = self._docstore.get_node_dict(cur_graph_node_ids)\n            cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)\n            # this layer is all leaf nodes, consolidate and split leaf nodes\n            # consolidate and split leaf nodes in half\n            # TODO: do better splitting (with a GPT prompt etc.)\n            half1 = cur_graph_node_list[: len(cur_graph_nodes) // 2]\n            half2 = cur_graph_node_list[len(cur_graph_nodes) // 2 :]\n\n            truncated_chunks = self._prompt_helper.truncate(\n                prompt=self.summary_prompt,\n                text_chunks=[\n                    node.get_content(metadata_mode=MetadataMode.LLM) for node in half1\n                ],\n                llm=self._llm,\n            )\n            text_chunk1 = \"\\n\".join(truncated_chunks)\n\n            summary1 = self._llm.predict(self.summary_prompt, context_str=text_chunk1)\n            node1 = TextNode(text=summary1)\n            self.index_graph.insert(node1, children_nodes=half1)\n\n            truncated_chunks = self._prompt_helper.truncate(\n                prompt=self.summary_prompt,\n                text_chunks=[\n                    node.get_content(metadata_mode=MetadataMode.LLM) for node in half2\n                ],\n                llm=self._llm,\n            )\n            text_chunk2 = \"\\n\".join(truncated_chunks)\n            summary2 = self._llm.predict(self.summary_prompt, context_str=text_chunk2)\n            node2 = TextNode(text=summary2)\n            self.index_graph.insert(node2, children_nodes=half2)\n\n            # insert half1 and half2 as new children of parent_node\n            # first remove child indices from parent node\n            if parent_node is not None:\n                self.index_graph.node_id_to_children_ids[parent_node.node_id] = []\n            else:\n                self.index_graph.root_nodes = {}\n            self.index_graph.insert_under_parent(\n                node1, parent_node, new_index=self.index_graph.get_index(node1)\n            )\n            self._docstore.add_documents([node1], allow_update=False)\n            self.index_graph.insert_under_parent(\n                node2, parent_node, new_index=self.index_graph.get_index(node2)\n            )\n            self._docstore.add_documents([node2], allow_update=False)\n\n    def _insert_node(\n        self, node: BaseNode, parent_node: Optional[BaseNode] = None\n    ) -> None:\n        \"\"\"Insert node.\"\"\"\n        cur_graph_node_ids = self.index_graph.get_children(parent_node)\n        cur_graph_nodes = self._docstore.get_node_dict(cur_graph_node_ids)\n        cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)\n        # if cur_graph_nodes is empty (start with empty graph), then insert under\n        # parent (insert new root node)\n        if len(cur_graph_nodes) == 0:\n            self._insert_under_parent_and_consolidate(node, parent_node)\n        # check if leaf nodes, then just insert under parent\n        elif len(self.index_graph.get_children(cur_graph_node_list[0])) == 0:\n            self._insert_under_parent_and_consolidate(node, parent_node)\n        # else try to find the right summary node to insert under\n        else:\n            text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n                prompt=self.insert_prompt,\n                num_chunks=len(cur_graph_node_list),\n                llm=self._llm,\n            )\n            numbered_text = get_numbered_text_from_nodes(\n                cur_graph_node_list, text_splitter=text_splitter\n            )\n            response = self._llm.predict(\n                self.insert_prompt,\n                new_chunk_text=node.get_content(metadata_mode=MetadataMode.LLM),\n                num_chunks=len(cur_graph_node_list),\n                context_list=numbered_text,\n            )\n            numbers = extract_numbers_given_response(response)\n            if numbers is None or len(numbers) == 0:\n                # NOTE: if we can't extract a number, then we just insert under parent\n                self._insert_under_parent_and_consolidate(node, parent_node)\n            elif int(numbers[0]) > len(cur_graph_node_list):\n                # NOTE: if number is out of range, then we just insert under parent\n                self._insert_under_parent_and_consolidate(node, parent_node)\n            else:\n                selected_node = cur_graph_node_list[int(numbers[0]) - 1]\n                self._insert_node(node, selected_node)\n\n        # now we need to update summary for parent node, since we\n        # need to bubble updated summaries up the tree\n        if parent_node is not None:\n            # refetch children\n            cur_graph_node_ids = self.index_graph.get_children(parent_node)\n            cur_graph_nodes = self._docstore.get_node_dict(cur_graph_node_ids)\n            cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)\n            truncated_chunks = self._prompt_helper.truncate(\n                prompt=self.summary_prompt,\n                text_chunks=[\n                    node.get_content(metadata_mode=MetadataMode.LLM)\n                    for node in cur_graph_node_list\n                ],\n                llm=self._llm,\n            )\n            text_chunk = \"\\n\".join(truncated_chunks)\n            new_summary = self._llm.predict(self.summary_prompt, context_str=text_chunk)\n\n            parent_node.set_content(new_summary)\n\n    def insert(self, nodes: Sequence[BaseNode]) -> None:\n        \"\"\"Insert into index_graph.\"\"\"\n        for node in nodes:\n            self._insert_node(node)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/base.py",
    "filename": "base.py",
    "relpath": "indices/tree/base.py",
    "start_line": 1,
    "end_line": 189,
    "length": 189,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "as_retriever",
      "_validate_build_tree_required",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info"
    ],
    "chunk_class_names": [
      "TreeRetrieverMode",
      "TreeIndex"
    ],
    "document_function_names": [
      "__init__",
      "as_retriever",
      "_validate_build_tree_required",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info"
    ],
    "document_class_names": [
      "TreeRetrieverMode",
      "TreeIndex"
    ],
    "content": "\"\"\"Tree-based index.\"\"\"\n\nfrom enum import Enum\nfrom typing import Any, Dict, Optional, Sequence, Union\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\n\n# from llama_index.core.data_structs.data_structs import IndexGraph\nfrom llama_index.core.data_structs.data_structs import IndexGraph\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.common_tree.base import GPTTreeIndexBuilder\nfrom llama_index.core.indices.tree.inserter import TreeIndexInserter\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_INSERT_PROMPT,\n    DEFAULT_SUMMARY_PROMPT,\n)\nfrom llama_index.core.schema import BaseNode, IndexNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore.types import RefDocInfo\n\n\nclass TreeRetrieverMode(str, Enum):\n    SELECT_LEAF = \"select_leaf\"\n    SELECT_LEAF_EMBEDDING = \"select_leaf_embedding\"\n    ALL_LEAF = \"all_leaf\"\n    ROOT = \"root\"\n\n\nREQUIRE_TREE_MODES = {\n    TreeRetrieverMode.SELECT_LEAF,\n    TreeRetrieverMode.SELECT_LEAF_EMBEDDING,\n    TreeRetrieverMode.ROOT,\n}\n\n\nclass TreeIndex(BaseIndex[IndexGraph]):\n    \"\"\"Tree Index.\n\n    The tree index is a tree-structured index, where each node is a summary of\n    the children nodes. During index construction, the tree is constructed\n    in a bottoms-up fashion until we end up with a set of root_nodes.\n\n    There are a few different options during query time (see :ref:`Ref-Query`).\n    The main option is to traverse down the tree from the root nodes.\n    A secondary answer is to directly synthesize the answer from the root nodes.\n\n    Args:\n        summary_template (Optional[BasePromptTemplate]): A Summarization Prompt\n            (see :ref:`Prompt-Templates`).\n        insert_prompt (Optional[BasePromptTemplate]): An Tree Insertion Prompt\n            (see :ref:`Prompt-Templates`).\n        num_children (int): The number of children each node should have.\n        build_tree (bool): Whether to build the tree during index construction.\n        show_progress (bool): Whether to show progress bars. Defaults to False.\n\n    \"\"\"\n\n    index_struct_cls = IndexGraph\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        objects: Optional[Sequence[IndexNode]] = None,\n        index_struct: Optional[IndexGraph] = None,\n        llm: Optional[LLM] = None,\n        summary_template: Optional[BasePromptTemplate] = None,\n        insert_prompt: Optional[BasePromptTemplate] = None,\n        num_children: int = 10,\n        build_tree: bool = True,\n        use_async: bool = False,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self.num_children = num_children\n        self.summary_template = summary_template or DEFAULT_SUMMARY_PROMPT\n        self.insert_prompt: BasePromptTemplate = insert_prompt or DEFAULT_INSERT_PROMPT\n        self.build_tree = build_tree\n        self._use_async = use_async\n        self._llm = llm or Settings.llm\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            show_progress=show_progress,\n            objects=objects,\n            **kwargs,\n        )\n\n    def as_retriever(\n        self,\n        retriever_mode: Union[str, TreeRetrieverMode] = TreeRetrieverMode.SELECT_LEAF,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        # NOTE: lazy import\n        from llama_index.core.indices.tree.all_leaf_retriever import (\n            TreeAllLeafRetriever,\n        )\n        from llama_index.core.indices.tree.select_leaf_embedding_retriever import (\n            TreeSelectLeafEmbeddingRetriever,\n        )\n        from llama_index.core.indices.tree.select_leaf_retriever import (\n            TreeSelectLeafRetriever,\n        )\n        from llama_index.core.indices.tree.tree_root_retriever import (\n            TreeRootRetriever,\n        )\n\n        self._validate_build_tree_required(TreeRetrieverMode(retriever_mode))\n\n        if retriever_mode == TreeRetrieverMode.SELECT_LEAF:\n            return TreeSelectLeafRetriever(self, object_map=self._object_map, **kwargs)\n        elif retriever_mode == TreeRetrieverMode.SELECT_LEAF_EMBEDDING:\n            embed_model = embed_model or Settings.embed_model\n            return TreeSelectLeafEmbeddingRetriever(\n                self, embed_model=embed_model, object_map=self._object_map, **kwargs\n            )\n        elif retriever_mode == TreeRetrieverMode.ROOT:\n            return TreeRootRetriever(self, object_map=self._object_map, **kwargs)\n        elif retriever_mode == TreeRetrieverMode.ALL_LEAF:\n            return TreeAllLeafRetriever(self, object_map=self._object_map, **kwargs)\n        else:\n            raise ValueError(f\"Unknown retriever mode: {retriever_mode}\")\n\n    def _validate_build_tree_required(self, retriever_mode: TreeRetrieverMode) -> None:\n        \"\"\"Check if index supports modes that require trees.\"\"\"\n        if retriever_mode in REQUIRE_TREE_MODES and not self.build_tree:\n            raise ValueError(\n                \"Index was constructed without building trees, \"\n                f\"but retriever mode {retriever_mode} requires trees.\"\n            )\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> IndexGraph:\n        \"\"\"Build the index from nodes.\"\"\"\n        index_builder = GPTTreeIndexBuilder(\n            self.num_children,\n            self.summary_template,\n            llm=self._llm,\n            use_async=self._use_async,\n            show_progress=self._show_progress,\n            docstore=self._docstore,\n        )\n        return index_builder.build_from_nodes(nodes, build_tree=self.build_tree)\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        # TODO: allow to customize insert prompt\n        inserter = TreeIndexInserter(\n            self.index_struct,\n            llm=self._llm,\n            num_children=self.num_children,\n            insert_prompt=self.insert_prompt,\n            summary_prompt=self.summary_template,\n            docstore=self._docstore,\n        )\n        inserter.insert(nodes)\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for tree index.\")\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        node_doc_ids = list(self.index_struct.all_nodes.values())\n        nodes = self.docstore.get_nodes(node_doc_ids)\n\n        all_ref_doc_info = {}\n        for node in nodes:\n            ref_node = node.source_node\n            if not ref_node:\n                continue\n\n            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)\n            if not ref_doc_info:\n                continue\n\n            all_ref_doc_info[ref_node.node_id] = ref_doc_info\n        return all_ref_doc_info\n\n\n# legacy\nGPTTreeIndex = TreeIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/tree_root_retriever.py",
    "filename": "tree_root_retriever.py",
    "relpath": "indices/tree/tree_root_retriever.py",
    "start_line": 1,
    "end_line": 48,
    "length": 48,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_retrieve"
    ],
    "chunk_class_names": [
      "TreeRootRetriever",
      "directly",
      "assumes"
    ],
    "document_function_names": [
      "__init__",
      "_retrieve"
    ],
    "document_class_names": [
      "TreeRootRetriever",
      "directly",
      "assumes"
    ],
    "content": "\"\"\"Retrieve query.\"\"\"\nimport logging\nfrom typing import Any, List, Optional\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.query.schema import QueryBundle\nfrom llama_index.core.indices.tree.base import TreeIndex\nfrom llama_index.core.indices.utils import get_sorted_node_list\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\n\nlogger = logging.getLogger(__name__)\n\n\nclass TreeRootRetriever(BaseRetriever):\n    \"\"\"Tree root retriever.\n\n    This class directly retrieves the answer from the root nodes.\n\n    Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores\n    the answer (because it was constructed with a query_str), so it does not\n    attempt to parse information down the graph in order to synthesize an answer.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: TreeIndex,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self._index = index\n        self._index_struct = index.index_struct\n        self._docstore = index.docstore\n        super().__init__(\n            callback_manager=callback_manager, object_map=object_map, verbose=verbose\n        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        logger.info(f\"> Starting query: {query_bundle.query_str}\")\n        root_nodes = self._docstore.get_node_dict(self._index_struct.root_nodes)\n        sorted_nodes = get_sorted_node_list(root_nodes)\n        return [NodeWithScore(node=node) for node in sorted_nodes]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_embedding_retriever.py",
    "filename": "select_leaf_embedding_retriever.py",
    "relpath": "indices/tree/select_leaf_embedding_retriever.py",
    "start_line": 1,
    "end_line": 157,
    "length": 157,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_query_level",
      "_get_query_text_embedding_similarities",
      "_get_most_similar_nodes",
      "_select_nodes"
    ],
    "chunk_class_names": [
      "TreeSelectLeafEmbeddingRetriever",
      "traverses"
    ],
    "document_function_names": [
      "__init__",
      "_query_level",
      "_get_query_text_embedding_similarities",
      "_get_most_similar_nodes",
      "_select_nodes"
    ],
    "document_class_names": [
      "TreeSelectLeafEmbeddingRetriever",
      "traverses"
    ],
    "content": "\"\"\"Query Tree using embedding similarity between query and node text.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, Tuple, cast\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.tree.base import TreeIndex\nfrom llama_index.core.indices.tree.select_leaf_retriever import (\n    TreeSelectLeafRetriever,\n)\nfrom llama_index.core.indices.utils import get_sorted_node_list\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.schema import BaseNode, MetadataMode, QueryBundle\nfrom llama_index.core.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass TreeSelectLeafEmbeddingRetriever(TreeSelectLeafRetriever):\n    \"\"\"Tree select leaf embedding retriever.\n\n    This class traverses the index graph using the embedding similarity between the\n    query and the node text.\n\n    Args:\n        query_template (Optional[BasePromptTemplate]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[BasePromptTemplate]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[BasePromptTemplate]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n        refine_template (Optional[BasePromptTemplate]): Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n        embed_model (Optional[BaseEmbedding]): Embedding model to use for\n            embedding similarity.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: TreeIndex,\n        embed_model: Optional[BaseEmbedding] = None,\n        query_template: Optional[BasePromptTemplate] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        refine_template: Optional[BasePromptTemplate] = None,\n        query_template_multiple: Optional[BasePromptTemplate] = None,\n        child_branch_factor: int = 1,\n        verbose: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        **kwargs: Any,\n    ):\n        super().__init__(\n            index,\n            query_template=query_template,\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n            query_template_multiple=query_template_multiple,\n            child_branch_factor=child_branch_factor,\n            verbose=verbose,\n            callback_manager=callback_manager,\n            object_map=object_map,\n            **kwargs,\n        )\n        self._embed_model = embed_model or Settings.embed_model\n\n    def _query_level(\n        self,\n        cur_node_ids: Dict[int, str],\n        query_bundle: QueryBundle,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Answer a query recursively.\"\"\"\n        cur_nodes = {\n            index: self._docstore.get_node(node_id)\n            for index, node_id in cur_node_ids.items()\n        }\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        # Get the node with the highest similarity to the query\n        selected_nodes, selected_indices = self._get_most_similar_nodes(\n            cur_node_list, query_bundle\n        )\n\n        result_response = None\n        for node, index in zip(selected_nodes, selected_indices):\n            logger.debug(\n                f\">[Level {level}] Node [{index + 1}] Summary text: \"\n                f\"{' '.join(node.get_content().splitlines())}\"\n            )\n\n            # Get the response for the selected node\n            result_response = self._query_with_selected_node(\n                node, query_bundle, level=level, prev_response=result_response\n            )\n\n        return cast(str, result_response)\n\n    def _get_query_text_embedding_similarities(\n        self, query_bundle: QueryBundle, nodes: List[BaseNode]\n    ) -> List[float]:\n        \"\"\"\n        Get query text embedding similarity.\n\n        Cache the query embedding and the node text embedding.\n\n        \"\"\"\n        if query_bundle.embedding is None:\n            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(\n                query_bundle.embedding_strs\n            )\n        similarities = []\n        for node in nodes:\n            if node.embedding is None:\n                node.embedding = self._embed_model.get_text_embedding(\n                    node.get_content(metadata_mode=MetadataMode.EMBED)\n                )\n\n            similarity = self._embed_model.similarity(\n                query_bundle.embedding, node.embedding\n            )\n            similarities.append(similarity)\n        return similarities\n\n    def _get_most_similar_nodes(\n        self, nodes: List[BaseNode], query_bundle: QueryBundle\n    ) -> Tuple[List[BaseNode], List[int]]:\n        \"\"\"Get the node with the highest similarity to the query.\"\"\"\n        similarities = self._get_query_text_embedding_similarities(query_bundle, nodes)\n\n        selected_nodes: List[BaseNode] = []\n        selected_indices: List[int] = []\n        for node, _ in sorted(\n            zip(nodes, similarities), key=lambda x: x[1], reverse=True\n        ):\n            if len(selected_nodes) < self.child_branch_factor:\n                selected_nodes.append(node)\n                selected_indices.append(nodes.index(node))\n            else:\n                break\n\n        return selected_nodes, selected_indices\n\n    def _select_nodes(\n        self,\n        cur_node_list: List[BaseNode],\n        query_bundle: QueryBundle,\n        level: int = 0,\n    ) -> List[BaseNode]:\n        selected_nodes, _ = self._get_most_similar_nodes(cur_node_list, query_bundle)\n        return selected_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/tree/__init__.py",
    "start_line": 1,
    "end_line": 22,
    "length": 22,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Tree-structured Index Data Structures.\"\"\"\n\n# indices\nfrom llama_index.core.indices.tree.all_leaf_retriever import TreeAllLeafRetriever\nfrom llama_index.core.indices.tree.base import GPTTreeIndex, TreeIndex\nfrom llama_index.core.indices.tree.select_leaf_embedding_retriever import (\n    TreeSelectLeafEmbeddingRetriever,\n)\nfrom llama_index.core.indices.tree.select_leaf_retriever import (\n    TreeSelectLeafRetriever,\n)\nfrom llama_index.core.indices.tree.tree_root_retriever import TreeRootRetriever\n\n__all__ = [\n    \"TreeIndex\",\n    \"TreeSelectLeafEmbeddingRetriever\",\n    \"TreeSelectLeafRetriever\",\n    \"TreeAllLeafRetriever\",\n    \"TreeRootRetriever\",\n    # legacy\n    \"GPTTreeIndex\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/utils.py",
    "filename": "utils.py",
    "relpath": "indices/tree/utils.py",
    "start_line": 1,
    "end_line": 26,
    "length": 26,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_numbered_text_from_nodes"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_numbered_text_from_nodes"
    ],
    "document_class_names": [],
    "content": "from typing import List, Optional\n\nfrom llama_index.core.node_parser.text import TokenTextSplitter\nfrom llama_index.core.node_parser.text.utils import truncate_text\nfrom llama_index.core.schema import BaseNode\n\n\ndef get_numbered_text_from_nodes(\n    node_list: List[BaseNode],\n    text_splitter: Optional[TokenTextSplitter] = None,\n) -> str:\n    \"\"\"Get text from nodes in the format of a numbered list.\n\n    Used by tree-structured indices.\n\n    \"\"\"\n    results = []\n    number = 1\n    for node in node_list:\n        node_text = \" \".join(node.get_content().splitlines())\n        if text_splitter is not None:\n            node_text = truncate_text(node_text, text_splitter)\n        text = f\"({number}) {node_text}\"\n        results.append(text)\n        number += 1\n    return \"\\n\\n\".join(results)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/all_leaf_retriever.py",
    "filename": "all_leaf_retriever.py",
    "relpath": "indices/tree/all_leaf_retriever.py",
    "start_line": 1,
    "end_line": 55,
    "length": 55,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_retrieve"
    ],
    "chunk_class_names": [
      "TreeAllLeafRetriever",
      "builds"
    ],
    "document_function_names": [
      "__init__",
      "_retrieve"
    ],
    "document_class_names": [
      "TreeAllLeafRetriever",
      "builds"
    ],
    "content": "\"\"\"Summarize query.\"\"\"\n\nimport logging\nfrom typing import Any, List, Optional, cast\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.data_structs.data_structs import IndexGraph\nfrom llama_index.core.indices.tree.base import TreeIndex\nfrom llama_index.core.indices.utils import get_sorted_node_list\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_NUM_CHILDREN = 10\n\n\nclass TreeAllLeafRetriever(BaseRetriever):\n    \"\"\"GPT all leaf retriever.\n\n    This class builds a query-specific tree from leaf nodes to return a response.\n    Using this query mode means that the tree index doesn't need to be built\n    when initialized, since we rebuild the tree for each query.\n\n    Args:\n        text_qa_template (Optional[BasePromptTemplate]): Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: TreeIndex,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self._index = index\n        self._index_struct = index.index_struct\n        self._docstore = index.docstore\n        super().__init__(\n            callback_manager=callback_manager, object_map=object_map, verbose=verbose\n        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        logger.info(f\"> Starting query: {query_bundle.query_str}\")\n        index_struct = cast(IndexGraph, self._index_struct)\n        all_nodes = self._docstore.get_node_dict(index_struct.all_nodes)\n        sorted_node_list = get_sorted_node_list(all_nodes)\n        return [NodeWithScore(node=node) for node in sorted_node_list]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/tree/select_leaf_retriever.py",
    "filename": "select_leaf_retriever.py",
    "relpath": "indices/tree/select_leaf_retriever.py",
    "start_line": 1,
    "end_line": 426,
    "length": 426,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_text_from_node",
      "__init__",
      "_query_with_selected_node",
      "_query_level",
      "_query",
      "_select_nodes",
      "_retrieve_level",
      "_retrieve"
    ],
    "chunk_class_names": [
      "TreeSelectLeafRetriever",
      "traverses",
      "info_str"
    ],
    "document_function_names": [
      "get_text_from_node",
      "__init__",
      "_query_with_selected_node",
      "_query_level",
      "_query",
      "_select_nodes",
      "_retrieve_level",
      "_retrieve"
    ],
    "document_class_names": [
      "TreeSelectLeafRetriever",
      "traverses",
      "info_str"
    ],
    "content": "\"\"\"Leaf query mechanism.\"\"\"\n\nimport logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.indices.query.schema import QueryBundle\nfrom llama_index.core.indices.tree.base import TreeIndex\nfrom llama_index.core.indices.tree.utils import get_numbered_text_from_nodes\nfrom llama_index.core.indices.utils import (\n    extract_numbers_given_response,\n    get_sorted_node_list,\n)\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_REFINE_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_QUERY_PROMPT,\n    DEFAULT_QUERY_PROMPT_MULTIPLE,\n    DEFAULT_TEXT_QA_PROMPT,\n)\nfrom llama_index.core.response_synthesizers import get_response_synthesizer\nfrom llama_index.core.schema import (\n    BaseNode,\n    MetadataMode,\n    NodeWithScore,\n    QueryBundle,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utils import print_text, truncate_text\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_text_from_node(\n    node: BaseNode,\n    level: Optional[int] = None,\n    verbose: bool = False,\n) -> str:\n    \"\"\"Get text from node.\"\"\"\n    level_str = \"\" if level is None else f\"[Level {level}]\"\n    fmt_text_chunk = truncate_text(node.get_content(metadata_mode=MetadataMode.LLM), 50)\n    logger.debug(f\">{level_str} Searching in chunk: {fmt_text_chunk}\")\n\n    response_txt = node.get_content(metadata_mode=MetadataMode.LLM)\n    fmt_response = truncate_text(response_txt, 200)\n    if verbose:\n        print_text(f\">{level_str} Got node text: {fmt_response}\\n\", color=\"blue\")\n    return response_txt\n\n\nclass TreeSelectLeafRetriever(BaseRetriever):\n    \"\"\"Tree select leaf retriever.\n\n    This class traverses the index graph and searches for a leaf node that can best\n    answer the query.\n\n    Args:\n        query_template (Optional[BasePromptTemplate]): Tree Select Query Prompt\n            (see :ref:`Prompt-Templates`).\n        query_template_multiple (Optional[BasePromptTemplate]): Tree Select\n            Query Prompt (Multiple)\n            (see :ref:`Prompt-Templates`).\n        child_branch_factor (int): Number of child nodes to consider at each level.\n            If child_branch_factor is 1, then the query will only choose one child node\n            to traverse for any given parent node.\n            If child_branch_factor is 2, then the query will choose two child nodes.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: TreeIndex,\n        query_template: Optional[BasePromptTemplate] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        refine_template: Optional[BasePromptTemplate] = None,\n        query_template_multiple: Optional[BasePromptTemplate] = None,\n        child_branch_factor: int = 1,\n        verbose: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        **kwargs: Any,\n    ):\n        self._index = index\n        self._llm = index._llm\n        self._index_struct = index.index_struct\n        self._docstore = index.docstore\n        self._prompt_helper = Settings._prompt_helper or PromptHelper.from_llm_metadata(\n            self._llm.metadata,\n        )\n\n        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self._refine_template = refine_template or DEFAULT_REFINE_PROMPT_SEL\n        self.query_template = query_template or DEFAULT_QUERY_PROMPT\n        self.query_template_multiple = (\n            query_template_multiple or DEFAULT_QUERY_PROMPT_MULTIPLE\n        )\n        self.child_branch_factor = child_branch_factor\n        super().__init__(\n            callback_manager=callback_manager or Settings.callback_manager,\n            object_map=object_map,\n            verbose=verbose,\n        )\n\n    def _query_with_selected_node(\n        self,\n        selected_node: BaseNode,\n        query_bundle: QueryBundle,\n        prev_response: Optional[str] = None,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Get response for selected node.\n\n        If not leaf node, it will recursively call _query on the child nodes.\n        If prev_response is provided, we will update prev_response with the answer.\n\n        \"\"\"\n        query_str = query_bundle.query_str\n\n        if len(self._index_struct.get_children(selected_node)) == 0:\n            response_builder = get_response_synthesizer(\n                llm=self._llm,\n                text_qa_template=self._text_qa_template,\n                refine_template=self._refine_template,\n                callback_manager=self.callback_manager,\n            )\n            # use response builder to get answer from node\n            node_text = get_text_from_node(selected_node, level=level)\n            cur_response = response_builder.get_response(\n                query_str, [node_text], prev_response=prev_response\n            )\n            cur_response = str(cur_response)\n            logger.debug(f\">[Level {level}] Current answer response: {cur_response} \")\n        else:\n            cur_response = self._query_level(\n                self._index_struct.get_children(selected_node),\n                query_bundle,\n                level=level + 1,\n            )\n\n        if prev_response is None:\n            return cur_response\n        else:\n            context_msg = selected_node.get_content(metadata_mode=MetadataMode.LLM)\n            cur_response = self._llm.predict(\n                self._refine_template,\n                query_str=query_str,\n                existing_answer=prev_response,\n                context_msg=context_msg,\n            )\n\n            logger.debug(f\">[Level {level}] Current refined response: {cur_response} \")\n            return str(cur_response)\n\n    def _query_level(\n        self,\n        cur_node_ids: Dict[int, str],\n        query_bundle: QueryBundle,\n        level: int = 0,\n    ) -> str:\n        \"\"\"Answer a query recursively.\"\"\"\n        query_str = query_bundle.query_str\n        cur_nodes = {\n            index: self._docstore.get_node(node_id)\n            for index, node_id in cur_node_ids.items()\n        }\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        if len(cur_node_list) == 1:\n            logger.debug(f\">[Level {level}] Only one node left. Querying node.\")\n            return self._query_with_selected_node(\n                cur_node_list[0], query_bundle, level=level\n            )\n        elif self.child_branch_factor == 1:\n            query_template = self.query_template.partial_format(\n                num_chunks=len(cur_node_list), query_str=query_str\n            )\n            text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n                prompt=query_template,\n                num_chunks=len(cur_node_list),\n                llm=self._llm,\n            )\n            numbered_node_text = get_numbered_text_from_nodes(\n                cur_node_list, text_splitter=text_splitter\n            )\n\n            response = self._llm.predict(\n                query_template,\n                context_list=numbered_node_text,\n            )\n        else:\n            query_template_multiple = self.query_template_multiple.partial_format(\n                num_chunks=len(cur_node_list),\n                query_str=query_str,\n                branching_factor=self.child_branch_factor,\n            )\n\n            text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n                prompt=query_template_multiple,\n                num_chunks=len(cur_node_list),\n                llm=self._llm,\n            )\n            numbered_node_text = get_numbered_text_from_nodes(\n                cur_node_list, text_splitter=text_splitter\n            )\n\n            response = self._llm.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,\n            )\n\n        debug_str = f\">[Level {level}] Current response: {response}\"\n        logger.debug(debug_str)\n        if self._verbose:\n            print_text(debug_str, end=\"\\n\")\n\n        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers is None:\n            debug_str = (\n                f\">[Level {level}] Could not retrieve response - no numbers present\"\n            )\n            logger.debug(debug_str)\n            if self._verbose:\n                print_text(debug_str, end=\"\\n\")\n            # just join text from current nodes as response\n            return response\n        result_response = None\n        for number_str in numbers:\n            number = int(number_str)\n            if number > len(cur_node_list):\n                logger.debug(\n                    f\">[Level {level}] Invalid response: {response} - \"\n                    f\"number {number} out of range\"\n                )\n                return response\n\n            # number is 1-indexed, so subtract 1\n            selected_node = cur_node_list[number - 1]\n\n            info_str = (\n                f\">[Level {level}] Selected node: \"\n                f\"[{number}]/[{','.join([str(int(n)) for n in numbers])}]\"\n            )\n            logger.info(info_str)\n            if self._verbose:\n                print_text(info_str, end=\"\\n\")\n            debug_str = \" \".join(\n                selected_node.get_content(metadata_mode=MetadataMode.LLM).splitlines()\n            )\n            full_debug_str = (\n                f\">[Level {level}] Node \"\n                f\"[{number}] Summary text: \"\n                f\"{selected_node.get_content(metadata_mode=MetadataMode.LLM)}\"\n            )\n            logger.debug(full_debug_str)\n            if self._verbose:\n                print_text(full_debug_str, end=\"\\n\")\n            result_response = self._query_with_selected_node(\n                selected_node,\n                query_bundle,\n                prev_response=result_response,\n                level=level,\n            )\n        # result_response should not be None\n        return cast(str, result_response)\n\n    def _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # NOTE: this overrides the _query method in the base class\n        info_str = f\"> Starting query: {query_bundle.query_str}\"\n        logger.info(info_str)\n        if self._verbose:\n            print_text(info_str, end=\"\\n\")\n        response_str = self._query_level(\n            self._index_struct.root_nodes,\n            query_bundle,\n            level=0,\n        ).strip()\n        # TODO: fix source nodes\n        return Response(response_str, source_nodes=[])\n\n    def _select_nodes(\n        self,\n        cur_node_list: List[BaseNode],\n        query_bundle: QueryBundle,\n        level: int = 0,\n    ) -> List[BaseNode]:\n        query_str = query_bundle.query_str\n\n        if self.child_branch_factor == 1:\n            query_template = self.query_template.partial_format(\n                num_chunks=len(cur_node_list), query_str=query_str\n            )\n            text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n                prompt=query_template,\n                num_chunks=len(cur_node_list),\n                llm=self._llm,\n            )\n            numbered_node_text = get_numbered_text_from_nodes(\n                cur_node_list, text_splitter=text_splitter\n            )\n\n            response = self._llm.predict(\n                query_template,\n                context_list=numbered_node_text,\n            )\n        else:\n            query_template_multiple = self.query_template_multiple.partial_format(\n                num_chunks=len(cur_node_list),\n                query_str=query_str,\n                branching_factor=self.child_branch_factor,\n            )\n\n            text_splitter = self._prompt_helper.get_text_splitter_given_prompt(\n                prompt=query_template_multiple,\n                num_chunks=len(cur_node_list),\n                llm=self._llm,\n            )\n            numbered_node_text = get_numbered_text_from_nodes(\n                cur_node_list, text_splitter=text_splitter\n            )\n\n            response = self._llm.predict(\n                query_template_multiple,\n                context_list=numbered_node_text,\n            )\n\n        debug_str = f\">[Level {level}] Current response: {response}\"\n        logger.debug(debug_str)\n        if self._verbose:\n            print_text(debug_str, end=\"\\n\")\n\n        numbers = extract_numbers_given_response(response, n=self.child_branch_factor)\n        if numbers is None:\n            debug_str = (\n                f\">[Level {level}] Could not retrieve response - no numbers present\"\n            )\n            logger.debug(debug_str)\n            if self._verbose:\n                print_text(debug_str, end=\"\\n\")\n            # just join text from current nodes as response\n            return []\n\n        selected_nodes = []\n        for number_str in numbers:\n            number = int(number_str)\n            if number > len(cur_node_list):\n                logger.debug(\n                    f\">[Level {level}] Invalid response: {response} - \"\n                    f\"number {number} out of range\"\n                )\n                continue\n\n            # number is 1-indexed, so subtract 1\n            selected_node = cur_node_list[number - 1]\n\n            info_str = (\n                f\">[Level {level}] Selected node: \"\n                f\"[{number}]/[{','.join([str(int(n)) for n in numbers])}]\"\n            )\n            logger.info(info_str)\n            if self._verbose:\n                print_text(info_str, end=\"\\n\")\n            debug_str = \" \".join(\n                selected_node.get_content(metadata_mode=MetadataMode.LLM).splitlines()\n            )\n            full_debug_str = (\n                f\">[Level {level}] Node \"\n                f\"[{number}] Summary text: \"\n                f\"{selected_node.get_content(metadata_mode=MetadataMode.LLM)}\"\n            )\n            logger.debug(full_debug_str)\n            if self._verbose:\n                print_text(full_debug_str, end=\"\\n\")\n            selected_nodes.append(selected_node)\n\n        return selected_nodes\n\n    def _retrieve_level(\n        self,\n        cur_node_ids: Dict[int, str],\n        query_bundle: QueryBundle,\n        level: int = 0,\n    ) -> List[BaseNode]:\n        \"\"\"Answer a query recursively.\"\"\"\n        cur_nodes = {\n            index: self._docstore.get_node(node_id)\n            for index, node_id in cur_node_ids.items()\n        }\n        cur_node_list = get_sorted_node_list(cur_nodes)\n\n        if len(cur_node_list) > self.child_branch_factor:\n            selected_nodes = self._select_nodes(\n                cur_node_list,\n                query_bundle,\n                level=level,\n            )\n        else:\n            selected_nodes = cur_node_list\n\n        children_nodes = {}\n        for node in selected_nodes:\n            node_dict = self._index_struct.get_children(node)\n            children_nodes.update(node_dict)\n\n        if len(children_nodes) == 0:\n            # NOTE: leaf level\n            return selected_nodes\n        else:\n            return self._retrieve_level(children_nodes, query_bundle, level + 1)\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        nodes = self._retrieve_level(\n            self._index_struct.root_nodes,\n            query_bundle,\n            level=0,\n        )\n        return [NodeWithScore(node=node) for node in nodes]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/base.py",
    "filename": "base.py",
    "relpath": "indices/property_graph/base.py",
    "start_line": 1,
    "end_line": 405,
    "length": 405,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_existing",
      "property_graph_store",
      "vector_store",
      "_insert_nodes",
      "_insert_nodes_to_vector_index",
      "_build_index_from_nodes",
      "as_retriever",
      "_delete_node",
      "_insert",
      "ref_doc_info"
    ],
    "chunk_class_names": [
      "PropertyGraphIndex",
      "params",
      "params"
    ],
    "document_function_names": [
      "__init__",
      "from_existing",
      "property_graph_store",
      "vector_store",
      "_insert_nodes",
      "_insert_nodes_to_vector_index",
      "_build_index_from_nodes",
      "as_retriever",
      "_delete_node",
      "_insert",
      "ref_doc_info"
    ],
    "document_class_names": [
      "PropertyGraphIndex",
      "params",
      "params"
    ],
    "content": "import asyncio\nfrom typing import Any, Dict, List, Optional, Sequence, Type, TYPE_CHECKING\n\nfrom llama_index.core.data_structs import IndexLPG\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.embeddings.utils import EmbedType, resolve_embed_model\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.graph_stores.simple_labelled import SimplePropertyGraphStore\nfrom llama_index.core.graph_stores.types import (\n    KG_NODES_KEY,\n    KG_RELATIONS_KEY,\n    VECTOR_SOURCE_KEY,\n)\nfrom llama_index.core.vector_stores.simple import DEFAULT_VECTOR_STORE\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.property_graph.transformations import (\n    SimpleLLMPathExtractor,\n    ImplicitPathExtractor,\n)\nfrom llama_index.core.ingestion.pipeline import (\n    run_transformations,\n    arun_transformations,\n)\nfrom llama_index.core.graph_stores.types import (\n    LabelledNode,\n    Relation,\n    PropertyGraphStore,\n    TRIPLET_SOURCE_KEY,\n)\nfrom llama_index.core.storage.docstore.types import RefDocInfo\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode, TransformComponent\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.vector_stores.types import BasePydanticVectorStore\n\nif TYPE_CHECKING:\n    from llama_index.core.indices.property_graph.sub_retrievers.base import (\n        BasePGRetriever,\n    )\n\n\nclass PropertyGraphIndex(BaseIndex[IndexLPG]):\n    \"\"\"An index for a property graph.\n\n    Args:\n        nodes (Optional[Sequence[BaseNode]]):\n            A list of nodes to insert into the index.\n        llm (Optional[LLM]):\n            The language model to use for extracting triplets. Defaults to `Settings.llm`.\n        kg_extractors (Optional[List[TransformComponent]]):\n            A list of transformations to apply to the nodes to extract triplets.\n            Defaults to `[SimpleLLMPathExtractor(llm=llm), ImplicitEdgeExtractor()]`.\n        property_graph_store (Optional[PropertyGraphStore]):\n            The property graph store to use. If not provided, a new `SimplePropertyGraphStore` will be created.\n        vector_store (Optional[BasePydanticVectorStore]):\n            The vector store index to use, if the graph store does not support vector queries.\n        use_async (bool):\n            Whether to use async for transformations. Defaults to `True`.\n        embed_model (Optional[EmbedType]):\n            The embedding model to use for embedding nodes.\n            If not provided, `Settings.embed_model` will be used if `embed_kg_nodes=True`.\n        embed_kg_nodes (bool):\n            Whether to embed the KG nodes. Defaults to `True`.\n        callback_manager (Optional[CallbackManager]):\n            The callback manager to use.\n        transformations (Optional[List[TransformComponent]]):\n            A list of transformations to apply to the nodes before inserting them into the index.\n            These are applied prior to the `kg_extractors`.\n        storage_context (Optional[StorageContext]):\n            The storage context to use.\n        show_progress (bool):\n            Whether to show progress bars for transformations. Defaults to `False`.\n    \"\"\"\n\n    index_struct_cls = IndexLPG\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        llm: Optional[LLM] = None,\n        kg_extractors: Optional[List[TransformComponent]] = None,\n        property_graph_store: Optional[PropertyGraphStore] = None,\n        # vector related params\n        vector_store: Optional[BasePydanticVectorStore] = None,\n        use_async: bool = True,\n        embed_model: Optional[EmbedType] = None,\n        embed_kg_nodes: bool = True,\n        # parent class params\n        callback_manager: Optional[CallbackManager] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        storage_context: Optional[StorageContext] = None,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        storage_context = storage_context or StorageContext.from_defaults(\n            property_graph_store=property_graph_store\n        )\n\n        # lazily initialize the graph store on the storage context\n        if property_graph_store is not None:\n            storage_context.property_graph_store = property_graph_store\n        elif storage_context.property_graph_store is None:\n            storage_context.property_graph_store = SimplePropertyGraphStore()\n\n        if vector_store is not None:\n            storage_context.vector_stores[DEFAULT_VECTOR_STORE] = vector_store\n\n        if embed_kg_nodes and (\n            storage_context.property_graph_store.supports_vector_queries\n            or embed_kg_nodes\n        ):\n            self._embed_model = (\n                resolve_embed_model(embed_model)\n                if embed_model\n                else Settings.embed_model\n            )\n        else:\n            self._embed_model = None  # type: ignore\n\n        self._kg_extractors = kg_extractors or [\n            SimpleLLMPathExtractor(llm=llm or Settings.llm),\n            ImplicitPathExtractor(),\n        ]\n        self._use_async = use_async\n        self._llm = llm\n        self._embed_kg_nodes = embed_kg_nodes\n        self._override_vector_store = (\n            vector_store is not None\n            or not storage_context.property_graph_store.supports_vector_queries\n        )\n\n        super().__init__(\n            nodes=nodes,\n            callback_manager=callback_manager,\n            storage_context=storage_context,\n            transformations=transformations,\n            show_progress=show_progress,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_existing(\n        cls: Type[\"PropertyGraphIndex\"],\n        property_graph_store: PropertyGraphStore,\n        vector_store: Optional[BasePydanticVectorStore] = None,\n        # general params\n        llm: Optional[LLM] = None,\n        kg_extractors: Optional[List[TransformComponent]] = None,\n        # vector related params\n        use_async: bool = True,\n        embed_model: Optional[EmbedType] = None,\n        embed_kg_nodes: bool = True,\n        # parent class params\n        callback_manager: Optional[CallbackManager] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        storage_context: Optional[StorageContext] = None,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> \"PropertyGraphIndex\":\n        \"\"\"Create an index from an existing property graph store (and optional vector store).\"\"\"\n        return cls(\n            nodes=[],  # no nodes to insert\n            property_graph_store=property_graph_store,\n            vector_store=vector_store,\n            llm=llm,\n            kg_extractors=kg_extractors,\n            use_async=use_async,\n            embed_model=embed_model,\n            embed_kg_nodes=embed_kg_nodes,\n            callback_manager=callback_manager,\n            transformations=transformations,\n            storage_context=storage_context,\n            show_progress=show_progress,\n            **kwargs,\n        )\n\n    @property\n    def property_graph_store(self) -> PropertyGraphStore:\n        \"\"\"Get the labelled property graph store.\"\"\"\n        assert self.storage_context.property_graph_store is not None\n\n        return self.storage_context.property_graph_store\n\n    @property\n    def vector_store(self) -> Optional[BasePydanticVectorStore]:\n        if self._embed_kg_nodes and self._override_vector_store:\n            return self.storage_context.vector_store\n        else:\n            return None\n\n    def _insert_nodes(self, nodes: Sequence[BaseNode]) -> Sequence[BaseNode]:\n        \"\"\"Insert nodes to the index struct.\"\"\"\n        if len(nodes) == 0:\n            return nodes\n\n        # run transformations on nodes to extract triplets\n        if self._use_async:\n            nodes = asyncio.run(\n                arun_transformations(\n                    nodes, self._kg_extractors, show_progress=self._show_progress\n                )\n            )\n        else:\n            nodes = run_transformations(\n                nodes, self._kg_extractors, show_progress=self._show_progress\n            )\n\n        # ensure all nodes have nodes and/or relations in metadata\n        assert all(\n            node.metadata.get(KG_NODES_KEY) is not None\n            or node.metadata.get(KG_RELATIONS_KEY) is not None\n            for node in nodes\n        )\n\n        kg_nodes_to_insert: List[LabelledNode] = []\n        kg_rels_to_insert: List[Relation] = []\n        for node in nodes:\n            # remove nodes and relations from metadata\n            kg_nodes = node.metadata.pop(KG_NODES_KEY, [])\n            kg_rels = node.metadata.pop(KG_RELATIONS_KEY, [])\n\n            # add source id to properties\n            for kg_node in kg_nodes:\n                kg_node.properties[TRIPLET_SOURCE_KEY] = node.id_\n            for kg_rel in kg_rels:\n                kg_rel.properties[TRIPLET_SOURCE_KEY] = node.id_\n\n            # add nodes and relations to insert lists\n            kg_nodes_to_insert.extend(kg_nodes)\n            kg_rels_to_insert.extend(kg_rels)\n\n        # filter out duplicate kg nodes\n        kg_node_ids = {node.id for node in kg_nodes_to_insert}\n        existing_kg_nodes = self.property_graph_store.get(ids=list(kg_node_ids))\n        existing_kg_node_ids = {node.id for node in existing_kg_nodes}\n        kg_nodes_to_insert = [\n            node for node in kg_nodes_to_insert if node.id not in existing_kg_node_ids\n        ]\n\n        # filter out duplicate llama nodes\n        existing_nodes = self.property_graph_store.get_llama_nodes(\n            [node.id_ for node in nodes]\n        )\n        existing_node_hashes = {node.hash for node in existing_nodes}\n        nodes = [node for node in nodes if node.hash not in existing_node_hashes]\n\n        # embed nodes (if needed)\n        if self._embed_kg_nodes:\n            # embed llama-index nodes\n            node_texts = [\n                node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes\n            ]\n\n            if self._use_async:\n                embeddings = asyncio.run(\n                    self._embed_model.aget_text_embedding_batch(\n                        node_texts, show_progress=self._show_progress\n                    )\n                )\n            else:\n                embeddings = self._embed_model.get_text_embedding_batch(\n                    node_texts, show_progress=self._show_progress\n                )\n\n            for node, embedding in zip(nodes, embeddings):\n                node.embedding = embedding\n\n            # embed kg nodes\n            kg_node_texts = [str(kg_node) for kg_node in kg_nodes_to_insert]\n\n            if self._use_async:\n                kg_embeddings = asyncio.run(\n                    self._embed_model.aget_text_embedding_batch(\n                        kg_node_texts, show_progress=self._show_progress\n                    )\n                )\n            else:\n                kg_embeddings = self._embed_model.get_text_embedding_batch(\n                    kg_node_texts,\n                    show_progress=self._show_progress,\n                )\n\n            for kg_node, embedding in zip(kg_nodes_to_insert, kg_embeddings):\n                kg_node.embedding = embedding\n\n        # if graph store doesn't support vectors, or the vector index was provided, use it\n        if self.vector_store is not None and len(kg_nodes_to_insert) > 0:\n            self._insert_nodes_to_vector_index(kg_nodes_to_insert)\n\n        if len(nodes) > 0:\n            self.property_graph_store.upsert_llama_nodes(nodes)\n\n        if len(kg_nodes_to_insert) > 0:\n            self.property_graph_store.upsert_nodes(kg_nodes_to_insert)\n\n        # important: upsert relations after nodes\n        if len(kg_rels_to_insert) > 0:\n            self.property_graph_store.upsert_relations(kg_rels_to_insert)\n\n        # refresh schema if needed\n        if self.property_graph_store.supports_structured_queries:\n            self.property_graph_store.get_schema(refresh=True)\n\n        return nodes\n\n    def _insert_nodes_to_vector_index(self, nodes: List[LabelledNode]) -> None:\n        \"\"\"Insert vector nodes.\"\"\"\n        assert self.vector_store is not None\n\n        llama_nodes: List[TextNode] = []\n        for node in nodes:\n            if node.embedding is not None:\n                llama_nodes.append(\n                    TextNode(\n                        text=str(node),\n                        metadata={VECTOR_SOURCE_KEY: node.id, **node.properties},\n                        embedding=[*node.embedding],\n                    )\n                )\n                if not self.vector_store.stores_text:\n                    llama_nodes[-1].id_ = node.id\n\n            # clear the embedding to save memory, its not used now\n            node.embedding = None\n\n        self.vector_store.add(llama_nodes)\n\n    def _build_index_from_nodes(\n        self, nodes: Optional[Sequence[BaseNode]], **build_kwargs: Any\n    ) -> IndexLPG:\n        \"\"\"Build index from nodes.\"\"\"\n        nodes = self._insert_nodes(nodes or [])\n\n        # this isn't really used or needed\n        return IndexLPG()\n\n    def as_retriever(\n        self,\n        sub_retrievers: Optional[List[\"BasePGRetriever\"]] = None,\n        include_text: bool = True,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        \"\"\"Return a retriever for the index.\n\n        Args:\n            sub_retrievers (Optional[List[BasePGRetriever]]):\n                A list of sub-retrievers to use. If not provided, a default list will be used:\n                `[LLMSynonymRetriever, VectorContextRetriever]` if the graph store supports vector queries.\n            include_text (bool):\n                Whether to include source-text in the retriever results.\n            **kwargs:\n                Additional kwargs to pass to the retriever.\n        \"\"\"\n        from llama_index.core.indices.property_graph.retriever import (\n            PGRetriever,\n        )\n        from llama_index.core.indices.property_graph.sub_retrievers.vector import (\n            VectorContextRetriever,\n        )\n        from llama_index.core.indices.property_graph.sub_retrievers.llm_synonym import (\n            LLMSynonymRetriever,\n        )\n\n        if sub_retrievers is None:\n            sub_retrievers = [\n                LLMSynonymRetriever(\n                    graph_store=self.property_graph_store,\n                    include_text=include_text,\n                    llm=self._llm,\n                    **kwargs,\n                ),\n            ]\n\n            if self._embed_model and (\n                self.property_graph_store.supports_vector_queries or self.vector_store\n            ):\n                sub_retrievers.append(\n                    VectorContextRetriever(\n                        graph_store=self.property_graph_store,\n                        vector_store=self.vector_store,\n                        include_text=include_text,\n                        embed_model=self._embed_model,\n                        **kwargs,\n                    )\n                )\n\n        return PGRetriever(sub_retrievers, use_async=self._use_async, **kwargs)\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        self.property_graph_store.delete(ids=[node_id])\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Index-specific logic for inserting nodes to the index struct.\"\"\"\n        self._insert_nodes(nodes)\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        raise NotImplementedError(\n            \"Ref doc info not implemented for PropertyGraphIndex. \"\n            \"All inserts are already upserts.\"\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/property_graph/__init__.py",
    "start_line": 1,
    "end_line": 53,
    "length": 53,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.indices.property_graph.base import PropertyGraphIndex\nfrom llama_index.core.indices.property_graph.retriever import PGRetriever\nfrom llama_index.core.indices.property_graph.sub_retrievers.base import BasePGRetriever\nfrom llama_index.core.indices.property_graph.sub_retrievers.custom import (\n    CustomPGRetriever,\n    CUSTOM_RETRIEVE_TYPE,\n)\nfrom llama_index.core.indices.property_graph.sub_retrievers.cypher_template import (\n    CypherTemplateRetriever,\n)\nfrom llama_index.core.indices.property_graph.sub_retrievers.llm_synonym import (\n    LLMSynonymRetriever,\n)\nfrom llama_index.core.indices.property_graph.sub_retrievers.text_to_cypher import (\n    TextToCypherRetriever,\n)\nfrom llama_index.core.indices.property_graph.sub_retrievers.vector import (\n    VectorContextRetriever,\n)\nfrom llama_index.core.indices.property_graph.transformations.implicit import (\n    ImplicitPathExtractor,\n)\nfrom llama_index.core.indices.property_graph.transformations.schema_llm import (\n    SchemaLLMPathExtractor,\n)\nfrom llama_index.core.indices.property_graph.transformations.simple_llm import (\n    SimpleLLMPathExtractor,\n)\nfrom llama_index.core.indices.property_graph.transformations.dynamic_llm import (\n    DynamicLLMPathExtractor,\n)\nfrom llama_index.core.indices.property_graph.utils import default_parse_triplets_fn\n\n__all__ = [\n    # Index\n    \"PropertyGraphIndex\",\n    # Retrievers\n    \"PGRetriever\",\n    \"BasePGRetriever\",\n    \"CustomPGRetriever\",\n    \"CypherTemplateRetriever\",\n    \"LLMSynonymRetriever\",\n    \"TextToCypherRetriever\",\n    \"VectorContextRetriever\",\n    # Transformations / Extractors\n    \"ImplicitPathExtractor\",\n    \"SchemaLLMPathExtractor\",\n    \"SimpleLLMPathExtractor\",\n    \"DynamicLLMPathExtractor\",\n    # Utils\n    \"default_parse_triplets_fn\",\n    \"CUSTOM_RETRIEVE_TYPE\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/utils.py",
    "filename": "utils.py",
    "relpath": "indices/property_graph/utils.py",
    "start_line": 1,
    "end_line": 37,
    "length": 37,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "default_parse_triplets_fn"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "default_parse_triplets_fn"
    ],
    "document_class_names": [],
    "content": "from typing import List, Tuple\n\n\ndef default_parse_triplets_fn(\n    response: str, max_length: int = 128\n) -> List[Tuple[str, str, str]]:\n    knowledge_strs = response.strip().split(\"\\n\")\n    results = []\n    for text in knowledge_strs:\n        if \"(\" not in text or \")\" not in text or text.index(\")\") < text.index(\"(\"):\n            # skip empty lines and non-triplets\n            continue\n        triplet_part = text[text.index(\"(\") + 1 : text.index(\")\")]\n        tokens = triplet_part.split(\",\")\n        if len(tokens) != 3:\n            continue\n\n        if any(len(s.encode(\"utf-8\")) > max_length for s in tokens):\n            # We count byte-length instead of len() for UTF-8 chars,\n            # will skip if any of the tokens are too long.\n            # This is normally due to a poorly formatted triplet\n            # extraction, in more serious KG building cases\n            # we'll need NLP models to better extract triplets.\n            continue\n\n        subj, pred, obj = map(str.strip, tokens)\n        if not subj or not pred or not obj:\n            # skip partial triplets\n            continue\n\n        # Strip double quotes and Capitalize triplets for disambiguation\n        subj, pred, obj = (\n            entity.strip('\"').capitalize() for entity in [subj, pred, obj]\n        )\n\n        results.append((subj, pred, obj))\n    return results"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/retriever.py",
    "filename": "retriever.py",
    "relpath": "indices/property_graph/retriever.py",
    "start_line": 1,
    "end_line": 69,
    "length": 69,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_deduplicate",
      "_retrieve",
      "_aretrieve"
    ],
    "chunk_class_names": [
      "PGRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_deduplicate",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "PGRetriever"
    ],
    "content": "from tqdm import tqdm\nfrom typing import Any, List\n\nfrom llama_index.core.async_utils import asyncio_run, run_jobs\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.indices.property_graph.sub_retrievers.base import (\n    BasePGRetriever,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\n\n\nclass PGRetriever(BaseRetriever):\n    \"\"\"A retriever that uses multiple sub-retrievers to retrieve nodes from a property graph.\n\n    Args:\n        sub_retrievers (List[BasePGRetriever]):\n            The sub-retrievers to use.\n        num_workers (int, optional):\n            The number of workers to use for async retrieval. Defaults to 4.\n        use_async (bool, optional):\n            Whether to use async retrieval. Defaults to True.\n        show_progress (bool, optional):\n            Whether to show progress bars. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        sub_retrievers: List[BasePGRetriever],\n        num_workers: int = 4,\n        use_async: bool = True,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self.sub_retrievers = sub_retrievers\n        self.use_async = use_async\n        self.num_workers = num_workers\n        self.show_progress = show_progress\n\n    def _deduplicate(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n        seen = set()\n        deduped = []\n        for node in nodes:\n            if node.text not in seen:\n                deduped.append(node)\n                seen.add(node.text)\n\n        return deduped\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        results = []\n        if self.use_async:\n            return asyncio_run(self._aretrieve(query_bundle))\n\n        for sub_retriever in tqdm(self.sub_retrievers, disable=not self.show_progress):\n            results.extend(sub_retriever.retrieve(query_bundle))\n\n        return self._deduplicate(results)\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        tasks = []\n        for sub_retriever in self.sub_retrievers:\n            tasks.append(sub_retriever.aretrieve(query_bundle))\n\n        async_results = await run_jobs(\n            tasks, workers=self.num_workers, show_progress=self.show_progress\n        )\n\n        # flatten the results\n        return self._deduplicate([node for nodes in async_results for node in nodes])"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/transformations/simple_llm.py",
    "filename": "simple_llm.py",
    "relpath": "indices/property_graph/transformations/simple_llm.py",
    "start_line": 1,
    "end_line": 128,
    "length": 128,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "__call__",
      "_aextract",
      "acall"
    ],
    "chunk_class_names": [
      "SimpleLLMPathExtractor"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "__call__",
      "_aextract",
      "acall"
    ],
    "document_class_names": [
      "SimpleLLMPathExtractor"
    ],
    "content": "import asyncio\nfrom typing import Any, Callable, Optional, Sequence, Union\n\nfrom llama_index.core.async_utils import run_jobs\nfrom llama_index.core.indices.property_graph.utils import (\n    default_parse_triplets_fn,\n)\nfrom llama_index.core.graph_stores.types import (\n    EntityNode,\n    Relation,\n    KG_NODES_KEY,\n    KG_RELATIONS_KEY,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n)\nfrom llama_index.core.schema import TransformComponent, BaseNode, MetadataMode\n\n\nclass SimpleLLMPathExtractor(TransformComponent):\n    \"\"\"Extract triples from a graph.\n\n    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) from text.\n\n    Args:\n        llm (LLM):\n            The language model to use.\n        extract_prompt (Union[str, PromptTemplate]):\n            The prompt to use for extracting triples.\n        parse_fn (callable):\n            A function to parse the output of the language model.\n        num_workers (int):\n            The number of workers to use for parallel processing.\n        max_paths_per_chunk (int):\n            The maximum number of paths to extract per chunk.\n    \"\"\"\n\n    llm: LLM\n    extract_prompt: PromptTemplate\n    parse_fn: Callable\n    num_workers: int\n    max_paths_per_chunk: int\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n        parse_fn: Callable = default_parse_triplets_fn,\n        max_paths_per_chunk: int = 10,\n        num_workers: int = 4,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        from llama_index.core import Settings\n\n        if isinstance(extract_prompt, str):\n            extract_prompt = PromptTemplate(extract_prompt)\n\n        super().__init__(\n            llm=llm or Settings.llm,\n            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n            parse_fn=parse_fn,\n            num_workers=num_workers,\n            max_paths_per_chunk=max_paths_per_chunk,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SimpleLLMPathExtractor\"\n\n    def __call__(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> Sequence[BaseNode]:\n        \"\"\"Extract triples from nodes.\"\"\"\n        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))\n\n    async def _aextract(self, node: BaseNode) -> BaseNode:\n        \"\"\"Extract triples from a node.\"\"\"\n        assert hasattr(node, \"text\")\n\n        text = node.get_content(metadata_mode=MetadataMode.LLM)\n        try:\n            llm_response = await self.llm.apredict(\n                self.extract_prompt,\n                text=text,\n                max_knowledge_triplets=self.max_paths_per_chunk,\n            )\n            triples = self.parse_fn(llm_response)\n        except ValueError:\n            triples = []\n\n        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n\n        metadata = node.metadata.copy()\n        for subj, rel, obj in triples:\n            subj_node = EntityNode(name=subj, properties=metadata)\n            obj_node = EntityNode(name=obj, properties=metadata)\n            rel_node = Relation(\n                label=rel,\n                source_id=subj_node.id,\n                target_id=obj_node.id,\n                properties=metadata,\n            )\n\n            existing_nodes.extend([subj_node, obj_node])\n            existing_relations.append(rel_node)\n\n        node.metadata[KG_NODES_KEY] = existing_nodes\n        node.metadata[KG_RELATIONS_KEY] = existing_relations\n\n        return node\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> Sequence[BaseNode]:\n        \"\"\"Extract triples from nodes async.\"\"\"\n        jobs = []\n        for node in nodes:\n            jobs.append(self._aextract(node))\n\n        return await run_jobs(\n            jobs,\n            workers=self.num_workers,\n            show_progress=show_progress,\n            desc=\"Extracting paths from text\",\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/transformations/dynamic_llm.py",
    "filename": "dynamic_llm.py",
    "relpath": "indices/property_graph/transformations/dynamic_llm.py",
    "start_line": 1,
    "end_line": 159,
    "length": 159,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "default_parse_dynamic_triplets",
      "default_parse_dynamic_triplets_with_props",
      "parse_props"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "default_parse_dynamic_triplets",
      "default_parse_dynamic_triplets_with_props",
      "parse_props",
      "__init__",
      "class_name",
      "__call__",
      "_apredict_without_props",
      "_apredict_with_props",
      "_aextract",
      "acall"
    ],
    "document_class_names": [
      "DynamicLLMPathExtractor"
    ],
    "content": "import asyncio\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union, Tuple\nimport re\nimport json\nfrom llama_index.core.async_utils import run_jobs\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.graph_stores.types import (\n    EntityNode,\n    Relation,\n    KG_NODES_KEY,\n    KG_RELATIONS_KEY,\n)\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_DYNAMIC_EXTRACT_PROMPT,\n    DEFAULT_DYNAMIC_EXTRACT_PROPS_PROMPT,\n)\nfrom llama_index.core.schema import TransformComponent, BaseNode, MetadataMode\n\n\ndef default_parse_dynamic_triplets(\n    llm_output: str,\n) -> List[Tuple[EntityNode, Relation, EntityNode]]:\n    \"\"\"\n    Parse the LLM output and convert it into a list of entity-relation-entity triplets.\n    This function is flexible and can handle various output formats.\n\n    Args:\n        llm_output (str): The output from the LLM, which may be JSON-like or plain text.\n\n    Returns:\n        List[Tuple[EntityNode, Relation, EntityNode]]: A list of triplets.\n    \"\"\"\n    triplets = []\n\n    try:\n        # Attempt to parse the output as JSON\n        data = json.loads(llm_output)\n        for item in data:\n            head = item.get(\"head\")\n            head_type = item.get(\"head_type\")\n            relation = item.get(\"relation\")\n            tail = item.get(\"tail\")\n            tail_type = item.get(\"tail_type\")\n\n            if head and head_type and relation and tail and tail_type:\n                head_node = EntityNode(name=head, label=head_type)\n                tail_node = EntityNode(name=tail, label=tail_type)\n                relation_node = Relation(\n                    source_id=head_node.id, target_id=tail_node.id, label=relation\n                )\n                triplets.append((head_node, relation_node, tail_node))\n\n    except json.JSONDecodeError:\n        # Flexible pattern to match the key-value pairs for head, head_type, relation, tail, and tail_type\n        pattern = r'[\\{\"\\']head[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\'],\\s*[\\{\"\\']head_type[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\'],\\s*[\\{\"\\']relation[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\'],\\s*[\\{\"\\']tail[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\'],\\s*[\\{\"\\']tail_type[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\']'\n\n        # Find all matches in the output\n        matches = re.findall(pattern, llm_output)\n\n        for match in matches:\n            head, head_type, relation, tail, tail_type = match\n            head_node = EntityNode(name=head, label=head_type)\n            tail_node = EntityNode(name=tail, label=tail_type)\n            relation_node = Relation(\n                source_id=head_node.id, target_id=tail_node.id, label=relation\n            )\n            triplets.append((head_node, relation_node, tail_node))\n    return triplets\n\n\ndef default_parse_dynamic_triplets_with_props(\n    llm_output: str,\n) -> List[Tuple[EntityNode, Relation, EntityNode]]:\n    \"\"\"\n    Parse the LLM output and convert it into a list of entity-relation-entity triplets.\n    This function is flexible and can handle various output formats.\n\n    Args:\n        llm_output (str): The output from the LLM, which may be JSON-like or plain text.\n\n    Returns:\n        List[Tuple[EntityNode, Relation, EntityNode]]: A list of triplets.\n    \"\"\"\n    triplets = []\n\n    try:\n        # Attempt to parse the output as JSON\n        data = json.loads(llm_output)\n        for item in data:\n            head = item.get(\"head\")\n            head_type = item.get(\"head_type\")\n            head_props = item.get(\"head_props\", {})\n            relation = item.get(\"relation\")\n            relation_props = item.get(\"relation_props\", {})\n            tail = item.get(\"tail\")\n            tail_type = item.get(\"tail_type\")\n            tail_props = item.get(\"tail_props\", {})\n\n            if head and head_type and relation and tail and tail_type:\n                head_node = EntityNode(\n                    name=head, label=head_type, properties=head_props\n                )\n                tail_node = EntityNode(\n                    name=tail, label=tail_type, properties=tail_props\n                )\n                relation_node = Relation(\n                    source_id=head_node.id,\n                    target_id=tail_node.id,\n                    label=relation,\n                    properties=relation_props,\n                )\n                triplets.append((head_node, relation_node, tail_node))\n    except json.JSONDecodeError:\n        # Flexible pattern to match the key-value pairs for head, head_type, head_props, relation, relation_props, tail, tail_type, and tail_props\n        pattern = r'[\\{\"\\']head[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\']\\s*,\\s*[\\{\"\\']head_type[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\']\\s*,\\s*[\\{\"\\']head_props[\\}\"\\']\\s*:\\s*\\{(.*?)\\}\\s*,\\s*[\\{\"\\']relation[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\']\\s*,\\s*[\\{\"\\']relation_props[\\}\"\\']\\s*:\\s*\\{(.*?)\\}\\s*,\\s*[\\{\"\\']tail[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\']\\s*,\\s*[\\{\"\\']tail_type[\\}\"\\']\\s*:\\s*[\\{\"\\'](.*?)[\\}\"\\']\\s*,\\s*[\\{\"\\']tail_props[\\}\"\\']\\s*:\\s*\\{(.*?)\\}\\s*'\n\n        # Find all matches in the output\n        matches = re.findall(pattern, llm_output)\n\n        for match in matches:\n            (\n                head,\n                head_type,\n                head_props,\n                relation,\n                relation_props,\n                tail,\n                tail_type,\n                tail_props,\n            ) = match\n\n            # Use more robust parsing for properties\n            def parse_props(props_str: str) -> Dict[str, Any]:\n                try:\n                    # Handle mixed quotes and convert to a proper dictionary\n                    props_str = props_str.replace(\"'\", '\"')\n                    return json.loads(f\"{{{props_str}}}\")\n                except json.JSONDecodeError:\n                    return {}\n\n            head_props_dict = parse_props(head_props)\n            relation_props_dict = parse_props(relation_props)\n            tail_props_dict = parse_props(tail_props)\n\n            head_node = EntityNode(\n                name=head, label=head_type, properties=head_props_dict\n            )\n            tail_node = EntityNode(\n                name=tail, label=tail_type, properties=tail_props_dict\n            )\n            relation_node = Relation(\n                source_id=head_node.id,\n                target_id=tail_node.id,\n                label=relation,\n                properties=relation_props_dict,\n            )\n            triplets.append((head_node, relation_node, tail_node))\n    return triplets"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/transformations/dynamic_llm.py",
    "filename": "dynamic_llm.py",
    "relpath": "indices/property_graph/transformations/dynamic_llm.py",
    "start_line": 159,
    "end_line": 408,
    "length": 250,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "__call__",
      "_apredict_without_props",
      "_apredict_with_props",
      "_aextract",
      "acall"
    ],
    "chunk_class_names": [
      "DynamicLLMPathExtractor"
    ],
    "document_function_names": [
      "default_parse_dynamic_triplets",
      "default_parse_dynamic_triplets_with_props",
      "parse_props",
      "__init__",
      "class_name",
      "__call__",
      "_apredict_without_props",
      "_apredict_with_props",
      "_aextract",
      "acall"
    ],
    "document_class_names": [
      "DynamicLLMPathExtractor"
    ],
    "content": "class DynamicLLMPathExtractor(TransformComponent):\n    \"\"\"\n    DynamicLLMPathExtractor is a component for extracting structured information from text\n    to build a knowledge graph. It uses an LLM to identify entities and their relationships,\n    with the ability to infer entity types and expand upon an initial ontology.\n\n    This extractor improves upon SimpleLLMPathExtractor by:\n    1. Detecting entity types instead of labeling them generically as \"entity\" and \"chunk\".\n    2. Accepting an initial ontology as input, specifying desired nodes and relationships.\n    3. Encouraging ontology expansion through its prompt design.\n\n    This extractor differs from SchemaLLMPathExtractor because:\n    1. It interprets the passed possible entities and relations as an initial ontology.\n    2. It encourages expansion of the initial ontology in the prompt.\n    3. It aims for flexibility in knowledge graph construction while still providing guidance.\n\n    Attributes:\n        llm (LLM): The language model used for extraction.\n        extract_prompt (PromptTemplate): The prompt template used to guide the LLM.\n        parse_fn (Callable): Function to parse the LLM output into triplets.\n        num_workers (int): Number of workers for parallel processing.\n        max_triplets_per_chunk (int): Maximum number of triplets to extract per text chunk.\n        allowed_entity_types (List[str]): List of initial entity types for the ontology.\n        allowed_entity_props (Optional[Union[List[str], List[Tuple[str, str]]]]):\n            List of initial entity properties for the ontology.\n            Can be either property names or tuples of (name, description).\n        allowed_relation_types (List[str]): List of initial relation types for the ontology.\n        allowed_relation_props (Optional[Union[List[str], List[Tuple[str, str]]]]):\n            List of initial relation properties for the ontology.\n            Can be either property names or tuples of (name, description).\n    \"\"\"\n\n    llm: LLM\n    extract_prompt: PromptTemplate\n    parse_fn: Callable\n    num_workers: int\n    max_triplets_per_chunk: int\n    allowed_entity_types: List[str]\n    allowed_entity_props: List[str]\n    allowed_relation_types: Optional[List[str]]\n    allowed_relation_props: Optional[List[str]]\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n        parse_fn: Optional[Callable] = None,\n        max_triplets_per_chunk: int = 10,\n        num_workers: int = 4,\n        allowed_entity_types: Optional[List[str]] = None,\n        allowed_entity_props: Optional[Union[List[str], List[Tuple[str, str]]]] = None,\n        allowed_relation_types: Optional[List[str]] = None,\n        allowed_relation_props: Optional[\n            Union[List[str], List[Tuple[str, str]]]\n        ] = None,\n    ) -> None:\n        \"\"\"\n        Initialize the DynamicLLMPathExtractor.\n\n        Args:\n            llm (Optional[LLM]): The language model to use. If None, uses the default from Settings.\n            extract_prompt (Optional[Union[str, PromptTemplate]]): The prompt template to use.\n            parse_fn (Callable): Function to parse LLM output into triplets.\n            max_triplets_per_chunk (int): Maximum number of triplets to extract per chunk.\n            num_workers (int): Number of workers for parallel processing.\n            allowed_entity_types (Optional[List[str]]): List of initial entity types for the ontology.\n            allowed_relation_types (Optional[List[str]]): List of initial relation types for the ontology.\n        \"\"\"\n        from llama_index.core import Settings\n\n        if isinstance(extract_prompt, str):\n            extract_prompt = PromptTemplate(extract_prompt)\n\n        if extract_prompt is None:\n            if allowed_entity_props is not None or allowed_relation_props is not None:\n                extract_prompt = DEFAULT_DYNAMIC_EXTRACT_PROPS_PROMPT\n            else:\n                extract_prompt = DEFAULT_DYNAMIC_EXTRACT_PROMPT\n\n        if parse_fn is None:\n            if allowed_entity_props is not None or allowed_relation_props is not None:\n                parse_fn = default_parse_dynamic_triplets_with_props\n            else:\n                parse_fn = default_parse_dynamic_triplets\n\n        # convert props to name -> description format if needed\n        if allowed_entity_props and isinstance(allowed_entity_props[0], tuple):\n            allowed_entity_props = [  # type: ignore\n                f\"Property `{k}` with description ({v})\"\n                for k, v in allowed_entity_props  # type: ignore\n            ]\n\n        if allowed_relation_props and isinstance(allowed_relation_props[0], tuple):\n            allowed_relation_props = [  # type: ignore\n                f\"Property `{k}` with description ({v})\"\n                for k, v in allowed_relation_props  # type: ignore\n            ]\n\n        super().__init__(\n            llm=llm or Settings.llm,\n            extract_prompt=extract_prompt,\n            parse_fn=parse_fn,\n            num_workers=num_workers,\n            max_triplets_per_chunk=max_triplets_per_chunk,\n            allowed_entity_types=allowed_entity_types or [],\n            allowed_entity_props=allowed_entity_props or [],\n            allowed_relation_types=allowed_relation_types or [],\n            allowed_relation_props=allowed_relation_props or [],\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Return the name of the class.\"\"\"\n        return \"DynamicLLMPathExtractor\"\n\n    def __call__(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        \"\"\"\n        Extract triples from nodes.\n\n        Args:\n            nodes (List[BaseNode]): List of nodes to process.\n            show_progress (bool): Whether to show a progress bar.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            List[BaseNode]: Processed nodes with extracted information.\n        \"\"\"\n        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))\n\n    async def _apredict_without_props(self, text: str) -> str:\n        \"\"\"\n        Asynchronously predict triples from text without properties.\n\n        Args:\n            text (str): The text to process.\n\n        Returns:\n            str: The predicted triples.\n        \"\"\"\n        return await self.llm.apredict(\n            self.extract_prompt,\n            text=text,\n            max_knowledge_triplets=self.max_triplets_per_chunk,\n            allowed_entity_types=\", \".join(self.allowed_entity_types)\n            if len(self.allowed_entity_types or []) > 0\n            else \"No entity types provided, You are free to define them.\",\n            allowed_relation_types=\", \".join(self.allowed_relation_types or [])\n            if len(self.allowed_relation_types or []) > 0\n            else \"No relation types provided, You are free to define them.\",\n        )\n\n    async def _apredict_with_props(self, text: str) -> str:\n        \"\"\"\n        Asynchronously predict triples from text with properties.\n\n        Args:\n            text (str): The text to process.\n\n        Returns:\n            str: The predicted triples.\n        \"\"\"\n        return await self.llm.apredict(\n            self.extract_prompt,\n            text=text,\n            max_knowledge_triplets=self.max_triplets_per_chunk,\n            allowed_entity_types=\", \".join(self.allowed_entity_types)\n            if len(self.allowed_entity_types or []) > 0\n            else \"No entity types provided, You are free to define them.\",\n            allowed_relation_types=\", \".join(self.allowed_relation_types or [])\n            if len(self.allowed_relation_types or []) > 0\n            else \"No relation types provided, You are free to define them.\",\n            allowed_entity_properties=\", \".join(self.allowed_entity_props)\n            if self.allowed_entity_props\n            else \"No entity properties provided, You are free to define them.\",\n            allowed_relation_properties=\", \".join(self.allowed_relation_props)\n            if self.allowed_relation_props\n            else \"No relation properties provided, You are free to define them.\",\n        )\n\n    async def _aextract(self, node: BaseNode) -> BaseNode:\n        \"\"\"\n        Asynchronously extract triples from a single node.\n\n        Args:\n            node (BaseNode): The node to process.\n\n        Returns:\n            BaseNode: The processed node with extracted information.\n        \"\"\"\n        text = node.get_content(metadata_mode=MetadataMode.LLM)\n        try:\n            if (\n                self.allowed_entity_props is not None\n                and self.allowed_relation_props is not None\n            ):\n                llm_response = await self._apredict_with_props(text)\n            else:\n                llm_response = await self._apredict_without_props(text)\n\n            triplets = self.parse_fn(llm_response)\n        except Exception as e:\n            print(f\"Error during extraction: {e!s}\")\n            triplets = []\n\n        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n\n        metadata = node.metadata.copy()\n        for subj, rel, obj in triplets:\n            subj.properties.update(metadata)\n            obj.properties.update(metadata)\n            rel.properties.update(metadata)\n\n            existing_nodes.extend([subj, obj])\n            existing_relations.append(rel)\n\n        node.metadata[KG_NODES_KEY] = existing_nodes\n        node.metadata[KG_RELATIONS_KEY] = existing_relations\n\n        return node\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        \"\"\"\n        Asynchronously extract triples from multiple nodes.\n\n        Args:\n            nodes (List[BaseNode]): List of nodes to process.\n            show_progress (bool): Whether to show a progress bar.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            List[BaseNode]: Processed nodes with extracted information.\n        \"\"\"\n        jobs = []\n        for node in nodes:\n            jobs.append(self._aextract(node))\n\n        return await run_jobs(\n            jobs,\n            workers=self.num_workers,\n            show_progress=show_progress,\n            desc=\"Extracting and inferring knowledge graph from text\",\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/transformations/schema_llm.py",
    "filename": "schema_llm.py",
    "relpath": "indices/property_graph/transformations/schema_llm.py",
    "start_line": 1,
    "end_line": 385,
    "length": 385,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "validate",
      "class_name",
      "__call__",
      "_prune_invalid_props",
      "_prune_invalid_triplets",
      "_aextract",
      "acall"
    ],
    "chunk_class_names": [
      "SchemaLLMPathExtractor",
      "to"
    ],
    "document_function_names": [
      "__init__",
      "validate",
      "class_name",
      "__call__",
      "_prune_invalid_props",
      "_prune_invalid_triplets",
      "_aextract",
      "acall"
    ],
    "document_class_names": [
      "SchemaLLMPathExtractor",
      "to"
    ],
    "content": "import asyncio\nfrom typing import Any, Dict, List, Literal, Optional, Sequence, Tuple, Type, Union\n\nfrom llama_index.core.async_utils import run_jobs\nfrom llama_index.core.bridge.pydantic import create_model, field_validator\nfrom llama_index.core.graph_stores.types import (\n    EntityNode,\n    Relation,\n    Triplet,\n    KG_NODES_KEY,\n    KG_RELATIONS_KEY,\n)\nfrom llama_index.core.indices.property_graph.transformations.utils import (\n    get_entity_class,\n    get_relation_class,\n)\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.schema import TransformComponent, BaseNode, MetadataMode\nfrom llama_index.core.llms.llm import LLM\n\n\nDEFAULT_ENTITIES = Literal[\n    \"PRODUCT\",\n    \"MARKET\",\n    \"TECHNOLOGY\",\n    \"EVENT\",\n    \"CONCEPT\",\n    \"ORGANIZATION\",\n    \"PERSON\",\n    \"LOCATION\",\n    \"TIME\",\n    \"MISCELLANEOUS\",\n]\n\nDEFAULT_RELATIONS = Literal[\n    \"USED_BY\",\n    \"USED_FOR\",\n    \"LOCATED_IN\",\n    \"PART_OF\",\n    \"WORKED_ON\",\n    \"HAS\",\n    \"IS_A\",\n    \"BORN_IN\",\n    \"DIED_IN\",\n    \"HAS_ALIAS\",\n]\n\n# Convert the above dict schema into a list of triples\nTriple = Tuple[str, str, str]\nDEFAULT_VALIDATION_SCHEMA: List[Triple] = [\n    (\"PRODUCT\", \"USED_BY\", \"PRODUCT\"),\n    (\"PRODUCT\", \"USED_FOR\", \"MARKET\"),\n    (\"PRODUCT\", \"HAS\", \"TECHNOLOGY\"),\n    (\"MARKET\", \"LOCATED_IN\", \"LOCATION\"),\n    (\"MARKET\", \"HAS\", \"TECHNOLOGY\"),\n    (\"TECHNOLOGY\", \"USED_BY\", \"PRODUCT\"),\n    (\"TECHNOLOGY\", \"USED_FOR\", \"MARKET\"),\n    (\"TECHNOLOGY\", \"LOCATED_IN\", \"LOCATION\"),\n    (\"TECHNOLOGY\", \"PART_OF\", \"ORGANIZATION\"),\n    (\"TECHNOLOGY\", \"IS_A\", \"PRODUCT\"),\n    (\"EVENT\", \"LOCATED_IN\", \"LOCATION\"),\n    (\"EVENT\", \"PART_OF\", \"ORGANIZATION\"),\n    (\"CONCEPT\", \"USED_BY\", \"TECHNOLOGY\"),\n    (\"CONCEPT\", \"USED_FOR\", \"PRODUCT\"),\n    (\"ORGANIZATION\", \"LOCATED_IN\", \"LOCATION\"),\n    (\"ORGANIZATION\", \"PART_OF\", \"ORGANIZATION\"),\n    (\"ORGANIZATION\", \"PART_OF\", \"MARKET\"),\n    (\"PERSON\", \"BORN_IN\", \"LOCATION\"),\n    (\"PERSON\", \"BORN_IN\", \"TIME\"),\n    (\"PERSON\", \"DIED_IN\", \"LOCATION\"),\n    (\"PERSON\", \"DIED_IN\", \"TIME\"),\n    (\"PERSON\", \"WORKED_ON\", \"EVENT\"),\n    (\"PERSON\", \"WORKED_ON\", \"PRODUCT\"),\n    (\"PERSON\", \"WORKED_ON\", \"CONCEPT\"),\n    (\"PERSON\", \"WORKED_ON\", \"TECHNOLOGY\"),\n    (\"LOCATION\", \"LOCATED_IN\", \"LOCATION\"),\n    (\"LOCATION\", \"PART_OF\", \"LOCATION\"),\n]\n\nDEFAULT_SCHEMA_PATH_EXTRACT_PROMPT = PromptTemplate(\n    \"Give the following text, extract the knowledge graph according to the provided schema. \"\n    \"Try to limit to the output {max_triplets_per_chunk} extracted paths.s\\n\"\n    \"-------\\n\"\n    \"{text}\\n\"\n    \"-------\\n\"\n)\n\n\nclass SchemaLLMPathExtractor(TransformComponent):\n    \"\"\"\n    Extract paths from a graph using a schema.\n\n    Args:\n        llm (LLM):\n            The language model to use.\n        extract_prompt (Union[PromptTemplate, str], optional):\n            The template to use for the extraction query. Defaults to None.\n        possible_entities (Optional[Type[Any]], optional):\n            The possible entities to extract. Defaults to None.\n        possible_entity_props (Optional[Union[List[str], List[Tuple[str, str]]], optional):\n            The possible entity properties to extract. Defaults to None.\n            Can be a list of strings or a list of tuples with the format (name, description).\n        possible_relations (Optional[Type[Any]], optional):\n            The possible relations to extract. Defaults to None.\n        possible_relation_props (Optional[Union[List[str], List[Tuple[str, str]]], optional):\n            The possible relation properties to extract. Defaults to None.\n            Can be a list of strings or a list of tuples with the format (name, description).\n        strict (bool, optional):\n            Whether to enforce strict validation of entities and relations. Defaults to True.\n            If false, values outside of the schema will be allowed.\n        kg_schema_cls (Any, optional):\n            The schema class to use. Defaults to None.\n        kg_validation_schema (Dict[str, str], optional):\n            The validation schema to use. Defaults to None.\n        max_triplets_per_chunk (int, optional):\n            The maximum number of triplets to extract per chunk. Defaults to 10.\n        num_workers (int, optional):\n            The number of workers to use. Defaults to 4.\n    \"\"\"\n\n    llm: LLM\n    extract_prompt: PromptTemplate\n    kg_schema_cls: Any\n    kg_validation_schema: Dict[str, Any]\n    num_workers: int\n    max_triplets_per_chunk: int\n    possible_entity_props: Optional[List[str]]\n    possible_relation_props: Optional[List[str]]\n    strict: bool\n\n    def __init__(\n        self,\n        llm: LLM,\n        extract_prompt: Optional[Union[PromptTemplate, str]] = None,\n        possible_entities: Optional[Type[Any]] = None,\n        possible_entity_props: Optional[Union[List[str], List[Tuple[str, str]]]] = None,\n        possible_relations: Optional[Type[Any]] = None,\n        possible_relation_props: Optional[\n            Union[List[str], List[Tuple[str, str]]]\n        ] = None,\n        strict: bool = True,\n        kg_schema_cls: Any = None,\n        kg_validation_schema: Optional[Union[Dict[str, str], List[Triple]]] = None,\n        max_triplets_per_chunk: int = 10,\n        num_workers: int = 4,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if isinstance(extract_prompt, str):\n            extract_prompt = PromptTemplate(extract_prompt)\n\n        # Build a pydantic model on the fly\n        if kg_schema_cls is None:\n            possible_entities = possible_entities or DEFAULT_ENTITIES  # type: ignore\n            if possible_entity_props and isinstance(possible_entity_props[0], tuple):\n                entity_props = [  # type: ignore\n                    f\"Property label `{k}` with description ({v})\"\n                    for k, v in possible_entity_props\n                ]\n            else:\n                entity_props = possible_entity_props  # type: ignore\n            entity_cls = get_entity_class(possible_entities, entity_props, strict)\n\n            possible_relations = possible_relations or DEFAULT_RELATIONS  # type: ignore\n            if possible_relation_props and isinstance(\n                possible_relation_props[0], tuple\n            ):\n                relation_props = [  # type: ignore\n                    f\"Property label `{k}` with description ({v})\"\n                    for k, v in possible_relation_props\n                ]\n            else:\n                relation_props = possible_relation_props  # type: ignore\n            relation_cls = get_relation_class(\n                possible_relations, relation_props, strict\n            )\n\n            triplet_cls = create_model(\n                \"Triplet\",\n                subject=(entity_cls, ...),\n                relation=(relation_cls, ...),\n                object=(entity_cls, ...),\n            )\n\n            def validate(v: Any) -> Any:\n                \"\"\"Validate triplets.\"\"\"\n                passing_triplets = []\n                for i, triplet in enumerate(v):\n                    # cleanup\n                    try:\n                        for key in triplet:\n                            triplet[key][\"type\"] = triplet[key][\"type\"].replace(\n                                \" \", \"_\"\n                            )\n                            triplet[key][\"type\"] = triplet[key][\"type\"].upper()\n\n                        # validate, skip if invalid\n                        _ = triplet_cls(**triplet)\n                        passing_triplets.append(v[i])\n                    except (KeyError, ValueError):\n                        continue\n\n                return passing_triplets\n\n            root = field_validator(\"triplets\", mode=\"before\")(validate)\n            kg_schema_cls = create_model(\n                \"KGSchema\",\n                __validators__={\"validator1\": root},  # type: ignore\n                triplets=(List[triplet_cls], ...),  # type: ignore\n            )\n            kg_schema_cls.__doc__ = \"Knowledge Graph Schema.\"\n\n        # Get validation schema\n        kg_validation_schema = kg_validation_schema or DEFAULT_VALIDATION_SCHEMA\n        # TODO: Remove this in a future version & encourage List[Triple] for validation schema\n        if isinstance(kg_validation_schema, list):\n            kg_validation_schema = {\"relationships\": kg_validation_schema}  # type: ignore\n\n        # flatten tuples now that we don't need the descriptions\n        if possible_relation_props and isinstance(possible_relation_props[0], tuple):\n            possible_relation_props = [x[0] for x in possible_relation_props]\n\n        if possible_entity_props and isinstance(possible_entity_props[0], tuple):\n            possible_entity_props = [x[0] for x in possible_entity_props]\n\n        super().__init__(\n            llm=llm,\n            extract_prompt=extract_prompt or DEFAULT_SCHEMA_PATH_EXTRACT_PROMPT,\n            kg_schema_cls=kg_schema_cls,\n            kg_validation_schema=kg_validation_schema,\n            num_workers=num_workers,\n            max_triplets_per_chunk=max_triplets_per_chunk,\n            possible_entity_props=possible_entity_props,\n            possible_relation_props=possible_relation_props,\n            strict=strict,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SchemaLLMPathExtractor\"\n\n    def __call__(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        \"\"\"Extract triplets from nodes.\"\"\"\n        return asyncio.run(self.acall(nodes, show_progress=show_progress, **kwargs))\n\n    def _prune_invalid_props(\n        self, props: Dict[str, Any], allowed_props: Optional[List[str]]\n    ) -> Dict[str, Any]:\n        \"\"\"Prune invalid properties.\"\"\"\n        if not allowed_props:\n            return props\n\n        props_to_remove = []\n        for key in props:\n            if key not in allowed_props:\n                props_to_remove.append(key)\n\n        for key in props_to_remove:\n            del props[key]\n\n        return props\n\n    def _prune_invalid_triplets(self, kg_schema: Any) -> Sequence[Triplet]:\n        \"\"\"Prune invalid triplets.\"\"\"\n        valid_triplets = []\n        for triplet in kg_schema.triplets:\n            subject = triplet.subject.name\n            subject_type = triplet.subject.type\n            subject_props: Dict[str, Any] = {}\n            if hasattr(triplet.subject, \"properties\"):\n                subject_props = triplet.subject.properties or {}\n                if self.strict:\n                    subject_props = self._prune_invalid_props(\n                        subject_props,\n                        self.possible_entity_props,\n                    )\n\n            relation = triplet.relation.type\n            relation_props: Dict[str, Any] = {}\n            if hasattr(triplet.relation, \"properties\"):\n                relation_props = triplet.relation.properties or {}\n                if self.strict:\n                    relation_props = self._prune_invalid_props(\n                        relation_props,\n                        self.possible_relation_props,\n                    )\n\n            obj = triplet.object.name\n            obj_type = triplet.object.type\n            obj_props: Dict[str, Any] = {}\n            if hasattr(triplet.object, \"properties\"):\n                obj_props = triplet.object.properties or {}\n                if self.strict:\n                    obj_props = self._prune_invalid_props(\n                        obj_props,\n                        self.possible_entity_props,\n                    )\n\n            # Check if the triplet is valid based on the schema format\n            if self.strict:\n                if (\n                    isinstance(self.kg_validation_schema, dict)\n                    and \"relationships\" in self.kg_validation_schema\n                ):\n                    # Schema is a dictionary with a 'relationships' key and triples as values\n                    if (\n                        subject_type,\n                        relation,\n                        obj_type,\n                    ) not in self.kg_validation_schema[\"relationships\"]:\n                        continue\n                else:\n                    # Schema is the backwards-compat format\n                    if relation not in self.kg_validation_schema.get(\n                        subject_type, [relation]\n                    ) and relation not in self.kg_validation_schema.get(\n                        obj_type, [relation]\n                    ):\n                        continue\n\n            # Remove self-references\n            if subject.lower() == obj.lower():\n                continue\n\n            subj_node = EntityNode(\n                label=subject_type, name=subject, properties=subject_props\n            )\n            obj_node = EntityNode(label=obj_type, name=obj, properties=obj_props)\n            rel_node = Relation(\n                label=relation,\n                source_id=subj_node.id,\n                target_id=obj_node.id,\n                properties=relation_props,\n            )\n            valid_triplets.append((subj_node, rel_node, obj_node))\n\n        return valid_triplets\n\n    async def _aextract(self, node: BaseNode) -> BaseNode:\n        \"\"\"Extract triplets from a node.\"\"\"\n        text = node.get_content(metadata_mode=MetadataMode.LLM)\n        try:\n            kg_schema = await self.llm.astructured_predict(\n                self.kg_schema_cls,\n                self.extract_prompt,\n                text=text,\n                max_triplets_per_chunk=self.max_triplets_per_chunk,\n            )\n            triplets = self._prune_invalid_triplets(kg_schema)\n        except (ValueError, TypeError, AttributeError):\n            triplets = []\n\n        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n\n        metadata = node.metadata.copy()\n        for subj, rel, obj in triplets:\n            subj.properties.update(metadata)\n            obj.properties.update(metadata)\n            rel.properties.update(metadata)\n\n            existing_relations.append(rel)\n            existing_nodes.append(subj)\n            existing_nodes.append(obj)\n\n        node.metadata[KG_NODES_KEY] = existing_nodes\n        node.metadata[KG_RELATIONS_KEY] = existing_relations\n\n        return node\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        \"\"\"Extract triplets from nodes async.\"\"\"\n        jobs = []\n        for node in nodes:\n            jobs.append(self._aextract(node))\n\n        return await run_jobs(\n            jobs,\n            workers=self.num_workers,\n            show_progress=show_progress,\n            desc=\"Extracting paths from text with schema\",\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/transformations/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/property_graph/transformations/__init__.py",
    "start_line": 1,
    "end_line": 19,
    "length": 19,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.indices.property_graph.transformations.implicit import (\n    ImplicitPathExtractor,\n)\nfrom llama_index.core.indices.property_graph.transformations.schema_llm import (\n    SchemaLLMPathExtractor,\n)\nfrom llama_index.core.indices.property_graph.transformations.simple_llm import (\n    SimpleLLMPathExtractor,\n)\nfrom llama_index.core.indices.property_graph.transformations.dynamic_llm import (\n    DynamicLLMPathExtractor,\n)\n\n__all__ = [\n    \"ImplicitPathExtractor\",\n    \"SchemaLLMPathExtractor\",\n    \"SimpleLLMPathExtractor\",\n    \"DynamicLLMPathExtractor\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/transformations/utils.py",
    "filename": "utils.py",
    "relpath": "indices/property_graph/transformations/utils.py",
    "start_line": 1,
    "end_line": 103,
    "length": 103,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_entity_class",
      "get_relation_class"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_entity_class",
      "get_relation_class"
    ],
    "document_class_names": [],
    "content": "from typing import Any, Dict, List, Optional\n\ntry:\n    from typing import TypeAlias  # type: ignore\nexcept ImportError:\n    # python 3.8 and 3.9 compatibility\n    from typing import Any as TypeAlias  # type: ignore\n\nfrom llama_index.core.bridge.pydantic import create_model, Field\n\n\ndef get_entity_class(\n    possible_entities: TypeAlias,\n    possible_entity_props: Optional[List[str]],\n    strict: bool,\n) -> Any:\n    \"\"\"Get entity class.\"\"\"\n    if not possible_entity_props:\n        return create_model(\n            \"Entity\",\n            type=(\n                possible_entities if strict else str,\n                Field(\n                    ...,\n                    description=(\n                        \"Entity in a knowledge graph. Only extract entities with types that are listed as valid: \"\n                        + str(possible_entities)\n                    ),\n                ),\n            ),\n            name=(str, ...),\n        )\n    else:\n        return create_model(\n            \"Entity\",\n            type=(\n                possible_entities if strict else str,\n                Field(\n                    ...,\n                    description=(\n                        \"Entity in a knowledge graph. Only extract entities with types that are listed as valid: \"\n                        + str(possible_entities)\n                    ),\n                ),\n            ),\n            name=(str, ...),\n            properties=(\n                Optional[Dict[str, Any]],\n                Field(\n                    None,\n                    description=(\n                        \"Properties of the entity. Only extract the following valid properties: \"\n                        + \"\\n\".join(possible_entity_props)\n                    ),\n                ),\n            ),\n        )\n\n\ndef get_relation_class(\n    possible_relations: TypeAlias,\n    possible_relation_props: Optional[List[str]],\n    strict: bool,\n) -> Any:\n    \"\"\"Get relation class.\"\"\"\n    if not possible_relation_props:\n        return create_model(\n            \"Relation\",\n            type=(\n                possible_relations if strict else str,\n                Field(\n                    ...,\n                    description=(\n                        \"Relation in a knowledge graph. Only extract relations with types that are listed as valid: \"\n                        + str(possible_relations)\n                    ),\n                ),\n            ),\n        )\n    else:\n        return create_model(\n            \"Relation\",\n            type=(\n                possible_relations if strict else str,\n                Field(\n                    ...,\n                    description=(\n                        \"Relation in a knowledge graph. Only extract relations with types that are listed as valid: \"\n                        + str(possible_relations)\n                    ),\n                ),\n            ),\n            properties=(\n                Optional[Dict[str, Any]],\n                Field(\n                    None,\n                    description=(\n                        \"Properties of the relation. Only extract the following valid properties: \"\n                        + \"\\n\".join(possible_relation_props)\n                    ),\n                ),\n            ),\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/transformations/implicit.py",
    "filename": "implicit.py",
    "relpath": "indices/property_graph/transformations/implicit.py",
    "start_line": 1,
    "end_line": 90,
    "length": 90,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_node_rel_string",
      "class_name",
      "__call__"
    ],
    "chunk_class_names": [
      "ImplicitPathExtractor"
    ],
    "document_function_names": [
      "get_node_rel_string",
      "class_name",
      "__call__"
    ],
    "document_class_names": [
      "ImplicitPathExtractor"
    ],
    "content": "from tqdm import tqdm\nfrom typing import Any, Sequence\n\nfrom llama_index.core.schema import TransformComponent, BaseNode, NodeRelationship\nfrom llama_index.core.graph_stores.types import Relation, KG_NODES_KEY, KG_RELATIONS_KEY\n\n\ndef get_node_rel_string(relationship: NodeRelationship) -> str:\n    return str(relationship).split(\".\")[-1]\n\n\nclass ImplicitPathExtractor(TransformComponent):\n    \"\"\"Extract edges from node relationships.\n\n    Uses `node.relationships` to extract relations between nodes.\n    \"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"ImplicitPathExtractor\"\n\n    def __call__(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> Sequence[BaseNode]:\n        \"\"\"Extract edges from node relationships.\"\"\"\n        if show_progress:\n            nodes = tqdm(nodes, desc=\"Extracting implicit paths\")\n\n        for node in nodes:\n            existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n            existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n\n            edges = []\n            metadata = node.metadata.copy()\n            if node.source_node:\n                edges.append(\n                    Relation(\n                        target_id=node.source_node.node_id,\n                        source_id=node.node_id,\n                        label=get_node_rel_string(NodeRelationship.SOURCE),\n                        properties=metadata,\n                    )\n                )\n\n            if node.parent_node:\n                edges.append(\n                    Relation(\n                        target_id=node.parent_node.node_id,\n                        source_id=node.node_id,\n                        label=get_node_rel_string(NodeRelationship.PARENT),\n                        properties=metadata,\n                    )\n                )\n\n            if node.prev_node:\n                edges.append(\n                    Relation(\n                        target_id=node.prev_node.node_id,\n                        source_id=node.node_id,\n                        label=get_node_rel_string(NodeRelationship.PREVIOUS),\n                        properties=metadata,\n                    )\n                )\n\n            if node.next_node:\n                edges.append(\n                    Relation(\n                        source_id=node.node_id,\n                        target_id=node.next_node.node_id,\n                        label=get_node_rel_string(NodeRelationship.NEXT),\n                        properties=metadata,\n                    )\n                )\n\n            if node.child_nodes:\n                for child_node in node.child_nodes:\n                    edges.append(\n                        Relation(\n                            source_id=node.node_id,\n                            target_id=child_node.node_id,\n                            label=get_node_rel_string(NodeRelationship.CHILD),\n                            properties=metadata,\n                        )\n                    )\n\n            existing_relations.extend(edges)\n            node.metadata[KG_RELATIONS_KEY] = existing_relations\n            node.metadata[KG_NODES_KEY] = existing_nodes\n\n        return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/llm_synonym.py",
    "filename": "llm_synonym.py",
    "relpath": "indices/property_graph/sub_retrievers/llm_synonym.py",
    "start_line": 1,
    "end_line": 139,
    "length": 139,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_parse_llm_output",
      "_prepare_matches",
      "_aprepare_matches",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "chunk_class_names": [
      "LLMSynonymRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_parse_llm_output",
      "_prepare_matches",
      "_aprepare_matches",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "document_class_names": [
      "LLMSynonymRetriever"
    ],
    "content": "from typing import Any, Callable, List, Optional, Union\n\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.indices.property_graph.sub_retrievers.base import (\n    BasePGRetriever,\n)\nfrom llama_index.core.graph_stores.types import (\n    PropertyGraphStore,\n    KG_SOURCE_REL,\n)\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.schema import (\n    NodeWithScore,\n    QueryBundle,\n)\n\nDEFAULT_SYNONYM_EXPAND_TEMPLATE = (\n    \"Given some initial query, generate synonyms or related keywords up to {max_keywords} in total, \"\n    \"considering possible cases of capitalization, pluralization, common expressions, etc.\\n\"\n    \"Provide all synonyms/keywords separated by '^' symbols: 'keyword1^keyword2^...'\\n\"\n    \"Note, result should be in one-line, separated by '^' symbols.\"\n    \"----\\n\"\n    \"QUERY: {query_str}\\n\"\n    \"----\\n\"\n    \"KEYWORDS: \"\n)\n\n\nclass LLMSynonymRetriever(BasePGRetriever):\n    \"\"\"A retriever that uses a language model to expand a query with synonyms.\n    The synonyms are then used to retrieve nodes from a property graph.\n\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool, optional):\n            Whether to include source text in the retrieved nodes. Defaults to True.\n        synonym_prompt (Union[BasePromptTemplate, str], optional):\n            The template to use for the synonym expansion query.\n            Defaults to DEFAULT_SYNONYM_EXPAND_TEMPLATE.\n        max_keywords (int, optional):\n            The maximum number of synonyms to generate. Defaults to 10.\n        path_depth (int, optional):\n            The depth of the path to retrieve for each node. Defaults to 1 (i.e. a triple).\n        output_parsing_fn (Optional[callable], optional):\n            A callable function to parse the output of the language model. Defaults to None.\n        llm (Optional[LLM], optional):\n            The language model to use. Defaults to Settings.llm.\n    \"\"\"\n\n    def __init__(\n        self,\n        graph_store: PropertyGraphStore,\n        include_text: bool = True,\n        include_properties: bool = False,\n        synonym_prompt: Union[\n            BasePromptTemplate, str\n        ] = DEFAULT_SYNONYM_EXPAND_TEMPLATE,\n        max_keywords: int = 10,\n        path_depth: int = 1,\n        limit: int = 30,\n        output_parsing_fn: Optional[Callable] = None,\n        llm: Optional[LLM] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._llm = llm or Settings.llm\n        if isinstance(synonym_prompt, str):\n            synonym_prompt = PromptTemplate(synonym_prompt)\n        self._synonym_prompt = synonym_prompt\n        self._output_parsing_fn = output_parsing_fn\n        self._max_keywords = max_keywords\n        self._path_depth = path_depth\n        self._limit = limit\n        super().__init__(\n            graph_store=graph_store,\n            include_text=include_text,\n            include_properties=include_properties,\n            **kwargs,\n        )\n\n    def _parse_llm_output(self, output: str) -> List[str]:\n        if self._output_parsing_fn:\n            matches = self._output_parsing_fn(output)\n        else:\n            matches = output.strip().split(\"^\")\n\n        # capitalize to normalize with ingestion\n        return [x.strip().capitalize() for x in matches if x.strip()]\n\n    def _prepare_matches(\n        self, matches: List[str], limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        kg_nodes = self._graph_store.get(ids=matches)\n        triplets = self._graph_store.get_rel_map(\n            kg_nodes,\n            depth=self._path_depth,\n            limit=limit or self._limit,\n            ignore_rels=[KG_SOURCE_REL],\n        )\n\n        return self._get_nodes_with_score(triplets)\n\n    async def _aprepare_matches(\n        self, matches: List[str], limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        kg_nodes = await self._graph_store.aget(ids=matches)\n        triplets = await self._graph_store.aget_rel_map(\n            kg_nodes,\n            depth=self._path_depth,\n            limit=limit or self._limit,\n            ignore_rels=[KG_SOURCE_REL],\n        )\n\n        return self._get_nodes_with_score(triplets)\n\n    def retrieve_from_graph(\n        self, query_bundle: QueryBundle, limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        response = self._llm.predict(\n            self._synonym_prompt,\n            query_str=query_bundle.query_str,\n            max_keywords=self._max_keywords,\n        )\n        matches = self._parse_llm_output(response)\n\n        return self._prepare_matches(matches, limit=limit or self._limit)\n\n    async def aretrieve_from_graph(\n        self, query_bundle: QueryBundle, limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        response = await self._llm.apredict(\n            self._synonym_prompt,\n            query_str=query_bundle.query_str,\n            max_keywords=self._max_keywords,\n        )\n        matches = self._parse_llm_output(response)\n\n        return await self._aprepare_matches(matches, limit=limit or self._limit)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/base.py",
    "filename": "base.py",
    "relpath": "indices/property_graph/sub_retrievers/base.py",
    "start_line": 1,
    "end_line": 163,
    "length": 163,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_nodes_with_score",
      "_add_source_text",
      "add_source_text",
      "async_add_source_text",
      "_retrieve",
      "_aretrieve",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "chunk_class_names": [
      "BasePGRetriever",
      "for"
    ],
    "document_function_names": [
      "__init__",
      "_get_nodes_with_score",
      "_add_source_text",
      "add_source_text",
      "async_add_source_text",
      "_retrieve",
      "_aretrieve",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "document_class_names": [
      "BasePGRetriever",
      "for"
    ],
    "content": "from abc import abstractmethod\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.graph_stores.types import PropertyGraphStore, Triplet\nfrom llama_index.core.indices.property_graph.base import (\n    TRIPLET_SOURCE_KEY,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    NodeWithScore,\n    NodeRelationship,\n    RelatedNodeInfo,\n    QueryBundle,\n    TextNode,\n)\n\n\nDEFAULT_PREAMBLE = \"Here are some facts extracted from the provided text:\\n\\n\"\n\n\nclass BasePGRetriever(BaseRetriever):\n    \"\"\"The base class for property graph retrievers.\n\n    By default, will retrieve nodes from the graph store and add source text to the nodes if needed.\n\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool, optional):\n            Whether to include source text in the retrieved nodes. Defaults to True.\n        include_text_preamble (Optional[str], optional):\n            The preamble to include before the source text. Defaults to DEFAULT_PREAMBLE.\n    \"\"\"\n\n    def __init__(\n        self,\n        graph_store: PropertyGraphStore,\n        include_text: bool = True,\n        include_text_preamble: Optional[str] = DEFAULT_PREAMBLE,\n        include_properties: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self._graph_store = graph_store\n        self.include_text = include_text\n        self._include_text_preamble = include_text_preamble\n        self.include_properties = include_properties\n        super().__init__(callback_manager=kwargs.get(\"callback_manager\", None))\n\n    def _get_nodes_with_score(\n        self, triplets: List[Triplet], scores: Optional[List[float]] = None\n    ) -> List[NodeWithScore]:\n        results = []\n        for i, triplet in enumerate(triplets):\n            source_id = triplet[0].properties.get(TRIPLET_SOURCE_KEY, None)\n            relationships = {}\n            if source_id is not None:\n                relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n                    node_id=source_id\n                )\n\n            if self.include_properties:\n                text = f\"{triplet[0]!s} -> {triplet[1]!s} -> {triplet[2]!s}\"\n            else:\n                text = f\"{triplet[0].id} -> {triplet[1].id} -> {triplet[2].id}\"\n            results.append(\n                NodeWithScore(\n                    node=TextNode(\n                        text=text,\n                        relationships=relationships,\n                    ),\n                    score=1.0 if scores is None else scores[i],\n                )\n            )\n\n        return results\n\n    def _add_source_text(\n        self, retrieved_nodes: List[NodeWithScore], og_node_map: Dict[str, BaseNode]\n    ) -> List[NodeWithScore]:\n        \"\"\"Combine retrieved nodes/triplets with their source text, using provided preamble.\"\"\"\n        # map of ref doc id to triplets/retrieved labelled nodes\n        graph_node_map: Dict[str, List[str]] = {}\n        for node in retrieved_nodes:\n            ref_doc_id = node.node.ref_doc_id or \"\"\n            if ref_doc_id not in graph_node_map:\n                graph_node_map[ref_doc_id] = []\n\n            graph_node_map[ref_doc_id].append(node.node.get_content())\n\n        result_nodes: List[NodeWithScore] = []\n        for node_with_score in retrieved_nodes:\n            mapped_node = og_node_map.get(node_with_score.node.ref_doc_id or \"\", None)\n\n            if mapped_node:\n                graph_content = graph_node_map.get(mapped_node.node_id, [])\n                if len(graph_content) > 0:\n                    graph_content_str = \"\\n\".join(graph_content)\n                    cur_content = mapped_node.get_content()\n                    preamble_text = (\n                        self._include_text_preamble\n                        if self._include_text_preamble\n                        else \"\"\n                    )\n                    new_content = (\n                        preamble_text + graph_content_str + \"\\n\\n\" + cur_content\n                    )\n                    mapped_node = TextNode(**mapped_node.dict())\n                    mapped_node.text = new_content\n                result_nodes.append(\n                    NodeWithScore(\n                        node=mapped_node,\n                        score=node_with_score.score,\n                    )\n                )\n            else:\n                result_nodes.append(node_with_score)\n\n        return result_nodes\n\n    def add_source_text(self, nodes: List[NodeWithScore]) -> List[NodeWithScore]:\n        \"\"\"Combine retrieved nodes/triplets with their source text.\"\"\"\n        og_nodes = self._graph_store.get_llama_nodes(\n            [x.node.ref_doc_id for x in nodes if x.node.ref_doc_id is not None]\n        )\n        node_map = {node.node_id: node for node in og_nodes}\n\n        return self._add_source_text(nodes, node_map)\n\n    async def async_add_source_text(\n        self, nodes: List[NodeWithScore]\n    ) -> List[NodeWithScore]:\n        \"\"\"Combine retrieved nodes/triplets with their source text.\"\"\"\n        og_nodes = await self._graph_store.aget_llama_nodes(\n            [x.node.ref_doc_id for x in nodes if x.node.ref_doc_id is not None]\n        )\n        og_node_map = {node.node_id: node for node in og_nodes}\n\n        return self._add_source_text(nodes, og_node_map)\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = self.retrieve_from_graph(query_bundle)\n        if self.include_text and nodes:\n            nodes = self.add_source_text(nodes)\n        return nodes\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        nodes = await self.aretrieve_from_graph(query_bundle)\n        if self.include_text and nodes:\n            nodes = await self.async_add_source_text(nodes)\n        return nodes\n\n    @abstractmethod\n    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes from the labelled property graph.\"\"\"\n        ...\n\n    @abstractmethod\n    async def aretrieve_from_graph(\n        self, query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes from the labelled property graph.\"\"\"\n        ..."
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/vector.py",
    "filename": "vector.py",
    "relpath": "indices/property_graph/sub_retrievers/vector.py",
    "start_line": 1,
    "end_line": 259,
    "length": 259,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_valid_vector_store_params",
      "_filter_vector_store_query_kwargs",
      "_get_vector_store_query",
      "_get_kg_ids",
      "_aget_vector_store_query",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "chunk_class_names": [
      "VectorContextRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_valid_vector_store_params",
      "_filter_vector_store_query_kwargs",
      "_get_vector_store_query",
      "_get_kg_ids",
      "_aget_vector_store_query",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "document_class_names": [
      "VectorContextRetriever"
    ],
    "content": "import dataclasses\nfrom typing import Any, List, Sequence, Optional, Dict, Set\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.indices.property_graph.sub_retrievers.base import (\n    BasePGRetriever,\n)\nfrom llama_index.core.graph_stores.types import (\n    PropertyGraphStore,\n    KG_SOURCE_REL,\n    VECTOR_SOURCE_KEY,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.schema import BaseNode, NodeWithScore, QueryBundle\nfrom llama_index.core.vector_stores.types import (\n    BasePydanticVectorStore,\n    VectorStoreQuery,\n    MetadataFilters,\n)\n\n\nclass VectorContextRetriever(BasePGRetriever):\n    \"\"\"A retriever that uses a vector store to retrieve nodes based on a query.\n\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool, optional):\n            Whether to include source text in the retrieved nodes. Defaults to True.\n        embed_model (Optional[BaseEmbedding], optional):\n            The embedding model to use. Defaults to Settings.embed_model.\n        vector_store (Optional[BasePydanticVectorStore], optional):\n            The vector store to use. Defaults to None.\n            Should be supplied if the graph store does not support vector queries.\n        similarity_top_k (int, optional):\n            The number of top similar kg nodes to retrieve. Defaults to 4.\n        path_depth (int, optional):\n            The depth of the path to retrieve for each node. Defaults to 1 (i.e. a triple).\n        similarity_score (float, optional):\n            The minimum similarity score to retrieve the nodes. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        graph_store: PropertyGraphStore,\n        include_text: bool = True,\n        include_properties: bool = False,\n        embed_model: Optional[BaseEmbedding] = None,\n        vector_store: Optional[BasePydanticVectorStore] = None,\n        similarity_top_k: int = 4,\n        path_depth: int = 1,\n        limit: int = 30,\n        similarity_score: Optional[float] = None,\n        filters: Optional[MetadataFilters] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._retriever_kwargs = self._filter_vector_store_query_kwargs(kwargs or {})\n        self._embed_model = embed_model or Settings.embed_model\n        self._similarity_top_k = similarity_top_k\n        self._vector_store = vector_store\n        self._path_depth = path_depth\n        self._limit = limit\n        self._similarity_score = similarity_score\n        self._filters = filters\n\n        super().__init__(\n            graph_store=graph_store,\n            include_text=include_text,\n            include_properties=include_properties,\n            **kwargs,\n        )\n\n    @staticmethod\n    def _get_valid_vector_store_params() -> Set[str]:\n        return {x.name for x in dataclasses.fields(VectorStoreQuery)}\n\n    def _filter_vector_store_query_kwargs(\n        self, kwargs: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        valid_params = self._get_valid_vector_store_params()\n        return {k: v for k, v in kwargs.items() if k in valid_params}\n\n    def _get_vector_store_query(self, query_bundle: QueryBundle) -> VectorStoreQuery:\n        if query_bundle.embedding is None:\n            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(\n                query_bundle.embedding_strs\n            )\n\n        return VectorStoreQuery(\n            query_embedding=query_bundle.embedding,\n            similarity_top_k=self._similarity_top_k,\n            filters=self._filters,\n            **self._retriever_kwargs,\n        )\n\n    def _get_kg_ids(self, kg_nodes: Sequence[BaseNode]) -> List[str]:\n        \"\"\"Backward compatibility method to get kg_ids from kg_nodes.\"\"\"\n        return [node.metadata.get(VECTOR_SOURCE_KEY, node.id_) for node in kg_nodes]\n\n    async def _aget_vector_store_query(\n        self, query_bundle: QueryBundle\n    ) -> VectorStoreQuery:\n        if query_bundle.embedding is None:\n            query_bundle.embedding = (\n                await self._embed_model.aget_agg_embedding_from_queries(\n                    query_bundle.embedding_strs\n                )\n            )\n\n        return VectorStoreQuery(\n            query_embedding=query_bundle.embedding,\n            similarity_top_k=self._similarity_top_k,\n            filters=self._filters,\n            **self._retriever_kwargs,\n        )\n\n    def retrieve_from_graph(\n        self, query_bundle: QueryBundle, limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        vector_store_query = self._get_vector_store_query(query_bundle)\n\n        triplets = []\n        kg_ids = []\n        new_scores = []\n        if self._graph_store.supports_vector_queries:\n            result = self._graph_store.vector_query(vector_store_query)\n            if len(result) != 2:\n                raise ValueError(\"No nodes returned by vector_query\")\n            kg_nodes, scores = result\n\n            kg_ids = [node.id for node in kg_nodes]\n            triplets = self._graph_store.get_rel_map(\n                kg_nodes,\n                depth=self._path_depth,\n                limit=limit or self._limit,\n                ignore_rels=[KG_SOURCE_REL],\n            )\n\n        elif self._vector_store is not None:\n            query_result = self._vector_store.query(vector_store_query)\n            if query_result.nodes is not None and query_result.similarities is not None:\n                kg_ids = self._get_kg_ids(query_result.nodes)\n                scores = query_result.similarities\n                kg_nodes = self._graph_store.get(ids=kg_ids)\n                triplets = self._graph_store.get_rel_map(\n                    kg_nodes,\n                    depth=self._path_depth,\n                    limit=limit or self._limit,\n                    ignore_rels=[KG_SOURCE_REL],\n                )\n\n            elif query_result.ids is not None and query_result.similarities is not None:\n                kg_ids = query_result.ids\n                scores = query_result.similarities\n                kg_nodes = self._graph_store.get(ids=kg_ids)\n                triplets = self._graph_store.get_rel_map(\n                    kg_nodes,\n                    depth=self._path_depth,\n                    limit=limit or self._limit,\n                    ignore_rels=[KG_SOURCE_REL],\n                )\n\n        for triplet in triplets:\n            score1 = (\n                scores[kg_ids.index(triplet[0].id)] if triplet[0].id in kg_ids else 0.0\n            )\n            score2 = (\n                scores[kg_ids.index(triplet[2].id)] if triplet[2].id in kg_ids else 0.0\n            )\n            new_scores.append(max(score1, score2))\n\n        assert len(triplets) == len(new_scores)\n\n        # filter by similarity score\n        if self._similarity_score:\n            filtered_data = [\n                (triplet, score)\n                for triplet, score in zip(triplets, new_scores)\n                if score >= self._similarity_score\n            ]\n            # sort by score\n            top_k = sorted(filtered_data, key=lambda x: x[1], reverse=True)\n        else:\n            # sort by score\n            top_k = sorted(zip(triplets, new_scores), key=lambda x: x[1], reverse=True)\n\n        return self._get_nodes_with_score([x[0] for x in top_k], [x[1] for x in top_k])\n\n    async def aretrieve_from_graph(\n        self, query_bundle: QueryBundle, limit: Optional[int] = None\n    ) -> List[NodeWithScore]:\n        vector_store_query = await self._aget_vector_store_query(query_bundle)\n\n        triplets = []\n        kg_ids = []\n        new_scores = []\n        if self._graph_store.supports_vector_queries:\n            result = await self._graph_store.avector_query(vector_store_query)\n            if len(result) != 2:\n                raise ValueError(\"No nodes returned by vector_query\")\n\n            kg_nodes, scores = result\n            kg_ids = [node.id for node in kg_nodes]\n            triplets = await self._graph_store.aget_rel_map(\n                kg_nodes,\n                depth=self._path_depth,\n                limit=limit or self._limit,\n                ignore_rels=[KG_SOURCE_REL],\n            )\n\n        elif self._vector_store is not None:\n            query_result = await self._vector_store.aquery(vector_store_query)\n            if query_result.nodes is not None and query_result.similarities is not None:\n                kg_ids = self._get_kg_ids(query_result.nodes)\n                scores = query_result.similarities\n                kg_nodes = await self._graph_store.aget(ids=kg_ids)\n                triplets = await self._graph_store.aget_rel_map(\n                    kg_nodes,\n                    depth=self._path_depth,\n                    limit=limit or self._limit,\n                    ignore_rels=[KG_SOURCE_REL],\n                )\n\n            elif query_result.ids is not None and query_result.similarities is not None:\n                kg_ids = query_result.ids\n                scores = query_result.similarities\n                kg_nodes = await self._graph_store.aget(ids=kg_ids)\n                triplets = await self._graph_store.aget_rel_map(\n                    kg_nodes,\n                    depth=self._path_depth,\n                    limit=limit or self._limit,\n                    ignore_rels=[KG_SOURCE_REL],\n                )\n\n        for triplet in triplets:\n            score1 = (\n                scores[kg_ids.index(triplet[0].id)] if triplet[0].id in kg_ids else 0.0\n            )\n            score2 = (\n                scores[kg_ids.index(triplet[2].id)] if triplet[2].id in kg_ids else 0.0\n            )\n            new_scores.append(max(score1, score2))\n\n        assert len(triplets) == len(new_scores)\n\n        # filter by similarity score\n        if self._similarity_score:\n            filtered_data = [\n                (triplet, score)\n                for triplet, score in zip(triplets, new_scores)\n                if score >= self._similarity_score\n            ]\n            # sort by score\n            top_k = sorted(filtered_data, key=lambda x: x[1], reverse=True)\n        else:\n            # sort by score\n            top_k = sorted(zip(triplets, new_scores), key=lambda x: x[1], reverse=True)\n\n        return self._get_nodes_with_score([x[0] for x in top_k], [x[1] for x in top_k])"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/text_to_cypher.py",
    "filename": "text_to_cypher.py",
    "relpath": "indices/property_graph/sub_retrievers/text_to_cypher.py",
    "start_line": 1,
    "end_line": 221,
    "length": 221,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_parse_generated_cypher",
      "_clean_query_output",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "chunk_class_names": [
      "TextToCypherRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_parse_generated_cypher",
      "_clean_query_output",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "document_class_names": [
      "TextToCypherRetriever"
    ],
    "content": "from typing import Any, Callable, List, Optional, Union\n\nfrom llama_index.core.graph_stores.types import PropertyGraphStore\nfrom llama_index.core.indices.property_graph.sub_retrievers.base import BasePGRetriever\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\nfrom llama_index.core.settings import Settings\n\nDEFAULT_RESPONSE_TEMPLATE = (\n    \"Generated Cypher query:\\n{query}\\n\\n\" \"Cypher Response:\\n{response}\"\n)\n\nDEFAULT_SUMMARY_TEMPLATE = PromptTemplate(\n    \"\"\"You are an assistant that helps to form nice and human understandable answers.\n        The information part contains the provided information you must use to construct an answer.\n        The provided information is authoritative, never doubt it or try to use your internal knowledge to correct it.\n        If the provided information is empty, say that you don't know the answer.\n        Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n        Here is an example:\n\n        Question: How many miles is the flight between the ANC and SEA airports?\n        Information:\n        [{\"r.dist\": 1440}]\n        Helpful Answer:\n        It is 1440 miles to fly between the ANC and SEA airports.\n\n        Follow this example when generating answers.\n        Question:\n        {question}\n        Information:\n        {context}\n        Helpful Answer:\"\"\"\n)\n\n\nclass TextToCypherRetriever(BasePGRetriever):\n    \"\"\"A Text-to-Cypher retriever that uses a language model to generate Cypher queries.\n\n    NOTE: Executing arbitrary cypher has its risks. Ensure you take the needed measures\n    (read-only roles, sandboxed env, etc.) to ensure safe usage in a production environment.\n\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        llm (Optional[LLM], optional):\n            The language model to use. Defaults to Settings.llm.\n        text_to_cypher_template (Optional[Union[PromptTemplate, str]], optional):\n            The template to use for the text-to-cypher query. Defaults to None.\n        response_template (Optional[str], optional):\n            The template to use for the response. Defaults to None.\n        cypher_validator (Optional[callable], optional):\n            A callable function to validate the generated Cypher query. Defaults to None.\n        allowed_query_fields (Optional[List[str]], optional):\n            The fields to allow in the query output. Defaults to [\"text\", \"label\", \"type\"].\n        include_raw_response_as_metadata (Optional[bool], optional):\n            If True this will add the query and raw response data to the metadata property. Defaults to False.\n        summarize_response (Optional[bool], optional):\n            If True this will run the response through the provided LLM to create a more human readable\n            response, If False this uses the provided or default response_template. Defaults to False.\n        summarization_template (Optional[str], optional):\n            The template to use for summarizing the response. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        graph_store: PropertyGraphStore,\n        llm: Optional[LLM] = None,\n        text_to_cypher_template: Optional[Union[PromptTemplate, str]] = None,\n        response_template: Optional[str] = None,\n        cypher_validator: Optional[Callable] = None,\n        allowed_output_fields: Optional[List[str]] = None,\n        include_raw_response_as_metadata: Optional[bool] = False,\n        summarize_response: Optional[bool] = False,\n        summarization_template: Optional[Union[PromptTemplate, str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        if not graph_store.supports_structured_queries:\n            raise ValueError(\n                \"The provided graph store does not support cypher queries.\"\n            )\n\n        self.llm = llm or Settings.llm\n\n        if isinstance(text_to_cypher_template, str):\n            text_to_cypher_template = PromptTemplate(text_to_cypher_template)\n\n        if isinstance(summarization_template, str):\n            summarization_template = PromptTemplate(summarization_template)\n\n        self.response_template = response_template or DEFAULT_RESPONSE_TEMPLATE\n        self.text_to_cypher_template = (\n            text_to_cypher_template or graph_store.text_to_cypher_template\n        )\n        self.cypher_validator = cypher_validator\n        self.allowed_output_fields = allowed_output_fields\n        self.include_raw_response_as_metadata = include_raw_response_as_metadata\n        self.summarize_response = summarize_response\n        self.summarization_template = summarization_template or DEFAULT_SUMMARY_TEMPLATE\n\n        super().__init__(\n            graph_store=graph_store, include_text=False, include_properties=False\n        )\n\n    def _parse_generated_cypher(self, cypher_query: str) -> str:\n        if self.cypher_validator is not None:\n            return self.cypher_validator(cypher_query)\n        return cypher_query\n\n    def _clean_query_output(self, query_output: Any) -> Any:\n        \"\"\"Iterate the cypher response, looking for the allowed fields.\"\"\"\n        if isinstance(query_output, dict):\n            filtered_dict = {}\n            for key, value in query_output.items():\n                if (\n                    self.allowed_output_fields is None\n                    or key in self.allowed_output_fields\n                ):\n                    filtered_dict[key] = value\n                elif isinstance(value, (dict, list)):\n                    filtered_value = self._clean_query_output(value)\n                    if filtered_value:\n                        filtered_dict[key] = filtered_value\n            return filtered_dict\n        elif isinstance(query_output, list):\n            filtered_list = []\n            for item in query_output:\n                filtered_item = self._clean_query_output(item)\n                if filtered_item:\n                    filtered_list.append(filtered_item)\n            return filtered_list\n\n        return None\n\n    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        schema = self._graph_store.get_schema_str()\n        question = query_bundle.query_str\n\n        response = self.llm.predict(\n            self.text_to_cypher_template,\n            schema=schema,\n            question=question,\n        )\n\n        parsed_cypher_query = self._parse_generated_cypher(response)\n\n        query_output = self._graph_store.structured_query(parsed_cypher_query)\n\n        cleaned_query_output = self._clean_query_output(query_output)\n\n        if self.summarize_response:\n            summarized_response = self.llm.predict(\n                self.summarization_template,\n                context=str(cleaned_query_output),\n                question=parsed_cypher_query,\n            )\n            node_text = summarized_response\n        else:\n            node_text = self.response_template.format(\n                query=parsed_cypher_query,\n                response=str(cleaned_query_output),\n            )\n\n        return [\n            NodeWithScore(\n                node=TextNode(\n                    text=node_text,\n                    metadata=(\n                        {\"query\": parsed_cypher_query, \"response\": cleaned_query_output}\n                        if self.include_raw_response_as_metadata\n                        else {}\n                    ),\n                ),\n                score=1.0,\n            )\n        ]\n\n    async def aretrieve_from_graph(\n        self, query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        schema = await self._graph_store.aget_schema_str()\n        question = query_bundle.query_str\n\n        response = await self.llm.apredict(\n            self.text_to_cypher_template,\n            schema=schema,\n            question=question,\n        )\n\n        parsed_cypher_query = self._parse_generated_cypher(response)\n\n        query_output = await self._graph_store.astructured_query(parsed_cypher_query)\n\n        cleaned_query_output = self._clean_query_output(query_output)\n\n        if self.summarize_response:\n            summarized_response = await self.llm.apredict(\n                self.summarization_template,\n                context=str(cleaned_query_output),\n                question=parsed_cypher_query,\n            )\n            node_text = summarized_response\n        else:\n            node_text = self.response_template.format(\n                query=parsed_cypher_query,\n                response=str(cleaned_query_output),\n            )\n\n        return [\n            NodeWithScore(\n                node=TextNode(\n                    text=node_text,\n                    metadata=(\n                        {\"query\": parsed_cypher_query, \"response\": cleaned_query_output}\n                        if self.include_raw_response_as_metadata\n                        else {}\n                    ),\n                ),\n                score=1.0,\n            )\n        ]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/cypher_template.py",
    "filename": "cypher_template.py",
    "relpath": "indices/property_graph/sub_retrievers/cypher_template.py",
    "start_line": 1,
    "end_line": 92,
    "length": 92,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "chunk_class_names": [
      "CypherTemplateRetriever",
      "to"
    ],
    "document_function_names": [
      "__init__",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "document_class_names": [
      "CypherTemplateRetriever",
      "to"
    ],
    "content": "from typing import Any, List, Optional, Type\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.graph_stores.types import PropertyGraphStore\nfrom llama_index.core.indices.property_graph.sub_retrievers.base import BasePGRetriever\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\nfrom llama_index.core.settings import Settings\n\n\nclass CypherTemplateRetriever(BasePGRetriever):\n    \"\"\"A Cypher retriever that fills in params for a cypher query using an LLM.\n\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        output_cls (Type[BaseModel]):\n            The output class to use for the LLM.\n            Should contain the params needed for the cypher query.\n        cypher_query (str):\n            The cypher query to use, with templated params.\n        llm (Optional[LLM], optional):\n            The language model to use. Defaults to Settings.llm.\n    \"\"\"\n\n    def __init__(\n        self,\n        graph_store: PropertyGraphStore,\n        output_cls: Type[BaseModel],\n        cypher_query: str,\n        llm: Optional[LLM] = None,\n        **kwargs: Any,\n    ) -> None:\n        if not graph_store.supports_structured_queries:\n            raise ValueError(\n                \"The provided graph store does not support cypher queries.\"\n            )\n\n        self.llm = llm or Settings.llm\n        # Explicit type hint to suppress:\n        #   `Expected type '_SpecialForm[BaseModel]', got 'Type[BaseModel]' instead`\n        self.output_cls: Type[BaseModel] = output_cls\n        self.cypher_query = cypher_query\n\n        super().__init__(\n            graph_store=graph_store, include_text=False, include_properties=False\n        )\n\n    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        question = query_bundle.query_str\n\n        response = self.llm.structured_predict(\n            self.output_cls, PromptTemplate(question)\n        )\n\n        cypher_response = self._graph_store.structured_query(\n            self.cypher_query,\n            param_map=response.model_dump(),\n        )\n\n        return [\n            NodeWithScore(\n                node=TextNode(\n                    text=str(cypher_response),\n                ),\n                score=1.0,\n            )\n        ]\n\n    async def aretrieve_from_graph(\n        self, query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        question = query_bundle.query_str\n\n        response = await self.llm.astructured_predict(\n            self.output_cls, PromptTemplate(question)\n        )\n\n        cypher_response = await self._graph_store.astructured_query(\n            self.cypher_query,\n            param_map=response.model_dump(),\n        )\n\n        return [\n            NodeWithScore(\n                node=TextNode(\n                    text=str(cypher_response),\n                ),\n                score=1.0,\n            )\n        ]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/property_graph/sub_retrievers/custom.py",
    "filename": "custom.py",
    "relpath": "indices/property_graph/sub_retrievers/custom.py",
    "start_line": 1,
    "end_line": 129,
    "length": 129,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "graph_store",
      "init",
      "custom_retrieve",
      "acustom_retrieve",
      "_parse_custom_return_type",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "chunk_class_names": [
      "CustomPGRetriever"
    ],
    "document_function_names": [
      "__init__",
      "graph_store",
      "init",
      "custom_retrieve",
      "acustom_retrieve",
      "_parse_custom_return_type",
      "retrieve_from_graph",
      "aretrieve_from_graph"
    ],
    "document_class_names": [
      "CustomPGRetriever"
    ],
    "content": "from abc import abstractmethod\nfrom typing import Any, List, Union\n\nfrom llama_index.core.graph_stores.types import PropertyGraphStore\nfrom llama_index.core.indices.property_graph.sub_retrievers.base import BasePGRetriever\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n\nCUSTOM_RETRIEVE_TYPE = Union[\n    str, List[str], TextNode, List[TextNode], NodeWithScore, List[NodeWithScore]\n]\n\n\nclass CustomPGRetriever(BasePGRetriever):\n    \"\"\"A retriever meant to be easily subclassed to implement custom retrieval logic.\n\n    The user only has to implement:\n    - `init` to initialize the retriever and assign any necessary attributes.\n    - `custom_retrieve` to implement the custom retrieval logic.\n    - `aretrieve_custom` (optional) to implement asynchronous retrieval logic.\n\n    Args:\n        graph_store (PropertyGraphStore):\n            The graph store to retrieve data from.\n        include_text (bool):\n            Whether to include text in the retrieved nodes. Only works for kg nodes\n            inserted by LlamaIndex.\n        **kwargs:\n            Additional keyword arguments passed to init().\n    \"\"\"\n\n    def __init__(\n        self,\n        graph_store: PropertyGraphStore,\n        include_text: bool = False,\n        include_properties: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(\n            graph_store=graph_store,\n            include_text=include_text,\n            include_properties=include_properties,\n            **kwargs,\n        )\n        self.init(**kwargs)\n\n    @property\n    def graph_store(self) -> PropertyGraphStore:\n        return self._graph_store\n\n    @abstractmethod\n    def init(self, **kwargs: Any) -> None:\n        \"\"\"Initialize the retriever.\n\n        Has access to all keyword arguments passed to the retriever, as well as:\n        - `self.graph_store`: The graph store to retrieve data from.\n        - `self.include_text``: Whether to include text in the retrieved nodes.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def custom_retrieve(self, query_str: str) -> CUSTOM_RETRIEVE_TYPE:\n        \"\"\"Retrieve data from the graph store based on the query string.\n\n        Args:\n            query_str (str): The query string to retrieve data for.\n\n        Returns:\n            The retrieved data. The return type can be one of:\n            - str: A single string.\n            - List[str]: A list of strings.\n            - TextNode: A single TextNode.\n            - List[TextNode]: A list of TextNodes.\n            - NodeWithScore: A single NodeWithScore.\n            - List[NodeWithScore]: A list of NodeWithScores.\n        \"\"\"\n        ...\n\n    async def acustom_retrieve(self, query_str: str) -> CUSTOM_RETRIEVE_TYPE:\n        \"\"\"Asynchronously retrieve data from the graph store based on the query string.\n\n        Args:\n            query_str (str): The query string to retrieve data for.\n\n        Returns:\n            The retrieved data. The return type can be one of:\n            - str: A single string.\n            - List[str]: A list of strings.\n            - TextNode: A single TextNode.\n            - List[TextNode]: A list of TextNodes.\n            - NodeWithScore: A single NodeWithScore.\n            - List[NodeWithScore]: A list of NodeWithScores.\n        \"\"\"\n        return self.custom_retrieve(query_str)\n\n    def _parse_custom_return_type(\n        self, result: CUSTOM_RETRIEVE_TYPE\n    ) -> List[NodeWithScore]:\n        if isinstance(result, str):\n            return [NodeWithScore(node=TextNode(text=result), score=1.0)]\n        elif isinstance(result, list):\n            if all(isinstance(item, str) for item in result):\n                return [\n                    NodeWithScore(node=TextNode(text=item), score=1.0)\n                    for item in result\n                ]\n            elif all(isinstance(item, TextNode) for item in result):\n                return [NodeWithScore(node=item, score=1.0) for item in result]\n            elif all(isinstance(item, NodeWithScore) for item in result):\n                return result  # type: ignore\n            else:\n                raise ValueError(\n                    \"Invalid return type. All items in the list must be of the same type.\"\n                )\n        elif isinstance(result, TextNode):\n            return [NodeWithScore(node=result, score=1.0)]\n        elif isinstance(result, NodeWithScore):\n            return [result]\n        else:\n            raise ValueError(f\"Invalid return type: {type(result)}\")\n\n    def retrieve_from_graph(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        result = self.custom_retrieve(query_bundle.query_str)\n        return self._parse_custom_return_type(result)\n\n    async def aretrieve_from_graph(\n        self, query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        result = await self.acustom_retrieve(query_bundle.query_str)\n        return self._parse_custom_return_type(result)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/list/base.py",
    "filename": "base.py",
    "relpath": "indices/list/base.py",
    "start_line": 1,
    "end_line": 152,
    "length": 152,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "as_retriever",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info"
    ],
    "chunk_class_names": [
      "ListRetrieverMode",
      "SummaryIndex"
    ],
    "document_function_names": [
      "__init__",
      "as_retriever",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info"
    ],
    "document_class_names": [
      "ListRetrieverMode",
      "SummaryIndex"
    ],
    "content": "\"\"\"Summary index.\n\nA simple data structure where LlamaIndex iterates through document chunks\nin sequence in order to answer a given query.\n\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Any, Dict, Optional, Sequence, Union\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.data_structs.data_structs import IndexList\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.schema import BaseNode, IndexNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore.types import RefDocInfo\nfrom llama_index.core.utils import get_tqdm_iterable\n\n\nclass ListRetrieverMode(str, Enum):\n    DEFAULT = \"default\"\n    EMBEDDING = \"embedding\"\n    LLM = \"llm\"\n\n\nclass SummaryIndex(BaseIndex[IndexList]):\n    \"\"\"Summary Index.\n\n    The summary index is a simple data structure where nodes are stored in\n    a sequence. During index construction, the document texts are\n    chunked up, converted to nodes, and stored in a list.\n\n    During query time, the summary index iterates through the nodes\n    with some optional filter parameters, and synthesizes an\n    answer from all the nodes.\n\n    Args:\n        text_qa_template (Optional[BasePromptTemplate]): A Question-Answer Prompt\n            (see :ref:`Prompt-Templates`).\n            NOTE: this is a deprecated field.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n\n    \"\"\"\n\n    index_struct_cls = IndexList\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        objects: Optional[Sequence[IndexNode]] = None,\n        index_struct: Optional[IndexList] = None,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            show_progress=show_progress,\n            objects=objects,\n            **kwargs,\n        )\n\n    def as_retriever(\n        self,\n        retriever_mode: Union[str, ListRetrieverMode] = ListRetrieverMode.DEFAULT,\n        llm: Optional[LLM] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        from llama_index.core.indices.list.retrievers import (\n            SummaryIndexEmbeddingRetriever,\n            SummaryIndexLLMRetriever,\n            SummaryIndexRetriever,\n        )\n\n        if retriever_mode == ListRetrieverMode.DEFAULT:\n            return SummaryIndexRetriever(self, object_map=self._object_map, **kwargs)\n        elif retriever_mode == ListRetrieverMode.EMBEDDING:\n            embed_model = embed_model or Settings.embed_model\n            return SummaryIndexEmbeddingRetriever(\n                self, object_map=self._object_map, embed_model=embed_model, **kwargs\n            )\n        elif retriever_mode == ListRetrieverMode.LLM:\n            llm = llm or Settings.llm\n            return SummaryIndexLLMRetriever(\n                self, object_map=self._object_map, llm=llm, **kwargs\n            )\n        else:\n            raise ValueError(f\"Unknown retriever mode: {retriever_mode}\")\n\n    def _build_index_from_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **build_kwargs: Any,\n    ) -> IndexList:\n        \"\"\"Build the index from documents.\n\n        Args:\n            documents (List[BaseDocument]): A list of documents.\n\n        Returns:\n            IndexList: The created summary index.\n        \"\"\"\n        index_struct = IndexList()\n        nodes_with_progress = get_tqdm_iterable(\n            nodes, show_progress, \"Processing nodes\"\n        )\n        for n in nodes_with_progress:\n            index_struct.add_node(n)\n        return index_struct\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        for n in nodes:\n            self._index_struct.add_node(n)\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        cur_node_ids = self._index_struct.nodes\n        cur_nodes = self._docstore.get_nodes(cur_node_ids)\n        nodes_to_keep = [n for n in cur_nodes if n.node_id != node_id]\n        self._index_struct.nodes = [n.node_id for n in nodes_to_keep]\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        node_doc_ids = self._index_struct.nodes\n        nodes = self.docstore.get_nodes(node_doc_ids)\n\n        all_ref_doc_info = {}\n        for node in nodes:\n            ref_node = node.source_node\n            if not ref_node:\n                continue\n\n            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)\n            if not ref_doc_info:\n                continue\n\n            all_ref_doc_info[ref_node.node_id] = ref_doc_info\n        return all_ref_doc_info\n\n\n# Legacy\nGPTListIndex = SummaryIndex\n\n# New name\nListIndex = SummaryIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/list/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/list/__init__.py",
    "start_line": 1,
    "end_line": 28,
    "length": 28,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"List-based data structures.\"\"\"\n\nfrom llama_index.core.indices.list.base import (\n    GPTListIndex,\n    ListIndex,\n    SummaryIndex,\n)\nfrom llama_index.core.indices.list.retrievers import (\n    ListIndexEmbeddingRetriever,\n    ListIndexLLMRetriever,\n    ListIndexRetriever,\n    SummaryIndexEmbeddingRetriever,\n    SummaryIndexLLMRetriever,\n    SummaryIndexRetriever,\n)\n\n__all__ = [\n    \"SummaryIndex\",\n    \"SummaryIndexRetriever\",\n    \"SummaryIndexEmbeddingRetriever\",\n    \"SummaryIndexLLMRetriever\",\n    # legacy\n    \"ListIndex\",\n    \"GPTListIndex\",\n    \"ListIndexRetriever\",\n    \"ListIndexEmbeddingRetriever\",\n    \"ListIndexLLMRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/list/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/list/retrievers.py",
    "start_line": 1,
    "end_line": 224,
    "length": 224,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_retrieve",
      "__init__",
      "_retrieve",
      "_get_embeddings",
      "__init__",
      "_retrieve"
    ],
    "chunk_class_names": [
      "SummaryIndexRetriever",
      "SummaryIndexEmbeddingRetriever",
      "SummaryIndexLLMRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_retrieve",
      "__init__",
      "_retrieve",
      "_get_embeddings",
      "__init__",
      "_retrieve"
    ],
    "document_class_names": [
      "SummaryIndexRetriever",
      "SummaryIndexEmbeddingRetriever",
      "SummaryIndexLLMRetriever"
    ],
    "content": "\"\"\"Retrievers for SummaryIndex.\"\"\"\n\nimport logging\nfrom typing import Any, Callable, List, Optional, Tuple\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.list.base import SummaryIndex\nfrom llama_index.core.indices.query.embedding_utils import get_top_k_embeddings\nfrom llama_index.core.indices.utils import (\n    default_format_node_batch_fn,\n    default_parse_choice_select_answer_fn,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_CHOICE_SELECT_PROMPT,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    MetadataMode,\n    NodeWithScore,\n    QueryBundle,\n)\nfrom llama_index.core.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass SummaryIndexRetriever(BaseRetriever):\n    \"\"\"Simple retriever for SummaryIndex that returns all nodes.\n\n    Args:\n        index (SummaryIndex): The index to retrieve from.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: SummaryIndex,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self._index = index\n        super().__init__(\n            callback_manager=callback_manager, object_map=object_map, verbose=verbose\n        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        del query_bundle\n\n        node_ids = self._index.index_struct.nodes\n        nodes = self._index.docstore.get_nodes(node_ids)\n        return [NodeWithScore(node=node) for node in nodes]\n\n\nclass SummaryIndexEmbeddingRetriever(BaseRetriever):\n    \"\"\"Embedding based retriever for SummaryIndex.\n\n    Generates embeddings in a lazy fashion for all\n    nodes that are traversed.\n\n    Args:\n        index (SummaryIndex): The index to retrieve from.\n        similarity_top_k (Optional[int]): The number of top nodes to return.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: SummaryIndex,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_top_k: Optional[int] = 1,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self._index = index\n        self._similarity_top_k = similarity_top_k\n        self._embed_model = embed_model or Settings.embed_model\n\n        super().__init__(\n            callback_manager=callback_manager, object_map=object_map, verbose=verbose\n        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        node_ids = self._index.index_struct.nodes\n        # top k nodes\n        nodes = self._index.docstore.get_nodes(node_ids)\n        query_embedding, node_embeddings = self._get_embeddings(query_bundle, nodes)\n\n        top_similarities, top_idxs = get_top_k_embeddings(\n            query_embedding,\n            node_embeddings,\n            similarity_top_k=self._similarity_top_k,\n            embedding_ids=list(range(len(nodes))),\n        )\n\n        top_k_nodes = [nodes[i] for i in top_idxs]\n\n        node_with_scores = []\n        for node, similarity in zip(top_k_nodes, top_similarities):\n            node_with_scores.append(NodeWithScore(node=node, score=similarity))\n\n        logger.debug(f\"> Top {len(top_idxs)} nodes:\\n\")\n        nl = \"\\n\"\n        logger.debug(f\"{nl.join([n.get_content() for n in top_k_nodes])}\")\n        return node_with_scores\n\n    def _get_embeddings(\n        self, query_bundle: QueryBundle, nodes: List[BaseNode]\n    ) -> Tuple[List[float], List[List[float]]]:\n        \"\"\"Get top nodes by similarity to the query.\"\"\"\n        if query_bundle.embedding is None:\n            query_bundle.embedding = self._embed_model.get_agg_embedding_from_queries(\n                query_bundle.embedding_strs\n            )\n\n        node_embeddings: List[List[float]] = []\n        nodes_embedded = 0\n        for node in nodes:\n            if node.embedding is None:\n                nodes_embedded += 1\n                node.embedding = self._embed_model.get_text_embedding(\n                    node.get_content(metadata_mode=MetadataMode.EMBED)\n                )\n\n            node_embeddings.append(node.embedding)\n        return query_bundle.embedding, node_embeddings\n\n\nclass SummaryIndexLLMRetriever(BaseRetriever):\n    \"\"\"LLM retriever for SummaryIndex.\n\n    Args:\n        index (SummaryIndex): The index to retrieve from.\n        choice_select_prompt (Optional[PromptTemplate]): A Choice-Select Prompt\n           (see :ref:`Prompt-Templates`).)\n        choice_batch_size (int): The number of nodes to query at a time.\n        format_node_batch_fn (Optional[Callable]): A function that formats a\n            batch of nodes.\n        parse_choice_select_answer_fn (Optional[Callable]): A function that parses the\n            choice select answer.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: SummaryIndex,\n        llm: Optional[LLM] = None,\n        choice_select_prompt: Optional[PromptTemplate] = None,\n        choice_batch_size: int = 10,\n        format_node_batch_fn: Optional[Callable] = None,\n        parse_choice_select_answer_fn: Optional[Callable] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self._index = index\n        self._choice_select_prompt = (\n            choice_select_prompt or DEFAULT_CHOICE_SELECT_PROMPT\n        )\n        self._choice_batch_size = choice_batch_size\n        self._format_node_batch_fn = (\n            format_node_batch_fn or default_format_node_batch_fn\n        )\n        self._parse_choice_select_answer_fn = (\n            parse_choice_select_answer_fn or default_parse_choice_select_answer_fn\n        )\n        self._llm = llm or Settings.llm\n        super().__init__(\n            callback_manager=callback_manager, object_map=object_map, verbose=verbose\n        )\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        node_ids = self._index.index_struct.nodes\n        results = []\n        for idx in range(0, len(node_ids), self._choice_batch_size):\n            node_ids_batch = node_ids[idx : idx + self._choice_batch_size]\n            nodes_batch = self._index.docstore.get_nodes(node_ids_batch)\n\n            query_str = query_bundle.query_str\n            fmt_batch_str = self._format_node_batch_fn(nodes_batch)\n            # call each batch independently\n            raw_response = self._llm.predict(\n                self._choice_select_prompt,\n                context_str=fmt_batch_str,\n                query_str=query_str,\n            )\n\n            raw_choices, relevances = self._parse_choice_select_answer_fn(\n                raw_response, len(nodes_batch)\n            )\n            choice_idxs = [int(choice) - 1 for choice in raw_choices]\n            choice_node_ids = [node_ids_batch[idx] for idx in choice_idxs]\n\n            choice_nodes = self._index.docstore.get_nodes(choice_node_ids)\n            relevances = relevances or [1.0 for _ in choice_nodes]\n            results.extend(\n                [\n                    NodeWithScore(node=node, score=relevance)\n                    for node, relevance in zip(choice_nodes, relevances)\n                ]\n            )\n        return results\n\n\n# for backwards compatibility\nListIndexEmbeddingRetriever = SummaryIndexEmbeddingRetriever\nListIndexLLMRetriever = SummaryIndexLLMRetriever\nListIndexRetriever = SummaryIndexRetriever"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/base.py",
    "filename": "base.py",
    "relpath": "indices/vector_store/base.py",
    "start_line": 1,
    "end_line": 441,
    "length": 441,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_vector_store",
      "vector_store",
      "as_retriever",
      "_get_node_with_embedding",
      "_aget_node_with_embedding",
      "_async_add_nodes_to_index",
      "_add_nodes_to_index",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "_delete_node",
      "delete_nodes",
      "_delete_from_index_struct",
      "_delete_from_docstore",
      "delete_ref_doc",
      "_adelete_from_index_struct",
      "_adelete_from_docstore",
      "adelete_ref_doc",
      "ref_doc_info"
    ],
    "chunk_class_names": [
      "VectorStoreIndex",
      "params"
    ],
    "document_function_names": [
      "__init__",
      "from_vector_store",
      "vector_store",
      "as_retriever",
      "_get_node_with_embedding",
      "_aget_node_with_embedding",
      "_async_add_nodes_to_index",
      "_add_nodes_to_index",
      "_build_index_from_nodes",
      "build_index_from_nodes",
      "_insert",
      "insert_nodes",
      "_delete_node",
      "delete_nodes",
      "_delete_from_index_struct",
      "_delete_from_docstore",
      "delete_ref_doc",
      "_adelete_from_index_struct",
      "_adelete_from_docstore",
      "adelete_ref_doc",
      "ref_doc_info"
    ],
    "document_class_names": [
      "VectorStoreIndex",
      "params"
    ],
    "content": "\"\"\"\nBase vector store index.\n\nAn index that is built on top of an existing vector store.\n\n\"\"\"\n\nimport asyncio\nimport logging\nfrom typing import Any, Dict, List, Optional, Sequence\n\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.data_structs.data_structs import IndexDict\nfrom llama_index.core.embeddings.utils import EmbedType, resolve_embed_model\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.utils import async_embed_nodes, embed_nodes\nfrom llama_index.core.schema import (\n    BaseNode,\n    ImageNode,\n    IndexNode,\n    MetadataMode,\n    TransformComponent,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore.types import RefDocInfo\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.utils import iter_batch\nfrom llama_index.core.vector_stores.types import BasePydanticVectorStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass VectorStoreIndex(BaseIndex[IndexDict]):\n    \"\"\"\n    Vector Store Index.\n\n    Args:\n        use_async (bool): Whether to use asynchronous calls. Defaults to False.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n        store_nodes_override (bool): set to True to always store Node objects in index\n            store and document store even if vector store keeps text. Defaults to False\n    \"\"\"\n\n    index_struct_cls = IndexDict\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        # vector store index params\n        use_async: bool = False,\n        store_nodes_override: bool = False,\n        embed_model: Optional[EmbedType] = None,\n        insert_batch_size: int = 2048,\n        # parent class params\n        objects: Optional[Sequence[IndexNode]] = None,\n        index_struct: Optional[IndexDict] = None,\n        storage_context: Optional[StorageContext] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._use_async = use_async\n        self._store_nodes_override = store_nodes_override\n        self._embed_model = (\n            resolve_embed_model(embed_model, callback_manager=callback_manager)\n            if embed_model\n            else Settings.embed_model\n        )\n\n        self._insert_batch_size = insert_batch_size\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            storage_context=storage_context,\n            show_progress=show_progress,\n            objects=objects,\n            callback_manager=callback_manager,\n            transformations=transformations,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_vector_store(\n        cls,\n        vector_store: BasePydanticVectorStore,\n        embed_model: Optional[EmbedType] = None,\n        **kwargs: Any,\n    ) -> \"VectorStoreIndex\":\n        if not vector_store.stores_text:\n            raise ValueError(\n                \"Cannot initialize from a vector store that does not store text.\"\n            )\n\n        kwargs.pop(\"storage_context\", None)\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n        return cls(\n            nodes=[],\n            embed_model=embed_model,\n            storage_context=storage_context,\n            **kwargs,\n        )\n\n    @property\n    def vector_store(self) -> BasePydanticVectorStore:\n        return self._vector_store\n\n    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\n        # NOTE: lazy import\n        from llama_index.core.indices.vector_store.retrievers import (\n            VectorIndexRetriever,\n        )\n\n        return VectorIndexRetriever(\n            self,\n            node_ids=list(self.index_struct.nodes_dict.values()),\n            callback_manager=self._callback_manager,\n            object_map=self._object_map,\n            **kwargs,\n        )\n\n    def _get_node_with_embedding(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"\n        Get tuples of id, node, and embedding.\n\n        Allows us to store these nodes in a vector store.\n        Embeddings are called in batches.\n\n        \"\"\"\n        id_to_embed_map = embed_nodes(\n            nodes, self._embed_model, show_progress=show_progress\n        )\n\n        results = []\n        for node in nodes:\n            embedding = id_to_embed_map[node.node_id]\n            result = node.model_copy()\n            result.embedding = embedding\n            results.append(result)\n        return results\n\n    async def _aget_node_with_embedding(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"\n        Asynchronously get tuples of id, node, and embedding.\n\n        Allows us to store these nodes in a vector store.\n        Embeddings are called in batches.\n\n        \"\"\"\n        id_to_embed_map = await async_embed_nodes(\n            nodes=nodes,\n            embed_model=self._embed_model,\n            show_progress=show_progress,\n        )\n\n        results = []\n        for node in nodes:\n            embedding = id_to_embed_map[node.node_id]\n            result = node.model_copy()\n            result.embedding = embedding\n            results.append(result)\n        return results\n\n    async def _async_add_nodes_to_index(\n        self,\n        index_struct: IndexDict,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **insert_kwargs: Any,\n    ) -> None:\n        \"\"\"Asynchronously add nodes to index.\"\"\"\n        if not nodes:\n            return\n\n        for nodes_batch in iter_batch(nodes, self._insert_batch_size):\n            nodes_batch = await self._aget_node_with_embedding(\n                nodes_batch, show_progress\n            )\n            new_ids = await self._vector_store.async_add(nodes_batch, **insert_kwargs)\n\n            # if the vector store doesn't store text, we need to add the nodes to the\n            # index struct and document store\n            if not self._vector_store.stores_text or self._store_nodes_override:\n                for node, new_id in zip(nodes_batch, new_ids):\n                    # NOTE: remove embedding from node to avoid duplication\n                    node_without_embedding = node.model_copy()\n                    node_without_embedding.embedding = None\n\n                    index_struct.add_node(node_without_embedding, text_id=new_id)\n                    self._docstore.add_documents(\n                        [node_without_embedding], allow_update=True\n                    )\n            else:\n                # NOTE: if the vector store keeps text,\n                # we only need to add image and index nodes\n                for node, new_id in zip(nodes_batch, new_ids):\n                    if isinstance(node, (ImageNode, IndexNode)):\n                        # NOTE: remove embedding from node to avoid duplication\n                        node_without_embedding = node.model_copy()\n                        node_without_embedding.embedding = None\n\n                        index_struct.add_node(node_without_embedding, text_id=new_id)\n                        self._docstore.add_documents(\n                            [node_without_embedding], allow_update=True\n                        )\n\n    def _add_nodes_to_index(\n        self,\n        index_struct: IndexDict,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **insert_kwargs: Any,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        if not nodes:\n            return\n\n        for nodes_batch in iter_batch(nodes, self._insert_batch_size):\n            nodes_batch = self._get_node_with_embedding(nodes_batch, show_progress)\n            new_ids = self._vector_store.add(nodes_batch, **insert_kwargs)\n\n            if not self._vector_store.stores_text or self._store_nodes_override:\n                # NOTE: if the vector store doesn't store text,\n                # we need to add the nodes to the index struct and document store\n                for node, new_id in zip(nodes_batch, new_ids):\n                    # NOTE: remove embedding from node to avoid duplication\n                    node_without_embedding = node.model_copy()\n                    node_without_embedding.embedding = None\n\n                    index_struct.add_node(node_without_embedding, text_id=new_id)\n                    self._docstore.add_documents(\n                        [node_without_embedding], allow_update=True\n                    )\n            else:\n                # NOTE: if the vector store keeps text,\n                # we only need to add image and index nodes\n                for node, new_id in zip(nodes_batch, new_ids):\n                    if isinstance(node, (ImageNode, IndexNode)):\n                        # NOTE: remove embedding from node to avoid duplication\n                        node_without_embedding = node.model_copy()\n                        node_without_embedding.embedding = None\n\n                        index_struct.add_node(node_without_embedding, text_id=new_id)\n                        self._docstore.add_documents(\n                            [node_without_embedding], allow_update=True\n                        )\n\n    def _build_index_from_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        **insert_kwargs: Any,\n    ) -> IndexDict:\n        \"\"\"Build index from nodes.\"\"\"\n        index_struct = self.index_struct_cls()\n        if self._use_async:\n            tasks = [\n                self._async_add_nodes_to_index(\n                    index_struct,\n                    nodes,\n                    show_progress=self._show_progress,\n                    **insert_kwargs,\n                )\n            ]\n            run_async_tasks(tasks)\n        else:\n            self._add_nodes_to_index(\n                index_struct,\n                nodes,\n                show_progress=self._show_progress,\n                **insert_kwargs,\n            )\n        return index_struct\n\n    def build_index_from_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        **insert_kwargs: Any,\n    ) -> IndexDict:\n        \"\"\"\n        Build the index from nodes.\n\n        NOTE: Overrides BaseIndex.build_index_from_nodes.\n            VectorStoreIndex only stores nodes in document store\n            if vector store does not store text\n        \"\"\"\n        # Filter out the nodes that don't have content\n        content_nodes = [\n            node\n            for node in nodes\n            if node.get_content(metadata_mode=MetadataMode.EMBED) != \"\"\n        ]\n\n        # Report if some nodes are missing content\n        if len(content_nodes) != len(nodes):\n            print(\"Some nodes are missing content, skipping them...\")\n\n        return self._build_index_from_nodes(content_nodes, **insert_kwargs)\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_nodes_to_index(self._index_struct, nodes, **insert_kwargs)\n\n    def insert_nodes(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"\n        Insert nodes.\n\n        NOTE: overrides BaseIndex.insert_nodes.\n            VectorStoreIndex only stores nodes in document store\n            if vector store does not store text\n        \"\"\"\n        for node in nodes:\n            if isinstance(node, IndexNode):\n                try:\n                    node.dict()\n                except ValueError:\n                    self._object_map[node.index_id] = node.obj\n                    node.obj = None\n\n        with self._callback_manager.as_trace(\"insert_nodes\"):\n            self._insert(nodes, **insert_kwargs)\n            self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        pass\n\n    def delete_nodes(\n        self,\n        node_ids: List[str],\n        delete_from_docstore: bool = False,\n        **delete_kwargs: Any,\n    ) -> None:\n        \"\"\"\n        Delete a list of nodes from the index.\n\n        Args:\n            node_ids (List[str]): A list of node_ids from the nodes to delete\n\n        \"\"\"\n        # delete nodes from vector store\n        self._vector_store.delete_nodes(node_ids, **delete_kwargs)\n\n        # delete from docstore only if needed\n        if (\n            not self._vector_store.stores_text or self._store_nodes_override\n        ) and delete_from_docstore:\n            for node_id in node_ids:\n                self._docstore.delete_document(node_id, raise_error=False)\n\n    def _delete_from_index_struct(self, ref_doc_id: str) -> None:\n        # delete from index_struct only if needed\n        if not self._vector_store.stores_text or self._store_nodes_override:\n            ref_doc_info = self._docstore.get_ref_doc_info(ref_doc_id)\n            if ref_doc_info is not None:\n                for node_id in ref_doc_info.node_ids:\n                    self._index_struct.delete(node_id)\n                    self._vector_store.delete(node_id)\n\n    def _delete_from_docstore(self, ref_doc_id: str) -> None:\n        # delete from docstore only if needed\n        if not self._vector_store.stores_text or self._store_nodes_override:\n            self._docstore.delete_ref_doc(ref_doc_id, raise_error=False)\n\n    def delete_ref_doc(\n        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any\n    ) -> None:\n        \"\"\"Delete a document and it's nodes by using ref_doc_id.\"\"\"\n        self._vector_store.delete(ref_doc_id, **delete_kwargs)\n        self._delete_from_index_struct(ref_doc_id)\n        if delete_from_docstore:\n            self._delete_from_docstore(ref_doc_id)\n        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    async def _adelete_from_index_struct(self, ref_doc_id: str) -> None:\n        \"\"\"Delete from index_struct only if needed.\"\"\"\n        if not self._vector_store.stores_text or self._store_nodes_override:\n            ref_doc_info = await self._docstore.aget_ref_doc_info(ref_doc_id)\n            if ref_doc_info is not None:\n                for node_id in ref_doc_info.node_ids:\n                    self._index_struct.delete(node_id)\n                    self._vector_store.delete(node_id)\n\n    async def _adelete_from_docstore(self, ref_doc_id: str) -> None:\n        \"\"\"Delete from docstore only if needed.\"\"\"\n        if not self._vector_store.stores_text or self._store_nodes_override:\n            await self._docstore.adelete_ref_doc(ref_doc_id, raise_error=False)\n\n    async def adelete_ref_doc(\n        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any\n    ) -> None:\n        \"\"\"Delete a document and it's nodes by using ref_doc_id.\"\"\"\n        tasks = [\n            self._vector_store.adelete(ref_doc_id, **delete_kwargs),\n            self._adelete_from_index_struct(ref_doc_id),\n        ]\n        if delete_from_docstore:\n            tasks.append(self._adelete_from_docstore(ref_doc_id))\n\n        await asyncio.gather(*tasks)\n\n        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        if not self._vector_store.stores_text or self._store_nodes_override:\n            node_doc_ids = list(self.index_struct.nodes_dict.values())\n            nodes = self.docstore.get_nodes(node_doc_ids)\n\n            all_ref_doc_info = {}\n            for node in nodes:\n                ref_node = node.source_node\n                if not ref_node:\n                    continue\n\n                ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)\n                if not ref_doc_info:\n                    continue\n\n                all_ref_doc_info[ref_node.node_id] = ref_doc_info\n            return all_ref_doc_info\n        else:\n            raise NotImplementedError(\n                \"Vector store integrations that store text in the vector store are \"\n                \"not supported by ref_doc_info yet.\"\n            )\n\n\nGPTVectorStoreIndex = VectorStoreIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/vector_store/__init__.py",
    "start_line": 1,
    "end_line": 18,
    "length": 18,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Vector-store based data structures.\"\"\"\n\nfrom llama_index.core.indices.vector_store.base import (\n    GPTVectorStoreIndex,\n    VectorStoreIndex,\n)\nfrom llama_index.core.indices.vector_store.retrievers import (\n    VectorIndexAutoRetriever,\n    VectorIndexRetriever,\n)\n\n__all__ = [\n    \"VectorStoreIndex\",\n    \"VectorIndexRetriever\",\n    \"VectorIndexAutoRetriever\",\n    # legacy\n    \"GPTVectorStoreIndex\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/vector_store/retrievers/__init__.py",
    "start_line": 1,
    "end_line": 11,
    "length": 11,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.indices.vector_store.retrievers.retriever import (\n    VectorIndexRetriever,\n)\nfrom llama_index.core.indices.vector_store.retrievers.auto_retriever import (\n    VectorIndexAutoRetriever,\n)\n\n__all__ = [\n    \"VectorIndexRetriever\",\n    \"VectorIndexAutoRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/retriever.py",
    "filename": "retriever.py",
    "relpath": "indices/vector_store/retrievers/retriever.py",
    "start_line": 1,
    "end_line": 188,
    "length": 188,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "similarity_top_k",
      "similarity_top_k",
      "_retrieve",
      "_aretrieve",
      "_build_vector_store_query",
      "_build_node_list_from_query_result",
      "_get_nodes_with_embeddings",
      "_aget_nodes_with_embeddings"
    ],
    "chunk_class_names": [
      "VectorIndexRetriever"
    ],
    "document_function_names": [
      "__init__",
      "similarity_top_k",
      "similarity_top_k",
      "_retrieve",
      "_aretrieve",
      "_build_vector_store_query",
      "_build_node_list_from_query_result",
      "_get_nodes_with_embeddings",
      "_aget_nodes_with_embeddings"
    ],
    "document_class_names": [
      "VectorIndexRetriever"
    ],
    "content": "\"\"\"Base vector store index query.\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\nfrom llama_index.core.data_structs.data_structs import IndexDict\nfrom llama_index.core.indices.utils import log_vector_store_query_result\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.core.schema import NodeWithScore, ObjectType, QueryBundle\nfrom llama_index.core.vector_stores.types import (\n    MetadataFilters,\n    VectorStoreQuery,\n    VectorStoreQueryMode,\n    VectorStoreQueryResult,\n)\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass VectorIndexRetriever(BaseRetriever):\n    \"\"\"Vector index retriever.\n\n    Args:\n        index (VectorStoreIndex): vector store index.\n        similarity_top_k (int): number of top k results to return.\n        vector_store_query_mode (str): vector store query mode\n            See reference for VectorStoreQueryMode for full list of supported modes.\n        filters (Optional[MetadataFilters]): metadata filters, defaults to None\n        alpha (float): weight for sparse/dense retrieval, only used for\n            hybrid query mode.\n        doc_ids (Optional[List[str]]): list of documents to constrain search.\n        vector_store_kwargs (dict): Additional vector store specific kwargs to pass\n            through to the vector store at query time.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: VectorStoreIndex,\n        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n        vector_store_query_mode: VectorStoreQueryMode = VectorStoreQueryMode.DEFAULT,\n        filters: Optional[MetadataFilters] = None,\n        alpha: Optional[float] = None,\n        node_ids: Optional[List[str]] = None,\n        doc_ids: Optional[List[str]] = None,\n        sparse_top_k: Optional[int] = None,\n        hybrid_top_k: Optional[int] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._index = index\n        self._vector_store = self._index.vector_store\n        self._embed_model = embed_model or self._index._embed_model\n        self._docstore = self._index.docstore\n\n        self._similarity_top_k = similarity_top_k\n        self._vector_store_query_mode = VectorStoreQueryMode(vector_store_query_mode)\n        self._alpha = alpha\n        self._node_ids = node_ids\n        self._doc_ids = doc_ids\n        self._filters = filters\n        self._sparse_top_k = sparse_top_k\n        self._hybrid_top_k = hybrid_top_k\n        self._kwargs: Dict[str, Any] = kwargs.get(\"vector_store_kwargs\", {})\n\n        callback_manager = callback_manager or CallbackManager()\n        super().__init__(\n            callback_manager=callback_manager,\n            object_map=object_map,\n            verbose=verbose,\n        )\n\n    @property\n    def similarity_top_k(self) -> int:\n        \"\"\"Return similarity top k.\"\"\"\n        return self._similarity_top_k\n\n    @similarity_top_k.setter\n    def similarity_top_k(self, similarity_top_k: int) -> None:\n        \"\"\"Set similarity top k.\"\"\"\n        self._similarity_top_k = similarity_top_k\n\n    @dispatcher.span\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        if self._vector_store.is_embedding_query:\n            if query_bundle.embedding is None and len(query_bundle.embedding_strs) > 0:\n                query_bundle.embedding = (\n                    self._embed_model.get_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )\n                )\n        return self._get_nodes_with_embeddings(query_bundle)\n\n    @dispatcher.span\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        embedding = query_bundle.embedding\n        if self._vector_store.is_embedding_query:\n            if query_bundle.embedding is None and len(query_bundle.embedding_strs) > 0:\n                embed_model = self._embed_model\n                embedding = await embed_model.aget_agg_embedding_from_queries(\n                    query_bundle.embedding_strs\n                )\n        return await self._aget_nodes_with_embeddings(\n            QueryBundle(query_str=query_bundle.query_str, embedding=embedding)\n        )\n\n    def _build_vector_store_query(\n        self, query_bundle_with_embeddings: QueryBundle\n    ) -> VectorStoreQuery:\n        return VectorStoreQuery(\n            query_embedding=query_bundle_with_embeddings.embedding,\n            similarity_top_k=self._similarity_top_k,\n            node_ids=self._node_ids,\n            doc_ids=self._doc_ids,\n            query_str=query_bundle_with_embeddings.query_str,\n            mode=self._vector_store_query_mode,\n            alpha=self._alpha,\n            filters=self._filters,\n            sparse_top_k=self._sparse_top_k,\n            hybrid_top_k=self._hybrid_top_k,\n        )\n\n    def _build_node_list_from_query_result(\n        self, query_result: VectorStoreQueryResult\n    ) -> List[NodeWithScore]:\n        if query_result.nodes is None:\n            # NOTE: vector store does not keep text and returns node indices.\n            # Need to recover all nodes from docstore\n            if query_result.ids is None:\n                raise ValueError(\n                    \"Vector store query result should return at \"\n                    \"least one of nodes or ids.\"\n                )\n            assert isinstance(self._index.index_struct, IndexDict)\n            node_ids = [\n                self._index.index_struct.nodes_dict[idx] for idx in query_result.ids\n            ]\n            nodes = self._docstore.get_nodes(node_ids)\n            query_result.nodes = nodes\n        else:\n            # NOTE: vector store keeps text, returns nodes.\n            # Only need to recover image or index nodes from docstore\n            for i in range(len(query_result.nodes)):\n                source_node = query_result.nodes[i].source_node\n                if (not self._vector_store.stores_text) or (\n                    source_node is not None and source_node.node_type != ObjectType.TEXT\n                ):\n                    node_id = query_result.nodes[i].node_id\n                    if self._docstore.document_exists(node_id):\n                        query_result.nodes[i] = self._docstore.get_node(  # type: ignore\n                            node_id\n                        )\n\n        log_vector_store_query_result(query_result)\n\n        node_with_scores: List[NodeWithScore] = []\n        for ind, node in enumerate(query_result.nodes):\n            score: Optional[float] = None\n            if query_result.similarities is not None:\n                score = query_result.similarities[ind]\n            node_with_scores.append(NodeWithScore(node=node, score=score))\n\n        return node_with_scores\n\n    def _get_nodes_with_embeddings(\n        self, query_bundle_with_embeddings: QueryBundle\n    ) -> List[NodeWithScore]:\n        query = self._build_vector_store_query(query_bundle_with_embeddings)\n        query_result = self._vector_store.query(query, **self._kwargs)\n        return self._build_node_list_from_query_result(query_result)\n\n    async def _aget_nodes_with_embeddings(\n        self, query_bundle_with_embeddings: QueryBundle\n    ) -> List[NodeWithScore]:\n        query = self._build_vector_store_query(query_bundle_with_embeddings)\n        query_result = await self._vector_store.aquery(query, **self._kwargs)\n        return self._build_node_list_from_query_result(query_result)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/output_parser.py",
    "filename": "output_parser.py",
    "relpath": "indices/vector_store/retrievers/auto_retriever/output_parser.py",
    "start_line": 1,
    "end_line": 17,
    "length": 17,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "parse",
      "format"
    ],
    "chunk_class_names": [
      "VectorStoreQueryOutputParser"
    ],
    "document_function_names": [
      "parse",
      "format"
    ],
    "document_class_names": [
      "VectorStoreQueryOutputParser"
    ],
    "content": "from typing import Any\n\nfrom llama_index.core.output_parsers.base import StructuredOutput\nfrom llama_index.core.output_parsers.utils import parse_json_markdown\nfrom llama_index.core.types import BaseOutputParser\nfrom llama_index.core.vector_stores.types import VectorStoreQuerySpec\n\n\nclass VectorStoreQueryOutputParser(BaseOutputParser):\n    def parse(self, output: str) -> Any:\n        json_dict = parse_json_markdown(output)\n        query_and_filters = VectorStoreQuerySpec.model_validate(json_dict)\n\n        return StructuredOutput(raw_output=output, parsed_output=query_and_filters)\n\n    def format(self, prompt_template: str) -> str:\n        return prompt_template"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/auto_retriever.py",
    "filename": "auto_retriever.py",
    "relpath": "indices/vector_store/retrievers/auto_retriever/auto_retriever.py",
    "start_line": 1,
    "end_line": 243,
    "length": 243,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_query_bundle",
      "_parse_generated_spec",
      "generate_retrieval_spec",
      "agenerate_retrieval_spec",
      "_build_retriever_from_spec"
    ],
    "chunk_class_names": [
      "VectorIndexAutoRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_query_bundle",
      "_parse_generated_spec",
      "generate_retrieval_spec",
      "agenerate_retrieval_spec",
      "_build_retriever_from_spec"
    ],
    "document_class_names": [
      "VectorIndexAutoRetriever"
    ],
    "content": "import logging\nfrom typing import Any, List, Optional, Tuple, cast\n\nfrom llama_index.core.base.base_auto_retriever import BaseAutoRetriever\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\nfrom llama_index.core.indices.vector_store.base import VectorStoreIndex\nfrom llama_index.core.indices.vector_store.retrievers import VectorIndexRetriever\nfrom llama_index.core.indices.vector_store.retrievers.auto_retriever.output_parser import (\n    VectorStoreQueryOutputParser,\n)\nfrom llama_index.core.indices.vector_store.retrievers.auto_retriever.prompts import (\n    DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.output_parsers.base import (\n    OutputParserException,\n    StructuredOutput,\n)\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import IndexNode, QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.vector_stores.types import (\n    FilterCondition,\n    MetadataFilters,\n    VectorStoreInfo,\n    VectorStoreQueryMode,\n    VectorStoreQuerySpec,\n)\n\n_logger = logging.getLogger(__name__)\n\n\nclass VectorIndexAutoRetriever(BaseAutoRetriever):\n    \"\"\"\n    Vector store auto retriever.\n\n    A retriever for vector store index that uses an LLM to automatically set\n    vector store query parameters.\n\n    Args:\n        index (VectorStoreIndex): vector store index\n        vector_store_info (VectorStoreInfo): additional information about\n            vector store content and supported metadata filters. The natural language\n            description is used by an LLM to automatically set vector store query\n            parameters.\n        prompt_template_str: custom prompt template string for LLM.\n            Uses default template string if None.\n        similarity_top_k (int): number of top k results to return.\n        empty_query_top_k (Optional[int]): number of top k results to return\n            if the inferred query string is blank (uses metadata filters only).\n            Can be set to None, which would use the similarity_top_k instead.\n            By default, set to 10.\n        max_top_k (int):\n            the maximum top_k allowed. The top_k set by LLM or similarity_top_k will\n            be clamped to this value.\n        vector_store_query_mode (str): vector store query mode\n            See reference for VectorStoreQueryMode for full list of supported modes.\n        default_empty_query_vector (Optional[List[float]]): default empty query vector.\n            Defaults to None. If not None, then this vector will be used as the query\n            vector if the query is empty.\n        callback_manager (Optional[CallbackManager]): callback manager\n        verbose (bool): verbose mode\n    \"\"\"\n\n    def __init__(\n        self,\n        index: VectorStoreIndex,\n        vector_store_info: VectorStoreInfo,\n        llm: Optional[LLM] = None,\n        prompt_template_str: Optional[str] = None,\n        max_top_k: int = 10,\n        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n        empty_query_top_k: Optional[int] = 10,\n        vector_store_query_mode: VectorStoreQueryMode = VectorStoreQueryMode.DEFAULT,\n        default_empty_query_vector: Optional[List[float]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        extra_filters: Optional[MetadataFilters] = None,\n        object_map: Optional[dict] = None,\n        objects: Optional[List[IndexNode]] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._index = index\n        self._vector_store_info = vector_store_info\n        self._default_empty_query_vector = default_empty_query_vector\n        self._llm = llm or Settings.llm\n        callback_manager = callback_manager or Settings.callback_manager\n\n        # prompt\n        prompt_template_str = (\n            prompt_template_str or DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL\n        )\n        self._output_parser = VectorStoreQueryOutputParser()\n        self._prompt: BasePromptTemplate = PromptTemplate(template=prompt_template_str)\n\n        # additional config\n        self._max_top_k = max_top_k\n        self._similarity_top_k = similarity_top_k\n        self._empty_query_top_k = empty_query_top_k\n        self._vector_store_query_mode = vector_store_query_mode\n        # if extra_filters is OR condition, we don't support that yet\n        if extra_filters is not None and extra_filters.condition == FilterCondition.OR:\n            raise ValueError(\"extra_filters cannot be OR condition\")\n        self._extra_filters = extra_filters or MetadataFilters(filters=[])\n        self._kwargs = kwargs\n        super().__init__(\n            callback_manager=callback_manager,\n            object_map=object_map or self._index._object_map,\n            objects=objects,\n            verbose=verbose,\n        )\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"prompt\": self._prompt,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Get prompt modules.\"\"\"\n        if \"prompt\" in prompts:\n            self._prompt = prompts[\"prompt\"]\n\n    def _get_query_bundle(self, query: str) -> QueryBundle:\n        \"\"\"Get query bundle.\"\"\"\n        if not query and self._default_empty_query_vector is not None:\n            return QueryBundle(\n                query_str=\"\",\n                embedding=self._default_empty_query_vector,\n            )\n        else:\n            return QueryBundle(query_str=query)\n\n    def _parse_generated_spec(\n        self, output: str, query_bundle: QueryBundle\n    ) -> BaseModel:\n        \"\"\"Parse generated spec.\"\"\"\n        try:\n            structured_output = cast(\n                StructuredOutput, self._output_parser.parse(output)\n            )\n            query_spec = cast(VectorStoreQuerySpec, structured_output.parsed_output)\n        except OutputParserException:\n            _logger.warning(\"Failed to parse query spec, using defaults as fallback.\")\n            query_spec = VectorStoreQuerySpec(\n                query=query_bundle.query_str,\n                filters=[],\n                top_k=None,\n            )\n\n        return query_spec\n\n    def generate_retrieval_spec(\n        self, query_bundle: QueryBundle, **kwargs: Any\n    ) -> BaseModel:\n        # prepare input\n        info_str = self._vector_store_info.model_dump_json(indent=4)\n        schema_str = VectorStoreQuerySpec.model_json_schema()\n\n        # call LLM\n        output = self._llm.predict(\n            self._prompt,\n            schema_str=schema_str,\n            info_str=info_str,\n            query_str=query_bundle.query_str,\n        )\n\n        # parse output\n        return self._parse_generated_spec(output, query_bundle)\n\n    async def agenerate_retrieval_spec(\n        self, query_bundle: QueryBundle, **kwargs: Any\n    ) -> BaseModel:\n        # prepare input\n        info_str = self._vector_store_info.model_dump_json(indent=4)\n        schema_str = VectorStoreQuerySpec.model_json_schema()\n\n        # call LLM\n        output = await self._llm.apredict(\n            self._prompt,\n            schema_str=schema_str,\n            info_str=info_str,\n            query_str=query_bundle.query_str,\n        )\n\n        # parse output\n        return self._parse_generated_spec(output, query_bundle)\n\n    def _build_retriever_from_spec(  # type: ignore\n        self, spec: VectorStoreQuerySpec\n    ) -> Tuple[BaseRetriever, QueryBundle]:\n        # construct new query bundle from query_spec\n        # insert 0 vector if query is empty and default_empty_query_vector is not None\n        new_query_bundle = self._get_query_bundle(spec.query)\n\n        _logger.info(f\"Using query str: {spec.query}\")\n        filter_list = [\n            (filter.key, filter.operator.value, filter.value) for filter in spec.filters\n        ]\n        _logger.info(f\"Using filters: {filter_list}\")\n        if self._verbose:\n            print(f\"Using query str: {spec.query}\")\n            print(f\"Using filters: {filter_list}\")\n\n        # define similarity_top_k\n        # if query is specified, then use similarity_top_k\n        # if query is blank, then use empty_query_top_k\n        if spec.query or self._empty_query_top_k is None:\n            similarity_top_k = self._similarity_top_k\n        else:\n            similarity_top_k = self._empty_query_top_k\n\n        # if query_spec.top_k is specified, then use it\n        # as long as below max_top_k and similarity_top_k\n        if spec.top_k is not None:\n            similarity_top_k = min(spec.top_k, self._max_top_k, similarity_top_k)\n\n        _logger.info(f\"Using top_k: {similarity_top_k}\")\n\n        # avoid passing empty filters to retriever\n        if len(spec.filters) + len(self._extra_filters.filters) == 0:\n            filters = None\n        else:\n            filters = MetadataFilters(\n                filters=[*spec.filters, *self._extra_filters.filters]\n            )\n\n        return (\n            VectorIndexRetriever(\n                self._index,\n                filters=filters,\n                similarity_top_k=similarity_top_k,\n                vector_store_query_mode=self._vector_store_query_mode,\n                object_map=self.object_map,\n                verbose=self._verbose,\n                **self._kwargs,\n            ),\n            new_query_bundle,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/vector_store/retrievers/auto_retriever/__init__.py",
    "start_line": 1,
    "end_line": 7,
    "length": 7,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.indices.vector_store.retrievers.auto_retriever.auto_retriever import (\n    VectorIndexAutoRetriever,\n)\n\n__all__ = [\n    \"VectorIndexAutoRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/vector_store/retrievers/auto_retriever/prompts.py",
    "filename": "prompts.py",
    "relpath": "indices/vector_store/retrievers/auto_retriever/prompts.py",
    "start_line": 1,
    "end_line": 160,
    "length": 160,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Autoretriever prompts.\"\"\"\n\n\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.prompts.prompt_type import PromptType\nfrom llama_index.core.vector_stores.types import (\n    FilterOperator,\n    MetadataFilter,\n    MetadataInfo,\n    VectorStoreInfo,\n    VectorStoreQuerySpec,\n)\n\n# NOTE: these prompts are inspired from langchain's self-query prompt,\n# and adapted to our use case.\n# https://github.com/hwchase17/langchain/tree/main/langchain/chains/query_constructor/prompt.py\n\n\nPREFIX = \"\"\"\\\nYour goal is to structure the user's query to match the request schema provided below.\n\n<< Structured Request Schema >>\nWhen responding use a markdown code snippet with a JSON object formatted in the \\\nfollowing schema:\n\n{schema_str}\n\nThe query string should contain only text that is expected to match the contents of \\\ndocuments. Any conditions in the filter should not be mentioned in the query as well.\n\nMake sure that filters only refer to attributes that exist in the data source.\nMake sure that filters take into account the descriptions of attributes.\nMake sure that filters are only used as needed. If there are no filters that should be \\\napplied return [] for the filter value.\\\n\nIf the user's query explicitly mentions number of documents to retrieve, set top_k to \\\nthat number, otherwise do not set top_k.\n\n\"\"\"\n\nexample_info = VectorStoreInfo(\n    content_info=\"Lyrics of a song\",\n    metadata_info=[\n        MetadataInfo(name=\"artist\", type=\"str\", description=\"Name of the song artist\"),\n        MetadataInfo(\n            name=\"genre\",\n            type=\"str\",\n            description='The song genre, one of \"pop\", \"rock\" or \"rap\"',\n        ),\n    ],\n)\n\nexample_query = \"What are songs by Taylor Swift or Katy Perry in the dance pop genre\"\n\nexample_output = VectorStoreQuerySpec(\n    query=\"teenager love\",\n    filters=[\n        MetadataFilter(key=\"artist\", value=\"Taylor Swift\"),\n        MetadataFilter(key=\"artist\", value=\"Katy Perry\"),\n        MetadataFilter(key=\"genre\", value=\"pop\"),\n    ],\n)\n\nexample_info_2 = VectorStoreInfo(\n    content_info=\"Classic literature\",\n    metadata_info=[\n        MetadataInfo(name=\"author\", type=\"str\", description=\"Author name\"),\n        MetadataInfo(\n            name=\"book_title\",\n            type=\"str\",\n            description=\"Book title\",\n        ),\n        MetadataInfo(\n            name=\"year\",\n            type=\"int\",\n            description=\"Year Published\",\n        ),\n        MetadataInfo(\n            name=\"pages\",\n            type=\"int\",\n            description=\"Number of pages\",\n        ),\n        MetadataInfo(\n            name=\"summary\",\n            type=\"str\",\n            description=\"A short summary of the book\",\n        ),\n    ],\n)\n\nexample_query_2 = \"What are some books by Jane Austen published after 1813 that explore the theme of marriage for social standing?\"\n\nexample_output_2 = VectorStoreQuerySpec(\n    query=\"Books related to theme of marriage for social standing\",\n    filters=[\n        MetadataFilter(key=\"year\", value=\"1813\", operator=FilterOperator.GT),\n        MetadataFilter(key=\"author\", value=\"Jane Austen\"),\n    ],\n)\n\nEXAMPLES = f\"\"\"\\\n<< Example 1. >>\nData Source:\n```json\n{example_info.model_dump_json(indent=4)}\n```\n\nUser Query:\n{example_query}\n\nStructured Request:\n```json\n{example_output.model_dump_json()}\n\n\n<< Example 2. >>\nData Source:\n```json\n{example_info_2.model_dump_json(indent=4)}\n```\n\nUser Query:\n{example_query_2}\n\nStructured Request:\n```json\n{example_output_2.model_dump_json()}\n\n```\n\"\"\".replace(\n    \"{\", \"{{\"\n).replace(\n    \"}\", \"}}\"\n)\n\n\nSUFFIX = \"\"\"\n<< Example 3. >>\nData Source:\n```json\n{info_str}\n```\n\nUser Query:\n{query_str}\n\nStructured Request:\n\"\"\"\n\nDEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL = PREFIX + EXAMPLES + SUFFIX\n\n\n# deprecated, kept for backwards compatibility\n\"\"\"Vector store query prompt.\"\"\"\nVectorStoreQueryPrompt = PromptTemplate\n\nDEFAULT_VECTOR_STORE_QUERY_PROMPT = PromptTemplate(\n    template=DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL,\n    prompt_type=PromptType.VECTOR_STORE_QUERY,\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/query/base.py",
    "filename": "base.py",
    "relpath": "indices/query/base.py",
    "start_line": 1,
    "end_line": 6,
    "length": 6,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# for backwards compatibility\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\n\n__all__ = [\n    \"BaseQueryEngine\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/query/schema.py",
    "filename": "schema.py",
    "relpath": "indices/query/schema.py",
    "start_line": 1,
    "end_line": 4,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# for backwards compatibility\nfrom llama_index.core.schema import QueryBundle, QueryType\n\n__all__ = [\"QueryBundle\", \"QueryType\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/query/embedding_utils.py",
    "filename": "embedding_utils.py",
    "relpath": "indices/query/embedding_utils.py",
    "start_line": 1,
    "end_line": 165,
    "length": 165,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_top_k_embeddings",
      "get_top_k_embeddings_learner",
      "get_top_k_mmr_embeddings"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_top_k_embeddings",
      "get_top_k_embeddings_learner",
      "get_top_k_mmr_embeddings"
    ],
    "document_class_names": [],
    "content": "\"\"\"Embedding utils for queries.\"\"\"\nimport heapq\nimport math\nfrom typing import Any, Callable, List, Optional, Tuple\n\nimport numpy as np\nfrom llama_index.core.base.embeddings.base import similarity as default_similarity_fn\nfrom llama_index.core.vector_stores.types import VectorStoreQueryMode\n\n\ndef get_top_k_embeddings(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_fn: Optional[Callable[..., float]] = None,\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    similarity_cutoff: Optional[float] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top nodes by similarity to the query.\"\"\"\n    if embedding_ids is None:\n        embedding_ids = list(range(len(embeddings)))\n\n    similarity_fn = similarity_fn or default_similarity_fn\n\n    embeddings_np = np.array(embeddings)\n    query_embedding_np = np.array(query_embedding)\n\n    similarity_heap: List[Tuple[float, Any]] = []\n    for i, emb in enumerate(embeddings_np):\n        similarity = similarity_fn(query_embedding_np, emb)  # type: ignore[arg-type]\n        if similarity_cutoff is None or similarity > similarity_cutoff:\n            heapq.heappush(similarity_heap, (similarity, embedding_ids[i]))\n            if similarity_top_k and len(similarity_heap) > similarity_top_k:\n                heapq.heappop(similarity_heap)\n    result_tups = sorted(similarity_heap, key=lambda x: x[0], reverse=True)\n\n    result_similarities = [s for s, _ in result_tups]\n    result_ids = [n for _, n in result_tups]\n\n    return result_similarities, result_ids\n\n\ndef get_top_k_embeddings_learner(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    query_mode: VectorStoreQueryMode = VectorStoreQueryMode.SVM,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top embeddings by fitting a learner against query.\n\n    Inspired by Karpathy's SVM demo:\n    https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb\n\n    Can fit SVM, linear regression, and more.\n\n    \"\"\"\n    try:\n        from sklearn import linear_model, svm\n    except ImportError:\n        raise ImportError(\"Please install scikit-learn to use this feature.\")\n\n    if embedding_ids is None:\n        embedding_ids = list(range(len(embeddings)))\n    query_embedding_np = np.array(query_embedding)\n    embeddings_np = np.array(embeddings)\n    # create dataset\n    dataset_len = len(embeddings) + 1\n    dataset = np.concatenate([query_embedding_np[None, ...], embeddings_np])\n    y = np.zeros(dataset_len)\n    y[0] = 1\n\n    if query_mode == VectorStoreQueryMode.SVM:\n        # train our SVM\n        # TODO: make params configurable\n        clf = svm.LinearSVC(\n            class_weight=\"balanced\", verbose=False, max_iter=10000, tol=1e-6, C=0.1\n        )\n    elif query_mode == VectorStoreQueryMode.LINEAR_REGRESSION:\n        clf = linear_model.LinearRegression()\n    elif query_mode == VectorStoreQueryMode.LOGISTIC_REGRESSION:\n        clf = linear_model.LogisticRegression(class_weight=\"balanced\")\n    else:\n        raise ValueError(f\"Unknown query mode: {query_mode}\")\n\n    clf.fit(dataset, y)  # train\n\n    # infer on whatever data you wish, e.g. the original data\n    similarities = clf.decision_function(dataset[1:])\n    sorted_ix = np.argsort(-similarities)\n    top_sorted_ix = sorted_ix[:similarity_top_k]\n\n    result_similarities = similarities[top_sorted_ix]\n    result_ids = [embedding_ids[ix] for ix in top_sorted_ix]\n\n    return result_similarities, result_ids\n\n\ndef get_top_k_mmr_embeddings(\n    query_embedding: List[float],\n    embeddings: List[List[float]],\n    similarity_fn: Optional[Callable[..., float]] = None,\n    similarity_top_k: Optional[int] = None,\n    embedding_ids: Optional[List] = None,\n    similarity_cutoff: Optional[float] = None,\n    mmr_threshold: Optional[float] = None,\n) -> Tuple[List[float], List]:\n    \"\"\"Get top nodes by similarity to the query,\n    discount by their similarity to previous results.\n\n    A mmr_threshold of 0 will strongly avoid similarity to previous results.\n    A mmr_threshold of 1 will check similarity the query and ignore previous results.\n\n    \"\"\"\n    threshold = mmr_threshold or 0.5\n    similarity_fn = similarity_fn or default_similarity_fn\n\n    if embedding_ids is None or embedding_ids == []:\n        embedding_ids = list(range(len(embeddings)))\n    full_embed_map = dict(zip(embedding_ids, range(len(embedding_ids))))\n    embed_map = full_embed_map.copy()\n    embed_similarity = {}\n    score: float = -math.inf\n    high_score_id = None\n\n    for i, emb in enumerate(embeddings):\n        similarity = similarity_fn(query_embedding, emb)\n        embed_similarity[embedding_ids[i]] = similarity\n        if similarity * threshold > score:\n            high_score_id = embedding_ids[i]\n            score = similarity * threshold\n\n    results: List[Tuple[Any, Any]] = []\n\n    embedding_length = len(embeddings or [])\n    similarity_top_k_count = similarity_top_k or embedding_length\n    while len(results) < min(similarity_top_k_count, embedding_length):\n        # Calculate the similarity score the for the leading one.\n        results.append((score, high_score_id))\n\n        # Reset so a new high scoring result can be found\n        del embed_map[high_score_id]\n        recent_embedding_id = high_score_id\n        score = -math.inf\n\n        # Iterate through results to find high score\n        for embed_id in embed_map:\n            overlap_with_recent = similarity_fn(\n                embeddings[embed_map[embed_id]],\n                embeddings[full_embed_map[recent_embedding_id]],\n            )\n            if (\n                threshold * embed_similarity[embed_id]\n                - ((1 - threshold) * overlap_with_recent)\n                > score\n            ):\n                score = threshold * embed_similarity[embed_id] - (\n                    (1 - threshold) * overlap_with_recent\n                )\n                high_score_id = embed_id\n\n    result_similarities = [s for s, _ in results]\n    result_ids = [n for _, n in results]\n\n    return result_similarities, result_ids"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/base.py",
    "filename": "base.py",
    "relpath": "indices/query/query_transform/base.py",
    "start_line": 1,
    "end_line": 375,
    "length": 375,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompt_modules",
      "_run",
      "run",
      "__call__",
      "_as_query_component",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "BaseQueryTransform",
      "for",
      "IdentityQueryTransform",
      "HyDEQueryTransform",
      "DecomposeQueryTransform",
      "ImageOutputQueryTransform",
      "StepDecomposeQueryTransform",
      "QueryTransformComponent"
    ],
    "document_function_names": [
      "_get_prompt_modules",
      "_run",
      "run",
      "__call__",
      "_as_query_component",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "BaseQueryTransform",
      "for",
      "IdentityQueryTransform",
      "HyDEQueryTransform",
      "DecomposeQueryTransform",
      "ImageOutputQueryTransform",
      "StepDecomposeQueryTransform",
      "QueryTransformComponent"
    ],
    "content": "\"\"\"Query transform.\"\"\"\n\nimport dataclasses\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, cast\n\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict\nfrom llama_index.core.indices.query.query_transform.prompts import (\n    DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT,\n    DEFAULT_IMAGE_OUTPUT_PROMPT,\n    DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT,\n    DecomposeQueryTransformPrompt,\n    ImageOutputQueryTransformPrompt,\n    StepDecomposeQueryTransformPrompt,\n)\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_HYDE_PROMPT\nfrom llama_index.core.prompts.mixin import (\n    PromptDictType,\n    PromptMixin,\n    PromptMixinType,\n)\nfrom llama_index.core.schema import QueryBundle, QueryType\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utils import print_text\n\n\nclass BaseQueryTransform(ChainableMixin, PromptMixin, DispatcherSpanMixin):\n    \"\"\"\n    Base class for query transform.\n\n    A query transform augments a raw query string with associated transformations\n    to improve index querying.\n\n    The query transformation is performed before the query is sent to the index.\n\n    \"\"\"\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        # TODO: keep this for now since response synthesizers don't generally have sub-modules\n        return {}\n\n    @abstractmethod\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n\n    def run(\n        self,\n        query_bundle_or_str: QueryType,\n        metadata: Optional[Dict] = None,\n    ) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        metadata = metadata or {}\n        if isinstance(query_bundle_or_str, str):\n            query_bundle = QueryBundle(\n                query_str=query_bundle_or_str,\n                custom_embedding_strs=[query_bundle_or_str],\n            )\n        else:\n            query_bundle = query_bundle_or_str\n\n        return self._run(query_bundle, metadata=metadata)\n\n    def __call__(\n        self,\n        query_bundle_or_str: QueryType,\n        metadata: Optional[Dict] = None,\n    ) -> QueryBundle:\n        \"\"\"Run query processor.\"\"\"\n        return self.run(query_bundle_or_str, metadata=metadata)\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"As query component.\"\"\"\n        return QueryTransformComponent(query_transform=self)\n\n\nclass IdentityQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Identity query transform.\n\n    Do nothing to the query.\n\n    \"\"\"\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        return query_bundle\n\n\nclass HyDEQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Hypothetical Document Embeddings (HyDE) query transform.\n\n    It uses an LLM to generate hypothetical answer(s) to a given query,\n    and use the resulting documents as embedding strings.\n\n    As described in `[Precise Zero-Shot Dense Retrieval without Relevance Labels]\n    (https://arxiv.org/abs/2212.10496)`\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        hyde_prompt: Optional[BasePromptTemplate] = None,\n        include_original: bool = True,\n    ) -> None:\n        \"\"\"\n        Initialize HyDEQueryTransform.\n\n        Args:\n            llm_predictor (Optional[LLM]): LLM for generating\n                hypothetical documents\n            hyde_prompt (Optional[BasePromptTemplate]): Custom prompt for HyDE\n            include_original (bool): Whether to include original query\n                string as one of the embedding strings\n        \"\"\"\n        super().__init__()\n\n        self._llm = llm or Settings.llm\n        self._hyde_prompt = hyde_prompt or DEFAULT_HYDE_PROMPT\n        self._include_original = include_original\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"hyde_prompt\": self._hyde_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"hyde_prompt\" in prompts:\n            self._hyde_prompt = prompts[\"hyde_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # TODO: support generating multiple hypothetical docs\n        query_str = query_bundle.query_str\n        hypothetical_doc = self._llm.predict(self._hyde_prompt, context_str=query_str)\n        embedding_strs = [hypothetical_doc]\n        if self._include_original:\n            embedding_strs.extend(query_bundle.embedding_strs)\n        return QueryBundle(\n            query_str=query_str,\n            custom_embedding_strs=embedding_strs,\n        )\n\n\nclass DecomposeQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Decompose query transform.\n\n    Decomposes query into a subquery given the current index struct.\n    Performs a single step transformation.\n\n    Args:\n        llm_predictor (Optional[LLM]): LLM for generating\n            hypothetical documents\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        decompose_query_prompt: Optional[DecomposeQueryTransformPrompt] = None,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self._llm = llm or Settings.llm\n        self._decompose_query_prompt: BasePromptTemplate = (\n            decompose_query_prompt or DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT\n        )\n        self.verbose = verbose\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"decompose_query_prompt\": self._decompose_query_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"decompose_query_prompt\" in prompts:\n            self._decompose_query_prompt = prompts[\"decompose_query_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        # currently, just get text from the index structure\n        index_summary = cast(str, metadata.get(\"index_summary\", \"None\"))\n\n        # given the text from the index, we can use the query bundle to generate\n        # a new query bundle\n        query_str = query_bundle.query_str\n        new_query_str = self._llm.predict(\n            self._decompose_query_prompt,\n            query_str=query_str,\n            context_str=index_summary,\n        )\n\n        if self.verbose:\n            print_text(f\"> Current query: {query_str}\\n\", color=\"yellow\")\n            print_text(f\"> New query: {new_query_str}\\n\", color=\"pink\")\n\n        return QueryBundle(\n            query_str=new_query_str,\n            custom_embedding_strs=[new_query_str],\n        )\n\n\nclass ImageOutputQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Image output query transform.\n\n    Adds instructions for formatting image output.\n    By default, this prompts the LLM to format image output as an HTML <img> tag,\n    which can be displayed nicely in jupyter notebook.\n    \"\"\"\n\n    def __init__(\n        self,\n        width: int = 400,\n        query_prompt: Optional[ImageOutputQueryTransformPrompt] = None,\n    ) -> None:\n        \"\"\"\n        Init ImageOutputQueryTransform.\n\n        Args:\n            width (int): desired image display width in pixels\n            query_prompt (ImageOutputQueryTransformPrompt): custom prompt for\n                augmenting query with image output instructions.\n        \"\"\"\n        self._width = width\n        self._query_prompt: BasePromptTemplate = (\n            query_prompt or DEFAULT_IMAGE_OUTPUT_PROMPT\n        )\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"query_prompt\": self._query_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"query_prompt\" in prompts:\n            self._query_prompt = prompts[\"query_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        del metadata  # Unused\n        new_query_str = self._query_prompt.format(\n            query_str=query_bundle.query_str, image_width=self._width\n        )\n        return dataclasses.replace(query_bundle, query_str=new_query_str)\n\n\nclass StepDecomposeQueryTransform(BaseQueryTransform):\n    \"\"\"\n    Step decompose query transform.\n\n    Decomposes query into a subquery given the current index struct\n    and previous reasoning.\n\n    NOTE: doesn't work yet.\n\n    Args:\n        llm_predictor (Optional[LLM]): LLM for generating\n            hypothetical documents\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        step_decompose_query_prompt: Optional[StepDecomposeQueryTransformPrompt] = None,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__()\n        self._llm = llm or Settings.llm\n        self._step_decompose_query_prompt: BasePromptTemplate = (\n            step_decompose_query_prompt or DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT\n        )\n        self.verbose = verbose\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"step_decompose_query_prompt\": self._step_decompose_query_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"step_decompose_query_prompt\" in prompts:\n            self._step_decompose_query_prompt = prompts[\"step_decompose_query_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        \"\"\"Run query transform.\"\"\"\n        index_summary = cast(\n            str,\n            metadata.get(\"index_summary\", \"None\"),\n        )\n        prev_reasoning = cast(Response, metadata.get(\"prev_reasoning\"))\n        fmt_prev_reasoning = f\"\\n{prev_reasoning}\" if prev_reasoning else \"None\"\n\n        # given the text from the index, we can use the query bundle to generate\n        # a new query bundle\n        query_str = query_bundle.query_str\n        new_query_str = self._llm.predict(\n            self._step_decompose_query_prompt,\n            prev_reasoning=fmt_prev_reasoning,\n            query_str=query_str,\n            context_str=index_summary,\n        )\n        if self.verbose:\n            print_text(f\"> Current query: {query_str}\\n\", color=\"yellow\")\n            print_text(f\"> New query: {new_query_str}\\n\", color=\"pink\")\n        return QueryBundle(\n            query_str=new_query_str,\n            custom_embedding_strs=query_bundle.custom_embedding_strs,\n        )\n\n\nclass QueryTransformComponent(QueryComponent):\n    \"\"\"Query transform component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    query_transform: BaseQueryTransform = Field(..., description=\"Query transform.\")\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: not implemented yet\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        if \"query_str\" not in input:\n            raise ValueError(\"Input must have key 'query_str'\")\n        input[\"query_str\"] = validate_and_convert_stringable(input[\"query_str\"])\n\n        input[\"metadata\"] = input.get(\"metadata\", {})\n\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = self.query_transform.run(\n            kwargs[\"query_str\"],\n            metadata=kwargs[\"metadata\"],\n        )\n        return {\"query_str\": output.query_str}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # TODO: true async not implemented yet\n        return self._run_component(**kwargs)\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"query_str\"}, optional_keys={\"metadata\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"query_str\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/query/query_transform/__init__.py",
    "start_line": 1,
    "end_line": 13,
    "length": 13,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Query Transforms.\"\"\"\n\nfrom llama_index.core.indices.query.query_transform.base import (\n    DecomposeQueryTransform,\n    HyDEQueryTransform,\n    StepDecomposeQueryTransform,\n)\n\n__all__ = [\n    \"HyDEQueryTransform\",\n    \"DecomposeQueryTransform\",\n    \"StepDecomposeQueryTransform\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/feedback_transform.py",
    "filename": "feedback_transform.py",
    "relpath": "indices/query/query_transform/feedback_transform.py",
    "start_line": 1,
    "end_line": 116,
    "length": 116,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "_construct_feedback",
      "_resynthesize_query"
    ],
    "chunk_class_names": [
      "FeedbackQueryTransformation"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_run",
      "_construct_feedback",
      "_resynthesize_query"
    ],
    "document_class_names": [
      "FeedbackQueryTransformation"
    ],
    "content": "import logging\nfrom typing import Dict, Optional\n\nfrom llama_index.core.evaluation.base import Evaluation\nfrom llama_index.core.indices.query.query_transform.base import BaseQueryTransform\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_RESYNTHESIS_PROMPT_TMPL = (\n    \"Here is the original query:\\n\"\n    \"{query_str}\\n\"\n    \"Here is the response given:\\n\"\n    \"{response}\\n\"\n    \"Here is some feedback from evaluator about the response given.\\n\"\n    \"{feedback}\\n\"\n    \"If you want to resynthesize the query, please return the modified query below.\\n\"\n    \"Otherwise, please return the original query.\\n\"\n)\n\nDEFAULT_RESYNTHESIS_PROMPT = PromptTemplate(DEFAULT_RESYNTHESIS_PROMPT_TMPL)\n\n\nclass FeedbackQueryTransformation(BaseQueryTransform):\n    \"\"\"Transform the query given the evaluation feedback.\n\n    Args:\n        eval(Evaluation): An evaluation object.\n        llm(LLM): An LLM.\n        resynthesize_query(bool): Whether to resynthesize the query.\n        resynthesis_prompt(BasePromptTemplate): A prompt for resynthesizing the query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        resynthesize_query: bool = False,\n        resynthesis_prompt: Optional[BasePromptTemplate] = None,\n    ) -> None:\n        super().__init__()\n        self.llm = llm or Settings.llm\n        self.should_resynthesize_query = resynthesize_query\n        self.resynthesis_prompt = resynthesis_prompt or DEFAULT_RESYNTHESIS_PROMPT\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"resynthesis_prompt\": self.resynthesis_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"resynthesis_prompt\" in prompts:\n            self.resynthesis_prompt = prompts[\"resynthesis_prompt\"]\n\n    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n        orig_query_str = query_bundle.query_str\n        if metadata.get(\"evaluation\") and isinstance(\n            metadata.get(\"evaluation\"), Evaluation\n        ):\n            self.evaluation = metadata.get(\"evaluation\")\n        if self.evaluation is None or not isinstance(self.evaluation, Evaluation):\n            raise ValueError(\"Evaluation is not set.\")\n        if self.evaluation.response is None or self.evaluation.feedback is None:\n            raise ValueError(\"Evaluation result must contain response and feedback.\")\n\n        if self.evaluation.feedback == \"YES\" or self.evaluation.feedback == \"NO\":\n            new_query = (\n                orig_query_str\n                + \"\\n----------------\\n\"\n                + self._construct_feedback(response=self.evaluation.response)\n            )\n        else:\n            if self.should_resynthesize_query:\n                new_query_str = self._resynthesize_query(\n                    orig_query_str, self.evaluation.response, self.evaluation.feedback\n                )\n            else:\n                new_query_str = orig_query_str\n            new_query = (\n                self._construct_feedback(response=self.evaluation.response)\n                + \"\\n\"\n                + \"Here is some feedback from the evaluator about the response given.\\n\"\n                + self.evaluation.feedback\n                + \"\\n\"\n                + \"Now answer the question.\\n\"\n                + new_query_str\n            )\n        return QueryBundle(new_query, custom_embedding_strs=[orig_query_str])\n\n    @staticmethod\n    def _construct_feedback(response: Optional[str]) -> str:\n        \"\"\"Construct feedback from response.\"\"\"\n        if response is None:\n            return \"\"\n        else:\n            return \"Here is a previous bad answer.\\n\" + response\n\n    def _resynthesize_query(\n        self, query_str: str, response: str, feedback: Optional[str]\n    ) -> str:\n        \"\"\"Resynthesize query given feedback.\"\"\"\n        if feedback is None:\n            return query_str\n        else:\n            new_query_str = self.llm.predict(\n                self.resynthesis_prompt,\n                query_str=query_str,\n                response=response,\n                feedback=feedback,\n            )\n            logger.debug(\"Resynthesized query: %s\", new_query_str)\n            return new_query_str"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/query/query_transform/prompts.py",
    "filename": "prompts.py",
    "relpath": "indices/query/query_transform/prompts.py",
    "start_line": 1,
    "end_line": 130,
    "length": 130,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Query transform prompts.\"\"\"\n\n\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.prompts.prompt_type import PromptType\n\n# deprecated, kept for backwards compatibility\n\"\"\"Decompose prompt for query transformation.\n\nPromptTemplate to \"decompose\" a query into another query\ngiven the existing context.\n\nRequired template variables: `context_str`, `query_str`\n\"\"\"\nDecomposeQueryTransformPrompt = PromptTemplate\n\n\"\"\"Step Decompose prompt for query transformation.\n\nPromptTemplate to \"decompose\" a query into another query\ngiven the existing context + previous reasoning (the previous steps).\n\nRequired template variables: `context_str`, `query_str`, `prev_reasoning`\n\"\"\"\nStepDecomposeQueryTransformPrompt = PromptTemplate\n\n\"\"\"Image output prompt for query transformation.\n\nPromptTemplate to add instructions for formatting image output.\n\nRequired template variables: `query_str`, `image_width`\n\"\"\"\nImageOutputQueryTransformPrompt = PromptTemplate\n\n\nDEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"\n    \"Context information for the knowledge source is provided below. \\n\"\n    \"Given the context, return a new question that can be answered from \"\n    \"the context. The question can be the same as the original question, \"\n    \"or a new question that represents a subcomponent of the overall question.\\n\"\n    \"As an example: \"\n    \"\\n\\n\"\n    \"Question: How many Grand Slam titles does the winner of the 2020 Australian \"\n    \"Open have?\\n\"\n    \"Knowledge source context: Provides information about the winners of the 2020 \"\n    \"Australian Open\\n\"\n    \"New question: Who was the winner of the 2020 Australian Open? \"\n    \"\\n\\n\"\n    \"Question: What is the current population of the city in which Paul Graham found \"\n    \"his first company, Viaweb?\\n\"\n    \"Knowledge source context: Provides information about Paul Graham's \"\n    \"professional career, including the startups he's founded. \"\n    \"New question: In which city did Paul Graham found his first company, Viaweb? \"\n    \"\\n\\n\"\n    \"Question: {query_str}\\n\"\n    \"Knowledge source context: {context_str}\\n\"\n    \"New question: \"\n)\n\nDEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT = PromptTemplate(\n    DEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL, prompt_type=PromptType.DECOMPOSE\n)\n\n\nDEFAULT_IMAGE_OUTPUT_TMPL = (\n    \"{query_str}\"\n    \"Show any image with a HTML <img/> tag with {image_width}.\"\n    'e.g., <image src=\"data/img.jpg\" width=\"{image_width}\" />.'\n)\n\nDEFAULT_IMAGE_OUTPUT_PROMPT = PromptTemplate(DEFAULT_IMAGE_OUTPUT_TMPL)\n\n\nDEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL = (\n    \"The original question is as follows: {query_str}\\n\"\n    \"We have an opportunity to answer some, or all of the question from a \"\n    \"knowledge source. \"\n    \"Context information for the knowledge source is provided below, as \"\n    \"well as previous reasoning steps.\\n\"\n    \"Given the context and previous reasoning, return a question that can \"\n    \"be answered from \"\n    \"the context. This question can be the same as the original question, \"\n    \"or this question can represent a subcomponent of the overall question.\"\n    \"It should not be irrelevant to the original question.\\n\"\n    \"If we cannot extract more information from the context, provide 'None' \"\n    \"as the answer. \"\n    \"Some examples are given below: \"\n    \"\\n\\n\"\n    \"Question: How many Grand Slam titles does the winner of the 2020 Australian \"\n    \"Open have?\\n\"\n    \"Knowledge source context: Provides names of the winners of the 2020 \"\n    \"Australian Open\\n\"\n    \"Previous reasoning: None\\n\"\n    \"Next question: Who was the winner of the 2020 Australian Open? \"\n    \"\\n\\n\"\n    \"Question: Who was the winner of the 2020 Australian Open?\\n\"\n    \"Knowledge source context: Provides names of the winners of the 2020 \"\n    \"Australian Open\\n\"\n    \"Previous reasoning: None.\\n\"\n    \"New question: Who was the winner of the 2020 Australian Open? \"\n    \"\\n\\n\"\n    \"Question: How many Grand Slam titles does the winner of the 2020 Australian \"\n    \"Open have?\\n\"\n    \"Knowledge source context: Provides information about the winners of the 2020 \"\n    \"Australian Open\\n\"\n    \"Previous reasoning:\\n\"\n    \"- Who was the winner of the 2020 Australian Open? \\n\"\n    \"- The winner of the 2020 Australian Open was Novak Djokovic.\\n\"\n    \"New question: None\"\n    \"\\n\\n\"\n    \"Question: How many Grand Slam titles does the winner of the 2020 Australian \"\n    \"Open have?\\n\"\n    \"Knowledge source context: Provides information about the winners of the 2020 \"\n    \"Australian Open - includes biographical information for each winner\\n\"\n    \"Previous reasoning:\\n\"\n    \"- Who was the winner of the 2020 Australian Open? \\n\"\n    \"- The winner of the 2020 Australian Open was Novak Djokovic.\\n\"\n    \"New question: How many Grand Slam titles does Novak Djokovic have? \"\n    \"\\n\\n\"\n    \"Question: {query_str}\\n\"\n    \"Knowledge source context: {context_str}\\n\"\n    \"Previous reasoning: {prev_reasoning}\\n\"\n    \"New question: \"\n)\n\nDEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT = PromptTemplate(\n    DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/document_summary/base.py",
    "filename": "base.py",
    "relpath": "indices/document_summary/base.py",
    "start_line": 1,
    "end_line": 311,
    "length": 311,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "vector_store",
      "as_retriever",
      "get_document_summary",
      "_add_nodes_to_index",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "delete_nodes",
      "delete_ref_doc",
      "ref_doc_info"
    ],
    "chunk_class_names": [
      "DocumentSummaryRetrieverMode",
      "DocumentSummaryIndex"
    ],
    "document_function_names": [
      "__init__",
      "vector_store",
      "as_retriever",
      "get_document_summary",
      "_add_nodes_to_index",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "delete_nodes",
      "delete_ref_doc",
      "ref_doc_info"
    ],
    "document_class_names": [
      "DocumentSummaryRetrieverMode",
      "DocumentSummaryIndex"
    ],
    "content": "\"\"\"Document summary index.\n\nA data structure where LlamaIndex stores the summary per document, and maps\nthe summary to the underlying Nodes.\nThis summary can be used for retrieval.\n\n\"\"\"\n\nimport logging\nfrom collections import defaultdict\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Sequence, Union, cast\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.data_structs.document_summary import IndexDocumentSummary\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.utils import embed_nodes\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    ResponseMode,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    IndexNode,\n    NodeRelationship,\n    NodeWithScore,\n    RelatedNodeInfo,\n    TextNode,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore.types import RefDocInfo\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.utils import get_tqdm_iterable\nfrom llama_index.core.vector_stores.types import BasePydanticVectorStore\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_SUMMARY_QUERY = (\n    \"Describe what the provided text is about. \"\n    \"Also describe some of the questions that this text can answer. \"\n)\n\n\nclass DocumentSummaryRetrieverMode(str, Enum):\n    EMBEDDING = \"embedding\"\n    LLM = \"llm\"\n\n\n_RetrieverMode = DocumentSummaryRetrieverMode\n\n\nclass DocumentSummaryIndex(BaseIndex[IndexDocumentSummary]):\n    \"\"\"Document Summary Index.\n\n    Args:\n        response_synthesizer (BaseSynthesizer): A response synthesizer for generating\n            summaries.\n        summary_query (str): The query to use to generate the summary for each document.\n        show_progress (bool): Whether to show tqdm progress bars.\n            Defaults to False.\n        embed_summaries (bool): Whether to embed the summaries.\n            This is required for running the default embedding-based retriever.\n            Defaults to True.\n\n    \"\"\"\n\n    index_struct_cls = IndexDocumentSummary\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        objects: Optional[Sequence[IndexNode]] = None,\n        index_struct: Optional[IndexDocumentSummary] = None,\n        llm: Optional[LLM] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        storage_context: Optional[StorageContext] = None,\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        summary_query: str = DEFAULT_SUMMARY_QUERY,\n        show_progress: bool = False,\n        embed_summaries: bool = True,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or Settings.llm\n        self._embed_model = embed_model or Settings.embed_model\n        self._response_synthesizer = response_synthesizer or get_response_synthesizer(\n            llm=self._llm, response_mode=ResponseMode.TREE_SUMMARIZE\n        )\n        self._summary_query = summary_query\n        self._embed_summaries = embed_summaries\n\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            storage_context=storage_context,\n            show_progress=show_progress,\n            objects=objects,\n            **kwargs,\n        )\n\n    @property\n    def vector_store(self) -> BasePydanticVectorStore:\n        return self._vector_store\n\n    def as_retriever(\n        self,\n        retriever_mode: Union[str, _RetrieverMode] = _RetrieverMode.EMBEDDING,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        \"\"\"Get retriever.\n\n        Args:\n            retriever_mode (Union[str, DocumentSummaryRetrieverMode]): A retriever mode.\n                Defaults to DocumentSummaryRetrieverMode.EMBEDDING.\n\n        \"\"\"\n        from llama_index.core.indices.document_summary.retrievers import (\n            DocumentSummaryIndexEmbeddingRetriever,\n            DocumentSummaryIndexLLMRetriever,\n        )\n\n        LLMRetriever = DocumentSummaryIndexLLMRetriever\n        EmbeddingRetriever = DocumentSummaryIndexEmbeddingRetriever\n\n        if retriever_mode == _RetrieverMode.EMBEDDING:\n            if not self._embed_summaries:\n                raise ValueError(\n                    \"Cannot use embedding retriever if embed_summaries is False\"\n                )\n\n            return EmbeddingRetriever(\n                self,\n                object_map=self._object_map,\n                embed_model=self._embed_model,\n                **kwargs,\n            )\n        if retriever_mode == _RetrieverMode.LLM:\n            return LLMRetriever(\n                self, object_map=self._object_map, llm=self._llm, **kwargs\n            )\n        else:\n            raise ValueError(f\"Unknown retriever mode: {retriever_mode}\")\n\n    def get_document_summary(self, doc_id: str) -> str:\n        \"\"\"Get document summary by doc id.\n\n        Args:\n            doc_id (str): A document id.\n\n        \"\"\"\n        if doc_id not in self._index_struct.doc_id_to_summary_id:\n            raise ValueError(f\"doc_id {doc_id} not in index\")\n        summary_id = self._index_struct.doc_id_to_summary_id[doc_id]\n        return self.docstore.get_node(summary_id).get_content()\n\n    def _add_nodes_to_index(\n        self,\n        index_struct: IndexDocumentSummary,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n    ) -> None:\n        \"\"\"Add nodes to index.\"\"\"\n        doc_id_to_nodes = defaultdict(list)\n        for node in nodes:\n            if node.ref_doc_id is None:\n                raise ValueError(\n                    \"ref_doc_id of node cannot be None when building a document \"\n                    \"summary index\"\n                )\n            doc_id_to_nodes[node.ref_doc_id].append(node)\n\n        summary_node_dict = {}\n        items = doc_id_to_nodes.items()\n        iterable_with_progress = get_tqdm_iterable(\n            items, show_progress, \"Summarizing documents\"\n        )\n\n        for doc_id, nodes in iterable_with_progress:\n            print(f\"current doc id: {doc_id}\")\n            nodes_with_scores = [NodeWithScore(node=n) for n in nodes]\n            # get the summary for each doc_id\n            summary_response = self._response_synthesizer.synthesize(\n                query=self._summary_query,\n                nodes=nodes_with_scores,\n            )\n            summary_response = cast(Response, summary_response)\n            docid_first_node = doc_id_to_nodes.get(doc_id, [TextNode()])[0]\n            summary_node_dict[doc_id] = TextNode(\n                text=summary_response.response,\n                relationships={\n                    NodeRelationship.SOURCE: RelatedNodeInfo(node_id=doc_id)\n                },\n                metadata=docid_first_node.metadata,\n                excluded_embed_metadata_keys=docid_first_node.excluded_embed_metadata_keys,\n                excluded_llm_metadata_keys=docid_first_node.excluded_llm_metadata_keys,\n            )\n            self.docstore.add_documents([summary_node_dict[doc_id]])\n            logger.info(\n                f\"> Generated summary for doc {doc_id}: \" f\"{summary_response.response}\"\n            )\n\n        for doc_id, nodes in doc_id_to_nodes.items():\n            index_struct.add_summary_and_nodes(summary_node_dict[doc_id], nodes)\n\n        if self._embed_summaries:\n            summary_nodes = list(summary_node_dict.values())\n            id_to_embed_map = embed_nodes(\n                summary_nodes, self._embed_model, show_progress=show_progress\n            )\n\n            summary_nodes_with_embedding = []\n            for node in summary_nodes:\n                node_with_embedding = node.model_copy()\n                node_with_embedding.embedding = id_to_embed_map[node.node_id]\n                summary_nodes_with_embedding.append(node_with_embedding)\n            self._vector_store.add(summary_nodes_with_embedding)\n\n    def _build_index_from_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        **build_kwargs: Any,\n    ) -> IndexDocumentSummary:\n        \"\"\"Build index from nodes.\"\"\"\n        # first get doc_id to nodes_dict, generate a summary for each doc_id,\n        # then build the index struct\n        index_struct = IndexDocumentSummary()\n        self._add_nodes_to_index(index_struct, nodes, self._show_progress)\n        return index_struct\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        self._add_nodes_to_index(self._index_struct, nodes)\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        pass\n\n    def delete_nodes(\n        self,\n        node_ids: List[str],\n        delete_from_docstore: bool = False,\n        **delete_kwargs: Any,\n    ) -> None:\n        \"\"\"Delete a list of nodes from the index.\n\n        Args:\n            node_ids (List[str]): A list of node_ids from the nodes to delete\n\n        \"\"\"\n        index_nodes = self._index_struct.node_id_to_summary_id.keys()\n        for node in node_ids:\n            if node not in index_nodes:\n                logger.warning(f\"node_id {node} not found, will not be deleted.\")\n                node_ids.remove(node)\n\n        self._index_struct.delete_nodes(node_ids)\n\n        remove_summary_ids = [\n            summary_id\n            for summary_id in self._index_struct.summary_id_to_node_ids\n            if len(self._index_struct.summary_id_to_node_ids[summary_id]) == 0\n        ]\n\n        remove_docs = [\n            doc_id\n            for doc_id in self._index_struct.doc_id_to_summary_id\n            if self._index_struct.doc_id_to_summary_id[doc_id] in remove_summary_ids\n        ]\n\n        for doc_id in remove_docs:\n            self.delete_ref_doc(doc_id)\n\n    def delete_ref_doc(\n        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any\n    ) -> None:\n        \"\"\"Delete a document from the index.\n        All nodes in the index related to the document will be deleted.\n        \"\"\"\n        ref_doc_info = self.docstore.get_ref_doc_info(ref_doc_id)\n        if ref_doc_info is None:\n            logger.warning(f\"ref_doc_id {ref_doc_id} not found, nothing deleted.\")\n            return\n        self._index_struct.delete(ref_doc_id)\n        self._vector_store.delete(ref_doc_id)\n\n        if delete_from_docstore:\n            self.docstore.delete_ref_doc(ref_doc_id, raise_error=False)\n\n        self._storage_context.index_store.add_index_struct(self._index_struct)\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        ref_doc_ids = list(self._index_struct.doc_id_to_summary_id.keys())\n\n        all_ref_doc_info = {}\n        for ref_doc_id in ref_doc_ids:\n            ref_doc_info = self.docstore.get_ref_doc_info(ref_doc_id)\n            if not ref_doc_info:\n                continue\n\n            all_ref_doc_info[ref_doc_id] = ref_doc_info\n        return all_ref_doc_info\n\n\n# legacy\nGPTDocumentSummaryIndex = DocumentSummaryIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/document_summary/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/document_summary/__init__.py",
    "start_line": 1,
    "end_line": 21,
    "length": 21,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Document summary index.\"\"\"\n\n\nfrom llama_index.core.indices.document_summary.base import (\n    DocumentSummaryIndex,\n    GPTDocumentSummaryIndex,\n)\nfrom llama_index.core.indices.document_summary.retrievers import (\n    DocumentSummaryIndexEmbeddingRetriever,\n    DocumentSummaryIndexLLMRetriever,\n    DocumentSummaryIndexRetriever,\n)\n\n__all__ = [\n    \"DocumentSummaryIndex\",\n    \"DocumentSummaryIndexLLMRetriever\",\n    \"DocumentSummaryIndexEmbeddingRetriever\",\n    # legacy\n    \"GPTDocumentSummaryIndex\",\n    \"DocumentSummaryIndexRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/document_summary/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/document_summary/retrievers.py",
    "start_line": 1,
    "end_line": 192,
    "length": 192,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_retrieve",
      "__init__",
      "_retrieve"
    ],
    "chunk_class_names": [
      "DocumentSummaryIndexLLMRetriever",
      "DocumentSummaryIndexEmbeddingRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_retrieve",
      "__init__",
      "_retrieve"
    ],
    "document_class_names": [
      "DocumentSummaryIndexLLMRetriever",
      "DocumentSummaryIndexEmbeddingRetriever"
    ],
    "content": "\"\"\"Document summary retrievers.\n\nThis module contains retrievers for document summary indices.\n\n\"\"\"\n\nimport logging\nfrom typing import Any, Callable, List, Optional\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.document_summary.base import DocumentSummaryIndex\nfrom llama_index.core.indices.utils import (\n    default_format_node_batch_fn,\n    default_parse_choice_select_answer_fn,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_CHOICE_SELECT_PROMPT\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.vector_stores.types import VectorStoreQuery\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocumentSummaryIndexLLMRetriever(BaseRetriever):\n    \"\"\"Document Summary Index LLM Retriever.\n\n    By default, select relevant summaries from index using LLM calls.\n\n    Args:\n        index (DocumentSummaryIndex): The index to retrieve from.\n        choice_select_prompt (Optional[BasePromptTemplate]): The prompt to use for selecting relevant summaries.\n        choice_batch_size (int): The number of summary nodes to send to LLM at a time.\n        choice_top_k (int): The number of summary nodes to retrieve.\n        format_node_batch_fn (Callable): Function to format a batch of nodes for LLM.\n        parse_choice_select_answer_fn (Callable): Function to parse LLM response.\n        llm (LLM): The llm to use.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: DocumentSummaryIndex,\n        choice_select_prompt: Optional[BasePromptTemplate] = None,\n        choice_batch_size: int = 10,\n        choice_top_k: int = 1,\n        format_node_batch_fn: Optional[Callable] = None,\n        parse_choice_select_answer_fn: Optional[Callable] = None,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        self._index = index\n        self._choice_select_prompt = (\n            choice_select_prompt or DEFAULT_CHOICE_SELECT_PROMPT\n        )\n        self._choice_batch_size = choice_batch_size\n        self._choice_top_k = choice_top_k\n        self._format_node_batch_fn = (\n            format_node_batch_fn or default_format_node_batch_fn\n        )\n        self._parse_choice_select_answer_fn = (\n            parse_choice_select_answer_fn or default_parse_choice_select_answer_fn\n        )\n\n        self._llm = llm or Settings.llm\n\n        super().__init__(\n            callback_manager=callback_manager or Settings.callback_manager,\n            object_map=object_map,\n            verbose=verbose,\n        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        summary_ids = self._index.index_struct.summary_ids\n\n        all_summary_ids: List[str] = []\n        all_relevances: List[float] = []\n        for idx in range(0, len(summary_ids), self._choice_batch_size):\n            summary_ids_batch = summary_ids[idx : idx + self._choice_batch_size]\n            summary_nodes = self._index.docstore.get_nodes(summary_ids_batch)\n            query_str = query_bundle.query_str\n            fmt_batch_str = self._format_node_batch_fn(summary_nodes)\n            # call each batch independently\n            raw_response = self._llm.predict(\n                self._choice_select_prompt,\n                context_str=fmt_batch_str,\n                query_str=query_str,\n            )\n            raw_choices, relevances = self._parse_choice_select_answer_fn(\n                raw_response, len(summary_nodes)\n            )\n            choice_idxs = [choice - 1 for choice in raw_choices]\n\n            choice_summary_ids = [summary_ids_batch[ci] for ci in choice_idxs]\n\n            all_summary_ids.extend(choice_summary_ids)\n            all_relevances.extend(relevances)\n\n        zipped_list = list(zip(all_summary_ids, all_relevances))\n        sorted_list = sorted(zipped_list, key=lambda x: x[1], reverse=True)\n        top_k_list = sorted_list[: self._choice_top_k]\n\n        results = []\n        for summary_id, relevance in top_k_list:\n            node_ids = self._index.index_struct.summary_id_to_node_ids[summary_id]\n            nodes = self._index.docstore.get_nodes(node_ids)\n            results.extend([NodeWithScore(node=n, score=relevance) for n in nodes])\n\n        return results\n\n\nclass DocumentSummaryIndexEmbeddingRetriever(BaseRetriever):\n    \"\"\"Document Summary Index Embedding Retriever.\n\n    Args:\n        index (DocumentSummaryIndex): The index to retrieve from.\n        similarity_top_k (int): The number of summary nodes to retrieve.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: DocumentSummaryIndex,\n        similarity_top_k: int = 1,\n        embed_model: Optional[BaseEmbedding] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._index = index\n        self._vector_store = self._index.vector_store\n        self._embed_model = embed_model or Settings.embed_model\n        self._docstore = self._index.docstore\n        self._index_struct = self._index.index_struct\n        self._similarity_top_k = similarity_top_k\n        super().__init__(\n            callback_manager=callback_manager or Settings.callback_manager,\n            object_map=object_map,\n            verbose=verbose,\n        )\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes.\"\"\"\n        if self._vector_store.is_embedding_query:\n            if query_bundle.embedding is None:\n                query_bundle.embedding = (\n                    self._embed_model.get_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )\n                )\n\n        query = VectorStoreQuery(\n            query_embedding=query_bundle.embedding,\n            similarity_top_k=self._similarity_top_k,\n        )\n        query_result = self._vector_store.query(query)\n\n        top_k_summary_ids: List[str]\n        if query_result.ids is not None:\n            top_k_summary_ids = query_result.ids\n        elif query_result.nodes is not None:\n            top_k_summary_ids = [n.node_id for n in query_result.nodes]\n        else:\n            raise ValueError(\n                \"Vector store query result should return \"\n                \"at least one of nodes or ids.\"\n            )\n\n        results = []\n        for summary_id in top_k_summary_ids:\n            node_ids = self._index_struct.summary_id_to_node_ids[summary_id]\n            nodes = self._docstore.get_nodes(node_ids)\n            results.extend([NodeWithScore(node=n) for n in nodes])\n        return results\n\n\n# legacy, backward compatibility\nDocumentSummaryIndexRetriever = DocumentSummaryIndexLLMRetriever"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/empty/base.py",
    "filename": "base.py",
    "relpath": "indices/empty/base.py",
    "start_line": 1,
    "end_line": 91,
    "length": 91,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "as_retriever",
      "as_query_engine",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info"
    ],
    "chunk_class_names": [
      "EmptyIndex"
    ],
    "document_function_names": [
      "__init__",
      "as_retriever",
      "as_query_engine",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info"
    ],
    "document_class_names": [
      "EmptyIndex"
    ],
    "content": "\"\"\"Empty index.\n\nAn index that doesn't contain any documents. Can only be used for\npure LLM calls.\n\n\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.data_structs.data_structs import EmptyIndexStruct\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.llms.utils import LLMType\nfrom llama_index.core.schema import BaseNode\nfrom llama_index.core.storage.docstore.types import RefDocInfo\n\n\nclass EmptyIndex(BaseIndex[EmptyIndexStruct]):\n    \"\"\"Empty Index.\n\n    An index that doesn't contain any documents. Used for\n    pure LLM calls.\n    NOTE: this exists because an empty index it allows certain properties,\n    such as the ability to be composed with other indices + token\n    counting + others.\n\n    \"\"\"\n\n    index_struct_cls = EmptyIndexStruct\n\n    def __init__(\n        self,\n        index_struct: Optional[EmptyIndexStruct] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            nodes=None,\n            index_struct=index_struct or EmptyIndexStruct(),\n            **kwargs,\n        )\n\n    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\n        # NOTE: lazy import\n        from llama_index.core.indices.empty.retrievers import EmptyIndexRetriever\n\n        return EmptyIndexRetriever(self)\n\n    def as_query_engine(\n        self, llm: Optional[LLMType] = None, **kwargs: Any\n    ) -> BaseQueryEngine:\n        if \"response_mode\" not in kwargs:\n            kwargs[\"response_mode\"] = \"generation\"\n        else:\n            if kwargs[\"response_mode\"] != \"generation\":\n                raise ValueError(\"EmptyIndex only supports response_mode=generation.\")\n\n        return super().as_query_engine(llm=llm, **kwargs)\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> EmptyIndexStruct:\n        \"\"\"Build the index from documents.\n\n        Args:\n            documents (List[BaseDocument]): A list of documents.\n\n        Returns:\n            IndexList: The created summary index.\n        \"\"\"\n        del nodes  # Unused\n        return EmptyIndexStruct()\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        del nodes  # Unused\n        raise NotImplementedError(\"Cannot insert into an empty index.\")\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        raise NotImplementedError(\"Cannot delete from an empty index.\")\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        raise NotImplementedError(\"ref_doc_info not supported for an empty index.\")\n\n\n# legacy\nGPTEmptyIndex = EmptyIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/empty/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/empty/__init__.py",
    "start_line": 1,
    "end_line": 6,
    "length": 6,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Empty Index.\"\"\"\n\nfrom llama_index.core.indices.empty.base import EmptyIndex, GPTEmptyIndex\nfrom llama_index.core.indices.empty.retrievers import EmptyIndexRetriever\n\n__all__ = [\"EmptyIndex\", \"EmptyIndexRetriever\", \"GPTEmptyIndex\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/empty/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/empty/retrievers.py",
    "start_line": 1,
    "end_line": 38,
    "length": 38,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_retrieve"
    ],
    "chunk_class_names": [
      "EmptyIndexRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_retrieve"
    ],
    "document_class_names": [
      "EmptyIndexRetriever"
    ],
    "content": "\"\"\"Default query for EmptyIndex.\"\"\"\nfrom typing import Any, List, Optional\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.empty.base import EmptyIndex\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_SIMPLE_INPUT_PROMPT\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\n\n\nclass EmptyIndexRetriever(BaseRetriever):\n    \"\"\"EmptyIndex query.\n\n    Passes the raw LLM call to the underlying LLM model.\n\n    Args:\n        input_prompt (Optional[BasePromptTemplate]): A Simple Input Prompt\n            (see :ref:`Prompt-Templates`).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: EmptyIndex,\n        input_prompt: Optional[BasePromptTemplate] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._index = index\n        self._input_prompt = input_prompt or DEFAULT_SIMPLE_INPUT_PROMPT\n        super().__init__(callback_manager)\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve relevant nodes.\"\"\"\n        del query_bundle  # Unused\n        return []"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/keyword_table/base.py",
    "filename": "base.py",
    "relpath": "indices/keyword_table/base.py",
    "start_line": 1,
    "end_line": 252,
    "length": 252,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "as_retriever",
      "_extract_keywords",
      "_async_extract_keywords",
      "_add_nodes_to_index",
      "_async_add_nodes_to_index",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info",
      "_extract_keywords",
      "_async_extract_keywords"
    ],
    "chunk_class_names": [
      "KeywordTableRetrieverMode",
      "BaseKeywordTableIndex",
      "KeywordTableIndex"
    ],
    "document_function_names": [
      "__init__",
      "as_retriever",
      "_extract_keywords",
      "_async_extract_keywords",
      "_add_nodes_to_index",
      "_async_add_nodes_to_index",
      "_build_index_from_nodes",
      "_insert",
      "_delete_node",
      "ref_doc_info",
      "_extract_keywords",
      "_async_extract_keywords"
    ],
    "document_class_names": [
      "KeywordTableRetrieverMode",
      "BaseKeywordTableIndex",
      "KeywordTableIndex"
    ],
    "content": "\"\"\"Keyword-table based index.\n\nSimilar to a \"hash table\" in concept. LlamaIndex first tries\nto extract keywords from the source text, and stores the\nkeywords as keys per item. It similarly extracts keywords\nfrom the query text. Then, it tries to match those keywords to\nexisting keywords in the table.\n\n\"\"\"\n\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Any, Dict, Optional, Sequence, Set, Union\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.data_structs.data_structs import KeywordTable\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.keyword_table.utils import (\n    extract_keywords_given_response,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom llama_index.core.schema import BaseNode, IndexNode, MetadataMode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore.types import RefDocInfo\nfrom llama_index.core.utils import get_tqdm_iterable\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass KeywordTableRetrieverMode(str, Enum):\n    DEFAULT = \"default\"\n    SIMPLE = \"simple\"\n    RAKE = \"rake\"\n\n\nclass BaseKeywordTableIndex(BaseIndex[KeywordTable]):\n    \"\"\"Base Keyword Table Index.\n\n    This index extracts keywords from the text, and maps each\n    keyword to the node(s) that it corresponds to. In this sense it mimics a\n    \"hash table\". During index construction, the keyword table is constructed\n    by extracting keywords from each node and creating an internal mapping.\n\n    During query time, the keywords are extracted from the query text, and these\n    keywords are used to index into the keyword table. The retrieved nodes\n    are then used to answer the query.\n\n    Args:\n        keyword_extract_template (Optional[BasePromptTemplate]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n        use_async (bool): Whether to use asynchronous calls. Defaults to False.\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n\n    \"\"\"\n\n    index_struct_cls = KeywordTable\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        objects: Optional[Sequence[IndexNode]] = None,\n        index_struct: Optional[KeywordTable] = None,\n        llm: Optional[LLM] = None,\n        keyword_extract_template: Optional[BasePromptTemplate] = None,\n        max_keywords_per_chunk: int = 10,\n        use_async: bool = False,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # need to set parameters before building index in base class.\n        self._llm = llm or Settings.llm\n\n        self.max_keywords_per_chunk = max_keywords_per_chunk\n        self.keyword_extract_template = (\n            keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE\n        )\n        # NOTE: Partially format keyword extract template here.\n        self.keyword_extract_template = self.keyword_extract_template.partial_format(\n            max_keywords=self.max_keywords_per_chunk\n        )\n        self._use_async = use_async\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            show_progress=show_progress,\n            objects=objects,\n            **kwargs,\n        )\n\n    def as_retriever(\n        self,\n        retriever_mode: Union[\n            str, KeywordTableRetrieverMode\n        ] = KeywordTableRetrieverMode.DEFAULT,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        # NOTE: lazy import\n        from llama_index.core.indices.keyword_table.retrievers import (\n            KeywordTableGPTRetriever,\n            KeywordTableRAKERetriever,\n            KeywordTableSimpleRetriever,\n        )\n\n        if retriever_mode == KeywordTableRetrieverMode.DEFAULT:\n            return KeywordTableGPTRetriever(\n                self, object_map=self._object_map, llm=self._llm, **kwargs\n            )\n        elif retriever_mode == KeywordTableRetrieverMode.SIMPLE:\n            return KeywordTableSimpleRetriever(\n                self, object_map=self._object_map, **kwargs\n            )\n        elif retriever_mode == KeywordTableRetrieverMode.RAKE:\n            return KeywordTableRAKERetriever(\n                self, object_map=self._object_map, **kwargs\n            )\n        else:\n            raise ValueError(f\"Unknown retriever mode: {retriever_mode}\")\n\n    @abstractmethod\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n\n    async def _async_extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        # by default just call sync version\n        return self._extract_keywords(text)\n\n    def _add_nodes_to_index(\n        self,\n        index_struct: KeywordTable,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes_with_progress = get_tqdm_iterable(\n            nodes, show_progress, \"Extracting keywords from nodes\"\n        )\n        for n in nodes_with_progress:\n            keywords = self._extract_keywords(\n                n.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            index_struct.add_node(list(keywords), n)\n\n    async def _async_add_nodes_to_index(\n        self,\n        index_struct: KeywordTable,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n    ) -> None:\n        \"\"\"Add document to index.\"\"\"\n        nodes_with_progress = get_tqdm_iterable(\n            nodes, show_progress, \"Extracting keywords from nodes\"\n        )\n        for n in nodes_with_progress:\n            keywords = await self._async_extract_keywords(\n                n.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            index_struct.add_node(list(keywords), n)\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> KeywordTable:\n        \"\"\"Build the index from nodes.\"\"\"\n        # do simple concatenation\n        index_struct = KeywordTable(table={})\n        if self._use_async:\n            tasks = [\n                self._async_add_nodes_to_index(index_struct, nodes, self._show_progress)\n            ]\n            run_async_tasks(tasks)\n        else:\n            self._add_nodes_to_index(index_struct, nodes, self._show_progress)\n\n        return index_struct\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert nodes.\"\"\"\n        for n in nodes:\n            keywords = self._extract_keywords(\n                n.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            self._index_struct.add_node(list(keywords), n)\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        # delete node from the keyword table\n        keywords_to_delete = set()\n        for keyword, existing_node_ids in self._index_struct.table.items():\n            if node_id in existing_node_ids:\n                existing_node_ids.remove(node_id)\n                if len(existing_node_ids) == 0:\n                    keywords_to_delete.add(keyword)\n\n        # delete keywords that have zero nodes\n        for keyword in keywords_to_delete:\n            del self._index_struct.table[keyword]\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        node_doc_ids_sets = list(self._index_struct.table.values())\n        node_doc_ids = list(set().union(*node_doc_ids_sets))\n        nodes = self.docstore.get_nodes(node_doc_ids)\n\n        all_ref_doc_info = {}\n        for node in nodes:\n            ref_node = node.source_node\n            if not ref_node:\n                continue\n\n            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)\n            if not ref_doc_info:\n                continue\n\n            all_ref_doc_info[ref_node.node_id] = ref_doc_info\n        return all_ref_doc_info\n\n\nclass KeywordTableIndex(BaseKeywordTableIndex):\n    \"\"\"Keyword Table Index.\n\n    This index uses a GPT model to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = self._llm.predict(\n            self.keyword_extract_template,\n            text=text,\n        )\n        return extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n\n    async def _async_extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        response = await self._llm.apredict(\n            self.keyword_extract_template,\n            text=text,\n        )\n        return extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n\n\n# legacy\nGPTKeywordTableIndex = KeywordTableIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/keyword_table/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/keyword_table/__init__.py",
    "start_line": 1,
    "end_line": 33,
    "length": 33,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Keyword Table Index Data Structures.\"\"\"\n\n# indices\nfrom llama_index.core.indices.keyword_table.base import (\n    GPTKeywordTableIndex,\n    KeywordTableIndex,\n)\nfrom llama_index.core.indices.keyword_table.rake_base import (\n    GPTRAKEKeywordTableIndex,\n    RAKEKeywordTableIndex,\n)\nfrom llama_index.core.indices.keyword_table.retrievers import (\n    KeywordTableGPTRetriever,\n    KeywordTableRAKERetriever,\n    KeywordTableSimpleRetriever,\n)\nfrom llama_index.core.indices.keyword_table.simple_base import (\n    GPTSimpleKeywordTableIndex,\n    SimpleKeywordTableIndex,\n)\n\n__all__ = [\n    \"KeywordTableIndex\",\n    \"SimpleKeywordTableIndex\",\n    \"RAKEKeywordTableIndex\",\n    \"KeywordTableGPTRetriever\",\n    \"KeywordTableRAKERetriever\",\n    \"KeywordTableSimpleRetriever\",\n    # legacy\n    \"GPTKeywordTableIndex\",\n    \"GPTSimpleKeywordTableIndex\",\n    \"GPTRAKEKeywordTableIndex\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/keyword_table/utils.py",
    "filename": "utils.py",
    "relpath": "indices/keyword_table/utils.py",
    "start_line": 1,
    "end_line": 75,
    "length": 75,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "simple_extract_keywords",
      "rake_extract_keywords",
      "extract_keywords_given_response"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "simple_extract_keywords",
      "rake_extract_keywords",
      "extract_keywords_given_response"
    ],
    "document_class_names": [],
    "content": "\"\"\"Utils for keyword table.\"\"\"\n\nimport re\nfrom collections import Counter\nfrom typing import Optional, Set\n\nfrom llama_index.core.indices.utils import expand_tokens_with_subtokens\nfrom llama_index.core.utils import globals_helper\n\n\ndef simple_extract_keywords(\n    text_chunk: str, max_keywords: Optional[int] = None, filter_stopwords: bool = True\n) -> Set[str]:\n    \"\"\"Extract keywords with simple algorithm.\"\"\"\n    tokens = [t.strip().lower() for t in re.findall(r\"\\w+\", text_chunk)]\n    if filter_stopwords:\n        tokens = [t for t in tokens if t not in globals_helper.stopwords]\n\n    token_counts = Counter(tokens)\n    keywords = [keyword for keyword, count in token_counts.most_common(max_keywords)]\n    return set(keywords)\n\n\ndef rake_extract_keywords(\n    text_chunk: str,\n    max_keywords: Optional[int] = None,\n    expand_with_subtokens: bool = True,\n) -> Set[str]:\n    \"\"\"Extract keywords with RAKE.\"\"\"\n    try:\n        import nltk\n    except ImportError:\n        raise ImportError(\"Please install nltk: `pip install nltk`\")\n    try:\n        from rake_nltk import Rake\n    except ImportError:\n        raise ImportError(\"Please install rake_nltk: `pip install rake_nltk`\")\n\n    r = Rake(\n        sentence_tokenizer=nltk.tokenize.sent_tokenize,\n        word_tokenizer=nltk.tokenize.wordpunct_tokenize,\n    )\n    r.extract_keywords_from_text(text_chunk)\n    keywords = r.get_ranked_phrases()[:max_keywords]\n    if expand_with_subtokens:\n        return set(expand_tokens_with_subtokens(keywords))\n    else:\n        return set(keywords)\n\n\ndef extract_keywords_given_response(\n    response: str, lowercase: bool = True, start_token: str = \"\"\n) -> Set[str]:\n    \"\"\"Extract keywords given the GPT-generated response.\n\n    Used by keyword table indices.\n    Parses <start_token>: <word1>, <word2>, ... into [word1, word2, ...]\n    Raises exception if response doesn't start with <start_token>\n    \"\"\"\n    results = []\n    response = response.strip()  # Strip newlines from responses.\n\n    if response.startswith(start_token):\n        response = response[len(start_token) :]\n\n    keywords = response.split(\",\")\n    for k in keywords:\n        rk = k\n        if lowercase:\n            rk = rk.lower()\n        results.append(rk.strip())\n\n    # if keyword consists of multiple words, split into subwords\n    # (removing stopwords)\n    return expand_tokens_with_subtokens(set(results))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/keyword_table/rake_base.py",
    "filename": "rake_base.py",
    "relpath": "indices/keyword_table/rake_base.py",
    "start_line": 1,
    "end_line": 39,
    "length": 39,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_extract_keywords",
      "as_retriever"
    ],
    "chunk_class_names": [
      "RAKEKeywordTableIndex"
    ],
    "document_function_names": [
      "_extract_keywords",
      "as_retriever"
    ],
    "document_class_names": [
      "RAKEKeywordTableIndex"
    ],
    "content": "\"\"\"RAKE keyword-table based index.\n\nSimilar to KeywordTableIndex, but uses RAKE instead of GPT.\n\n\"\"\"\n\nfrom typing import Any, Set, Union\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.indices.keyword_table.base import (\n    BaseKeywordTableIndex,\n    KeywordTableRetrieverMode,\n)\nfrom llama_index.core.indices.keyword_table.utils import rake_extract_keywords\n\n\nclass RAKEKeywordTableIndex(BaseKeywordTableIndex):\n    \"\"\"RAKE Keyword Table Index.\n\n    This index uses a RAKE keyword extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return rake_extract_keywords(text, max_keywords=self.max_keywords_per_chunk)\n\n    def as_retriever(\n        self,\n        retriever_mode: Union[\n            str, KeywordTableRetrieverMode\n        ] = KeywordTableRetrieverMode.RAKE,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        return super().as_retriever(retriever_mode=retriever_mode, **kwargs)\n\n\n# legacy\nGPTRAKEKeywordTableIndex = RAKEKeywordTableIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/keyword_table/retrievers.py",
    "filename": "retrievers.py",
    "relpath": "indices/keyword_table/retrievers.py",
    "start_line": 1,
    "end_line": 195,
    "length": 195,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_keywords",
      "_retrieve",
      "__init__",
      "_get_keywords",
      "_get_keywords",
      "_get_keywords"
    ],
    "chunk_class_names": [
      "BaseKeywordTableRetriever",
      "KeywordTableGPTRetriever",
      "KeywordTableSimpleRetriever",
      "KeywordTableRAKERetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_keywords",
      "_retrieve",
      "__init__",
      "_get_keywords",
      "_get_keywords",
      "_get_keywords"
    ],
    "document_class_names": [
      "BaseKeywordTableRetriever",
      "KeywordTableGPTRetriever",
      "KeywordTableSimpleRetriever",
      "KeywordTableRAKERetriever"
    ],
    "content": "\"\"\"Query for KeywordTableIndex.\"\"\"\nimport logging\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.keyword_table.base import BaseKeywordTableIndex\nfrom llama_index.core.indices.keyword_table.utils import (\n    extract_keywords_given_response,\n    rake_extract_keywords,\n    simple_extract_keywords,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utils import truncate_text\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseKeywordTableRetriever(BaseRetriever):\n    \"\"\"Base Keyword Table Retriever.\n\n    Arguments are shared among subclasses.\n\n    Args:\n        keyword_extract_template (Optional[BasePromptTemplate]): A Keyword\n            Extraction Prompt\n            (see :ref:`Prompt-Templates`).\n        query_keyword_extract_template (Optional[BasePromptTemplate]): A Query\n            Keyword Extraction\n            Prompt (see :ref:`Prompt-Templates`).\n        refine_template (Optional[BasePromptTemplate]): A Refinement Prompt\n            (see :ref:`Prompt-Templates`).\n        text_qa_template (Optional[BasePromptTemplate]): A Question Answering Prompt\n            (see :ref:`Prompt-Templates`).\n        max_keywords_per_query (int): Maximum number of keywords to extract from query.\n        num_chunks_per_query (int): Maximum number of text chunks to query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: BaseKeywordTableIndex,\n        keyword_extract_template: Optional[BasePromptTemplate] = None,\n        query_keyword_extract_template: Optional[BasePromptTemplate] = None,\n        max_keywords_per_query: int = 10,\n        num_chunks_per_query: int = 10,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._index = index\n        self._index_struct = index.index_struct\n        self._docstore = index.docstore\n\n        self.max_keywords_per_query = max_keywords_per_query\n        self.num_chunks_per_query = num_chunks_per_query\n        self.keyword_extract_template = (\n            keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE\n        )\n        self.query_keyword_extract_template = query_keyword_extract_template or DQKET\n        super().__init__(\n            callback_manager=callback_manager or Settings.callback_manager,\n            object_map=object_map,\n            verbose=verbose,\n        )\n\n    @abstractmethod\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n\n    def _retrieve(\n        self,\n        query_bundle: QueryBundle,\n    ) -> List[NodeWithScore]:\n        \"\"\"Get nodes for response.\"\"\"\n        logger.info(f\"> Starting query: {query_bundle.query_str}\")\n        keywords = self._get_keywords(query_bundle.query_str)\n        logger.info(f\"query keywords: {keywords}\")\n\n        # go through text chunks in order of most matching keywords\n        chunk_indices_count: Dict[str, int] = defaultdict(int)\n        keywords = [k for k in keywords if k in self._index_struct.keywords]\n        logger.info(f\"> Extracted keywords: {keywords}\")\n        for k in keywords:\n            for node_id in self._index_struct.table[k]:\n                chunk_indices_count[node_id] += 1\n        sorted_chunk_indices = sorted(\n            chunk_indices_count.keys(),\n            key=lambda x: chunk_indices_count[x],\n            reverse=True,\n        )\n        sorted_chunk_indices = sorted_chunk_indices[: self.num_chunks_per_query]\n        sorted_nodes = self._docstore.get_nodes(sorted_chunk_indices)\n\n        if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:\n            for chunk_idx, node in zip(sorted_chunk_indices, sorted_nodes):\n                logger.debug(\n                    f\"> Querying with idx: {chunk_idx}: \"\n                    f\"{truncate_text(node.get_content(), 50)}\"\n                )\n        return [NodeWithScore(node=node) for node in sorted_nodes]\n\n\nclass KeywordTableGPTRetriever(BaseKeywordTableRetriever):\n    \"\"\"Keyword Table Index GPT Retriever.\n\n    Extracts keywords using GPT. Set when using `retriever_mode=\"default\"`.\n\n    See BaseGPTKeywordTableQuery for arguments.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        index: BaseKeywordTableIndex,\n        keyword_extract_template: Optional[BasePromptTemplate] = None,\n        query_keyword_extract_template: Optional[BasePromptTemplate] = None,\n        max_keywords_per_query: int = 10,\n        num_chunks_per_query: int = 10,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or Settings.llm\n\n        super().__init__(\n            index=index,\n            keyword_extract_template=keyword_extract_template,\n            query_keyword_extract_template=query_keyword_extract_template,\n            max_keywords_per_query=max_keywords_per_query,\n            num_chunks_per_query=num_chunks_per_query,\n            callback_manager=callback_manager or Settings.callback_manager,\n            object_map=object_map,\n            verbose=verbose,\n        )\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        response = self._llm.predict(\n            self.query_keyword_extract_template,\n            max_keywords=self.max_keywords_per_query,\n            question=query_str,\n        )\n        keywords = extract_keywords_given_response(response, start_token=\"KEYWORDS:\")\n        return list(keywords)\n\n\nclass KeywordTableSimpleRetriever(BaseKeywordTableRetriever):\n    \"\"\"Keyword Table Index Simple Retriever.\n\n    Extracts keywords using simple regex-based keyword extractor.\n    Set when `retriever_mode=\"simple\"`.\n\n    See BaseGPTKeywordTableQuery for arguments.\n\n    \"\"\"\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        return list(\n            simple_extract_keywords(query_str, max_keywords=self.max_keywords_per_query)\n        )\n\n\nclass KeywordTableRAKERetriever(BaseKeywordTableRetriever):\n    \"\"\"Keyword Table Index RAKE Retriever.\n\n    Extracts keywords using RAKE keyword extractor.\n    Set when `retriever_mode=\"rake\"`.\n\n    See BaseGPTKeywordTableQuery for arguments.\n\n    \"\"\"\n\n    def _get_keywords(self, query_str: str) -> List[str]:\n        \"\"\"Extract keywords.\"\"\"\n        return list(\n            rake_extract_keywords(query_str, max_keywords=self.max_keywords_per_query)\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/keyword_table/simple_base.py",
    "filename": "simple_base.py",
    "relpath": "indices/keyword_table/simple_base.py",
    "start_line": 1,
    "end_line": 45,
    "length": 45,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_extract_keywords",
      "as_retriever"
    ],
    "chunk_class_names": [
      "SimpleKeywordTableIndex"
    ],
    "document_function_names": [
      "_extract_keywords",
      "as_retriever"
    ],
    "document_class_names": [
      "SimpleKeywordTableIndex"
    ],
    "content": "\"\"\"Simple keyword-table based index.\n\nSimilar to KeywordTableIndex, but uses a simpler keyword extraction\ntechnique that doesn't involve GPT - just uses regex.\n\n\"\"\"\n\nfrom typing import Any, Set, Union\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.indices.keyword_table.base import (\n    BaseKeywordTableIndex,\n    KeywordTableRetrieverMode,\n)\nfrom llama_index.core.indices.keyword_table.utils import simple_extract_keywords\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,\n)\n\nDQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE\n\n\nclass SimpleKeywordTableIndex(BaseKeywordTableIndex):\n    \"\"\"Simple Keyword Table Index.\n\n    This index uses a simple regex extractor to extract keywords from the text.\n\n    \"\"\"\n\n    def _extract_keywords(self, text: str) -> Set[str]:\n        \"\"\"Extract keywords from text.\"\"\"\n        return simple_extract_keywords(text, self.max_keywords_per_chunk)\n\n    def as_retriever(\n        self,\n        retriever_mode: Union[\n            str, KeywordTableRetrieverMode\n        ] = KeywordTableRetrieverMode.SIMPLE,\n        **kwargs: Any,\n    ) -> BaseRetriever:\n        return super().as_retriever(retriever_mode=retriever_mode, **kwargs)\n\n\n# legacy\nGPTSimpleKeywordTableIndex = SimpleKeywordTableIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/common_tree/base.py",
    "filename": "base.py",
    "relpath": "indices/common_tree/base.py",
    "start_line": 1,
    "end_line": 238,
    "length": 238,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "docstore",
      "build_from_nodes",
      "_prepare_node_and_text_chunks",
      "_construct_parent_nodes",
      "build_index_from_nodes",
      "abuild_index_from_nodes"
    ],
    "chunk_class_names": [
      "GPTTreeIndexBuilder",
      "to"
    ],
    "document_function_names": [
      "__init__",
      "docstore",
      "build_from_nodes",
      "_prepare_node_and_text_chunks",
      "_construct_parent_nodes",
      "build_index_from_nodes",
      "abuild_index_from_nodes"
    ],
    "document_class_names": [
      "GPTTreeIndexBuilder",
      "to"
    ],
    "content": "\"\"\"Common classes/functions for tree index operations.\"\"\"\n\nimport asyncio\nimport logging\nfrom typing import Dict, List, Optional, Sequence, Tuple\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.data_structs.data_structs import IndexGraph\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.indices.utils import get_sorted_node_list, truncate_text\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore import BaseDocumentStore\nfrom llama_index.core.storage.docstore.registry import get_default_docstore\nfrom llama_index.core.utils import get_tqdm_iterable\n\nlogger = logging.getLogger(__name__)\n\n\nclass GPTTreeIndexBuilder:\n    \"\"\"GPT tree index builder.\n\n    Helper class to build the tree-structured index,\n    or to synthesize an answer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        num_children: int,\n        summary_prompt: BasePromptTemplate,\n        llm: Optional[LLM] = None,\n        docstore: Optional[BaseDocumentStore] = None,\n        show_progress: bool = False,\n        use_async: bool = False,\n    ) -> None:\n        \"\"\"Initialize with params.\"\"\"\n        if num_children < 2:\n            raise ValueError(\"Invalid number of children.\")\n        self.num_children = num_children\n        self.summary_prompt = summary_prompt\n        self._llm = llm or Settings.llm\n        self._prompt_helper = Settings._prompt_helper or PromptHelper.from_llm_metadata(\n            self._llm.metadata,\n        )\n        self._callback_manager = Settings.callback_manager\n        self._use_async = use_async\n        self._show_progress = show_progress\n        self._docstore = docstore or get_default_docstore()\n\n    @property\n    def docstore(self) -> BaseDocumentStore:\n        \"\"\"Return docstore.\"\"\"\n        return self._docstore\n\n    def build_from_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        build_tree: bool = True,\n    ) -> IndexGraph:\n        \"\"\"Build from text.\n\n        Returns:\n            IndexGraph: graph object consisting of all_nodes, root_nodes\n\n        \"\"\"\n        index_graph = IndexGraph()\n        for node in nodes:\n            index_graph.insert(node)\n\n        if build_tree:\n            return self.build_index_from_nodes(\n                index_graph, index_graph.all_nodes, index_graph.all_nodes, level=0\n            )\n        else:\n            return index_graph\n\n    def _prepare_node_and_text_chunks(\n        self, cur_node_ids: Dict[int, str]\n    ) -> Tuple[List[int], List[List[BaseNode]], List[str]]:\n        \"\"\"Prepare node and text chunks.\"\"\"\n        cur_nodes = {\n            index: self._docstore.get_node(node_id)\n            for index, node_id in cur_node_ids.items()\n        }\n        cur_node_list = get_sorted_node_list(cur_nodes)\n        logger.info(\n            f\"> Building index from nodes: {len(cur_nodes) // self.num_children} chunks\"\n        )\n        indices, cur_nodes_chunks, text_chunks = [], [], []\n        for i in range(0, len(cur_node_list), self.num_children):\n            cur_nodes_chunk = cur_node_list[i : i + self.num_children]\n            truncated_chunks = self._prompt_helper.truncate(\n                prompt=self.summary_prompt,\n                text_chunks=[\n                    node.get_content(metadata_mode=MetadataMode.LLM)\n                    for node in cur_nodes_chunk\n                ],\n                llm=self._llm,\n            )\n            text_chunk = \"\\n\".join(truncated_chunks)\n            indices.append(i)\n            cur_nodes_chunks.append(cur_nodes_chunk)\n            text_chunks.append(text_chunk)\n        return indices, cur_nodes_chunks, text_chunks\n\n    def _construct_parent_nodes(\n        self,\n        index_graph: IndexGraph,\n        indices: List[int],\n        cur_nodes_chunks: List[List[BaseNode]],\n        summaries: List[str],\n    ) -> Dict[int, str]:\n        \"\"\"Construct parent nodes.\n\n        Save nodes to docstore.\n\n        \"\"\"\n        new_node_dict = {}\n        for i, cur_nodes_chunk, new_summary in zip(\n            indices, cur_nodes_chunks, summaries\n        ):\n            logger.debug(\n                f\"> {i}/{len(cur_nodes_chunk)}, \"\n                f\"summary: {truncate_text(new_summary, 50)}\"\n            )\n            new_node = TextNode(text=new_summary)\n            index_graph.insert(new_node, children_nodes=cur_nodes_chunk)\n            index = index_graph.get_index(new_node)\n            new_node_dict[index] = new_node.node_id\n            self._docstore.add_documents([new_node], allow_update=False)\n        return new_node_dict\n\n    def build_index_from_nodes(\n        self,\n        index_graph: IndexGraph,\n        cur_node_ids: Dict[int, str],\n        all_node_ids: Dict[int, str],\n        level: int = 0,\n    ) -> IndexGraph:\n        \"\"\"Consolidates chunks recursively, in a bottoms-up fashion.\"\"\"\n        if len(cur_node_ids) <= self.num_children:\n            index_graph.root_nodes = cur_node_ids\n            return index_graph\n\n        indices, cur_nodes_chunks, text_chunks = self._prepare_node_and_text_chunks(\n            cur_node_ids\n        )\n\n        with self._callback_manager.event(\n            CBEventType.TREE, payload={EventPayload.CHUNKS: text_chunks}\n        ) as event:\n            if self._use_async:\n                tasks = [\n                    self._llm.apredict(self.summary_prompt, context_str=text_chunk)\n                    for text_chunk in text_chunks\n                ]\n                outputs: List[Tuple[str, str]] = run_async_tasks(\n                    tasks,\n                    show_progress=self._show_progress,\n                    progress_bar_desc=\"Generating summaries\",\n                )\n                summaries = [output[0] for output in outputs]\n            else:\n                text_chunks_progress = get_tqdm_iterable(\n                    text_chunks,\n                    show_progress=self._show_progress,\n                    desc=\"Generating summaries\",\n                )\n                summaries = [\n                    self._llm.predict(self.summary_prompt, context_str=text_chunk)\n                    for text_chunk in text_chunks_progress\n                ]\n\n            event.on_end(payload={\"summaries\": summaries, \"level\": level})\n\n        new_node_dict = self._construct_parent_nodes(\n            index_graph, indices, cur_nodes_chunks, summaries\n        )\n        all_node_ids.update(new_node_dict)\n\n        index_graph.root_nodes = new_node_dict\n\n        if len(new_node_dict) <= self.num_children:\n            return index_graph\n        else:\n            return self.build_index_from_nodes(\n                index_graph, new_node_dict, all_node_ids, level=level + 1\n            )\n\n    async def abuild_index_from_nodes(\n        self,\n        index_graph: IndexGraph,\n        cur_node_ids: Dict[int, str],\n        all_node_ids: Dict[int, str],\n        level: int = 0,\n    ) -> IndexGraph:\n        \"\"\"Consolidates chunks recursively, in a bottoms-up fashion.\"\"\"\n        if len(cur_node_ids) <= self.num_children:\n            index_graph.root_nodes = cur_node_ids\n            return index_graph\n\n        indices, cur_nodes_chunks, text_chunks = self._prepare_node_and_text_chunks(\n            cur_node_ids\n        )\n\n        with self._callback_manager.event(\n            CBEventType.TREE, payload={EventPayload.CHUNKS: text_chunks}\n        ) as event:\n            text_chunks_progress = get_tqdm_iterable(\n                text_chunks,\n                show_progress=self._show_progress,\n                desc=\"Generating summaries\",\n            )\n            tasks = [\n                self._llm.apredict(self.summary_prompt, context_str=text_chunk)\n                for text_chunk in text_chunks_progress\n            ]\n            summaries = await asyncio.gather(*tasks)\n\n            event.on_end(payload={\"summaries\": summaries, \"level\": level})\n\n        new_node_dict = self._construct_parent_nodes(\n            index_graph, indices, cur_nodes_chunks, summaries\n        )\n        all_node_ids.update(new_node_dict)\n\n        index_graph.root_nodes = new_node_dict\n\n        if len(new_node_dict) <= self.num_children:\n            return index_graph\n        else:\n            return await self.abuild_index_from_nodes(\n                index_graph, new_node_dict, all_node_ids, level=level + 1\n            )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/common_tree/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/common_tree/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/managed/types.py",
    "filename": "types.py",
    "relpath": "indices/managed/types.py",
    "start_line": 1,
    "end_line": 10,
    "length": 10,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "ManagedIndexQueryMode"
    ],
    "document_function_names": [],
    "document_class_names": [
      "ManagedIndexQueryMode"
    ],
    "content": "\"\"\"Managed index types.\"\"\"\n\nfrom enum import Enum\n\n\nclass ManagedIndexQueryMode(str, Enum):\n    \"\"\"Managed Index query mode.\"\"\"\n\n    DEFAULT = \"default\"\n    MMR = \"mmr\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/managed/base.py",
    "filename": "base.py",
    "relpath": "indices/managed/base.py",
    "start_line": 1,
    "end_line": 95,
    "length": 95,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_insert",
      "delete_ref_doc",
      "update_ref_doc",
      "as_retriever",
      "_build_index_from_nodes",
      "_delete_node",
      "ref_doc_info",
      "from_documents"
    ],
    "chunk_class_names": [
      "BaseManagedIndex"
    ],
    "document_function_names": [
      "__init__",
      "_insert",
      "delete_ref_doc",
      "update_ref_doc",
      "as_retriever",
      "_build_index_from_nodes",
      "_delete_node",
      "ref_doc_info",
      "from_documents"
    ],
    "document_class_names": [
      "BaseManagedIndex"
    ],
    "content": "\"\"\"Base Managed Service index.\n\nAn index that is built on top of a managed service.\n\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional, Sequence, Type\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.data_structs.data_structs import IndexDict\nfrom llama_index.core.indices.base import BaseIndex, IndexType\nfrom llama_index.core.schema import BaseNode, Document, TransformComponent\nfrom llama_index.core.storage.docstore.types import RefDocInfo\nfrom llama_index.core.storage.storage_context import StorageContext\n\n\nclass BaseManagedIndex(BaseIndex[IndexDict], ABC):\n    \"\"\"Managed Index.\n    The managed service can index documents into a managed service.\n    How documents are structured into nodes is a detail for the managed service,\n    and not exposed in this interface (although could be controlled by\n    configuration parameters).\n\n    Args:\n        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        index_struct: Optional[IndexDict] = None,\n        storage_context: Optional[StorageContext] = None,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            storage_context=storage_context,\n            show_progress=show_progress,\n            **kwargs,\n        )\n\n    @abstractmethod\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a set of documents (each a node).\"\"\"\n\n    @abstractmethod\n    def delete_ref_doc(\n        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any\n    ) -> None:\n        \"\"\"Delete a document and it's nodes by using ref_doc_id.\"\"\"\n\n    @abstractmethod\n    def update_ref_doc(self, document: Document, **update_kwargs: Any) -> None:\n        \"\"\"Update a document and it's corresponding nodes.\"\"\"\n\n    @abstractmethod\n    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\n        \"\"\"Return a Retriever for this managed index.\"\"\"\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> IndexDict:\n        \"\"\"Build the index from nodes.\"\"\"\n        raise NotImplementedError(\n            \"_build_index_from_nodes not implemented for BaseManagedIndex.\"\n        )\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        raise NotImplementedError(\"_delete_node not implemented for BaseManagedIndex.\")\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        raise NotImplementedError(\"ref_doc_info not implemented for BaseManagedIndex.\")\n\n    @classmethod\n    def from_documents(\n        cls: Type[IndexType],\n        documents: Sequence[Document],\n        storage_context: Optional[StorageContext] = None,\n        show_progress: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        **kwargs: Any,\n    ) -> IndexType:\n        \"\"\"Build an index from a sequence of documents.\"\"\"\n        raise NotImplementedError(\n            \"from_documents not implemented for BaseManagedIndex.\"\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/managed/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/managed/__init__.py",
    "start_line": 1,
    "end_line": 5,
    "length": 5,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.indices.managed.base import BaseManagedIndex\n\n__all__ = [\n    \"BaseManagedIndex\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/pandas.py",
    "filename": "pandas.py",
    "relpath": "indices/struct_store/pandas.py",
    "start_line": 1,
    "end_line": 25,
    "length": 25,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__"
    ],
    "chunk_class_names": [
      "PandasIndex"
    ],
    "document_function_names": [
      "__init__"
    ],
    "document_class_names": [
      "PandasIndex"
    ],
    "content": "\"\"\"Pandas csv structured store.\n\nDEPRECATED: Please use :class:`PandasQueryEngine` in `llama-index-experimental` instead.\n\"\"\"\n\nfrom typing import Any\n\n\nclass PandasIndex:\n    def __init__(\n        self,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        raise DeprecationWarning(\n            \"PandasQueryEngine has been moved to `llama-index-experimental`.\\n\"\n            \"`pip install llama-index-experimental`\\n\"\n            \"`from llama_index.experimental.query_engine import PandasQueryEngine`\\n\"\n            \"Note that the PandasQueryEngine allows for arbitrary code execution, \\n\"\n            \"and should be used in a secure environment.\"\n        )\n\n\n# Legacy\nGPTPandasIndex = PandasIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/base.py",
    "filename": "base.py",
    "relpath": "indices/struct_store/base.py",
    "start_line": 1,
    "end_line": 67,
    "length": 67,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "default_output_parser",
      "__init__",
      "_delete_node",
      "ref_doc_info"
    ],
    "chunk_class_names": [
      "BaseStructStoreIndex"
    ],
    "document_function_names": [
      "default_output_parser",
      "__init__",
      "_delete_node",
      "ref_doc_info"
    ],
    "document_class_names": [
      "BaseStructStoreIndex"
    ],
    "content": "\"\"\"Struct store.\"\"\"\n\nimport re\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar\n\nfrom llama_index.core.data_structs.table import BaseStructTable\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT\nfrom llama_index.core.schema import BaseNode\nfrom llama_index.core.storage.docstore.types import RefDocInfo\n\nBST = TypeVar(\"BST\", bound=BaseStructTable)\n\n\ndef default_output_parser(output: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Parse output of schema extraction.\n\n    Attempt to parse the following format from the default prompt:\n    field1: <value>, field2: <value>, ...\n\n    \"\"\"\n    tups = output.split(\"\\n\")\n\n    fields = {}\n    for tup in tups:\n        if \":\" in tup:\n            tokens = tup.split(\":\")\n            field = re.sub(r\"\\W+\", \"\", tokens[0])\n            value = re.sub(r\"\\W+\", \"\", tokens[1])\n            fields[field] = value\n    return fields\n\n\nOUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]\n\n\nclass BaseStructStoreIndex(BaseIndex[BST], Generic[BST]):\n    \"\"\"Base Struct Store Index.\"\"\"\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        index_struct: Optional[BST] = None,\n        schema_extract_prompt: Optional[BasePromptTemplate] = None,\n        output_parser: Optional[OUTPUT_PARSER_TYPE] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self.schema_extract_prompt = (\n            schema_extract_prompt or DEFAULT_SCHEMA_EXTRACT_PROMPT\n        )\n        self.output_parser = output_parser or default_output_parser\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            **kwargs,\n        )\n\n    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"Delete a node.\"\"\"\n        raise NotImplementedError(\"Delete not implemented for Struct Store Index.\")\n\n    @property\n    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n        raise NotImplementedError(\"Struct Store Index does not support ref_doc_info.\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
    "filename": "sql_retriever.py",
    "relpath": "indices/struct_store/sql_retriever.py",
    "start_line": 1,
    "end_line": 175,
    "length": 175,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_format_node_results",
      "retrieve_with_metadata",
      "aretrieve_with_metadata",
      "_retrieve",
      "parse_response_to_sql",
      "parse_response_to_sql",
      "__init__",
      "parse_response_to_sql"
    ],
    "chunk_class_names": [
      "SQLRetriever",
      "SQLParserMode",
      "BaseSQLParser",
      "DefaultSQLParser",
      "PGVectorSQLParser"
    ],
    "document_function_names": [
      "__init__",
      "_format_node_results",
      "retrieve_with_metadata",
      "aretrieve_with_metadata",
      "_retrieve",
      "parse_response_to_sql",
      "parse_response_to_sql",
      "__init__",
      "parse_response_to_sql",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "_load_sql_parser",
      "_load_get_tables_fn",
      "retrieve_with_metadata",
      "aretrieve_with_metadata",
      "_retrieve",
      "_aretrieve",
      "_get_table_context"
    ],
    "document_class_names": [
      "SQLRetriever",
      "SQLParserMode",
      "BaseSQLParser",
      "DefaultSQLParser",
      "PGVectorSQLParser",
      "NLSQLRetriever"
    ],
    "content": "\"\"\"SQL Retriever.\"\"\"\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union, cast\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.objects.table_node_mapping import SQLTableSchema\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_TEXT_TO_SQL_PROMPT,\n)\nfrom llama_index.core.prompts.mixin import (\n    PromptDictType,\n    PromptMixin,\n    PromptMixinType,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, QueryType, TextNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\nfrom sqlalchemy import Table\n\nlogger = logging.getLogger(__name__)\n\n\nclass SQLRetriever(BaseRetriever):\n    \"\"\"SQL Retriever.\n\n    Retrieves via raw SQL statements.\n\n    Args:\n        sql_database (SQLDatabase): SQL database.\n        return_raw (bool): Whether to return raw results or format results.\n            Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        return_raw: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._sql_database = sql_database\n        self._return_raw = return_raw\n        super().__init__(callback_manager)\n\n    def _format_node_results(\n        self, results: List[List[Any]], col_keys: List[str]\n    ) -> List[NodeWithScore]:\n        \"\"\"Format node results.\"\"\"\n        nodes = []\n        for result in results:\n            # associate column keys with result tuple\n            metadata = dict(zip(col_keys, result))\n            # NOTE: leave text field blank for now\n            text_node = TextNode(\n                text=\"\",\n                metadata=metadata,\n            )\n            nodes.append(NodeWithScore(node=text_node))\n        return nodes\n\n    def retrieve_with_metadata(\n        self, str_or_query_bundle: QueryType\n    ) -> Tuple[List[NodeWithScore], Dict]:\n        \"\"\"Retrieve with metadata.\"\"\"\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n        raw_response_str, metadata = self._sql_database.run_sql(query_bundle.query_str)\n        if self._return_raw:\n            return [\n                NodeWithScore(\n                    node=TextNode(\n                        text=raw_response_str,\n                        metadata={\n                            \"sql_query\": query_bundle.query_str,\n                            \"result\": metadata[\"result\"],\n                            \"col_keys\": metadata[\"col_keys\"],\n                        },\n                        excluded_embed_metadata_keys=[\n                            \"sql_query\",\n                            \"result\",\n                            \"col_keys\",\n                        ],\n                        excluded_llm_metadata_keys=[\"sql_query\", \"result\", \"col_keys\"],\n                    )\n                )\n            ], metadata\n        else:\n            # return formatted\n            results = metadata[\"result\"]\n            col_keys = metadata[\"col_keys\"]\n            return self._format_node_results(results, col_keys), metadata\n\n    async def aretrieve_with_metadata(\n        self, str_or_query_bundle: QueryType\n    ) -> Tuple[List[NodeWithScore], Dict]:\n        return self.retrieve_with_metadata(str_or_query_bundle)\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\"\"\"\n        retrieved_nodes, _ = self.retrieve_with_metadata(query_bundle)\n        return retrieved_nodes\n\n\nclass SQLParserMode(str, Enum):\n    \"\"\"SQL Parser Mode.\"\"\"\n\n    DEFAULT = \"default\"\n    PGVECTOR = \"pgvector\"\n\n\nclass BaseSQLParser(DispatcherSpanMixin, ABC):\n    \"\"\"Base SQL Parser.\"\"\"\n\n    @abstractmethod\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n\n\nclass DefaultSQLParser(BaseSQLParser):\n    \"\"\"Default SQL Parser.\"\"\"\n\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:\n            response = response[sql_query_start:]\n            # TODO: move to removeprefix after Python 3.9+\n            if response.startswith(\"SQLQuery:\"):\n                response = response[len(\"SQLQuery:\") :]\n        sql_result_start = response.find(\"SQLResult:\")\n        if sql_result_start != -1:\n            response = response[:sql_result_start]\n        return response.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n\n\nclass PGVectorSQLParser(BaseSQLParser):\n    \"\"\"PGVector SQL Parser.\"\"\"\n\n    def __init__(\n        self,\n        embed_model: BaseEmbedding,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._embed_model = embed_model\n\n    def parse_response_to_sql(self, response: str, query_bundle: QueryBundle) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:\n            response = response[sql_query_start:]\n            # TODO: move to removeprefix after Python 3.9+\n            if response.startswith(\"SQLQuery:\"):\n                response = response[len(\"SQLQuery:\") :]\n        sql_result_start = response.find(\"SQLResult:\")\n        if sql_result_start != -1:\n            response = response[:sql_result_start]\n\n        # this gets you the sql string with [query_vector] placeholders\n        raw_sql_str = response.strip().strip(\"```sql\").strip(\"```\").strip()\n        query_embedding = self._embed_model.get_query_embedding(query_bundle.query_str)\n        query_embedding_str = str(query_embedding)\n        return raw_sql_str.replace(\"[query_vector]\", query_embedding_str)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_retriever.py",
    "filename": "sql_retriever.py",
    "relpath": "indices/struct_store/sql_retriever.py",
    "start_line": 175,
    "end_line": 428,
    "length": 254,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "_load_sql_parser",
      "_load_get_tables_fn",
      "retrieve_with_metadata",
      "aretrieve_with_metadata",
      "_retrieve",
      "_aretrieve",
      "_get_table_context"
    ],
    "chunk_class_names": [
      "NLSQLRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_format_node_results",
      "retrieve_with_metadata",
      "aretrieve_with_metadata",
      "_retrieve",
      "parse_response_to_sql",
      "parse_response_to_sql",
      "__init__",
      "parse_response_to_sql",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "_load_sql_parser",
      "_load_get_tables_fn",
      "retrieve_with_metadata",
      "aretrieve_with_metadata",
      "_retrieve",
      "_aretrieve",
      "_get_table_context"
    ],
    "document_class_names": [
      "SQLRetriever",
      "SQLParserMode",
      "BaseSQLParser",
      "DefaultSQLParser",
      "PGVectorSQLParser",
      "NLSQLRetriever"
    ],
    "content": "class NLSQLRetriever(BaseRetriever, PromptMixin):\n    \"\"\"Text-to-SQL Retriever.\n\n    Retrieves via text.\n\n    Args:\n        sql_database (SQLDatabase): SQL database.\n        text_to_sql_prompt (BasePromptTemplate): Prompt template for text-to-sql.\n            Defaults to DEFAULT_TEXT_TO_SQL_PROMPT.\n        context_query_kwargs (dict): Mapping from table name to context query.\n            Defaults to None.\n        tables (Union[List[str], List[Table]]): List of table names or Table objects.\n        table_retriever (ObjectRetriever[SQLTableSchema]): Object retriever for\n            SQLTableSchema objects. Defaults to None.\n        rows_retriever (Dict[str, VectorIndexRetriever]): a mapping between table name and\n            a vector index retriever of its rows. Defaults to None.\n        context_str_prefix (str): Prefix for context string. Defaults to None.\n        return_raw (bool): Whether to return plain-text dump of SQL results, or parsed into Nodes.\n        handle_sql_errors (bool): Whether to handle SQL errors. Defaults to True.\n        sql_only (bool) : Whether to get only sql and not the sql query result.\n            Default to False.\n        llm (Optional[LLM]): Language model to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        text_to_sql_prompt: Optional[BasePromptTemplate] = None,\n        context_query_kwargs: Optional[dict] = None,\n        tables: Optional[Union[List[str], List[Table]]] = None,\n        table_retriever: Optional[ObjectRetriever[SQLTableSchema]] = None,\n        rows_retrievers: Optional[dict[str, BaseRetriever]] = None,\n        context_str_prefix: Optional[str] = None,\n        sql_parser_mode: SQLParserMode = SQLParserMode.DEFAULT,\n        llm: Optional[LLM] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        return_raw: bool = True,\n        handle_sql_errors: bool = True,\n        sql_only: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._sql_retriever = SQLRetriever(sql_database, return_raw=return_raw)\n        self._sql_database = sql_database\n        self._get_tables = self._load_get_tables_fn(\n            sql_database, tables, context_query_kwargs, table_retriever\n        )\n        self._context_str_prefix = context_str_prefix\n        self._llm = llm or Settings.llm\n        self._text_to_sql_prompt = text_to_sql_prompt or DEFAULT_TEXT_TO_SQL_PROMPT\n        self._sql_parser_mode = sql_parser_mode\n\n        embed_model = embed_model or Settings.embed_model\n        self._sql_parser = self._load_sql_parser(sql_parser_mode, embed_model)\n        self._handle_sql_errors = handle_sql_errors\n        self._sql_only = sql_only\n        self._verbose = verbose\n\n        # To retrieve relevant rows from each retrieved table\n        self._rows_retrievers = rows_retrievers\n        super().__init__(\n            callback_manager=callback_manager or Settings.callback_manager,\n            verbose=verbose,\n        )\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"text_to_sql_prompt\": self._text_to_sql_prompt,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"text_to_sql_prompt\" in prompts:\n            self._text_to_sql_prompt = prompts[\"text_to_sql_prompt\"]\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _load_sql_parser(\n        self, sql_parser_mode: SQLParserMode, embed_model: BaseEmbedding\n    ) -> BaseSQLParser:\n        \"\"\"Load SQL parser.\"\"\"\n        if sql_parser_mode == SQLParserMode.DEFAULT:\n            return DefaultSQLParser()\n        elif sql_parser_mode == SQLParserMode.PGVECTOR:\n            return PGVectorSQLParser(embed_model=embed_model)\n        else:\n            raise ValueError(f\"Unknown SQL parser mode: {sql_parser_mode}\")\n\n    def _load_get_tables_fn(\n        self,\n        sql_database: SQLDatabase,\n        tables: Optional[Union[List[str], List[Table]]] = None,\n        context_query_kwargs: Optional[dict] = None,\n        table_retriever: Optional[ObjectRetriever[SQLTableSchema]] = None,\n    ) -> Callable[[str], List[SQLTableSchema]]:\n        \"\"\"Load get_tables function.\"\"\"\n        context_query_kwargs = context_query_kwargs or {}\n        if table_retriever is not None:\n            return lambda query_str: cast(Any, table_retriever).retrieve(query_str)\n        else:\n            if tables is not None:\n                table_names: List[str] = [\n                    t.name if isinstance(t, Table) else t for t in tables\n                ]\n            else:\n                table_names = list(sql_database.get_usable_table_names())\n            context_strs = [context_query_kwargs.get(t, None) for t in table_names]\n            table_schemas = [\n                SQLTableSchema(table_name=t, context_str=c)\n                for t, c in zip(table_names, context_strs)\n            ]\n            return lambda _: table_schemas\n\n    def retrieve_with_metadata(\n        self, str_or_query_bundle: QueryType\n    ) -> Tuple[List[NodeWithScore], Dict]:\n        \"\"\"Retrieve with metadata.\"\"\"\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n        if self._verbose:\n            print(f\"> Table desc str: {table_desc_str}\")\n\n        response_str = self._llm.predict(\n            self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,\n            schema=table_desc_str,\n            dialect=self._sql_database.dialect,\n        )\n\n        sql_query_str = self._sql_parser.parse_response_to_sql(\n            response_str, query_bundle\n        )\n        # assume that it's a valid SQL query\n        logger.debug(f\"> Predicted SQL query: {sql_query_str}\")\n        if self._verbose:\n            print(f\"> Predicted SQL query: {sql_query_str}\")\n\n        if self._sql_only:\n            sql_only_node = TextNode(text=f\"{sql_query_str}\")\n            retrieved_nodes = [NodeWithScore(node=sql_only_node)]\n            metadata = {\"result\": sql_query_str}\n        else:\n            try:\n                retrieved_nodes, metadata = self._sql_retriever.retrieve_with_metadata(\n                    sql_query_str\n                )\n            except BaseException as e:\n                # if handle_sql_errors is True, then return error message\n                if self._handle_sql_errors:\n                    err_node = TextNode(text=f\"Error: {e!s}\")\n                    retrieved_nodes = [NodeWithScore(node=err_node)]\n                    metadata = {}\n                else:\n                    raise\n\n        return retrieved_nodes, {\"sql_query\": sql_query_str, **metadata}\n\n    async def aretrieve_with_metadata(\n        self, str_or_query_bundle: QueryType\n    ) -> Tuple[List[NodeWithScore], Dict]:\n        \"\"\"Async retrieve with metadata.\"\"\"\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n\n        response_str = await self._llm.apredict(\n            self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,\n            schema=table_desc_str,\n            dialect=self._sql_database.dialect,\n        )\n\n        sql_query_str = self._sql_parser.parse_response_to_sql(\n            response_str, query_bundle\n        )\n        # assume that it's a valid SQL query\n        logger.debug(f\"> Predicted SQL query: {sql_query_str}\")\n\n        if self._sql_only:\n            sql_only_node = TextNode(text=f\"{sql_query_str}\")\n            retrieved_nodes = [NodeWithScore(node=sql_only_node)]\n            metadata: Dict[str, Any] = {}\n        else:\n            try:\n                (\n                    retrieved_nodes,\n                    metadata,\n                ) = await self._sql_retriever.aretrieve_with_metadata(sql_query_str)\n            except BaseException as e:\n                # if handle_sql_errors is True, then return error message\n                if self._handle_sql_errors:\n                    err_node = TextNode(text=f\"Error: {e!s}\")\n                    retrieved_nodes = [NodeWithScore(node=err_node)]\n                    metadata = {}\n                else:\n                    raise\n        return retrieved_nodes, {\"sql_query\": sql_query_str, **metadata}\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\"\"\"\n        retrieved_nodes, _ = self.retrieve_with_metadata(query_bundle)\n        return retrieved_nodes\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Async retrieve nodes given query.\"\"\"\n        retrieved_nodes, _ = await self.aretrieve_with_metadata(query_bundle)\n        return retrieved_nodes\n\n    def _get_table_context(self, query_bundle: QueryBundle) -> str:\n        \"\"\"Get table context string.\"\"\"\n        table_schema_objs = self._get_tables(query_bundle.query_str)\n        context_strs = []\n        for table_schema_obj in table_schema_objs:\n            # first append table info + additional context\n            table_info = self._sql_database.get_single_table_info(\n                table_schema_obj.table_name\n            )\n            if table_schema_obj.context_str:\n                table_opt_context = \" The table description is: \"\n                table_opt_context += table_schema_obj.context_str\n                table_info += table_opt_context\n\n            # also lookup vector index to return relevant table rows\n            # if rows_retrievers was not passed, no rows will be returned\n            if self._rows_retrievers is not None:\n                rows_retriever = self._rows_retrievers[table_schema_obj.table_name]\n                relevant_nodes = rows_retriever.retrieve(query_bundle.query_str)\n                if len(relevant_nodes) > 0:\n                    table_row_context = \"\\nHere are some relevant example rows (values in the same order as columns above)\\n\"\n                    for node in relevant_nodes:\n                        table_row_context += str(node.get_content()) + \"\\n\"\n                    table_info += table_row_context\n\n            if self._verbose:\n                print(f\"> Table Info: {table_info}\")\n            context_strs.append(table_info)\n\n        return \"\\n\\n\".join(context_strs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
    "filename": "sql_query.py",
    "relpath": "indices/struct_store/sql_query.py",
    "start_line": 1,
    "end_line": 315,
    "length": 315,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_run_with_sql_only_check",
      "_query",
      "_aquery",
      "__init__",
      "_get_prompt_modules",
      "_parse_response_to_sql",
      "_get_table_context",
      "_run_with_sql_only_check",
      "_query",
      "_aquery",
      "_validate_prompt"
    ],
    "chunk_class_names": [
      "SQLStructStoreQueryEngine",
      "NLStructStoreQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_run_with_sql_only_check",
      "_query",
      "_aquery",
      "__init__",
      "_get_prompt_modules",
      "_parse_response_to_sql",
      "_get_table_context",
      "_run_with_sql_only_check",
      "_query",
      "_aquery",
      "_validate_prompt",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "sql_retriever",
      "_format_result_markdown",
      "_query",
      "_aquery",
      "__init__",
      "sql_retriever",
      "__init__",
      "sql_retriever",
      "__init__",
      "sql_retriever"
    ],
    "document_class_names": [
      "SQLStructStoreQueryEngine",
      "NLStructStoreQueryEngine",
      "BaseSQLTableQueryEngine",
      "NLSQLTableQueryEngine",
      "PGVectorSQLQueryEngine",
      "SQLTableRetrieverQueryEngine"
    ],
    "content": "\"\"\"Default query for SQLStructStoreIndex.\"\"\"\n\nimport logging\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, Tuple, Union, cast\nfrom llama_index.core.indices.base import BaseRetriever\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import (\n    RESPONSE_TYPE,\n    AsyncStreamingResponse,\n    Response,\n    StreamingResponse,\n)\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.indices.struct_store.container_builder import (\n    SQLContextContainerBuilder,\n)\nfrom llama_index.core.indices.struct_store.sql import SQLStructStoreIndex\nfrom llama_index.core.indices.struct_store.sql_retriever import (\n    NLSQLRetriever,\n    SQLParserMode,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.objects.table_node_mapping import SQLTableSchema\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_TEXT_TO_SQL_PGVECTOR_PROMPT,\n    DEFAULT_TEXT_TO_SQL_PROMPT,\n    DEFAULT_REFINE_PROMPT,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixinType\nfrom llama_index.core.prompts.prompt_type import PromptType\nfrom llama_index.core.response_synthesizers import (\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\nfrom sqlalchemy import Table\n\nlogger = logging.getLogger(__name__)\n\n\n# **NOTE**: deprecated (for older versions of sql query engine)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given an input question, synthesize a response from the query results.\\n\"\n    \"Query: {query_str}\\n\"\n    \"SQL: {sql_query}\\n\"\n    \"SQL Response: {sql_response_str}\\n\"\n    \"Response: \"\n)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS,\n)\n\n# **NOTE**: newer version of sql query engine\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL_V2 = (\n    \"Given an input question, synthesize a response from the query results.\\n\"\n    \"Query: {query_str}\\n\"\n    \"SQL: {sql_query}\\n\"\n    \"SQL Response: {context_str}\\n\"\n    \"Response: \"\n)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_V2 = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL_V2,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS_V2,\n)\n\n\nclass SQLStructStoreQueryEngine(BaseQueryEngine):\n    \"\"\"GPT SQL query engine over a structured database.\n\n    NOTE: deprecated in favor of SQLTableRetriever, kept for backward compatibility.\n\n    Runs raw SQL over a SQLStructStoreIndex. No LLM calls are made here.\n\n    NOTE: this query cannot work with composed indices - if the index\n    contains subindices, those subindices will not be queried.\n\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: SQLStructStoreIndex,\n        sql_context_container: Optional[SQLContextContainerBuilder] = None,\n        sql_only: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._sql_database = index.sql_database\n        self._sql_context_container = (\n            sql_context_container or index.sql_context_container\n        )\n        self._sql_only = sql_only\n        super().__init__(callback_manager=Settings.callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _run_with_sql_only_check(\n        self, sql_query_str: str\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Don't run sql if sql_only is true, else continue with normal path.\"\"\"\n        if self._sql_only:\n            metadata: Dict[str, Any] = {}\n            raw_response_str = sql_query_str\n        else:\n            raw_response_str, metadata = self._sql_database.run_sql(sql_query_str)\n\n        return raw_response_str, metadata\n\n    def _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        # NOTE: override query method in order to fetch the right results.\n        # NOTE: since the query_str is a SQL query, it doesn't make sense\n        # to use ResponseBuilder anywhere.\n        response_str, metadata = self._run_with_sql_only_check(query_bundle.query_str)\n        return Response(response=response_str, metadata=metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:\n        return self._query(query_bundle)\n\n\nclass NLStructStoreQueryEngine(BaseQueryEngine):\n    \"\"\"GPT natural language query engine over a structured database.\n\n    NOTE: deprecated in favor of SQLTableRetriever, kept for backward compatibility.\n\n    Given a natural language query, we will extract the query to SQL.\n    Runs raw SQL over a SQLStructStoreIndex. No LLM calls are made during\n    the SQL execution.\n\n    NOTE: this query cannot work with composed indices - if the index\n    contains subindices, those subindices will not be queried.\n\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.\n\n    Args:\n        index (SQLStructStoreIndex): A SQL Struct Store Index\n        text_to_sql_prompt (Optional[BasePromptTemplate]): A Text to SQL\n            BasePromptTemplate to use for the query.\n            Defaults to DEFAULT_TEXT_TO_SQL_PROMPT.\n        context_query_kwargs (Optional[dict]): Keyword arguments for the\n            context query. Defaults to {}.\n        synthesize_response (bool): Whether to synthesize a response from the\n            query results. Defaults to True.\n        sql_only (bool) : Whether to get only sql and not the sql query result.\n            Default to False.\n        response_synthesis_prompt (Optional[BasePromptTemplate]): A\n            Response Synthesis BasePromptTemplate to use for the query. Defaults to\n            DEFAULT_RESPONSE_SYNTHESIS_PROMPT.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: SQLStructStoreIndex,\n        text_to_sql_prompt: Optional[BasePromptTemplate] = None,\n        context_query_kwargs: Optional[dict] = None,\n        synthesize_response: bool = True,\n        response_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        sql_only: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._index = index\n        self._llm = Settings.llm\n        self._sql_database = index.sql_database\n        self._sql_context_container = index.sql_context_container\n        self._ref_doc_id_column = index.ref_doc_id_column\n\n        self._text_to_sql_prompt = text_to_sql_prompt or DEFAULT_TEXT_TO_SQL_PROMPT\n        self._response_synthesis_prompt = (\n            response_synthesis_prompt or DEFAULT_RESPONSE_SYNTHESIS_PROMPT\n        )\n        self._context_query_kwargs = context_query_kwargs or {}\n        self._synthesize_response = synthesize_response\n        self._sql_only = sql_only\n        super().__init__(callback_manager=Settings.callback_manager)\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _parse_response_to_sql(self, response: str) -> str:\n        \"\"\"Parse response to SQL.\"\"\"\n        # Find and remove SQLResult part\n        sql_result_start = response.find(\"SQLResult:\")\n        if sql_result_start != -1:\n            response = response[:sql_result_start]\n        # If LLM returns SQLQuery: or ```sql, extract the SQL query\n        sql_query_start = response.find(\"SQLQuery:\")\n        if sql_query_start != -1:\n            response = response[sql_query_start:]\n            response = response.replace(\"SQLQuery:\", \"\")\n        sql_markdown_start = response.find(\"```sql\")\n        if sql_markdown_start != -1:\n            response = response.replace(\"```sql\", \"\")\n        response = response.replace(\"```\", \"\")\n        # If LLM talks between the end of query and SQL result\n        # find semi-colon and remove everything after it\n        semi_colon = response.find(\";\")\n        if semi_colon != -1:\n            response = response[: semi_colon + 1]\n        # Replace escaped single quotes, happens when\n        # there's a ' in the value (e.g. \"I'm\")\n        response = response.replace(\"\\\\'\", \"''\")\n        return response.strip()\n\n    def _get_table_context(self, query_bundle: QueryBundle) -> str:\n        \"\"\"Get table context.\n\n        Get tables schema + optional context as a single string. Taken from\n        SQLContextContainer.\n\n        \"\"\"\n        if self._sql_context_container.context_str is not None:\n            tables_desc_str = self._sql_context_container.context_str\n        else:\n            table_desc_list = []\n            context_dict = self._sql_context_container.context_dict\n            if context_dict is None:\n                raise ValueError(\n                    \"context_dict must be provided. There is currently no \"\n                    \"table context.\"\n                )\n            for table_desc in context_dict.values():\n                table_desc_list.append(table_desc)\n            tables_desc_str = \"\\n\\n\".join(table_desc_list)\n\n        return tables_desc_str\n\n    def _run_with_sql_only_check(self, sql_query_str: str) -> Tuple[str, Dict]:\n        \"\"\"Don't run sql if sql_only is true, else continue with normal path.\"\"\"\n        if self._sql_only:\n            metadata: Dict[str, Any] = {}\n            raw_response_str = sql_query_str\n        else:\n            raw_response_str, metadata = self._sql_database.run_sql(sql_query_str)\n\n        return raw_response_str, metadata\n\n    def _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n\n        response_str = self._llm.predict(\n            self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,\n            schema=table_desc_str,\n            dialect=self._sql_database.dialect,\n        )\n\n        sql_query_str = self._parse_response_to_sql(response_str)\n        # assume that it's a valid SQL query\n        logger.debug(f\"> Predicted SQL query: {sql_query_str}\")\n\n        raw_response_str, metadata = self._run_with_sql_only_check(sql_query_str)\n\n        metadata[\"sql_query\"] = sql_query_str\n\n        if self._synthesize_response:\n            response_str = self._llm.predict(\n                self._response_synthesis_prompt,\n                query_str=query_bundle.query_str,\n                sql_query=sql_query_str,\n                sql_response_str=raw_response_str,\n            )\n        else:\n            response_str = raw_response_str\n\n        return Response(response=response_str, metadata=metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        table_desc_str = self._get_table_context(query_bundle)\n        logger.info(f\"> Table desc str: {table_desc_str}\")\n\n        response_str = await self._llm.apredict(\n            self._text_to_sql_prompt,\n            query_str=query_bundle.query_str,\n            schema=table_desc_str,\n            dialect=self._sql_database.dialect,\n        )\n\n        sql_query_str = self._parse_response_to_sql(response_str)\n        # assume that it's a valid SQL query\n        logger.debug(f\"> Predicted SQL query: {sql_query_str}\")\n\n        response_str, metadata = self._run_with_sql_only_check(sql_query_str)\n        metadata[\"sql_query\"] = sql_query_str\n        return Response(response=response_str, metadata=metadata)\n\n\ndef _validate_prompt(\n    custom_prompt: BasePromptTemplate,\n    default_prompt: BasePromptTemplate,\n) -> None:\n    \"\"\"Validate prompt.\"\"\"\n    if custom_prompt.template_vars != default_prompt.template_vars:\n        raise ValueError(\n            \"custom_prompt must have the following template variables: \"\n            f\"{default_prompt.template_vars}\"\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql_query.py",
    "filename": "sql_query.py",
    "relpath": "indices/struct_store/sql_query.py",
    "start_line": 315,
    "end_line": 635,
    "length": 321,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "sql_retriever",
      "_format_result_markdown",
      "_query",
      "_aquery",
      "__init__",
      "sql_retriever",
      "__init__",
      "sql_retriever",
      "__init__",
      "sql_retriever"
    ],
    "chunk_class_names": [
      "BaseSQLTableQueryEngine",
      "NLSQLTableQueryEngine",
      "PGVectorSQLQueryEngine",
      "SQLTableRetrieverQueryEngine"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_run_with_sql_only_check",
      "_query",
      "_aquery",
      "__init__",
      "_get_prompt_modules",
      "_parse_response_to_sql",
      "_get_table_context",
      "_run_with_sql_only_check",
      "_query",
      "_aquery",
      "_validate_prompt",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "sql_retriever",
      "_format_result_markdown",
      "_query",
      "_aquery",
      "__init__",
      "sql_retriever",
      "__init__",
      "sql_retriever",
      "__init__",
      "sql_retriever"
    ],
    "document_class_names": [
      "SQLStructStoreQueryEngine",
      "NLStructStoreQueryEngine",
      "BaseSQLTableQueryEngine",
      "NLSQLTableQueryEngine",
      "PGVectorSQLQueryEngine",
      "SQLTableRetrieverQueryEngine"
    ],
    "content": "class BaseSQLTableQueryEngine(BaseQueryEngine):\n    \"\"\"Base SQL Table query engine.\n\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        synthesize_response: bool = True,\n        markdown_response: bool = False,\n        response_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        refine_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        verbose: bool = False,\n        streaming: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._llm = llm or Settings.llm\n        if callback_manager is not None:\n            self._llm.callback_manager = callback_manager\n\n        self._response_synthesis_prompt = (\n            response_synthesis_prompt or DEFAULT_RESPONSE_SYNTHESIS_PROMPT_V2\n        )\n        self._refine_synthesis_prompt = refine_synthesis_prompt or DEFAULT_REFINE_PROMPT\n\n        # do some basic prompt validation\n        _validate_prompt(\n            self._response_synthesis_prompt, DEFAULT_RESPONSE_SYNTHESIS_PROMPT_V2\n        )\n        _validate_prompt(self._refine_synthesis_prompt, DEFAULT_REFINE_PROMPT)\n\n        self._synthesize_response = synthesize_response\n        self._markdown_response = markdown_response\n        self._verbose = verbose\n        self._streaming = streaming\n        super().__init__(callback_manager=callback_manager or Settings.callback_manager)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\"response_synthesis_prompt\": self._response_synthesis_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"response_synthesis_prompt\" in prompts:\n            self._response_synthesis_prompt = prompts[\"response_synthesis_prompt\"]\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {\"sql_retriever\": self.sql_retriever}\n\n    @property\n    @abstractmethod\n    def sql_retriever(self) -> NLSQLRetriever:\n        \"\"\"Get SQL retriever.\"\"\"\n\n    def _format_result_markdown(self, retrieved_nodes: List[NodeWithScore]) -> str:\n        \"\"\"Format the result in markdown.\"\"\"\n        tables = []\n        for node_with_score in retrieved_nodes:\n            node = node_with_score.node\n            metadata = node.metadata\n\n            col_keys = metadata.get(\"col_keys\", [])\n            results = metadata.get(\"result\", [])\n            table_header = \"| \" + \" | \".join(col_keys) + \" |\\n\"\n            table_separator = \"|\" + \"|\".join([\"---\"] * len(col_keys)) + \"|\\n\"\n\n            table_rows = \"\"\n            for row in results:\n                table_rows += \"| \" + \" | \".join(str(item) for item in row) + \" |\\n\"\n\n            markdown_table = table_header + table_separator + table_rows\n            tables.append(markdown_table)\n\n        return \"\\n\\n\".join(tables).strip()\n\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        retrieved_nodes, metadata = self.sql_retriever.retrieve_with_metadata(\n            query_bundle\n        )\n\n        sql_query_str = metadata[\"sql_query\"]\n        if self._synthesize_response:\n            partial_synthesis_prompt = self._response_synthesis_prompt.partial_format(\n                sql_query=sql_query_str,\n            )\n            response_synthesizer = get_response_synthesizer(\n                llm=self._llm,\n                callback_manager=self.callback_manager,\n                text_qa_template=partial_synthesis_prompt,\n                refine_template=self._refine_synthesis_prompt,\n                verbose=self._verbose,\n                streaming=self._streaming,\n            )\n            response = response_synthesizer.synthesize(\n                query=query_bundle.query_str,\n                nodes=retrieved_nodes,\n            )\n            cast(Dict, response.metadata).update(metadata)\n            if self._streaming:\n                return cast(StreamingResponse, response)\n            return cast(Response, response)\n        else:\n            if self._markdown_response:\n                response_str = self._format_result_markdown(retrieved_nodes)\n            else:\n                response_str = \"\\n\".join([node.text for node in retrieved_nodes])\n            return Response(response=response_str, metadata=metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        \"\"\"Answer a query.\"\"\"\n        retrieved_nodes, metadata = await self.sql_retriever.aretrieve_with_metadata(\n            query_bundle\n        )\n\n        sql_query_str = metadata[\"sql_query\"]\n        if self._synthesize_response:\n            partial_synthesis_prompt = self._response_synthesis_prompt.partial_format(\n                sql_query=sql_query_str,\n            )\n\n            response_synthesizer = get_response_synthesizer(\n                llm=self._llm,\n                callback_manager=self.callback_manager,\n                text_qa_template=partial_synthesis_prompt,\n                refine_template=self._refine_synthesis_prompt,\n                verbose=self._verbose,\n                streaming=self._streaming,\n            )\n            response = await response_synthesizer.asynthesize(\n                query=query_bundle.query_str,\n                nodes=retrieved_nodes,\n            )\n            cast(Dict, response.metadata).update(metadata)\n            if self._streaming:\n                return cast(AsyncStreamingResponse, response)\n            return cast(Response, response)\n        else:\n            response_str = \"\\n\".join([node.text for node in retrieved_nodes])\n            return Response(response=response_str, metadata=metadata)\n\n\nclass NLSQLTableQueryEngine(BaseSQLTableQueryEngine):\n    \"\"\"\n    Natural language SQL Table query engine.\n\n    Read NLStructStoreQueryEngine's docstring for more info on NL SQL.\n\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        llm: Optional[LLM] = None,\n        text_to_sql_prompt: Optional[BasePromptTemplate] = None,\n        context_query_kwargs: Optional[dict] = None,\n        synthesize_response: bool = True,\n        markdown_response: bool = False,\n        response_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        refine_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        tables: Optional[Union[List[str], List[Table]]] = None,\n        context_str_prefix: Optional[str] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        sql_only: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        # self._tables = tables\n        self._sql_retriever = NLSQLRetriever(\n            sql_database,\n            llm=llm,\n            text_to_sql_prompt=text_to_sql_prompt,\n            context_query_kwargs=context_query_kwargs,\n            tables=tables,\n            context_str_prefix=context_str_prefix,\n            embed_model=embed_model,\n            sql_only=sql_only,\n            callback_manager=callback_manager,\n            verbose=verbose,\n        )\n        super().__init__(\n            synthesize_response=synthesize_response,\n            markdown_response=markdown_response,\n            response_synthesis_prompt=response_synthesis_prompt,\n            refine_synthesis_prompt=refine_synthesis_prompt,\n            llm=llm,\n            callback_manager=callback_manager,\n            verbose=verbose,\n            **kwargs,\n        )\n\n    @property\n    def sql_retriever(self) -> NLSQLRetriever:\n        \"\"\"Get SQL retriever.\"\"\"\n        return self._sql_retriever\n\n\nclass PGVectorSQLQueryEngine(BaseSQLTableQueryEngine):\n    \"\"\"PGvector SQL query engine.\n\n    A modified version of the normal text-to-SQL query engine because\n    we can infer embedding vectors in the sql query.\n\n    NOTE: this is a beta feature\n\n    NOTE: Any Text-to-SQL application should be aware that executing\n    arbitrary SQL queries can be a security risk. It is recommended to\n    take precautions as needed, such as using restricted roles, read-only\n    databases, sandboxing, etc.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        llm: Optional[LLM] = None,\n        text_to_sql_prompt: Optional[BasePromptTemplate] = None,\n        context_query_kwargs: Optional[dict] = None,\n        synthesize_response: bool = True,\n        response_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        refine_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        tables: Optional[Union[List[str], List[Table]]] = None,\n        context_str_prefix: Optional[str] = None,\n        sql_only: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        text_to_sql_prompt = text_to_sql_prompt or DEFAULT_TEXT_TO_SQL_PGVECTOR_PROMPT\n        self._sql_retriever = NLSQLRetriever(\n            sql_database,\n            llm=llm,\n            text_to_sql_prompt=text_to_sql_prompt,\n            context_query_kwargs=context_query_kwargs,\n            tables=tables,\n            sql_parser_mode=SQLParserMode.PGVECTOR,\n            context_str_prefix=context_str_prefix,\n            sql_only=sql_only,\n            callback_manager=callback_manager,\n        )\n        super().__init__(\n            synthesize_response=synthesize_response,\n            response_synthesis_prompt=response_synthesis_prompt,\n            refine_synthesis_prompt=refine_synthesis_prompt,\n            llm=llm,\n            callback_manager=callback_manager,\n            **kwargs,\n        )\n\n    @property\n    def sql_retriever(self) -> NLSQLRetriever:\n        \"\"\"Get SQL retriever.\"\"\"\n        return self._sql_retriever\n\n\nclass SQLTableRetrieverQueryEngine(BaseSQLTableQueryEngine):\n    \"\"\"SQL Table retriever query engine.\"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        table_retriever: ObjectRetriever[SQLTableSchema],\n        rows_retrievers: Optional[dict[str, BaseRetriever]] = None,\n        llm: Optional[LLM] = None,\n        text_to_sql_prompt: Optional[BasePromptTemplate] = None,\n        context_query_kwargs: Optional[dict] = None,\n        synthesize_response: bool = True,\n        response_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        refine_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        context_str_prefix: Optional[str] = None,\n        sql_only: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._sql_retriever = NLSQLRetriever(\n            sql_database,\n            llm=llm,\n            text_to_sql_prompt=text_to_sql_prompt,\n            context_query_kwargs=context_query_kwargs,\n            table_retriever=table_retriever,\n            rows_retrievers=rows_retrievers,\n            context_str_prefix=context_str_prefix,\n            sql_only=sql_only,\n            callback_manager=callback_manager,\n            verbose=kwargs.get(\"verbose\", False),\n        )\n        super().__init__(\n            synthesize_response=synthesize_response,\n            response_synthesis_prompt=response_synthesis_prompt,\n            refine_synthesis_prompt=refine_synthesis_prompt,\n            llm=llm,\n            callback_manager=callback_manager,\n            **kwargs,\n        )\n\n    @property\n    def sql_retriever(self) -> NLSQLRetriever:\n        \"\"\"Get SQL retriever.\"\"\"\n        return self._sql_retriever\n\n\n# legacy\nGPTNLStructStoreQueryEngine = NLStructStoreQueryEngine\nGPTSQLStructStoreQueryEngine = SQLStructStoreQueryEngine"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/struct_store/__init__.py",
    "start_line": 1,
    "end_line": 36,
    "length": 36,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Structured store indices.\"\"\"\n\nfrom llama_index.core.indices.struct_store.json_query import JSONQueryEngine\nfrom llama_index.core.indices.struct_store.pandas import (\n    GPTPandasIndex,\n    PandasIndex,\n)\nfrom llama_index.core.indices.struct_store.sql import (\n    GPTSQLStructStoreIndex,\n    SQLContextContainerBuilder,\n    SQLStructStoreIndex,\n)\nfrom llama_index.core.indices.struct_store.sql_query import (\n    GPTNLStructStoreQueryEngine,\n    GPTSQLStructStoreQueryEngine,\n    NLSQLTableQueryEngine,\n    NLStructStoreQueryEngine,\n    SQLStructStoreQueryEngine,\n    SQLTableRetrieverQueryEngine,\n)\n\n__all__ = [\n    \"SQLStructStoreIndex\",\n    \"SQLContextContainerBuilder\",\n    \"PandasIndex\",\n    \"NLStructStoreQueryEngine\",\n    \"SQLStructStoreQueryEngine\",\n    \"JSONQueryEngine\",\n    # legacy\n    \"GPTSQLStructStoreIndex\",\n    \"GPTPandasIndex\",\n    \"GPTNLStructStoreQueryEngine\",\n    \"GPTSQLStructStoreQueryEngine\",\n    \"SQLTableRetrieverQueryEngine\",\n    \"NLSQLTableQueryEngine\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/container_builder.py",
    "filename": "container_builder.py",
    "relpath": "indices/struct_store/container_builder.py",
    "start_line": 1,
    "end_line": 158,
    "length": 158,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_documents",
      "_build_context_from_sql_database",
      "_get_context_dict",
      "derive_index_from_context",
      "query_index_for_context",
      "build_context_container"
    ],
    "chunk_class_names": [
      "SQLContextContainerBuilder"
    ],
    "document_function_names": [
      "__init__",
      "from_documents",
      "_build_context_from_sql_database",
      "_get_context_dict",
      "derive_index_from_context",
      "query_index_for_context",
      "build_context_container"
    ],
    "document_class_names": [
      "SQLContextContainerBuilder"
    ],
    "content": "\"\"\"SQL Container builder.\"\"\"\n\n\nfrom typing import Any, Dict, List, Optional, Type\n\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.common.struct_store.base import (\n    SQLDocumentContextBuilder,\n)\nfrom llama_index.core.indices.common.struct_store.schema import SQLContextContainer\nfrom llama_index.core.readers.base import Document\nfrom llama_index.core.schema import BaseNode, QueryType\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\n\nDEFAULT_CONTEXT_QUERY_TMPL = (\n    \"Please return the relevant tables (including the full schema) \"\n    \"for the following query: {orig_query_str}\"\n)\n\n\nclass SQLContextContainerBuilder:\n    \"\"\"SQLContextContainerBuilder.\n\n    Build a SQLContextContainer that can be passed to the SQL index\n    during index construction or during query-time.\n\n    NOTE: if context_str is specified, that will be used as context\n    instead of context_dict\n\n    Args:\n        sql_database (SQLDatabase): SQL database\n        context_dict (Optional[Dict[str, str]]): context dict\n\n    \"\"\"\n\n    def __init__(\n        self,\n        sql_database: SQLDatabase,\n        context_dict: Optional[Dict[str, str]] = None,\n        context_str: Optional[str] = None,\n    ):\n        \"\"\"Initialize params.\"\"\"\n        self.sql_database = sql_database\n\n        # if context_dict provided, validate that all keys are valid table names\n        if context_dict is not None:\n            # validate context_dict keys are valid table names\n            context_keys = set(context_dict.keys())\n            if not context_keys.issubset(\n                set(self.sql_database.get_usable_table_names())\n            ):\n                raise ValueError(\n                    \"Invalid context table names: \"\n                    f\"{context_keys - set(self.sql_database.get_usable_table_names())}\"\n                )\n        self.context_dict = context_dict or {}\n        # build full context from sql_database\n        self.full_context_dict = self._build_context_from_sql_database(\n            self.sql_database, current_context=self.context_dict\n        )\n        self.context_str = context_str\n\n    @classmethod\n    def from_documents(\n        cls,\n        documents_dict: Dict[str, List[BaseNode]],\n        sql_database: SQLDatabase,\n        **context_builder_kwargs: Any,\n    ) -> \"SQLContextContainerBuilder\":\n        \"\"\"Build context from documents.\"\"\"\n        context_builder = SQLDocumentContextBuilder(\n            sql_database, **context_builder_kwargs\n        )\n        context_dict = context_builder.build_all_context_from_documents(documents_dict)\n        return SQLContextContainerBuilder(sql_database, context_dict=context_dict)\n\n    def _build_context_from_sql_database(\n        self,\n        sql_database: SQLDatabase,\n        current_context: Optional[Dict[str, str]] = None,\n    ) -> Dict[str, str]:\n        \"\"\"Get tables schema + optional context as a single string.\"\"\"\n        current_context = current_context or {}\n        result_context = {}\n        for table_name in sql_database.get_usable_table_names():\n            table_desc = sql_database.get_single_table_info(table_name)\n            table_text = f\"Schema of table {table_name}:\\n\" f\"{table_desc}\\n\"\n            if table_name in current_context:\n                table_text += f\"Context of table {table_name}:\\n\"\n                table_text += current_context[table_name]\n            result_context[table_name] = table_text\n        return result_context\n\n    def _get_context_dict(self, ignore_db_schema: bool) -> Dict[str, str]:\n        \"\"\"Get full context dict.\"\"\"\n        if ignore_db_schema:\n            return self.context_dict\n        else:\n            return self.full_context_dict\n\n    def derive_index_from_context(\n        self,\n        index_cls: Type[BaseIndex],\n        ignore_db_schema: bool = False,\n        **index_kwargs: Any,\n    ) -> BaseIndex:\n        \"\"\"Derive index from context.\"\"\"\n        full_context_dict = self._get_context_dict(ignore_db_schema)\n        context_docs = []\n        for table_name, context_str in full_context_dict.items():\n            doc = Document(text=context_str, metadata={\"table_name\": table_name})\n            context_docs.append(doc)\n        return index_cls.from_documents(\n            documents=context_docs,\n            **index_kwargs,\n        )\n\n    def query_index_for_context(\n        self,\n        index: BaseIndex,\n        query_str: QueryType,\n        query_tmpl: Optional[str] = DEFAULT_CONTEXT_QUERY_TMPL,\n        store_context_str: bool = True,\n        **index_kwargs: Any,\n    ) -> str:\n        \"\"\"Query index for context.\n\n        A simple wrapper around the index.query call which\n        injects a query template to specifically fetch table information,\n        and can store a context_str.\n\n        Args:\n            index (BaseIndex): index data structure\n            query_str (QueryType): query string\n            query_tmpl (Optional[str]): query template\n            store_context_str (bool): store context_str\n\n        \"\"\"\n        if query_tmpl is None:\n            context_query_str = query_str\n        else:\n            context_query_str = query_tmpl.format(orig_query_str=query_str)\n        query_engine = index.as_query_engine()\n        response = query_engine.query(context_query_str)\n        context_str = str(response)\n        if store_context_str:\n            self.context_str = context_str\n        return context_str\n\n    def build_context_container(\n        self, ignore_db_schema: bool = False\n    ) -> SQLContextContainer:\n        \"\"\"Build index structure.\"\"\"\n        full_context_dict = self._get_context_dict(ignore_db_schema)\n        return SQLContextContainer(\n            context_str=self.context_str,\n            context_dict=full_context_dict,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/json_query.py",
    "filename": "json_query.py",
    "relpath": "indices/struct_store/json_query.py",
    "start_line": 1,
    "end_line": 247,
    "length": 247,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "default_output_response_parser",
      "default_output_processor",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "_get_schema_context",
      "_query",
      "_aquery"
    ],
    "chunk_class_names": [
      "JSONQueryEngine"
    ],
    "document_function_names": [
      "default_output_response_parser",
      "default_output_processor",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "_get_schema_context",
      "_query",
      "_aquery"
    ],
    "document_class_names": [
      "JSONQueryEngine"
    ],
    "content": "import json\nimport logging\nimport re\nfrom typing import Any, Callable, Dict, List, Optional, Union\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_JSON_PATH_PROMPT\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixinType\nfrom llama_index.core.prompts.prompt_type import PromptType\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utils import print_text\n\nlogger = logging.getLogger(__name__)\nIMPORT_ERROR_MSG = (\n    \"`jsonpath_ng` package not found, please run `pip install jsonpath-ng`\"\n)\n\nJSONType = Union[Dict[str, \"JSONType\"], List[\"JSONType\"], str, int, float, bool, None]\n\n\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (\n    \"Given a query, synthesize a response \"\n    \"to satisfy the query using the JSON results. \"\n    \"Only include details that are relevant to the query. \"\n    \"If you don't know the answer, then say that.\\n\"\n    \"JSON Schema: {json_schema}\\n\"\n    \"JSON Path: {json_path}\\n\"\n    \"Value at path: {json_path_value}\\n\"\n    \"Query: {query_str}\\n\"\n    \"Response: \"\n)\nDEFAULT_RESPONSE_SYNTHESIS_PROMPT = PromptTemplate(\n    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL,\n    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS,\n)\n\n\ndef default_output_response_parser(llm_output: str) -> str:\n    \"\"\"Attempts to parse the JSON path prompt output. Only applicable if the default prompt is used.\"\"\"\n    try:\n        llm_output_parsed = re.search(  # type: ignore\n            pattern=r\"JSONPath:\\s+(.*)\", string=llm_output\n        ).groups()[0]\n    except Exception:\n        logger.warning(\n            f\"JSON Path could not be parsed in the LLM response after the 'JSONPath' identifier. \"\n            \"Try passing a custom JSON path prompt and processor. \"\n            \"Proceeding with output as-is.\"\n        )\n        return llm_output\n\n    return llm_output_parsed\n\n\ndef default_output_processor(llm_output: str, json_value: JSONType) -> Dict[str, str]:\n    \"\"\"Default output processor that extracts values based on JSON Path expressions.\"\"\"\n    # Post-process the LLM output to remove the JSONPath: prefix\n    llm_output = llm_output.replace(\"JSONPath: \", \"\").replace(\"JSON Path: \", \"\").strip()\n\n    # Split the given string into separate JSON Path expressions\n    expressions = [expr.strip() for expr in llm_output.split(\",\")]\n\n    try:\n        from jsonpath_ng.ext import parse  # pants: no-infer-dep\n        from jsonpath_ng.jsonpath import DatumInContext  # pants: no-infer-dep\n    except ImportError as exc:\n        IMPORT_ERROR_MSG = \"You need to install jsonpath-ng to use this function!\"\n        raise ImportError(IMPORT_ERROR_MSG) from exc\n\n    results: Dict[str, str] = {}\n\n    for expression in expressions:\n        try:\n            datum: List[DatumInContext] = parse(expression).find(json_value)\n            if datum:\n                key = expression.split(\".\")[\n                    -1\n                ]  # Extracting \"title\" from \"$.title\", for example\n                results[key] = \", \".join(str(i.value) for i in datum)\n        except Exception as exc:\n            raise ValueError(f\"Invalid JSON Path: {expression}\") from exc\n\n    return results\n\n\nclass JSONQueryEngine(BaseQueryEngine):\n    \"\"\"GPT JSON Query Engine.\n\n    Converts natural language to JSON Path queries.\n\n    Args:\n        json_value (JSONType): JSON value\n        json_schema (JSONType): JSON schema\n        json_path_prompt (BasePromptTemplate): The JSON Path prompt to use.\n        output_processor (Callable): The output processor that executes the\n            JSON Path query.\n        output_kwargs (dict): Additional output processor kwargs for the\n            output_processor function.\n        verbose (bool): Whether to print verbose output.\n    \"\"\"\n\n    def __init__(\n        self,\n        json_value: JSONType,\n        json_schema: JSONType,\n        llm: Optional[LLM] = None,\n        json_path_prompt: Optional[BasePromptTemplate] = None,\n        output_processor: Optional[Callable] = None,\n        output_kwargs: Optional[dict] = None,\n        synthesize_response: bool = True,\n        response_synthesis_prompt: Optional[BasePromptTemplate] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._json_value = json_value\n        self._json_schema = json_schema\n        self._llm = llm or Settings.llm\n        self._json_path_prompt = json_path_prompt or DEFAULT_JSON_PATH_PROMPT\n        self._output_processor = output_processor or default_output_processor\n        self._output_kwargs = output_kwargs or {}\n        self._verbose = verbose\n        self._synthesize_response = synthesize_response\n        self._response_synthesis_prompt = (\n            response_synthesis_prompt or DEFAULT_RESPONSE_SYNTHESIS_PROMPT\n        )\n\n        super().__init__(callback_manager=Settings.callback_manager)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"json_path_prompt\": self._json_path_prompt,\n            \"response_synthesis_prompt\": self._response_synthesis_prompt,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"json_path_prompt\" in prompts:\n            self._json_path_prompt = prompts[\"json_path_prompt\"]\n        if \"response_synthesis_prompt\" in prompts:\n            self._response_synthesis_prompt = prompts[\"response_synthesis_prompt\"]\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {}\n\n    def _get_schema_context(self) -> str:\n        \"\"\"Get JSON schema context.\"\"\"\n        return json.dumps(self._json_schema)\n\n    def _query(self, query_bundle: QueryBundle) -> Response:\n        \"\"\"Answer a query.\"\"\"\n        schema = self._get_schema_context()\n\n        json_path_response_str = self._llm.predict(\n            self._json_path_prompt,\n            schema=schema,\n            query_str=query_bundle.query_str,\n        )\n\n        if self._verbose:\n            print_text(\n                f\"> JSONPath Instructions:\\n\" f\"```\\n{json_path_response_str}\\n```\\n\"\n            )\n\n        json_path_output = self._output_processor(\n            json_path_response_str,\n            self._json_value,\n            **self._output_kwargs,\n        )\n\n        # removes JSONPath: prefix from returned JSON path prompt call\n        if self._json_path_prompt == DEFAULT_JSON_PATH_PROMPT:\n            json_path_response_str = default_output_response_parser(\n                json_path_response_str\n            )\n\n        if self._verbose:\n            print_text(f\"> JSONPath Output: {json_path_output}\\n\")\n\n        if self._synthesize_response:\n            response_str = self._llm.predict(\n                self._response_synthesis_prompt,\n                query_str=query_bundle.query_str,\n                json_schema=self._json_schema,\n                json_path=json_path_response_str,\n                json_path_value=json_path_output,\n            )\n        else:\n            response_str = json.dumps(json_path_output)\n\n        response_metadata = {\n            \"json_path_response_str\": json_path_response_str,\n        }\n\n        return Response(response=response_str, metadata=response_metadata)\n\n    async def _aquery(self, query_bundle: QueryBundle) -> Response:\n        schema = self._get_schema_context()\n\n        json_path_response_str = await self._llm.apredict(\n            self._json_path_prompt,\n            schema=schema,\n            query_str=query_bundle.query_str,\n        )\n\n        # removes JSONPath: prefix from returned JSON path prompt call\n        if self._json_path_prompt == DEFAULT_JSON_PATH_PROMPT:\n            json_path_response_str = default_output_response_parser(\n                json_path_response_str\n            )\n\n        if self._verbose:\n            print_text(\n                f\"> JSONPath Instructions:\\n\" f\"```\\n{json_path_response_str}\\n```\\n\"\n            )\n\n        json_path_output = self._output_processor(\n            json_path_response_str,\n            self._json_value,\n            **self._output_kwargs,\n        )\n\n        if self._verbose:\n            print_text(f\"> JSONPath Output: {json_path_output}\\n\")\n\n        if self._synthesize_response:\n            response_str = await self._llm.apredict(\n                self._response_synthesis_prompt,\n                query_str=query_bundle.query_str,\n                json_schema=self._json_schema,\n                json_path=json_path_response_str,\n                json_path_value=json_path_output,\n            )\n        else:\n            response_str = json.dumps(json_path_output)\n\n        response_metadata = {\n            \"json_path_response_str\": json_path_response_str,\n        }\n\n        return Response(response=response_str, metadata=response_metadata)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/struct_store/sql.py",
    "filename": "sql.py",
    "relpath": "indices/struct_store/sql.py",
    "start_line": 1,
    "end_line": 167,
    "length": 167,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "ref_doc_id_column",
      "_build_index_from_nodes",
      "_insert",
      "as_retriever",
      "as_query_engine"
    ],
    "chunk_class_names": [
      "SQLQueryMode",
      "SQLStructStoreIndex"
    ],
    "document_function_names": [
      "__init__",
      "ref_doc_id_column",
      "_build_index_from_nodes",
      "_insert",
      "as_retriever",
      "as_query_engine"
    ],
    "document_class_names": [
      "SQLQueryMode",
      "SQLStructStoreIndex"
    ],
    "content": "\"\"\"SQL Structured Store.\"\"\"\n\nfrom collections import defaultdict\nfrom enum import Enum\nfrom typing import Any, Optional, Sequence, Union\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.data_structs.table import SQLStructTable\nfrom llama_index.core.indices.common.struct_store.schema import SQLContextContainer\nfrom llama_index.core.indices.common.struct_store.sql import (\n    SQLStructDatapointExtractor,\n)\nfrom llama_index.core.indices.struct_store.base import BaseStructStoreIndex\nfrom llama_index.core.indices.struct_store.container_builder import (\n    SQLContextContainerBuilder,\n)\nfrom llama_index.core.llms.utils import LLMType\nfrom llama_index.core.schema import BaseNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utilities.sql_wrapper import SQLDatabase\nfrom sqlalchemy import Table\n\n\nclass SQLQueryMode(str, Enum):\n    SQL = \"sql\"\n    NL = \"nl\"\n\n\nclass SQLStructStoreIndex(BaseStructStoreIndex[SQLStructTable]):\n    \"\"\"SQL Struct Store Index.\n\n    The SQLStructStoreIndex is an index that uses a SQL database\n    under the hood. During index construction, the data can be inferred\n    from unstructured documents given a schema extract prompt,\n    or it can be pre-loaded in the database.\n\n    During query time, the user can either specify a raw SQL query\n    or a natural language query to retrieve their data.\n\n    NOTE: this is deprecated.\n\n    Args:\n        documents (Optional[Sequence[DOCUMENTS_INPUT]]): Documents to index.\n            NOTE: in the SQL index, this is an optional field.\n        sql_database (Optional[SQLDatabase]): SQL database to use,\n            including table names to specify.\n            See :ref:`Ref-Struct-Store` for more details.\n        table_name (Optional[str]): Name of the table to use\n            for extracting data.\n            Either table_name or table must be specified.\n        table (Optional[Table]): SQLAlchemy Table object to use.\n            Specifying the Table object explicitly, instead of\n            the table name, allows you to pass in a view.\n            Either table_name or table must be specified.\n        sql_context_container (Optional[SQLContextContainer]): SQL context container.\n            an be generated from a SQLContextContainerBuilder.\n            See :ref:`Ref-Struct-Store` for more details.\n\n    \"\"\"\n\n    index_struct_cls = SQLStructTable\n\n    def __init__(\n        self,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        index_struct: Optional[SQLStructTable] = None,\n        sql_database: Optional[SQLDatabase] = None,\n        table_name: Optional[str] = None,\n        table: Optional[Table] = None,\n        ref_doc_id_column: Optional[str] = None,\n        sql_context_container: Optional[SQLContextContainer] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        if sql_database is None:\n            raise ValueError(\"sql_database must be specified\")\n        self.sql_database = sql_database\n        # needed here for data extractor\n        self._ref_doc_id_column = ref_doc_id_column\n        self._table_name = table_name\n        self._table = table\n\n        # if documents aren't specified, pass in a blank []\n        if index_struct is None:\n            nodes = nodes or []\n\n        super().__init__(\n            nodes=nodes,\n            index_struct=index_struct,\n            **kwargs,\n        )\n\n        # TODO: index_struct context_dict is deprecated,\n        # we're migrating storage of information to here.\n        if sql_context_container is None:\n            container_builder = SQLContextContainerBuilder(sql_database)\n            sql_context_container = container_builder.build_context_container()\n        self.sql_context_container = sql_context_container\n\n    @property\n    def ref_doc_id_column(self) -> Optional[str]:\n        return self._ref_doc_id_column\n\n    def _build_index_from_nodes(\n        self, nodes: Sequence[BaseNode], **build_kwargs: Any\n    ) -> SQLStructTable:\n        \"\"\"Build index from nodes.\"\"\"\n        index_struct = self.index_struct_cls()\n        if len(nodes) == 0:\n            return index_struct\n        else:\n            data_extractor = SQLStructDatapointExtractor(\n                Settings.llm,\n                self.schema_extract_prompt,\n                self.output_parser,\n                self.sql_database,\n                table_name=self._table_name,\n                table=self._table,\n                ref_doc_id_column=self._ref_doc_id_column,\n            )\n            # group nodes by ids\n            source_to_node = defaultdict(list)\n            for node in nodes:\n                source_to_node[node.ref_doc_id].append(node)\n\n            for node_set in source_to_node.values():\n                data_extractor.insert_datapoint_from_nodes(node_set)\n        return index_struct\n\n    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n        \"\"\"Insert a document.\"\"\"\n        data_extractor = SQLStructDatapointExtractor(\n            Settings.llm,\n            self.schema_extract_prompt,\n            self.output_parser,\n            self.sql_database,\n            table_name=self._table_name,\n            table=self._table,\n            ref_doc_id_column=self._ref_doc_id_column,\n        )\n        data_extractor.insert_datapoint_from_nodes(nodes)\n\n    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\n        raise NotImplementedError(\"Not supported\")\n\n    def as_query_engine(\n        self,\n        llm: Optional[LLMType] = None,\n        query_mode: Union[str, SQLQueryMode] = SQLQueryMode.NL,\n        **kwargs: Any,\n    ) -> BaseQueryEngine:\n        # NOTE: lazy import\n        from llama_index.core.indices.struct_store.sql_query import (\n            NLStructStoreQueryEngine,\n            SQLStructStoreQueryEngine,\n        )\n\n        if query_mode == SQLQueryMode.NL:\n            return NLStructStoreQueryEngine(self, **kwargs)\n        elif query_mode == SQLQueryMode.SQL:\n            return SQLStructStoreQueryEngine(self, **kwargs)\n        else:\n            raise ValueError(f\"Unknown query mode: {query_mode}\")\n\n\nGPTSQLStructStoreIndex = SQLStructStoreIndex"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/composability/__init__.py",
    "filename": "__init__.py",
    "relpath": "indices/composability/__init__.py",
    "start_line": 1,
    "end_line": 6,
    "length": 6,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"This module contains all classes used for composing graphs over indices.\"\"\"\n\n\nfrom llama_index.core.indices.composability.graph import ComposableGraph\n\n__all__ = [\"ComposableGraph\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/indices/composability/graph.py",
    "filename": "graph.py",
    "relpath": "indices/composability/graph.py",
    "start_line": 1,
    "end_line": 127,
    "length": 127,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "root_id",
      "all_indices",
      "root_index",
      "index_struct",
      "from_indices",
      "get_index",
      "as_query_engine"
    ],
    "chunk_class_names": [
      "ComposableGraph",
      "as"
    ],
    "document_function_names": [
      "__init__",
      "root_id",
      "all_indices",
      "root_index",
      "index_struct",
      "from_indices",
      "get_index",
      "as_query_engine"
    ],
    "document_class_names": [
      "ComposableGraph",
      "as"
    ],
    "content": "\"\"\"Composability graphs.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Sequence, Type, cast\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.data_structs.data_structs import IndexStruct\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.schema import (\n    IndexNode,\n    NodeRelationship,\n    ObjectType,\n    RelatedNodeInfo,\n)\nfrom llama_index.core.storage.storage_context import StorageContext\n\n\nclass ComposableGraph:\n    \"\"\"Composable graph.\"\"\"\n\n    def __init__(\n        self,\n        all_indices: Dict[str, BaseIndex],\n        root_id: str,\n        storage_context: Optional[StorageContext] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._all_indices = all_indices\n        self._root_id = root_id\n        self.storage_context = storage_context\n\n    @property\n    def root_id(self) -> str:\n        return self._root_id\n\n    @property\n    def all_indices(self) -> Dict[str, BaseIndex]:\n        return self._all_indices\n\n    @property\n    def root_index(self) -> BaseIndex:\n        return self._all_indices[self._root_id]\n\n    @property\n    def index_struct(self) -> IndexStruct:\n        return self._all_indices[self._root_id].index_struct\n\n    @classmethod\n    def from_indices(\n        cls,\n        root_index_cls: Type[BaseIndex],\n        children_indices: Sequence[BaseIndex],\n        index_summaries: Optional[Sequence[str]] = None,\n        storage_context: Optional[StorageContext] = None,\n        **kwargs: Any,\n    ) -> \"ComposableGraph\":  # type: ignore\n        \"\"\"Create composable graph using this index class as the root.\"\"\"\n        from llama_index.core import Settings\n\n        with Settings.callback_manager.as_trace(\"graph_construction\"):\n            if index_summaries is None:\n                for index in children_indices:\n                    if index.index_struct.summary is None:\n                        raise ValueError(\n                            \"Summary must be set for children indices. \"\n                            \"If the index does a summary \"\n                            \"(through index.index_struct.summary), then \"\n                            \"it must be specified with then `index_summaries` \"\n                            \"argument in this function. We will support \"\n                            \"automatically setting the summary in the future.\"\n                        )\n                index_summaries = [\n                    index.index_struct.summary for index in children_indices\n                ]\n            else:\n                # set summaries for each index\n                for index, summary in zip(children_indices, index_summaries):\n                    index.index_struct.summary = summary\n\n            if len(children_indices) != len(index_summaries):\n                raise ValueError(\"indices and index_summaries must have same length!\")\n\n            # construct index nodes\n            index_nodes = []\n            for index, summary in zip(children_indices, index_summaries):\n                assert isinstance(index.index_struct, IndexStruct)\n                index_node = IndexNode(\n                    text=summary,\n                    index_id=index.index_id,\n                    relationships={\n                        NodeRelationship.SOURCE: RelatedNodeInfo(\n                            node_id=index.index_id, node_type=ObjectType.INDEX\n                        )\n                    },\n                )\n                index_nodes.append(index_node)\n\n            # construct root index\n            root_index = root_index_cls(\n                nodes=index_nodes,\n                storage_context=storage_context,\n                **kwargs,\n            )\n            # type: ignore\n            all_indices: List[BaseIndex] = [\n                *cast(List[BaseIndex], children_indices),\n                root_index,\n            ]\n\n            return cls(\n                all_indices={index.index_id: index for index in all_indices},\n                root_id=root_index.index_id,\n                storage_context=storage_context,\n            )\n\n    def get_index(self, index_struct_id: Optional[str] = None) -> BaseIndex:\n        \"\"\"Get index from index struct id.\"\"\"\n        if index_struct_id is None:\n            index_struct_id = self._root_id\n        return self._all_indices[index_struct_id]\n\n    def as_query_engine(self, **kwargs: Any) -> BaseQueryEngine:\n        # NOTE: lazy import\n        from llama_index.core.query_engine.graph_query_engine import (\n            ComposableGraphQueryEngine,\n        )\n\n        return ComposableGraphQueryEngine(self, **kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/base.py",
    "filename": "base.py",
    "relpath": "callbacks/base.py",
    "start_line": 1,
    "end_line": 301,
    "length": 301,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "on_event_start",
      "on_event_end",
      "add_handler",
      "remove_handler",
      "set_handlers",
      "event",
      "as_trace",
      "start_trace",
      "end_trace",
      "_reset_trace_events",
      "trace_map",
      "__get_pydantic_core_schema__",
      "__get_pydantic_json_schema__",
      "__init__",
      "on_start",
      "on_end"
    ],
    "chunk_class_names": [
      "CallbackManager",
      "EventContext"
    ],
    "document_function_names": [
      "__init__",
      "on_event_start",
      "on_event_end",
      "add_handler",
      "remove_handler",
      "set_handlers",
      "event",
      "as_trace",
      "start_trace",
      "end_trace",
      "_reset_trace_events",
      "trace_map",
      "__get_pydantic_core_schema__",
      "__get_pydantic_json_schema__",
      "__init__",
      "on_start",
      "on_end"
    ],
    "document_class_names": [
      "CallbackManager",
      "EventContext"
    ],
    "content": "import logging\nimport uuid\nfrom abc import ABC\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom typing import Any, Dict, Generator, List, Optional, cast, Type\n\nfrom llama_index.core.callbacks.base_handler import BaseCallbackHandler\nfrom llama_index.core.callbacks.schema import (\n    BASE_TRACE_EVENT,\n    LEAF_EVENTS,\n    CBEventType,\n    EventPayload,\n)\nfrom llama_index.core.bridge.pydantic import (\n    GetCoreSchemaHandler,\n    GetJsonSchemaHandler,\n)\nfrom llama_index.core.bridge.pydantic_core import CoreSchema, core_schema\n\nlogger = logging.getLogger(__name__)\nglobal_stack_trace = ContextVar(\"trace\", default=[BASE_TRACE_EVENT])\nempty_trace_ids: List[str] = []\nglobal_stack_trace_ids = ContextVar(\"trace_ids\", default=empty_trace_ids)\n\n\nclass CallbackManager(BaseCallbackHandler, ABC):\n    \"\"\"\n    Callback manager that handles callbacks for events within LlamaIndex.\n\n    The callback manager provides a way to call handlers on event starts/ends.\n\n    Additionally, the callback manager traces the current stack of events.\n    It does this by using a few key attributes.\n    - trace_stack - The current stack of events that have not ended yet.\n                    When an event ends, it's removed from the stack.\n                    Since this is a contextvar, it is unique to each\n                    thread/task.\n    - trace_map - A mapping of event ids to their children events.\n                  On the start of events, the bottom of the trace stack\n                  is used as the current parent event for the trace map.\n    - trace_id - A simple name for the current trace, usually denoting the\n                 entrypoint (query, index_construction, insert, etc.)\n\n    Args:\n        handlers (List[BaseCallbackHandler]): list of handlers to use.\n\n    Usage:\n        with callback_manager.event(CBEventType.QUERY) as event:\n            event.on_start(payload={key, val})\n            ...\n            event.on_end(payload={key, val})\n\n    \"\"\"\n\n    def __init__(self, handlers: Optional[List[BaseCallbackHandler]] = None):\n        \"\"\"Initialize the manager with a list of handlers.\"\"\"\n        from llama_index.core import global_handler\n\n        handlers = handlers or []\n\n        # add eval handlers based on global defaults\n        if global_handler is not None:\n            new_handler = global_handler\n            # go through existing handlers, check if any are same type as new handler\n            # if so, error\n            for existing_handler in handlers:\n                if isinstance(existing_handler, type(new_handler)):\n                    raise ValueError(\n                        \"Cannot add two handlers of the same type \"\n                        f\"{type(new_handler)} to the callback manager.\"\n                    )\n            handlers.append(new_handler)\n\n        # if we passed in no handlers, use the global default\n        if len(handlers) == 0:\n            from llama_index.core.settings import Settings\n\n            # hidden var access to prevent recursion in getter\n            cb_manager = Settings._callback_manager\n            if cb_manager is not None:\n                handlers = cb_manager.handlers\n\n        self.handlers: List[BaseCallbackHandler] = handlers\n        self._trace_map: Dict[str, List[str]] = defaultdict(list)\n\n    def on_event_start(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: Optional[str] = None,\n        parent_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Run handlers when an event starts and return id of event.\"\"\"\n        event_id = event_id or str(uuid.uuid4())\n\n        # if no trace is running, start a default trace\n        try:\n            parent_id = parent_id or global_stack_trace.get()[-1]\n        except IndexError:\n            self.start_trace(\"llama-index\")\n            parent_id = global_stack_trace.get()[-1]\n        parent_id = cast(str, parent_id)\n        self._trace_map[parent_id].append(event_id)\n        for handler in self.handlers:\n            if event_type not in handler.event_starts_to_ignore:\n                handler.on_event_start(\n                    event_type,\n                    payload,\n                    event_id=event_id,\n                    parent_id=parent_id,\n                    **kwargs,\n                )\n\n        if event_type not in LEAF_EVENTS:\n            # copy the stack trace to prevent conflicts with threads/coroutines\n            current_trace_stack = global_stack_trace.get().copy()\n            current_trace_stack.append(event_id)\n            global_stack_trace.set(current_trace_stack)\n\n        return event_id\n\n    def on_event_end(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run handlers when an event ends.\"\"\"\n        event_id = event_id or str(uuid.uuid4())\n        for handler in self.handlers:\n            if event_type not in handler.event_ends_to_ignore:\n                handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)\n\n        if event_type not in LEAF_EVENTS:\n            # copy the stack trace to prevent conflicts with threads/coroutines\n            current_trace_stack = global_stack_trace.get().copy()\n            current_trace_stack.pop()\n            global_stack_trace.set(current_trace_stack)\n\n    def add_handler(self, handler: BaseCallbackHandler) -> None:\n        \"\"\"Add a handler to the callback manager.\"\"\"\n        self.handlers.append(handler)\n\n    def remove_handler(self, handler: BaseCallbackHandler) -> None:\n        \"\"\"Remove a handler from the callback manager.\"\"\"\n        self.handlers.remove(handler)\n\n    def set_handlers(self, handlers: List[BaseCallbackHandler]) -> None:\n        \"\"\"Set handlers as the only handlers on the callback manager.\"\"\"\n        self.handlers = handlers\n\n    @contextmanager\n    def event(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: Optional[str] = None,\n    ) -> Generator[\"EventContext\", None, None]:\n        \"\"\"Context manager for lanching and shutdown of events.\n\n        Handles sending on_evnt_start and on_event_end to handlers for specified event.\n\n        Usage:\n            with callback_manager.event(CBEventType.QUERY, payload={key, val}) as event:\n                ...\n                event.on_end(payload={key, val})  # optional\n        \"\"\"\n        # create event context wrapper\n        event = EventContext(self, event_type, event_id=event_id)\n        event.on_start(payload=payload)\n\n        payload = None\n        try:\n            yield event\n        except Exception as e:\n            # data already logged to trace?\n            if not hasattr(e, \"event_added\"):\n                payload = {EventPayload.EXCEPTION: e}\n                e.event_added = True  # type: ignore\n                if not event.finished:\n                    event.on_end(payload=payload)\n            raise\n        finally:\n            # ensure event is ended\n            if not event.finished:\n                event.on_end(payload=payload)\n\n    @contextmanager\n    def as_trace(self, trace_id: str) -> Generator[None, None, None]:\n        \"\"\"Context manager tracer for lanching and shutdown of traces.\"\"\"\n        self.start_trace(trace_id=trace_id)\n\n        try:\n            yield\n        except Exception as e:\n            # event already added to trace?\n            if not hasattr(e, \"event_added\"):\n                self.on_event_start(\n                    CBEventType.EXCEPTION, payload={EventPayload.EXCEPTION: e}\n                )\n                e.event_added = True  # type: ignore\n\n            raise\n        finally:\n            # ensure trace is ended\n            self.end_trace(trace_id=trace_id)\n\n    def start_trace(self, trace_id: Optional[str] = None) -> None:\n        \"\"\"Run when an overall trace is launched.\"\"\"\n        current_trace_stack_ids = global_stack_trace_ids.get().copy()\n        if trace_id is not None:\n            if len(current_trace_stack_ids) == 0:\n                self._reset_trace_events()\n\n                for handler in self.handlers:\n                    handler.start_trace(trace_id=trace_id)\n\n                current_trace_stack_ids = [trace_id]\n            else:\n                current_trace_stack_ids.append(trace_id)\n\n        global_stack_trace_ids.set(current_trace_stack_ids)\n\n    def end_trace(\n        self,\n        trace_id: Optional[str] = None,\n        trace_map: Optional[Dict[str, List[str]]] = None,\n    ) -> None:\n        \"\"\"Run when an overall trace is exited.\"\"\"\n        current_trace_stack_ids = global_stack_trace_ids.get().copy()\n        if trace_id is not None and len(current_trace_stack_ids) > 0:\n            current_trace_stack_ids.pop()\n            if len(current_trace_stack_ids) == 0:\n                for handler in self.handlers:\n                    handler.end_trace(trace_id=trace_id, trace_map=self._trace_map)\n                current_trace_stack_ids = []\n\n        global_stack_trace_ids.set(current_trace_stack_ids)\n\n    def _reset_trace_events(self) -> None:\n        \"\"\"Helper function to reset the current trace.\"\"\"\n        self._trace_map = defaultdict(list)\n        global_stack_trace.set([BASE_TRACE_EVENT])\n\n    @property\n    def trace_map(self) -> Dict[str, List[str]]:\n        return self._trace_map\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, source: Type[Any], handler: GetCoreSchemaHandler\n    ) -> CoreSchema:\n        return core_schema.any_schema()\n\n    @classmethod\n    def __get_pydantic_json_schema__(\n        cls, core_schema: CoreSchema, handler: GetJsonSchemaHandler\n    ) -> Dict[str, Any]:\n        json_schema = handler(core_schema)\n        return handler.resolve_ref_schema(json_schema)\n\n\nclass EventContext:\n    \"\"\"\n    Simple wrapper to call callbacks on event starts and ends\n    with an event type and id.\n    \"\"\"\n\n    def __init__(\n        self,\n        callback_manager: CallbackManager,\n        event_type: CBEventType,\n        event_id: Optional[str] = None,\n    ):\n        self._callback_manager = callback_manager\n        self._event_type = event_type\n        self._event_id = event_id or str(uuid.uuid4())\n        self.started = False\n        self.finished = False\n\n    def on_start(self, payload: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:\n        if not self.started:\n            self.started = True\n            self._callback_manager.on_event_start(\n                self._event_type, payload=payload, event_id=self._event_id, **kwargs\n            )\n        else:\n            logger.warning(\n                f\"Event {self._event_type!s}: {self._event_id} already started!\"\n            )\n\n    def on_end(self, payload: Optional[Dict[str, Any]] = None, **kwargs: Any) -> None:\n        if not self.finished:\n            self.finished = True\n            self._callback_manager.on_event_end(\n                self._event_type, payload=payload, event_id=self._event_id, **kwargs\n            )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/simple_llm_handler.py",
    "filename": "simple_llm_handler.py",
    "relpath": "callbacks/simple_llm_handler.py",
    "start_line": 1,
    "end_line": 70,
    "length": 70,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "start_trace",
      "end_trace",
      "_print_llm_event",
      "on_event_start",
      "on_event_end"
    ],
    "chunk_class_names": [
      "SimpleLLMHandler"
    ],
    "document_function_names": [
      "__init__",
      "start_trace",
      "end_trace",
      "_print_llm_event",
      "on_event_start",
      "on_event_end"
    ],
    "document_class_names": [
      "SimpleLLMHandler"
    ],
    "content": "import logging\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom llama_index.core.callbacks.pythonically_printing_base_handler import (\n    PythonicallyPrintingBaseHandler,\n)\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\n\n\nclass SimpleLLMHandler(PythonicallyPrintingBaseHandler):\n    \"\"\"Callback handler for printing llms inputs/outputs.\"\"\"\n\n    def __init__(self, logger: Optional[logging.Logger] = None) -> None:\n        super().__init__(\n            event_starts_to_ignore=[], event_ends_to_ignore=[], logger=logger\n        )\n\n    def start_trace(self, trace_id: Optional[str] = None) -> None:\n        return\n\n    def end_trace(\n        self,\n        trace_id: Optional[str] = None,\n        trace_map: Optional[Dict[str, List[str]]] = None,\n    ) -> None:\n        return\n\n    def _print_llm_event(self, payload: dict) -> None:\n        from llama_index.core.llms import ChatMessage\n\n        if EventPayload.PROMPT in payload:\n            prompt = str(payload.get(EventPayload.PROMPT))\n            completion = str(payload.get(EventPayload.COMPLETION))\n\n            self._print(f\"** Prompt: **\\n{prompt}\")\n            self._print(\"*\" * 50)\n            self._print(f\"** Completion: **\\n{completion}\")\n            self._print(\"*\" * 50)\n            self._print(\"\\n\")\n        elif EventPayload.MESSAGES in payload:\n            messages = cast(List[ChatMessage], payload.get(EventPayload.MESSAGES, []))\n            messages_str = \"\\n\".join([str(x) for x in messages])\n            response = str(payload.get(EventPayload.RESPONSE))\n\n            self._print(f\"** Messages: **\\n{messages_str}\")\n            self._print(\"*\" * 50)\n            self._print(f\"** Response: **\\n{response}\")\n            self._print(\"*\" * 50)\n            self._print(\"\\n\")\n\n    def on_event_start(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        parent_id: str = \"\",\n        **kwargs: Any,\n    ) -> str:\n        return event_id\n\n    def on_event_end(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Count the LLM or Embedding tokens as needed.\"\"\"\n        if event_type == CBEventType.LLM and payload is not None:\n            self._print_llm_event(payload)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/schema.py",
    "filename": "schema.py",
    "relpath": "callbacks/schema.py",
    "start_line": 1,
    "end_line": 98,
    "length": 98,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__post_init__"
    ],
    "chunk_class_names": [
      "from",
      "CBEventType",
      "EventPayload",
      "class",
      "to",
      "class"
    ],
    "document_function_names": [
      "__post_init__"
    ],
    "document_class_names": [
      "from",
      "CBEventType",
      "EventPayload",
      "class",
      "to",
      "class"
    ],
    "content": "\"\"\"Base schema for callback managers.\"\"\"\nimport uuid\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, Optional\n\n# timestamp for callback events\nTIMESTAMP_FORMAT = \"%m/%d/%Y, %H:%M:%S.%f\"\n\n# base trace_id for the tracemap in callback_manager\nBASE_TRACE_EVENT = \"root\"\n\n\nclass CBEventType(str, Enum):\n    \"\"\"Callback manager event types.\n\n    Attributes:\n        CHUNKING: Logs for the before and after of text splitting.\n        NODE_PARSING: Logs for the documents and the nodes that they are parsed into.\n        EMBEDDING: Logs for the number of texts embedded.\n        LLM: Logs for the template and response of LLM calls.\n        QUERY: Keeps track of the start and end of each query.\n        RETRIEVE: Logs for the nodes retrieved for a query.\n        SYNTHESIZE: Logs for the result for synthesize calls.\n        TREE: Logs for the summary and level of summaries generated.\n        SUB_QUESTION: Logs for a generated sub question and answer.\n    \"\"\"\n\n    CHUNKING = \"chunking\"\n    NODE_PARSING = \"node_parsing\"\n    EMBEDDING = \"embedding\"\n    LLM = \"llm\"\n    QUERY = \"query\"\n    RETRIEVE = \"retrieve\"\n    SYNTHESIZE = \"synthesize\"\n    TREE = \"tree\"\n    SUB_QUESTION = \"sub_question\"\n    TEMPLATING = \"templating\"\n    FUNCTION_CALL = \"function_call\"\n    RERANKING = \"reranking\"\n    EXCEPTION = \"exception\"\n    AGENT_STEP = \"agent_step\"\n\n\nclass EventPayload(str, Enum):\n    DOCUMENTS = \"documents\"  # list of documents before parsing\n    CHUNKS = \"chunks\"  # list of text chunks\n    NODES = \"nodes\"  # list of nodes\n    PROMPT = \"formatted_prompt\"  # formatted prompt sent to LLM\n    MESSAGES = \"messages\"  # list of messages sent to LLM\n    COMPLETION = \"completion\"  # completion from LLM\n    RESPONSE = \"response\"  # message response from LLM\n    QUERY_STR = \"query_str\"  # query used for query engine\n    SUB_QUESTION = \"sub_question\"  # a sub question & answer + sources\n    EMBEDDINGS = \"embeddings\"  # list of embeddings\n    TOP_K = \"top_k\"  # top k nodes retrieved\n    ADDITIONAL_KWARGS = \"additional_kwargs\"  # additional kwargs for event call\n    SERIALIZED = \"serialized\"  # serialized object for event caller\n    FUNCTION_CALL = \"function_call\"  # function call for the LLM\n    FUNCTION_OUTPUT = \"function_call_response\"  # function call output\n    TOOL = \"tool\"  # tool used in LLM call\n    MODEL_NAME = \"model_name\"  # model name used in an event\n    TEMPLATE = \"template\"  # template used in LLM call\n    TEMPLATE_VARS = \"template_vars\"  # template variables used in LLM call\n    SYSTEM_PROMPT = \"system_prompt\"  # system prompt used in LLM call\n    QUERY_WRAPPER_PROMPT = \"query_wrapper_prompt\"  # query wrapper prompt used in LLM\n    EXCEPTION = \"exception\"  # exception raised in an event\n\n\n# events that will never have children events\nLEAF_EVENTS = (CBEventType.CHUNKING, CBEventType.LLM, CBEventType.EMBEDDING)\n\n\n@dataclass\nclass CBEvent:\n    \"\"\"Generic class to store event information.\"\"\"\n\n    event_type: CBEventType\n    payload: Optional[Dict[str, Any]] = None\n    time: str = \"\"\n    id_: str = \"\"\n\n    def __post_init__(self) -> None:\n        \"\"\"Init time and id if needed.\"\"\"\n        if not self.time:\n            self.time = datetime.now().strftime(TIMESTAMP_FORMAT)\n        if not self.id_:\n            self.id = str(uuid.uuid4())\n\n\n@dataclass\nclass EventStats:\n    \"\"\"Time-based Statistics for events.\"\"\"\n\n    total_secs: float\n    average_secs: float\n    total_count: int"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/global_handlers.py",
    "filename": "global_handlers.py",
    "relpath": "callbacks/global_handlers.py",
    "start_line": 1,
    "end_line": 149,
    "length": 149,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "set_global_handler",
      "create_global_handler"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "set_global_handler",
      "create_global_handler"
    ],
    "document_class_names": [],
    "content": "from typing import Any, Optional\nfrom llama_index.core.callbacks.base_handler import BaseCallbackHandler\nfrom llama_index.core.callbacks.simple_llm_handler import SimpleLLMHandler\n\n\ndef set_global_handler(eval_mode: str, **eval_params: Any) -> None:\n    \"\"\"Set global eval handlers.\"\"\"\n    import llama_index.core\n\n    handler = create_global_handler(eval_mode, **eval_params)\n    if handler:\n        llama_index.core.global_handler = handler\n\n\ndef create_global_handler(\n    eval_mode: str, **eval_params: Any\n) -> Optional[BaseCallbackHandler]:\n    \"\"\"Get global eval handler.\"\"\"\n    handler: Optional[BaseCallbackHandler] = None\n    if eval_mode == \"wandb\":\n        try:\n            from llama_index.callbacks.wandb import (\n                WandbCallbackHandler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"WandbCallbackHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-wandb`\"\n            )\n\n        handler = WandbCallbackHandler(**eval_params)\n    elif eval_mode == \"openinference\":\n        try:\n            from llama_index.callbacks.openinference import (\n                OpenInferenceCallbackHandler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"OpenInferenceCallbackHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-openinference`\"\n            )\n\n        handler = OpenInferenceCallbackHandler(**eval_params)\n    elif eval_mode == \"arize_phoenix\":\n        try:\n            from llama_index.callbacks.arize_phoenix import (\n                arize_phoenix_callback_handler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"ArizePhoenixCallbackHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-arize-phoenix`\"\n            )\n\n        handler = arize_phoenix_callback_handler(**eval_params)\n    elif eval_mode == \"honeyhive\":\n        try:\n            from llama_index.callbacks.honeyhive import (\n                honeyhive_callback_handler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"HoneyHiveCallbackHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-honeyhive`\"\n            )\n        handler = honeyhive_callback_handler(**eval_params)\n    elif eval_mode == \"promptlayer\":\n        try:\n            from llama_index.callbacks.promptlayer import (\n                PromptLayerHandler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"PromptLayerHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-promptlayer`\"\n            )\n        handler = PromptLayerHandler(**eval_params)\n    elif eval_mode == \"deepeval\":\n        try:\n            from llama_index.callbacks.deepeval import (\n                deepeval_callback_handler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"DeepEvalCallbackHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-deepeval`\"\n            )\n        handler = deepeval_callback_handler(**eval_params)\n    elif eval_mode == \"simple\":\n        handler = SimpleLLMHandler(**eval_params)\n    elif eval_mode == \"argilla\":\n        try:\n            from llama_index.callbacks.argilla import (\n                argilla_callback_handler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"ArgillaCallbackHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-argilla`\"\n            )\n        handler = argilla_callback_handler(**eval_params)\n    elif eval_mode == \"langfuse\":\n        try:\n            from llama_index.callbacks.langfuse import (\n                langfuse_callback_handler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"LangfuseCallbackHandler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-langfuse`\"\n            )\n        handler = langfuse_callback_handler(**eval_params)\n    elif eval_mode == \"agentops\":\n        try:\n            from llama_index.callbacks.agentops import (\n                AgentOpsHandler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"AgentOpsHandler is not installed. \"\n                \"Please install it using `pip install llama-index-instrumentation-agentops`\"\n            )\n        AgentOpsHandler.init(**eval_params)\n    elif eval_mode == \"literalai\":\n        try:\n            from llama_index.callbacks.literalai import (\n                literalai_callback_handler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"Literal AI Handler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-literalai`\"\n            )\n        handler = literalai_callback_handler(**eval_params)\n    elif eval_mode == \"opik\":\n        try:\n            from llama_index.callbacks.opik import (\n                opik_callback_handler,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"Opik Handler is not installed. \"\n                \"Please install it using `pip install llama-index-callbacks-opik`\"\n            )\n        handler = opik_callback_handler(**eval_params)\n    else:\n        raise ValueError(f\"Eval mode {eval_mode} not supported.\")\n\n    return handler"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/__init__.py",
    "filename": "__init__.py",
    "relpath": "callbacks/__init__.py",
    "start_line": 1,
    "end_line": 17,
    "length": 17,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from .base import CallbackManager\nfrom .llama_debug import LlamaDebugHandler\nfrom .schema import CBEvent, CBEventType, EventPayload\nfrom .token_counting import TokenCountingHandler\nfrom .pythonically_printing_base_handler import PythonicallyPrintingBaseHandler\nfrom .utils import trace_method\n\n__all__ = [\n    \"CallbackManager\",\n    \"CBEvent\",\n    \"CBEventType\",\n    \"EventPayload\",\n    \"LlamaDebugHandler\",\n    \"TokenCountingHandler\",\n    \"trace_method\",\n    \"PythonicallyPrintingBaseHandler\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/pythonically_printing_base_handler.py",
    "filename": "pythonically_printing_base_handler.py",
    "relpath": "callbacks/pythonically_printing_base_handler.py",
    "start_line": 1,
    "end_line": 38,
    "length": 38,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_print"
    ],
    "chunk_class_names": [
      "PythonicallyPrintingBaseHandler",
      "is"
    ],
    "document_function_names": [
      "__init__",
      "_print"
    ],
    "document_class_names": [
      "PythonicallyPrintingBaseHandler",
      "is"
    ],
    "content": "import logging\nfrom typing import List, Optional\n\nfrom llama_index.core.callbacks.base_handler import BaseCallbackHandler\nfrom llama_index.core.callbacks.schema import (\n    CBEventType,\n)\n\n\nclass PythonicallyPrintingBaseHandler(BaseCallbackHandler):\n    \"\"\"\n    Callback handler that prints logs in a Pythonic way. That is, not using `print` at all; use the logger instead.\n    See https://stackoverflow.com/a/6918596/1147061 for why you should prefer using a logger over `print`.\n\n    This class is meant to be subclassed, not used directly.\n\n    Using this class, your LlamaIndex Callback Handlers can now make use of vanilla Python logging handlers now.\n    One popular choice is https://rich.readthedocs.io/en/stable/logging.html#logging-handler.\n    \"\"\"\n\n    def __init__(\n        self,\n        event_starts_to_ignore: Optional[List[CBEventType]] = None,\n        event_ends_to_ignore: Optional[List[CBEventType]] = None,\n        logger: Optional[logging.Logger] = None,\n    ) -> None:\n        self.logger: Optional[logging.Logger] = logger\n        super().__init__(\n            event_starts_to_ignore=event_starts_to_ignore or [],\n            event_ends_to_ignore=event_ends_to_ignore or [],\n        )\n\n    def _print(self, print_str: str) -> None:\n        if self.logger:\n            self.logger.debug(print_str)\n        else:\n            # This branch is to preserve existing behavior.\n            print(print_str, flush=True)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/utils.py",
    "filename": "utils.py",
    "relpath": "callbacks/utils.py",
    "start_line": 1,
    "end_line": 60,
    "length": 60,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "trace_method",
      "my_method",
      "decorator",
      "wrapper",
      "async_wrapper"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "trace_method",
      "my_method",
      "decorator",
      "wrapper",
      "async_wrapper"
    ],
    "document_class_names": [],
    "content": "import asyncio\nimport functools\nimport logging\nfrom typing import Any, Callable, cast\n\nfrom llama_index.core.callbacks.base import CallbackManager\n\nlogger = logging.getLogger(__name__)\n\n\ndef trace_method(\n    trace_id: str, callback_manager_attr: str = \"callback_manager\"\n) -> Callable[[Callable], Callable]:\n    \"\"\"\n    Decorator to trace a method.\n\n    Example:\n        @trace_method(\"my_trace_id\")\n        def my_method(self):\n            pass\n\n    Assumes that the self instance has a CallbackManager instance in an attribute\n    named `callback_manager`.\n    This can be overridden by passing in a `callback_manager_attr` keyword argument.\n    \"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)  # preserve signature, name, etc. of func\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -> Any:\n            try:\n                callback_manager = getattr(self, callback_manager_attr)\n            except AttributeError:\n                logger.warning(\n                    \"Could not find attribute %s on %s.\",\n                    callback_manager_attr,\n                    type(self),\n                )\n                return func(self, *args, **kwargs)\n            callback_manager = cast(CallbackManager, callback_manager)\n            with callback_manager.as_trace(trace_id):\n                return func(self, *args, **kwargs)\n\n        @functools.wraps(func)  # preserve signature, name, etc. of func\n        async def async_wrapper(self: Any, *args: Any, **kwargs: Any) -> Any:\n            try:\n                callback_manager = getattr(self, callback_manager_attr)\n            except AttributeError:\n                logger.warning(\n                    \"Could not find attribute %s on %s.\",\n                    callback_manager_attr,\n                    type(self),\n                )\n                return await func(self, *args, **kwargs)\n            callback_manager = cast(CallbackManager, callback_manager)\n            with callback_manager.as_trace(trace_id):\n                return await func(self, *args, **kwargs)\n\n        return async_wrapper if asyncio.iscoroutinefunction(func) else wrapper\n\n    return decorator"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/llama_debug.py",
    "filename": "llama_debug.py",
    "relpath": "callbacks/llama_debug.py",
    "start_line": 1,
    "end_line": 207,
    "length": 207,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "on_event_start",
      "on_event_end",
      "get_events",
      "_get_event_pairs",
      "_get_time_stats_from_event_pairs",
      "get_event_pairs",
      "get_llm_inputs_outputs",
      "get_event_time_info",
      "flush_event_logs",
      "start_trace",
      "end_trace",
      "_print_trace_map",
      "print_trace_map",
      "event_pairs_by_type",
      "events_pairs_by_id",
      "sequential_events"
    ],
    "chunk_class_names": [
      "LlamaDebugHandler"
    ],
    "document_function_names": [
      "__init__",
      "on_event_start",
      "on_event_end",
      "get_events",
      "_get_event_pairs",
      "_get_time_stats_from_event_pairs",
      "get_event_pairs",
      "get_llm_inputs_outputs",
      "get_event_time_info",
      "flush_event_logs",
      "start_trace",
      "end_trace",
      "_print_trace_map",
      "print_trace_map",
      "event_pairs_by_type",
      "events_pairs_by_id",
      "sequential_events"
    ],
    "document_class_names": [
      "LlamaDebugHandler"
    ],
    "content": "from collections import defaultdict\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional\nimport logging\nfrom llama_index.core.callbacks.pythonically_printing_base_handler import (\n    PythonicallyPrintingBaseHandler,\n)\nfrom llama_index.core.callbacks.schema import (\n    BASE_TRACE_EVENT,\n    TIMESTAMP_FORMAT,\n    CBEvent,\n    CBEventType,\n    EventStats,\n)\n\n\nclass LlamaDebugHandler(PythonicallyPrintingBaseHandler):\n    \"\"\"Callback handler that keeps track of debug info.\n\n    NOTE: this is a beta feature. The usage within our codebase, and the interface\n    may change.\n\n    This handler simply keeps track of event starts/ends, separated by event types.\n    You can use this callback handler to keep track of and debug events.\n\n    Args:\n        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to\n            ignore when tracking event starts.\n        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to\n            ignore when tracking event ends.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        event_starts_to_ignore: Optional[List[CBEventType]] = None,\n        event_ends_to_ignore: Optional[List[CBEventType]] = None,\n        print_trace_on_end: bool = True,\n        logger: Optional[logging.Logger] = None,\n    ) -> None:\n        \"\"\"Initialize the llama debug handler.\"\"\"\n        self._event_pairs_by_type: Dict[CBEventType, List[CBEvent]] = defaultdict(list)\n        self._event_pairs_by_id: Dict[str, List[CBEvent]] = defaultdict(list)\n        self._sequential_events: List[CBEvent] = []\n        self._cur_trace_id: Optional[str] = None\n        self._trace_map: Dict[str, List[str]] = defaultdict(list)\n        self.print_trace_on_end = print_trace_on_end\n        event_starts_to_ignore = (\n            event_starts_to_ignore if event_starts_to_ignore else []\n        )\n        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []\n        super().__init__(\n            event_starts_to_ignore=event_starts_to_ignore,\n            event_ends_to_ignore=event_ends_to_ignore,\n            logger=logger,\n        )\n\n    def on_event_start(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        parent_id: str = \"\",\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Store event start data by event type.\n\n        Args:\n            event_type (CBEventType): event type to store.\n            payload (Optional[Dict[str, Any]]): payload to store.\n            event_id (str): event id to store.\n            parent_id (str): parent event id.\n\n        \"\"\"\n        event = CBEvent(event_type, payload=payload, id_=event_id)\n        self._event_pairs_by_type[event.event_type].append(event)\n        self._event_pairs_by_id[event.id_].append(event)\n        self._sequential_events.append(event)\n        return event.id_\n\n    def on_event_end(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Store event end data by event type.\n\n        Args:\n            event_type (CBEventType): event type to store.\n            payload (Optional[Dict[str, Any]]): payload to store.\n            event_id (str): event id to store.\n\n        \"\"\"\n        event = CBEvent(event_type, payload=payload, id_=event_id)\n        self._event_pairs_by_type[event.event_type].append(event)\n        self._event_pairs_by_id[event.id_].append(event)\n        self._sequential_events.append(event)\n        self._trace_map = defaultdict(list)\n\n    def get_events(self, event_type: Optional[CBEventType] = None) -> List[CBEvent]:\n        \"\"\"Get all events for a specific event type.\"\"\"\n        if event_type is not None:\n            return self._event_pairs_by_type[event_type]\n\n        return self._sequential_events\n\n    def _get_event_pairs(self, events: List[CBEvent]) -> List[List[CBEvent]]:\n        \"\"\"Helper function to pair events according to their ID.\"\"\"\n        event_pairs: Dict[str, List[CBEvent]] = defaultdict(list)\n        for event in events:\n            event_pairs[event.id_].append(event)\n\n        return sorted(\n            event_pairs.values(),\n            key=lambda x: datetime.strptime(x[0].time, TIMESTAMP_FORMAT),\n        )\n\n    def _get_time_stats_from_event_pairs(\n        self, event_pairs: List[List[CBEvent]]\n    ) -> EventStats:\n        \"\"\"Calculate time-based stats for a set of event pairs.\"\"\"\n        total_secs = 0.0\n        for event_pair in event_pairs:\n            start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)\n            end_time = datetime.strptime(event_pair[-1].time, TIMESTAMP_FORMAT)\n            total_secs += (end_time - start_time).total_seconds()\n\n        return EventStats(\n            total_secs=total_secs,\n            average_secs=total_secs / len(event_pairs),\n            total_count=len(event_pairs),\n        )\n\n    def get_event_pairs(\n        self, event_type: Optional[CBEventType] = None\n    ) -> List[List[CBEvent]]:\n        \"\"\"Pair events by ID, either all events or a specific type.\"\"\"\n        if event_type is not None:\n            return self._get_event_pairs(self._event_pairs_by_type[event_type])\n\n        return self._get_event_pairs(self._sequential_events)\n\n    def get_llm_inputs_outputs(self) -> List[List[CBEvent]]:\n        \"\"\"Get the exact LLM inputs and outputs.\"\"\"\n        return self._get_event_pairs(self._event_pairs_by_type[CBEventType.LLM])\n\n    def get_event_time_info(\n        self, event_type: Optional[CBEventType] = None\n    ) -> EventStats:\n        event_pairs = self.get_event_pairs(event_type)\n        return self._get_time_stats_from_event_pairs(event_pairs)\n\n    def flush_event_logs(self) -> None:\n        \"\"\"Clear all events from memory.\"\"\"\n        self._event_pairs_by_type = defaultdict(list)\n        self._event_pairs_by_id = defaultdict(list)\n        self._sequential_events = []\n\n    def start_trace(self, trace_id: Optional[str] = None) -> None:\n        \"\"\"Launch a trace.\"\"\"\n        self._trace_map = defaultdict(list)\n        self._cur_trace_id = trace_id\n\n    def end_trace(\n        self,\n        trace_id: Optional[str] = None,\n        trace_map: Optional[Dict[str, List[str]]] = None,\n    ) -> None:\n        \"\"\"Shutdown the current trace.\"\"\"\n        self._trace_map = trace_map or defaultdict(list)\n        if self.print_trace_on_end:\n            self.print_trace_map()\n\n    def _print_trace_map(self, cur_event_id: str, level: int = 0) -> None:\n        \"\"\"Recursively print trace map to terminal for debugging.\"\"\"\n        event_pair = self._event_pairs_by_id[cur_event_id]\n        if event_pair:\n            time_stats = self._get_time_stats_from_event_pairs([event_pair])\n            indent = \" \" * level * 2\n            self._print(\n                f\"{indent}|_{event_pair[0].event_type} -> {time_stats.total_secs} seconds\",\n            )\n\n        child_event_ids = self._trace_map[cur_event_id]\n        for child_event_id in child_event_ids:\n            self._print_trace_map(child_event_id, level=level + 1)\n\n    def print_trace_map(self) -> None:\n        \"\"\"Print simple trace map to terminal for debugging of the most recent trace.\"\"\"\n        self._print(\"*\" * 10)\n        self._print(f\"Trace: {self._cur_trace_id}\")\n        self._print_trace_map(BASE_TRACE_EVENT, level=1)\n        self._print(\"*\" * 10)\n\n    @property\n    def event_pairs_by_type(self) -> Dict[CBEventType, List[CBEvent]]:\n        return self._event_pairs_by_type\n\n    @property\n    def events_pairs_by_id(self) -> Dict[str, List[CBEvent]]:\n        return self._event_pairs_by_id\n\n    @property\n    def sequential_events(self) -> List[CBEvent]:\n        return self._sequential_events"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/token_counting.py",
    "filename": "token_counting.py",
    "relpath": "callbacks/token_counting.py",
    "start_line": 1,
    "end_line": 263,
    "length": 263,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__post_init__",
      "get_tokens_from_response",
      "get_llm_token_counts",
      "__init__",
      "start_trace",
      "end_trace",
      "on_event_start",
      "on_event_end",
      "total_llm_token_count",
      "prompt_llm_token_count",
      "completion_llm_token_count",
      "total_embedding_token_count",
      "reset_counts"
    ],
    "chunk_class_names": [
      "from",
      "class",
      "TokenCountingHandler"
    ],
    "document_function_names": [
      "__post_init__",
      "get_tokens_from_response",
      "get_llm_token_counts",
      "__init__",
      "start_trace",
      "end_trace",
      "on_event_start",
      "on_event_end",
      "total_llm_token_count",
      "prompt_llm_token_count",
      "completion_llm_token_count",
      "total_embedding_token_count",
      "reset_counts"
    ],
    "document_class_names": [
      "from",
      "class",
      "TokenCountingHandler"
    ],
    "content": "from dataclasses import dataclass\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Tuple,\n    Union,\n    cast,\n)\n\nfrom llama_index.core.callbacks.pythonically_printing_base_handler import (\n    PythonicallyPrintingBaseHandler,\n)\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.utilities.token_counting import TokenCounter\nfrom llama_index.core.utils import get_tokenizer\nimport logging\n\nif TYPE_CHECKING:\n    from llama_index.core.llms import ChatResponse, CompletionResponse\n\n\n@dataclass\nclass TokenCountingEvent:\n    prompt: str\n    completion: str\n    completion_token_count: int\n    prompt_token_count: int\n    total_token_count: int = 0\n    event_id: str = \"\"\n\n    def __post_init__(self) -> None:\n        self.total_token_count = self.prompt_token_count + self.completion_token_count\n\n\ndef get_tokens_from_response(\n    response: Union[\"CompletionResponse\", \"ChatResponse\"]\n) -> Tuple[int, int]:\n    \"\"\"Get the token counts from a raw response.\"\"\"\n    raw_response = response.raw\n    if not isinstance(raw_response, dict):\n        raw_response = dict(raw_response or {})\n\n    usage = raw_response.get(\"usage\", {})\n    if usage is None:\n        usage = response.additional_kwargs\n\n    if not usage:\n        return 0, 0\n\n    if not isinstance(usage, dict):\n        usage = usage.model_dump()\n\n    possible_input_keys = (\"prompt_tokens\", \"input_tokens\")\n    possible_output_keys = (\"completion_tokens\", \"output_tokens\")\n\n    prompt_tokens = 0\n    for input_key in possible_input_keys:\n        if input_key in usage:\n            prompt_tokens = usage[input_key]\n            break\n\n    completion_tokens = 0\n    for output_key in possible_output_keys:\n        if output_key in usage:\n            completion_tokens = usage[output_key]\n            break\n\n    return prompt_tokens, completion_tokens\n\n\ndef get_llm_token_counts(\n    token_counter: TokenCounter, payload: Dict[str, Any], event_id: str = \"\"\n) -> TokenCountingEvent:\n    from llama_index.core.llms import ChatMessage\n\n    if EventPayload.PROMPT in payload:\n        prompt = payload.get(EventPayload.PROMPT)\n        completion = payload.get(EventPayload.COMPLETION)\n\n        if completion:\n            # get from raw or additional_kwargs\n            prompt_tokens, completion_tokens = get_tokens_from_response(completion)\n        else:\n            prompt_tokens, completion_tokens = 0, 0\n\n        if prompt_tokens == 0:\n            prompt_tokens = token_counter.get_string_tokens(str(prompt))\n\n        if completion_tokens == 0:\n            completion_tokens = token_counter.get_string_tokens(str(completion))\n\n        return TokenCountingEvent(\n            event_id=event_id,\n            prompt=str(prompt),\n            prompt_token_count=prompt_tokens,\n            completion=str(completion),\n            completion_token_count=completion_tokens,\n        )\n\n    elif EventPayload.MESSAGES in payload:\n        messages = cast(List[ChatMessage], payload.get(EventPayload.MESSAGES, []))\n        messages_str = \"\\n\".join([str(x) for x in messages])\n\n        response = payload.get(EventPayload.RESPONSE)\n        response_str = str(response)\n\n        if response:\n            prompt_tokens, completion_tokens = get_tokens_from_response(response)\n        else:\n            prompt_tokens, completion_tokens = 0, 0\n\n        if prompt_tokens == 0:\n            prompt_tokens = token_counter.estimate_tokens_in_messages(messages)\n\n        if completion_tokens == 0:\n            completion_tokens = token_counter.get_string_tokens(response_str)\n\n        return TokenCountingEvent(\n            event_id=event_id,\n            prompt=messages_str,\n            prompt_token_count=prompt_tokens,\n            completion=response_str,\n            completion_token_count=completion_tokens,\n        )\n    else:\n        return TokenCountingEvent(\n            event_id=event_id,\n            prompt=\"\",\n            prompt_token_count=0,\n            completion=\"\",\n            completion_token_count=0,\n        )\n\n\nclass TokenCountingHandler(PythonicallyPrintingBaseHandler):\n    \"\"\"Callback handler for counting tokens in LLM and Embedding events.\n\n    Args:\n        tokenizer:\n            Tokenizer to use. Defaults to the global tokenizer\n            (see llama_index.core.utils.globals_helper).\n        event_starts_to_ignore: List of event types to ignore at the start of a trace.\n        event_ends_to_ignore: List of event types to ignore at the end of a trace.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer: Optional[Callable[[str], List]] = None,\n        event_starts_to_ignore: Optional[List[CBEventType]] = None,\n        event_ends_to_ignore: Optional[List[CBEventType]] = None,\n        verbose: bool = False,\n        logger: Optional[logging.Logger] = None,\n    ) -> None:\n        self.llm_token_counts: List[TokenCountingEvent] = []\n        self.embedding_token_counts: List[TokenCountingEvent] = []\n        self.tokenizer = tokenizer or get_tokenizer()\n\n        self._token_counter = TokenCounter(tokenizer=self.tokenizer)\n        self._verbose = verbose\n\n        super().__init__(\n            event_starts_to_ignore=event_starts_to_ignore or [],\n            event_ends_to_ignore=event_ends_to_ignore or [],\n            logger=logger,\n        )\n\n    def start_trace(self, trace_id: Optional[str] = None) -> None:\n        return\n\n    def end_trace(\n        self,\n        trace_id: Optional[str] = None,\n        trace_map: Optional[Dict[str, List[str]]] = None,\n    ) -> None:\n        return\n\n    def on_event_start(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        parent_id: str = \"\",\n        **kwargs: Any,\n    ) -> str:\n        return event_id\n\n    def on_event_end(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Count the LLM or Embedding tokens as needed.\"\"\"\n        if (\n            event_type == CBEventType.LLM\n            and event_type not in self.event_ends_to_ignore\n            and payload is not None\n        ):\n            self.llm_token_counts.append(\n                get_llm_token_counts(\n                    token_counter=self._token_counter,\n                    payload=payload,\n                    event_id=event_id,\n                )\n            )\n\n            if self._verbose:\n                self._print(\n                    \"LLM Prompt Token Usage: \"\n                    f\"{self.llm_token_counts[-1].prompt_token_count}\\n\"\n                    \"LLM Completion Token Usage: \"\n                    f\"{self.llm_token_counts[-1].completion_token_count}\",\n                )\n        elif (\n            event_type == CBEventType.EMBEDDING\n            and event_type not in self.event_ends_to_ignore\n            and payload is not None\n        ):\n            total_chunk_tokens = 0\n            for chunk in payload.get(EventPayload.CHUNKS, []):\n                self.embedding_token_counts.append(\n                    TokenCountingEvent(\n                        event_id=event_id,\n                        prompt=chunk,\n                        prompt_token_count=self._token_counter.get_string_tokens(chunk),\n                        completion=\"\",\n                        completion_token_count=0,\n                    )\n                )\n                total_chunk_tokens += self.embedding_token_counts[-1].total_token_count\n\n            if self._verbose:\n                self._print(f\"Embedding Token Usage: {total_chunk_tokens}\")\n\n    @property\n    def total_llm_token_count(self) -> int:\n        \"\"\"Get the current total LLM token count.\"\"\"\n        return sum([x.total_token_count for x in self.llm_token_counts])\n\n    @property\n    def prompt_llm_token_count(self) -> int:\n        \"\"\"Get the current total LLM prompt token count.\"\"\"\n        return sum([x.prompt_token_count for x in self.llm_token_counts])\n\n    @property\n    def completion_llm_token_count(self) -> int:\n        \"\"\"Get the current total LLM completion token count.\"\"\"\n        return sum([x.completion_token_count for x in self.llm_token_counts])\n\n    @property\n    def total_embedding_token_count(self) -> int:\n        \"\"\"Get the current total Embedding token count.\"\"\"\n        return sum([x.total_token_count for x in self.embedding_token_counts])\n\n    def reset_counts(self) -> None:\n        \"\"\"Reset the token counts.\"\"\"\n        self.llm_token_counts = []\n        self.embedding_token_counts = []"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/callbacks/base_handler.py",
    "filename": "base_handler.py",
    "relpath": "callbacks/base_handler.py",
    "start_line": 1,
    "end_line": 55,
    "length": 55,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "on_event_start",
      "on_event_end",
      "start_trace",
      "end_trace"
    ],
    "chunk_class_names": [
      "BaseCallbackHandler"
    ],
    "document_function_names": [
      "__init__",
      "on_event_start",
      "on_event_end",
      "start_trace",
      "end_trace"
    ],
    "document_class_names": [
      "BaseCallbackHandler"
    ],
    "content": "import logging\nfrom abc import ABC, abstractmethod\nfrom contextvars import ContextVar\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.callbacks.schema import BASE_TRACE_EVENT, CBEventType\n\nlogger = logging.getLogger(__name__)\nglobal_stack_trace = ContextVar(\"trace\", default=[BASE_TRACE_EVENT])\n\n\nclass BaseCallbackHandler(ABC):\n    \"\"\"Base callback handler that can be used to track event starts and ends.\"\"\"\n\n    def __init__(\n        self,\n        event_starts_to_ignore: List[CBEventType],\n        event_ends_to_ignore: List[CBEventType],\n    ) -> None:\n        \"\"\"Initialize the base callback handler.\"\"\"\n        self.event_starts_to_ignore = tuple(event_starts_to_ignore)\n        self.event_ends_to_ignore = tuple(event_ends_to_ignore)\n\n    @abstractmethod\n    def on_event_start(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        parent_id: str = \"\",\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Run when an event starts and return id of event.\"\"\"\n\n    @abstractmethod\n    def on_event_end(\n        self,\n        event_type: CBEventType,\n        payload: Optional[Dict[str, Any]] = None,\n        event_id: str = \"\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when an event ends.\"\"\"\n\n    @abstractmethod\n    def start_trace(self, trace_id: Optional[str] = None) -> None:\n        \"\"\"Run when an overall trace is launched.\"\"\"\n\n    @abstractmethod\n    def end_trace(\n        self,\n        trace_id: Optional[str] = None,\n        trace_map: Optional[Dict[str, List[str]]] = None,\n    ) -> None:\n        \"\"\"Run when an overall trace is exited.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/memory/types.py",
    "filename": "types.py",
    "relpath": "memory/types.py",
    "start_line": 1,
    "end_line": 117,
    "length": 117,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "from_defaults",
      "get",
      "get_all",
      "put",
      "aput",
      "put_messages",
      "aput_messages",
      "set",
      "reset",
      "serialize_courses_in_order",
      "class_name",
      "from_defaults",
      "get_all",
      "put",
      "aput",
      "set",
      "reset"
    ],
    "chunk_class_names": [
      "BaseMemory",
      "for",
      "name",
      "BaseChatStoreMemory",
      "for",
      "name"
    ],
    "document_function_names": [
      "class_name",
      "from_defaults",
      "get",
      "get_all",
      "put",
      "aput",
      "put_messages",
      "aput_messages",
      "set",
      "reset",
      "serialize_courses_in_order",
      "class_name",
      "from_defaults",
      "get_all",
      "put",
      "aput",
      "set",
      "reset"
    ],
    "document_class_names": [
      "BaseMemory",
      "for",
      "name",
      "BaseChatStoreMemory",
      "for",
      "name"
    ],
    "content": "from abc import abstractmethod\nfrom typing import Any, List, Optional\n\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.schema import BaseComponent\nfrom llama_index.core.storage.chat_store import BaseChatStore, SimpleChatStore\nfrom llama_index.core.bridge.pydantic import Field, field_serializer, SerializeAsAny\n\nDEFAULT_CHAT_STORE_KEY = \"chat_history\"\n\n\nclass BaseMemory(BaseComponent):\n    \"\"\"Base class for all memory types.\n\n    NOTE: The interface for memory is not yet finalized and is subject to change.\n    \"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"BaseMemory\"\n\n    @classmethod\n    @abstractmethod\n    def from_defaults(\n        cls,\n        **kwargs: Any,\n    ) -> \"BaseMemory\":\n        \"\"\"Create a chat memory from defaults.\"\"\"\n\n    @abstractmethod\n    def get(self, input: Optional[str] = None, **kwargs: Any) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n\n    @abstractmethod\n    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n\n    @abstractmethod\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n\n    async def aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        self.put(message)\n\n    def put_messages(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Put chat history.\"\"\"\n        for message in messages:\n            self.put(message)\n\n    async def aput_messages(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Put chat history.\"\"\"\n        for message in messages:\n            await self.aput(message)\n\n    @abstractmethod\n    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n\n\nclass BaseChatStoreMemory(BaseMemory):\n    \"\"\"Base class for any .\n\n    NOTE: The interface for memory is not yet finalized and is subject to change.\n    \"\"\"\n\n    chat_store: SerializeAsAny[BaseChatStore] = Field(default_factory=SimpleChatStore)\n    chat_store_key: str = Field(default=DEFAULT_CHAT_STORE_KEY)\n\n    @field_serializer(\"chat_store\")\n    def serialize_courses_in_order(self, chat_store: BaseChatStore) -> dict:\n        res = chat_store.model_dump()\n        res.update({\"class_name\": chat_store.class_name()})\n        return res\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"BaseChatStoreMemory\"\n\n    @classmethod\n    @abstractmethod\n    def from_defaults(\n        cls,\n        chat_history: Optional[List[ChatMessage]] = None,\n        llm: Optional[LLM] = None,\n        **kwargs: Any,\n    ) -> \"BaseChatStoreMemory\":\n        \"\"\"Create a chat memory from defaults.\"\"\"\n\n    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key)\n\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        self.chat_store.add_message(self.chat_store_key, message)\n\n    async def aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        await self.chat_store.async_add_message(self.chat_store_key, message)\n\n    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/memory/vector_memory.py",
    "filename": "vector_memory.py",
    "relpath": "memory/vector_memory.py",
    "start_line": 1,
    "end_line": 201,
    "length": 201,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_stringify_obj",
      "_stringify_chat_message",
      "_get_starter_node_for_new_batch",
      "validate_vector_index",
      "class_name",
      "from_defaults",
      "get",
      "get_all",
      "_commit_node",
      "put",
      "set",
      "reset"
    ],
    "chunk_class_names": [
      "VectorMemory",
      "requires",
      "name"
    ],
    "document_function_names": [
      "_stringify_obj",
      "_stringify_chat_message",
      "_get_starter_node_for_new_batch",
      "validate_vector_index",
      "class_name",
      "from_defaults",
      "get",
      "get_all",
      "_commit_node",
      "put",
      "set",
      "reset"
    ],
    "document_class_names": [
      "VectorMemory",
      "requires",
      "name"
    ],
    "content": "\"\"\"Vector memory.\n\nMemory backed by a vector database.\n\n\"\"\"\n\nimport uuid\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.bridge.pydantic import Field, field_validator\nfrom llama_index.core.embeddings.utils import EmbedType\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.schema import TextNode\nfrom llama_index.core.vector_stores.types import BasePydanticVectorStore\n\n\ndef _stringify_obj(d: Any) -> Union[str, list, dict]:\n    \"\"\"Utility function to convert all keys in a dictionary to strings.\"\"\"\n    if isinstance(d, list):\n        return [_stringify_obj(v) for v in d]\n    elif isinstance(d, Dict):\n        return {str(k): _stringify_obj(v) for k, v in d.items()}\n    else:\n        return str(d)\n\n\ndef _stringify_chat_message(msg: ChatMessage) -> Dict:\n    \"\"\"Utility function to convert chatmessage to serializable dict.\"\"\"\n    msg_dict = msg.dict()\n    msg_dict[\"additional_kwargs\"] = _stringify_obj(msg_dict[\"additional_kwargs\"])\n    msg_dict[\"content\"] = msg.content\n    return msg_dict\n\n\ndef _get_starter_node_for_new_batch() -> TextNode:\n    \"\"\"Generates a new starter node for a new batch or group of messages.\"\"\"\n    return TextNode(\n        id_=str(uuid.uuid4()),\n        text=\"\",\n        metadata={\"sub_dicts\": []},\n        excluded_embed_metadata_keys=[\"sub_dicts\"],\n        excluded_llm_metadata_keys=[\"sub_dicts\"],\n    )\n\n\nclass VectorMemory(BaseMemory):\n    \"\"\"Memory backed by a vector index.\n\n    NOTE: This class requires the `delete_nodes` method to be implemented\n    by the vector store underlying the vector index. At time of writing (May 2024),\n    Chroma, Qdrant and SimpleVectorStore all support delete_nodes.\n    \"\"\"\n\n    vector_index: Any\n    retriever_kwargs: Dict[str, Any] = Field(default_factory=dict)\n\n    # Whether to combine a user message with all subsequent messages\n    # until the next user message into a single message\n    # This is on by default, ensuring that we always fetch contiguous blocks of user/response pairs.\n    # Turning this off may lead to errors in the function calling API of the LLM.\n    # If this is on, then any message that's not a user message will be combined with the last user message\n    # in the vector store.\n    batch_by_user_message: bool = True\n\n    cur_batch_textnode: TextNode = Field(\n        default_factory=_get_starter_node_for_new_batch,\n        description=\"The super node for the current active user-message batch.\",\n    )\n\n    @field_validator(\"vector_index\")\n    @classmethod\n    def validate_vector_index(cls, value: Any) -> Any:\n        \"\"\"Validate vector index.\"\"\"\n        # NOTE: we can't import VectorStoreIndex directly due to circular imports,\n        # which is why the type is Any\n        from llama_index.core.indices.vector_store import VectorStoreIndex\n\n        if not isinstance(value, VectorStoreIndex):\n            raise ValueError(\n                f\"Expected 'vector_index' to be an instance of VectorStoreIndex, got {type(value)}\"\n            )\n        return value\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"VectorMemory\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        vector_store: Optional[BasePydanticVectorStore] = None,\n        embed_model: Optional[EmbedType] = None,\n        index_kwargs: Optional[Dict] = None,\n        retriever_kwargs: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> \"VectorMemory\":\n        \"\"\"Create vector memory.\n\n        Args:\n            vector_store (Optional[BasePydanticVectorStore]): vector store (note: delete_nodes must\n                be implemented. At time of writing (May 2024), Chroma, Qdrant and\n                SimpleVectorStore all support delete_nodes.\n            embed_model (Optional[EmbedType]): embedding model\n            index_kwargs (Optional[Dict]): kwargs for initializing the index\n            retriever_kwargs (Optional[Dict]): kwargs for initializing the retriever\n\n        \"\"\"\n        from llama_index.core.indices.vector_store import VectorStoreIndex\n\n        if kwargs:\n            raise ValueError(f\"Unexpected kwargs: {kwargs}\")\n\n        index_kwargs = index_kwargs or {}\n        retriever_kwargs = retriever_kwargs or {}\n\n        if vector_store is None:\n            # initialize a blank in-memory vector store\n            # NOTE: can't easily do that from `from_vector_store` at the moment.\n            index = VectorStoreIndex.from_documents(\n                [], embed_model=embed_model, **index_kwargs\n            )\n        else:\n            index = VectorStoreIndex.from_vector_store(\n                vector_store, embed_model=embed_model, **index_kwargs\n            )\n        return cls(vector_index=index, retriever_kwargs=retriever_kwargs)\n\n    def get(\n        self, input: Optional[str] = None, initial_token_count: int = 0, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        if input is None:\n            return []\n\n        # retrieve from index\n        retriever = self.vector_index.as_retriever(**self.retriever_kwargs)\n        nodes = retriever.retrieve(input or \"\")\n\n        # retrieve underlying messages\n        return [\n            ChatMessage.model_validate(sub_dict)\n            for node in nodes\n            for sub_dict in node.metadata[\"sub_dicts\"]\n        ]\n\n    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        # TODO: while we could implement get_all, would be hacky through metadata filtering\n        # since vector stores don't easily support get()\n        raise ValueError(\n            \"Vector memory does not support get_all method, can only retrieve based on input.\"\n        )\n\n    def _commit_node(self, override_last: bool = False) -> None:\n        \"\"\"Commit new node to vector store.\"\"\"\n        if self.cur_batch_textnode.text == \"\":\n            return\n\n        if override_last:\n            # delete the last node\n            # This is needed since we're updating the last node in the vector\n            # index as its being updated. When a new user-message batch starts\n            # we already will have the last user message group committed to the\n            # vector store index and so we don't need to override_last (i.e. see\n            # logic in self.put().)\n            self.vector_index.delete_nodes([self.cur_batch_textnode.id_])\n\n        self.vector_index.insert_nodes([self.cur_batch_textnode])\n\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        if not self.batch_by_user_message or message.role in [\n            MessageRole.USER,\n            MessageRole.SYSTEM,\n        ]:\n            # if not batching by user message, commit to vector store immediately after adding\n            self.cur_batch_textnode = _get_starter_node_for_new_batch()\n\n        # update current batch textnode\n        sub_dict = _stringify_chat_message(message)\n        if self.cur_batch_textnode.text == \"\":\n            self.cur_batch_textnode.text += sub_dict[\"content\"] or \"\"\n        else:\n            self.cur_batch_textnode.text += \" \" + (sub_dict[\"content\"] or \"\")\n        self.cur_batch_textnode.metadata[\"sub_dicts\"].append(sub_dict)\n        self._commit_node(override_last=True)\n\n    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n        self.reset()\n        for message in messages:\n            self.put(message)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.vector_index.vector_store.clear()\n\n\nVectorMemory.model_rebuild()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/memory/__init__.py",
    "filename": "__init__.py",
    "relpath": "memory/__init__.py",
    "start_line": 1,
    "end_line": 13,
    "length": 13,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer\nfrom llama_index.core.memory.chat_summary_memory_buffer import ChatSummaryMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.memory.vector_memory import VectorMemory\nfrom llama_index.core.memory.simple_composable_memory import SimpleComposableMemory\n\n__all__ = [\n    \"BaseMemory\",\n    \"ChatMemoryBuffer\",\n    \"ChatSummaryMemoryBuffer\",\n    \"SimpleComposableMemory\",\n    \"VectorMemory\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/memory/simple_composable_memory.py",
    "filename": "simple_composable_memory.py",
    "relpath": "memory/simple_composable_memory.py",
    "start_line": 1,
    "end_line": 158,
    "length": 158,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "from_defaults",
      "_format_secondary_messages",
      "get",
      "_compose_message_histories",
      "get_all",
      "put",
      "aput",
      "set",
      "reset"
    ],
    "chunk_class_names": [
      "SimpleComposableMemory"
    ],
    "document_function_names": [
      "class_name",
      "from_defaults",
      "_format_secondary_messages",
      "get",
      "_compose_message_histories",
      "get_all",
      "put",
      "aput",
      "set",
      "reset"
    ],
    "document_class_names": [
      "SimpleComposableMemory"
    ],
    "content": "from typing import Any, List, Optional\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.bridge.pydantic import Field, SerializeAsAny\nfrom llama_index.core.memory.types import (\n    BaseMemory,\n)\nfrom llama_index.core.memory import ChatMemoryBuffer\n\nDEFAULT_INTRO_HISTORY_MESSAGE = \"Below are a set of relevant dialogues retrieved from potentially several memory sources:\"\nDEFAULT_OUTRO_HISTORY_MESSAGE = \"This is the end of the retrieved message dialogues.\"\n\n\nclass SimpleComposableMemory(BaseMemory):\n    \"\"\"A simple composition of potentially several memory sources.\n\n    This composable memory considers one of the memory sources as the main\n    one and the others as secondary. The secondary memory sources get added to\n    the chat history only in either the system prompt or to the first user\n    message within the chat history.\n\n    Args:\n        primary_memory: (BaseMemory) The main memory buffer for agent.\n        secondary_memory_sources: (List(BaseMemory)) Secondary memory sources.\n            Retrieved messages from these sources get added to the system prompt message.\n    \"\"\"\n\n    primary_memory: SerializeAsAny[BaseMemory] = Field(\n        description=\"Primary memory source for chat agent.\",\n    )\n    secondary_memory_sources: List[SerializeAsAny[BaseMemory]] = Field(\n        default_factory=list, description=\"Secondary memory sources.\"\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SimpleComposableMemory\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        primary_memory: Optional[BaseMemory] = None,\n        secondary_memory_sources: Optional[List[BaseMemory]] = None,\n        **kwargs: Any,\n    ) -> \"SimpleComposableMemory\":\n        \"\"\"Create a simple composable memory from an LLM.\"\"\"\n        if kwargs:\n            raise ValueError(f\"Unexpected kwargs: {kwargs}\")\n\n        primary_memory = primary_memory or ChatMemoryBuffer.from_defaults()\n        secondary_memory_sources = secondary_memory_sources or []\n\n        return cls(\n            primary_memory=primary_memory,\n            secondary_memory_sources=secondary_memory_sources,\n        )\n\n    def _format_secondary_messages(\n        self, secondary_chat_histories: List[List[ChatMessage]]\n    ) -> str:\n        \"\"\"Formats retrieved historical messages into a single string.\"\"\"\n        # TODO: use PromptTemplate for this\n        formatted_history = \"\\n\\n\" + DEFAULT_INTRO_HISTORY_MESSAGE + \"\\n\"\n        for ix, chat_history in enumerate(secondary_chat_histories):\n            formatted_history += (\n                f\"\\n=====Relevant messages from memory source {ix + 1}=====\\n\\n\"\n            )\n            for m in chat_history:\n                formatted_history += f\"\\t{m.role.upper()}: {m.content}\\n\"\n            formatted_history += (\n                f\"\\n=====End of relevant messages from memory source {ix + 1}======\\n\\n\"\n            )\n\n        formatted_history += DEFAULT_OUTRO_HISTORY_MESSAGE\n        return formatted_history\n\n    def get(self, input: Optional[str] = None, **kwargs: Any) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self._compose_message_histories(input, **kwargs)\n\n    def _compose_message_histories(\n        self, input: Optional[str] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        # get from primary\n        messages = self.primary_memory.get(input=input, **kwargs)\n\n        # get from secondary\n        # TODO: remove any repeated messages in secondary and primary memory\n        secondary_histories = []\n        for mem in self.secondary_memory_sources:\n            secondary_history = mem.get(input, **kwargs)\n            secondary_history = [m for m in secondary_history if m not in messages]\n\n            if len(secondary_history) > 0:\n                secondary_histories.append(secondary_history)\n\n        # format secondary memory\n        if len(secondary_histories) > 0:\n            single_secondary_memory_str = self._format_secondary_messages(\n                secondary_histories\n            )\n\n            # add single_secondary_memory_str to chat_history\n            if len(messages) > 0 and messages[0].role == MessageRole.SYSTEM:\n                assert messages[0].content is not None\n                system_message = messages[0].content.split(\n                    DEFAULT_INTRO_HISTORY_MESSAGE\n                )[0]\n                messages[0] = ChatMessage(\n                    content=system_message.strip() + single_secondary_memory_str,\n                    role=MessageRole.SYSTEM,\n                )\n            else:\n                messages.insert(\n                    0,\n                    ChatMessage(\n                        content=\"You are a helpful assistant.\"\n                        + single_secondary_memory_str,\n                        role=MessageRole.SYSTEM,\n                    ),\n                )\n        return messages\n\n    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\n\n        Uses primary memory get_all only.\n        \"\"\"\n        return self.primary_memory.get_all()\n\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        self.primary_memory.put(message)\n        for mem in self.secondary_memory_sources:\n            mem.put(message)\n\n    async def aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        await self.primary_memory.aput(message)\n        for mem in self.secondary_memory_sources:\n            await mem.aput(message)\n\n    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n        self.primary_memory.set(messages)\n        for mem in self.secondary_memory_sources:\n            # finalize task often sets, but secondary memory is meant for\n            # long-term memory rather than main chat memory buffer\n            # so use put_messages instead\n            mem.put_messages(messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.primary_memory.reset()\n        for mem in self.secondary_memory_sources:\n            mem.reset()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/memory/chat_memory_buffer.py",
    "filename": "chat_memory_buffer.py",
    "relpath": "memory/chat_memory_buffer.py",
    "start_line": 1,
    "end_line": 153,
    "length": 153,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "validate_memory",
      "from_defaults",
      "to_string",
      "from_string",
      "to_dict",
      "from_dict",
      "get",
      "_token_count_for_messages"
    ],
    "chunk_class_names": [
      "ChatMemoryBuffer",
      "name"
    ],
    "document_function_names": [
      "class_name",
      "validate_memory",
      "from_defaults",
      "to_string",
      "from_string",
      "to_dict",
      "from_dict",
      "get",
      "_token_count_for_messages"
    ],
    "document_class_names": [
      "ChatMemoryBuffer",
      "name"
    ],
    "content": "import json\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.bridge.pydantic import Field, model_validator\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory.types import (\n    DEFAULT_CHAT_STORE_KEY,\n    BaseChatStoreMemory,\n)\nfrom llama_index.core.storage.chat_store import BaseChatStore, SimpleChatStore\nfrom llama_index.core.utils import get_tokenizer\n\nDEFAULT_TOKEN_LIMIT_RATIO = 0.75\nDEFAULT_TOKEN_LIMIT = 3000\n\n\nclass ChatMemoryBuffer(BaseChatStoreMemory):\n    \"\"\"Simple buffer for storing chat history.\"\"\"\n\n    token_limit: int\n    tokenizer_fn: Callable[[str], List] = Field(\n        default_factory=get_tokenizer,\n        exclude=True,\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"ChatMemoryBuffer\"\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_memory(cls, values: dict) -> dict:\n        # Validate token limit\n        token_limit = values.get(\"token_limit\", -1)\n        if token_limit < 1:\n            raise ValueError(\"Token limit must be set and greater than 0.\")\n\n        # Validate tokenizer -- this avoids errors when loading from json/dict\n        tokenizer_fn = values.get(\"tokenizer_fn\", None)\n        if tokenizer_fn is None:\n            values[\"tokenizer_fn\"] = get_tokenizer()\n\n        return values\n\n    @classmethod\n    def from_defaults(\n        cls,\n        chat_history: Optional[List[ChatMessage]] = None,\n        llm: Optional[LLM] = None,\n        chat_store: Optional[BaseChatStore] = None,\n        chat_store_key: str = DEFAULT_CHAT_STORE_KEY,\n        token_limit: Optional[int] = None,\n        tokenizer_fn: Optional[Callable[[str], List]] = None,\n        **kwargs: Any,\n    ) -> \"ChatMemoryBuffer\":\n        \"\"\"Create a chat memory buffer from an LLM.\"\"\"\n        if kwargs:\n            raise ValueError(f\"Unexpected kwargs: {kwargs}\")\n\n        if llm is not None:\n            context_window = llm.metadata.context_window\n            token_limit = token_limit or int(context_window * DEFAULT_TOKEN_LIMIT_RATIO)\n        elif token_limit is None:\n            token_limit = DEFAULT_TOKEN_LIMIT\n\n        if chat_history is not None:\n            chat_store = chat_store or SimpleChatStore()\n            chat_store.set_messages(chat_store_key, chat_history)\n\n        return cls(\n            token_limit=token_limit,\n            tokenizer_fn=tokenizer_fn or get_tokenizer(),\n            chat_store=chat_store or SimpleChatStore(),\n            chat_store_key=chat_store_key,\n        )\n\n    def to_string(self) -> str:\n        \"\"\"Convert memory to string.\"\"\"\n        return self.json()\n\n    @classmethod\n    def from_string(cls, json_str: str) -> \"ChatMemoryBuffer\":\n        \"\"\"Create a chat memory buffer from a string.\"\"\"\n        dict_obj = json.loads(json_str)\n        return cls.from_dict(dict_obj)\n\n    def to_dict(self, **kwargs: Any) -> dict:\n        \"\"\"Convert memory to dict.\"\"\"\n        return self.dict()\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> \"ChatMemoryBuffer\":\n        from llama_index.core.storage.chat_store.loading import load_chat_store\n\n        # NOTE: this handles backwards compatibility with the old chat history\n        if \"chat_history\" in data:\n            chat_history = data.pop(\"chat_history\")\n            simple_store = SimpleChatStore(store={DEFAULT_CHAT_STORE_KEY: chat_history})\n            data[\"chat_store\"] = simple_store\n        elif \"chat_store\" in data:\n            chat_store_dict = data.pop(\"chat_store\")\n            chat_store = load_chat_store(chat_store_dict)\n            data[\"chat_store\"] = chat_store\n\n        return cls(**data)\n\n    def get(\n        self, input: Optional[str] = None, initial_token_count: int = 0, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        chat_history = self.get_all()\n\n        if initial_token_count > self.token_limit:\n            raise ValueError(\"Initial token count exceeds token limit\")\n\n        message_count = len(chat_history)\n\n        cur_messages = chat_history[-message_count:]\n        token_count = self._token_count_for_messages(cur_messages) + initial_token_count\n\n        while token_count > self.token_limit and message_count > 1:\n            message_count -= 1\n            while chat_history[-message_count].role in (\n                MessageRole.TOOL,\n                MessageRole.ASSISTANT,\n            ):\n                # we cannot have an assistant message at the start of the chat history\n                # if after removal of the first, we have an assistant message,\n                # we need to remove the assistant message too\n                #\n                # all tool messages should be preceded by an assistant message\n                # if we remove a tool message, we need to remove the assistant message too\n                message_count -= 1\n\n            cur_messages = chat_history[-message_count:]\n            token_count = (\n                self._token_count_for_messages(cur_messages) + initial_token_count\n            )\n\n        # catch one message longer than token limit\n        if token_count > self.token_limit or message_count <= 0:\n            return []\n\n        return chat_history[-message_count:]\n\n    def _token_count_for_messages(self, messages: List[ChatMessage]) -> int:\n        if len(messages) <= 0:\n            return 0\n\n        msg_str = \" \".join(str(m.content) for m in messages)\n        return len(self.tokenizer_fn(msg_str))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/memory/chat_summary_memory_buffer.py",
    "filename": "chat_summary_memory_buffer.py",
    "relpath": "memory/chat_summary_memory_buffer.py",
    "start_line": 1,
    "end_line": 333,
    "length": 333,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "serialize_courses_in_order",
      "validate_memory",
      "from_defaults",
      "class_name",
      "to_string",
      "to_dict",
      "from_string",
      "from_dict",
      "get",
      "get_all",
      "put",
      "aput",
      "set",
      "reset",
      "get_token_count",
      "_token_count_for_messages",
      "_split_messages_summary_or_full_text",
      "_summarize_oldest_chat_history",
      "_get_prompt_to_summarize",
      "_handle_assistant_and_tool_messages"
    ],
    "chunk_class_names": [
      "ChatSummaryMemoryBuffer",
      "name"
    ],
    "document_function_names": [
      "serialize_courses_in_order",
      "validate_memory",
      "from_defaults",
      "class_name",
      "to_string",
      "to_dict",
      "from_string",
      "from_dict",
      "get",
      "get_all",
      "put",
      "aput",
      "set",
      "reset",
      "get_token_count",
      "_token_count_for_messages",
      "_split_messages_summary_or_full_text",
      "_summarize_oldest_chat_history",
      "_get_prompt_to_summarize",
      "_handle_assistant_and_tool_messages"
    ],
    "document_class_names": [
      "ChatSummaryMemoryBuffer",
      "name"
    ],
    "content": "import json\nimport logging\nfrom typing import Any, Callable, Dict, List, Tuple, Optional\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    PrivateAttr,\n    model_validator,\n    field_serializer,\n    SerializeAsAny,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory.types import DEFAULT_CHAT_STORE_KEY, BaseMemory\nfrom llama_index.core.storage.chat_store import BaseChatStore, SimpleChatStore\nfrom llama_index.core.utils import get_tokenizer\n\nDEFAULT_TOKEN_LIMIT_RATIO = 0.75\nDEFAULT_TOKEN_LIMIT = 2000\nSUMMARIZE_PROMPT = \"The following is a conversation between the user and assistant. Write a concise summary about the contents of this conversation.\"\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Add option for last N user/assistant history interactions instead of token limit\nclass ChatSummaryMemoryBuffer(BaseMemory):\n    \"\"\"Buffer for storing chat history that uses the full text for the latest\n    {token_limit}.\n\n    All older messages are iteratively summarized using the {llm} provided, with\n    the max number of tokens defined by the {llm}.\n\n    User can specify whether initial tokens (usually a system prompt)\n    should be counted as part of the {token_limit}\n    using the parameter {count_initial_tokens}.\n\n    This buffer is useful to retain the most important information from a\n    long chat history, while limiting the token count and latency\n    of each request to the LLM.\n    \"\"\"\n\n    token_limit: int\n    count_initial_tokens: bool = False\n    llm: Optional[SerializeAsAny[LLM]] = None\n    summarize_prompt: Optional[str] = None\n    tokenizer_fn: Callable[[str], List] = Field(\n        default_factory=get_tokenizer,\n        exclude=True,\n    )\n\n    chat_store: SerializeAsAny[BaseChatStore] = Field(default_factory=SimpleChatStore)\n    chat_store_key: str = Field(default=DEFAULT_CHAT_STORE_KEY)\n\n    _token_count: int = PrivateAttr(default=0)\n\n    @field_serializer(\"chat_store\")\n    def serialize_courses_in_order(self, chat_store: BaseChatStore) -> dict:\n        res = chat_store.model_dump()\n        res.update({\"class_name\": chat_store.class_name()})\n        return res\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_memory(cls, values: dict) -> dict:\n        \"\"\"Validate the memory.\"\"\"\n        # Validate token limits\n        token_limit = values.get(\"token_limit\", -1)\n        if token_limit < 1:\n            raise ValueError(\n                \"Token limit for full-text messages must be set and greater than 0.\"\n            )\n\n        # Validate tokenizer -- this avoids errors when loading from json/dict\n        tokenizer_fn = values.get(\"tokenizer_fn\", None)\n        if tokenizer_fn is None:\n            values[\"tokenizer_fn\"] = get_tokenizer()\n\n        return values\n\n    @classmethod\n    def from_defaults(\n        cls,\n        chat_history: Optional[List[ChatMessage]] = None,\n        llm: Optional[LLM] = None,\n        chat_store: Optional[BaseChatStore] = None,\n        chat_store_key: str = DEFAULT_CHAT_STORE_KEY,\n        token_limit: Optional[int] = None,\n        tokenizer_fn: Optional[Callable[[str], List]] = None,\n        summarize_prompt: Optional[str] = None,\n        count_initial_tokens: bool = False,\n        **kwargs: Any,\n    ) -> \"ChatSummaryMemoryBuffer\":\n        \"\"\"Create a chat memory buffer from an LLM\n        and an initial list of chat history messages.\n        \"\"\"\n        if kwargs:\n            raise ValueError(f\"Unexpected keyword arguments: {kwargs}\")\n\n        if llm is not None:\n            context_window = llm.metadata.context_window\n            token_limit = token_limit or int(context_window * DEFAULT_TOKEN_LIMIT_RATIO)\n        elif token_limit is None:\n            token_limit = DEFAULT_TOKEN_LIMIT\n\n        chat_store = chat_store or SimpleChatStore()\n\n        if chat_history is not None:\n            chat_store.set_messages(chat_store_key, chat_history)\n\n        summarize_prompt = summarize_prompt or SUMMARIZE_PROMPT\n        return cls(\n            llm=llm,\n            token_limit=token_limit,\n            # TODO: Check if we can get the tokenizer from the llm\n            tokenizer_fn=tokenizer_fn or get_tokenizer(),\n            summarize_prompt=summarize_prompt,\n            chat_store=chat_store,\n            chat_store_key=chat_store_key,\n            count_initial_tokens=count_initial_tokens,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"ChatSummaryMemoryBuffer\"\n\n    def to_string(self) -> str:\n        \"\"\"Convert memory to string.\"\"\"\n        return self.json()\n\n    def to_dict(self, **kwargs: Any) -> dict:\n        \"\"\"Convert memory to dict.\"\"\"\n        return self.dict()\n\n    @classmethod\n    def from_string(cls, json_str: str, **kwargs: Any) -> \"ChatSummaryMemoryBuffer\":\n        \"\"\"Create a chat memory buffer from a string.\"\"\"\n        dict_obj = json.loads(json_str)\n        return cls.from_dict(dict_obj, **kwargs)\n\n    @classmethod\n    def from_dict(\n        cls, data: Dict[str, Any], **kwargs: Any\n    ) -> \"ChatSummaryMemoryBuffer\":\n        from llama_index.core.storage.chat_store.loading import load_chat_store\n\n        # NOTE: this handles backwards compatibility with the old chat history\n        if \"chat_history\" in data:\n            chat_history = data.pop(\"chat_history\")\n            simple_store = SimpleChatStore(store={DEFAULT_CHAT_STORE_KEY: chat_history})\n            data[\"chat_store\"] = simple_store\n        elif \"chat_store\" in data:\n            chat_store_dict = data.pop(\"chat_store\")\n            chat_store = load_chat_store(chat_store_dict)\n            data[\"chat_store\"] = chat_store\n\n        # NOTE: The llm will have to be set manually in kwargs\n        if \"llm\" in data:\n            data.pop(\"llm\")\n\n        return cls(**data, **kwargs)\n\n    def get(\n        self, input: Optional[str] = None, initial_token_count: int = 0, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        chat_history = self.get_all()\n        if len(chat_history) == 0:\n            return []\n\n        # Give the user the choice whether to count the system prompt or not\n        if self.count_initial_tokens:\n            if initial_token_count > self.token_limit:\n                raise ValueError(\"Initial token count exceeds token limit\")\n            self._token_count = initial_token_count\n\n        (\n            chat_history_full_text,\n            chat_history_to_be_summarized,\n        ) = self._split_messages_summary_or_full_text(chat_history)\n\n        if self.llm is None or len(chat_history_to_be_summarized) == 0:\n            # Simply remove the message that don't fit the buffer anymore\n            updated_history = chat_history_full_text\n        else:\n            updated_history = [\n                self._summarize_oldest_chat_history(chat_history_to_be_summarized),\n                *chat_history_full_text,\n            ]\n\n        self.reset()\n        self._token_count = 0\n        self.set(updated_history)\n\n        return updated_history\n\n    def get_all(self) -> List[ChatMessage]:\n        \"\"\"Get all chat history.\"\"\"\n        return self.chat_store.get_messages(self.chat_store_key)\n\n    def put(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        # ensure everything is serialized\n        self.chat_store.add_message(self.chat_store_key, message)\n\n    async def aput(self, message: ChatMessage) -> None:\n        \"\"\"Put chat history.\"\"\"\n        await self.chat_store.async_add_message(self.chat_store_key, message)\n\n    def set(self, messages: List[ChatMessage]) -> None:\n        \"\"\"Set chat history.\"\"\"\n        self.chat_store.set_messages(self.chat_store_key, messages)\n\n    def reset(self) -> None:\n        \"\"\"Reset chat history.\"\"\"\n        self.chat_store.delete_messages(self.chat_store_key)\n\n    def get_token_count(self) -> int:\n        \"\"\"Returns the token count of the memory buffer (excluding the last assistant response).\"\"\"\n        return self._token_count\n\n    def _token_count_for_messages(self, messages: List[ChatMessage]) -> int:\n        \"\"\"Get token count for list of messages.\"\"\"\n        if len(messages) <= 0:\n            return 0\n\n        msg_str = \" \".join(str(m.content) for m in messages)\n        return len(self.tokenizer_fn(msg_str))\n\n    def _split_messages_summary_or_full_text(\n        self, chat_history: List[ChatMessage]\n    ) -> Tuple[List[ChatMessage], List[ChatMessage]]:\n        \"\"\"Determine which messages will be included as full text,\n        and which will have to be summarized by the llm.\n        \"\"\"\n        chat_history_full_text: List[ChatMessage] = []\n        message_count = len(chat_history)\n        while (\n            message_count > 0\n            and self.get_token_count()\n            + self._token_count_for_messages([chat_history[-1]])\n            <= self.token_limit\n        ):\n            # traverse the history in reverse order, when token limit is about to be\n            # exceeded, we stop, so remaining messages are summarized\n            self._token_count += self._token_count_for_messages([chat_history[-1]])\n            chat_history_full_text.insert(0, chat_history.pop())\n            message_count -= 1\n\n        chat_history_to_be_summarized = chat_history.copy()\n        self._handle_assistant_and_tool_messages(\n            chat_history_full_text, chat_history_to_be_summarized\n        )\n\n        return chat_history_full_text, chat_history_to_be_summarized\n\n    def _summarize_oldest_chat_history(\n        self, chat_history_to_be_summarized: List[ChatMessage]\n    ) -> ChatMessage:\n        \"\"\"Use the llm to summarize the messages that do not fit into the\n        buffer.\n        \"\"\"\n        assert self.llm is not None\n\n        # Only summarize if there is new information to be summarized\n        if (\n            len(chat_history_to_be_summarized) == 1\n            and chat_history_to_be_summarized[0].role == MessageRole.SYSTEM\n        ):\n            return chat_history_to_be_summarized[0]\n\n        summarize_prompt = [\n            ChatMessage(\n                role=MessageRole.SYSTEM,\n                content=self.summarize_prompt,\n            )\n        ]\n        summarize_prompt.append(\n            ChatMessage(\n                role=MessageRole.USER,\n                content=self._get_prompt_to_summarize(chat_history_to_be_summarized),\n            )\n        )\n        # TODO: Maybe it is better to pass a list of history to llm\n        r = self.llm.chat(summarize_prompt)\n        return ChatMessage(\n            role=MessageRole.SYSTEM,\n            content=r.message.content,\n        )\n\n    def _get_prompt_to_summarize(\n        self, chat_history_to_be_summarized: List[ChatMessage]\n    ) -> str:\n        \"\"\"Ask the LLM to summarize the chat history so far.\"\"\"\n        # TODO: This probably works better when question/answers are considered together.\n        prompt = '\"Transcript so far: '\n        for msg in chat_history_to_be_summarized:\n            if not isinstance(msg.content, str):\n                continue\n\n            prompt += msg.role + \": \"\n            if msg.content:\n                prompt += msg.content + \"\\n\\n\"\n            else:\n                prompt += (\n                    \"\\n\".join(\n                        [\n                            f\"Calling a function: {call!s}\"\n                            for call in msg.additional_kwargs.get(\"tool_calls\", [])\n                        ]\n                    )\n                    + \"\\n\\n\"\n                )\n        prompt += '\"\\n\\n'\n        return prompt\n\n    def _handle_assistant_and_tool_messages(\n        self,\n        chat_history_full_text: List[ChatMessage],\n        chat_history_to_be_summarized: List[ChatMessage],\n    ) -> None:\n        \"\"\"To avoid breaking API's, we need to ensure the following.\n\n        - the first message cannot be ASSISTANT\n        - ASSISTANT/TOOL should be considered in pairs\n        Therefore, we switch messages to summarized list until the first message is\n        not an ASSISTANT or TOOL message.\n        \"\"\"\n        while chat_history_full_text and chat_history_full_text[0].role in (\n            MessageRole.ASSISTANT,\n            MessageRole.TOOL,\n        ):\n            chat_history_to_be_summarized.append(chat_history_full_text.pop(0))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/text_splitter.py",
    "filename": "text_splitter.py",
    "relpath": "langchain_helpers/text_splitter.py",
    "start_line": 1,
    "end_line": 2,
    "length": 2,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# backward compatibility\nfrom llama_index.core.text_splitter import *"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/memory_wrapper.py",
    "filename": "memory_wrapper.py",
    "relpath": "langchain_helpers/memory_wrapper.py",
    "start_line": 1,
    "end_line": 199,
    "length": 199,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_prompt_input_key",
      "memory_variables",
      "_get_prompt_input_key",
      "load_memory_variables",
      "save_context",
      "clear",
      "__repr__",
      "memory_variables",
      "_get_prompt_input_key",
      "load_memory_variables",
      "save_context",
      "clear",
      "__repr__"
    ],
    "chunk_class_names": [
      "GPTIndexMemory",
      "GPTIndexChatMemory"
    ],
    "document_function_names": [
      "get_prompt_input_key",
      "memory_variables",
      "_get_prompt_input_key",
      "load_memory_variables",
      "save_context",
      "clear",
      "__repr__",
      "memory_variables",
      "_get_prompt_input_key",
      "load_memory_variables",
      "save_context",
      "clear",
      "__repr__"
    ],
    "document_class_names": [
      "GPTIndexMemory",
      "GPTIndexChatMemory"
    ],
    "content": "\"\"\"Langchain memory wrapper (for LlamaIndex).\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.bridge.langchain import (\n    AIMessage,\n    BaseChatMemory,\n    BaseMessage,\n    HumanMessage,\n)\nfrom llama_index.core.bridge.langchain import BaseMemory as Memory\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.schema import Document\nfrom llama_index.core.utils import get_new_id\n\n\ndef get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:\n    \"\"\"Get prompt input key.\n\n    Copied over from langchain.\n\n    \"\"\"\n    # \"stop\" is a special key that can be passed as input but is not used to\n    # format the prompt.\n    prompt_input_keys = list(set(inputs).difference([*memory_variables, \"stop\"]))\n    if len(prompt_input_keys) != 1:\n        raise ValueError(f\"One input key expected got {prompt_input_keys}\")\n    return prompt_input_keys[0]\n\n\nclass GPTIndexMemory(Memory):\n    \"\"\"Langchain memory wrapper (for LlamaIndex).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseIndex): LlamaIndex instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for LlamaIndex query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        # TODO: wrap in prompt\n        # TODO: add option to return the raw text\n        # NOTE: currently it's a hack\n        query_engine = self.index.as_query_engine(**self.query_kwargs)\n        response = query_engine.query(query_str)\n        return {self.memory_key: str(response)}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = next(iter(outputs.keys()))\n        else:\n            output_key = self.output_key\n        human = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai = f\"{self.ai_prefix}: \" + outputs[output_key]\n        doc_text = f\"{human}\\n{ai}\"\n        doc = Document(text=doc_text)\n        self.index.insert(doc)\n\n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\"\n\n\nclass GPTIndexChatMemory(BaseChatMemory):\n    \"\"\"Langchain chat memory wrapper (for LlamaIndex).\n\n    Args:\n        human_prefix (str): Prefix for human input. Defaults to \"Human\".\n        ai_prefix (str): Prefix for AI output. Defaults to \"AI\".\n        memory_key (str): Key for memory. Defaults to \"history\".\n        index (BaseIndex): LlamaIndex instance.\n        query_kwargs (Dict[str, Any]): Keyword arguments for LlamaIndex query.\n        input_key (Optional[str]): Input key. Defaults to None.\n        output_key (Optional[str]): Output key. Defaults to None.\n\n    \"\"\"\n\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    index: BaseIndex\n    query_kwargs: Dict = Field(default_factory=dict)\n    output_key: Optional[str] = None\n    input_key: Optional[str] = None\n\n    return_source: bool = False\n    id_to_message: Dict[str, BaseMessage] = Field(default_factory=dict)\n\n    @property\n    def memory_variables(self) -> List[str]:\n        \"\"\"Return memory variables.\"\"\"\n        return [self.memory_key]\n\n    def _get_prompt_input_key(self, inputs: Dict[str, Any]) -> str:\n        if self.input_key is None:\n            prompt_input_key = get_prompt_input_key(inputs, self.memory_variables)\n        else:\n            prompt_input_key = self.input_key\n        return prompt_input_key\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        query_str = inputs[prompt_input_key]\n\n        query_engine = self.index.as_query_engine(**self.query_kwargs)\n        response_obj = query_engine.query(query_str)\n        if self.return_source:\n            source_nodes = response_obj.source_nodes\n            if self.return_messages:\n                # get source messages from ids\n                source_ids = [sn.node.node_id for sn in source_nodes]\n                source_messages = [\n                    m for id, m in self.id_to_message.items() if id in source_ids\n                ]\n                # NOTE: type List[BaseMessage]\n                response: Any = source_messages\n            else:\n                source_texts = [sn.node.get_content() for sn in source_nodes]\n                response = \"\\n\\n\".join(source_texts)\n        else:\n            response = str(response_obj)\n        return {self.memory_key: response}\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        prompt_input_key = self._get_prompt_input_key(inputs)\n        if self.output_key is None:\n            if len(outputs) != 1:\n                raise ValueError(f\"One output key expected, got {outputs.keys()}\")\n            output_key = next(iter(outputs.keys()))\n        else:\n            output_key = self.output_key\n\n        # a bit different than existing langchain implementation\n        # because we want to track id's for messages\n        human_message = HumanMessage(content=inputs[prompt_input_key])\n        human_message_id = get_new_id(set(self.id_to_message.keys()))\n        ai_message = AIMessage(content=outputs[output_key])\n        ai_message_id = get_new_id(\n            set(self.id_to_message.keys()).union({human_message_id})\n        )\n\n        self.chat_memory.messages.append(human_message)\n        self.chat_memory.messages.append(ai_message)\n\n        self.id_to_message[human_message_id] = human_message\n        self.id_to_message[ai_message_id] = ai_message\n\n        human_txt = f\"{self.human_prefix}: \" + inputs[prompt_input_key]\n        ai_txt = f\"{self.ai_prefix}: \" + outputs[output_key]\n        human_doc = Document(text=human_txt, id_=human_message_id)\n        ai_doc = Document(text=ai_txt, id_=ai_message_id)\n        self.index.insert(human_doc)\n        self.index.insert(ai_doc)\n\n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"Return representation.\"\"\"\n        return \"GPTIndexMemory()\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/__init__.py",
    "filename": "__init__.py",
    "relpath": "langchain_helpers/__init__.py",
    "start_line": 1,
    "end_line": 9,
    "length": 9,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file for langchain helpers.\"\"\"\n\ntry:\n    import langchain  # noqa  # pants: no-infer-dep\nexcept ImportError:\n    raise ImportError(\n        \"langchain not installed. \"\n        \"Please install langchain with `pip install llama_index[langchain]`.\"\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/streaming.py",
    "filename": "streaming.py",
    "relpath": "langchain_helpers/streaming.py",
    "start_line": 1,
    "end_line": 60,
    "length": 60,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "__deepcopy__",
      "on_llm_new_token",
      "on_llm_end",
      "on_llm_error",
      "get_response_gen"
    ],
    "chunk_class_names": [
      "StreamingGeneratorCallbackHandler"
    ],
    "document_function_names": [
      "__init__",
      "__deepcopy__",
      "on_llm_new_token",
      "on_llm_end",
      "on_llm_error",
      "get_response_gen"
    ],
    "document_class_names": [
      "StreamingGeneratorCallbackHandler"
    ],
    "content": "import time\nfrom queue import Queue\nfrom threading import Event\nfrom typing import Any, Generator, List, Optional\nfrom uuid import UUID\n\nfrom llama_index.core.bridge.langchain import BaseCallbackHandler, LLMResult\n\n\nclass StreamingGeneratorCallbackHandler(BaseCallbackHandler):\n    \"\"\"Streaming callback handler.\"\"\"\n\n    def __init__(self) -> None:\n        self._token_queue: Queue = Queue()\n        self._done = Event()\n\n    def __deepcopy__(self, memo: Any) -> \"StreamingGeneratorCallbackHandler\":\n        # NOTE: hack to bypass deepcopy in langchain\n        return self\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n        self._token_queue.put_nowait(token)\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        self._done.set()\n\n    def on_llm_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        self._done.set()\n\n    def get_response_gen(self, timeout: float = 120.0) -> Generator:\n        \"\"\"Get response generator with timeout.\n\n        Args:\n            timeout (float): Maximum time in seconds to wait for the complete response.\n                            Defaults to 120 seconds.\n        \"\"\"\n        start_time = time.time()\n        while True:\n            if time.time() - start_time > timeout:\n                raise TimeoutError(\n                    f\"Response generation timed out after {timeout} seconds\"\n                )\n\n            if not self._token_queue.empty():\n                token = self._token_queue.get_nowait()\n                yield token\n            elif self._done.is_set():\n                break\n            else:\n                # Small sleep to prevent CPU spinning\n                time.sleep(0.01)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/agents/agents.py",
    "filename": "agents.py",
    "relpath": "langchain_helpers/agents/agents.py",
    "start_line": 1,
    "end_line": 91,
    "length": 91,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "create_llama_agent",
      "create_llama_chat_agent"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "create_llama_agent",
      "create_llama_chat_agent"
    ],
    "document_class_names": [],
    "content": "\"\"\"Create LlamaIndex agents.\"\"\"\n\nfrom typing import Any, Optional\n\nfrom llama_index.core.bridge.langchain import (\n    AgentExecutor,\n    AgentType,\n    BaseCallbackManager,\n    BaseLLM,\n    initialize_agent,\n)\nfrom llama_index.core.langchain_helpers.agents.toolkits import LlamaToolkit\n\n\ndef create_llama_agent(\n    toolkit: LlamaToolkit,\n    llm: BaseLLM,\n    agent: Optional[AgentType] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_path: Optional[str] = None,\n    agent_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    \"\"\"Load an agent executor given a Llama Toolkit and LLM.\n\n    NOTE: this is a light wrapper around initialize_agent in langchain.\n\n    Args:\n        toolkit: LlamaToolkit to use.\n        llm: Language model to use as the agent.\n        agent: A string that specified the agent type to use. Valid options are:\n            `zero-shot-react-description`\n            `react-docstore`\n            `self-ask-with-search`\n            `conversational-react-description`\n            `chat-zero-shot-react-description`,\n            `chat-conversational-react-description`,\n           If None and agent_path is also None, will default to\n            `zero-shot-react-description`.\n        callback_manager: CallbackManager to use. Global callback manager is used if\n            not provided. Defaults to None.\n        agent_path: Path to serialized agent to use.\n        agent_kwargs: Additional key word arguments to pass to the underlying agent\n        **kwargs: Additional key word arguments passed to the agent executor\n\n    Returns:\n        An agent executor\n    \"\"\"\n    llama_tools = toolkit.get_tools()\n    return initialize_agent(\n        llama_tools,\n        llm,\n        agent=agent,\n        callback_manager=callback_manager,\n        agent_path=agent_path,\n        agent_kwargs=agent_kwargs,\n        **kwargs,\n    )\n\n\ndef create_llama_chat_agent(\n    toolkit: LlamaToolkit,\n    llm: BaseLLM,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_kwargs: Optional[dict] = None,\n    **kwargs: Any,\n) -> AgentExecutor:\n    \"\"\"Load a chat llama agent given a Llama Toolkit and LLM.\n\n    Args:\n        toolkit: LlamaToolkit to use.\n        llm: Language model to use as the agent.\n        callback_manager: CallbackManager to use. Global callback manager is used if\n            not provided. Defaults to None.\n        agent_kwargs: Additional key word arguments to pass to the underlying agent\n        **kwargs: Additional key word arguments passed to the agent executor\n\n    Returns:\n        An agent executor\n    \"\"\"\n    # chat agent\n    # TODO: explore chat-conversational-react-description\n    agent_type = AgentType.CONVERSATIONAL_REACT_DESCRIPTION\n    return create_llama_agent(\n        toolkit,\n        llm,\n        agent=agent_type,\n        callback_manager=callback_manager,\n        agent_kwargs=agent_kwargs,\n        **kwargs,\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/agents/tools.py",
    "filename": "tools.py",
    "relpath": "langchain_helpers/agents/tools.py",
    "start_line": 1,
    "end_line": 68,
    "length": 68,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_response_with_sources",
      "from_tool_config",
      "_run",
      "_arun"
    ],
    "chunk_class_names": [
      "IndexToolConfig",
      "LlamaIndexTool"
    ],
    "document_function_names": [
      "_get_response_with_sources",
      "from_tool_config",
      "_run",
      "_arun"
    ],
    "document_class_names": [
      "IndexToolConfig",
      "LlamaIndexTool"
    ],
    "content": "\"\"\"LlamaIndex Tool classes.\"\"\"\n\nfrom typing import Any, Dict, List\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.bridge.langchain import BaseTool\nfrom llama_index.core.bridge.pydantic import BaseModel, ConfigDict, Field\nfrom llama_index.core.schema import TextNode\n\n\ndef _get_response_with_sources(response: RESPONSE_TYPE) -> str:\n    \"\"\"Return a response with source node info.\"\"\"\n    source_data: List[Dict[str, Any]] = []\n    for source_node in response.source_nodes:\n        metadata = {}\n        if isinstance(source_node.node, TextNode):\n            start = source_node.node.start_char_idx\n            end = source_node.node.end_char_idx\n            if start is not None and end is not None:\n                metadata.update({\"start_char_idx\": start, \"end_char_idx\": end})\n\n        source_data.append(metadata)\n        source_data[-1][\"ref_doc_id\"] = source_node.node.ref_doc_id\n        source_data[-1][\"score\"] = source_node.score\n    return str({\"answer\": str(response), \"sources\": source_data})\n\n\nclass IndexToolConfig(BaseModel):\n    \"\"\"Configuration for LlamaIndex index tool.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    query_engine: BaseQueryEngine\n    name: str\n    description: str\n    tool_kwargs: Dict = Field(default_factory=dict)\n\n\nclass LlamaIndexTool(BaseTool):\n    \"\"\"Tool for querying a LlamaIndex.\"\"\"\n\n    # NOTE: name/description still needs to be set\n    query_engine: BaseQueryEngine\n    return_sources: bool = False\n\n    @classmethod\n    def from_tool_config(cls, tool_config: IndexToolConfig) -> \"LlamaIndexTool\":\n        \"\"\"Create a tool from a tool config.\"\"\"\n        return_sources = tool_config.tool_kwargs.pop(\"return_sources\", False)\n        return cls(\n            query_engine=tool_config.query_engine,\n            name=tool_config.name,\n            description=tool_config.description,\n            return_sources=return_sources,\n            **tool_config.tool_kwargs,\n        )\n\n    def _run(self, input: str) -> str:\n        response = self.query_engine.query(input)\n        if self.return_sources:\n            return _get_response_with_sources(response)\n        return str(response)\n\n    async def _arun(self, input: str) -> str:\n        response = await self.query_engine.aquery(input)\n        if self.return_sources:\n            return _get_response_with_sources(response)\n        return str(response)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/agents/__init__.py",
    "filename": "__init__.py",
    "relpath": "langchain_helpers/agents/__init__.py",
    "start_line": 1,
    "end_line": 21,
    "length": 21,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Llama integration with Langchain agents.\"\"\"\n\nfrom llama_index.core.langchain_helpers.agents.agents import (\n    create_llama_agent,\n    create_llama_chat_agent,\n)\nfrom llama_index.core.langchain_helpers.agents.toolkits import LlamaToolkit\nfrom llama_index.core.langchain_helpers.agents.tools import (\n    IndexToolConfig,\n    LlamaIndexTool,\n)\n\n__all__ = [\n    \"LlamaIndexTool\",\n    \"LlamaGraphTool\",\n    \"create_llama_agent\",\n    \"create_llama_chat_agent\",\n    \"LlamaToolkit\",\n    \"IndexToolConfig\",\n    \"GraphToolConfig\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/langchain_helpers/agents/toolkits.py",
    "filename": "toolkits.py",
    "relpath": "langchain_helpers/agents/toolkits.py",
    "start_line": 1,
    "end_line": 26,
    "length": 26,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_tools"
    ],
    "chunk_class_names": [
      "LlamaToolkit"
    ],
    "document_function_names": [
      "get_tools"
    ],
    "document_class_names": [
      "LlamaToolkit"
    ],
    "content": "\"\"\"LlamaIndex toolkit.\"\"\"\n\nfrom typing import List\n\nfrom llama_index.core.bridge.langchain import BaseTool, BaseToolkit\nfrom llama_index.core.bridge.pydantic import ConfigDict, Field\nfrom llama_index.core.langchain_helpers.agents.tools import (\n    IndexToolConfig,\n    LlamaIndexTool,\n)\n\n\nclass LlamaToolkit(BaseToolkit):\n    \"\"\"Toolkit for interacting with Llama indices.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    index_configs: List[IndexToolConfig] = Field(default_factory=list)\n\n    def get_tools(self) -> List[BaseTool]:\n        \"\"\"Get the tools in the toolkit.\"\"\"\n        index_tools: List[BaseTool] = [\n            LlamaIndexTool.from_tool_config(tool_config=tool_config)\n            for tool_config in self.index_configs\n        ]\n\n        return index_tools"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/data_sources.py",
    "filename": "data_sources.py",
    "relpath": "ingestion/data_sources.py",
    "start_line": 1,
    "end_line": 476,
    "length": 476,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "file_name",
      "class_name",
      "lazy_load_data",
      "from_component",
      "build_configured_data_source",
      "build_configurable_data_source_enum",
      "from_component",
      "configurable_data_source_type"
    ],
    "chunk_class_names": [
      "DataSource",
      "containing",
      "DocumentGroup",
      "ConfigurableComponent",
      "ConfiguredDataSource",
      "containing"
    ],
    "document_function_names": [
      "file_name",
      "class_name",
      "lazy_load_data",
      "from_component",
      "build_configured_data_source",
      "build_configurable_data_source_enum",
      "from_component",
      "configurable_data_source_type"
    ],
    "document_class_names": [
      "DataSource",
      "containing",
      "DocumentGroup",
      "ConfigurableComponent",
      "ConfiguredDataSource",
      "containing"
    ],
    "content": "import uuid\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Generic, Iterable, List, Optional, Type, TypeVar, cast\n\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    ValidationError,\n)\nfrom llama_index.core.readers.base import BasePydanticReader, ReaderConfig\nfrom llama_index.core.schema import BaseComponent, Document, TextNode\n\n\nclass DataSource(BaseModel):\n    \"\"\"\n    A class containing metadata for a type of data source.\n    \"\"\"\n\n    name: str = Field(\n        description=\"Unique and human-readable name for the type of data source\"\n    )\n    component_type: Type[BaseComponent] = Field(\n        description=\"Type of component that implements the data source\"\n    )\n\n\nclass DocumentGroup(BasePydanticReader):\n    \"\"\"\n    A group of documents, usually separate pages from a single file.\n    \"\"\"\n\n    file_path: str = Field(description=\"Path to the file containing the documents\")\n    documents: List[Document] = Field(\n        description=\"Sequential group of documents, usually separate pages from a single file.\"\n    )\n\n    @property\n    def file_name(self) -> str:\n        return Path(self.file_path).name\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"DocumentGroup\"\n\n    def lazy_load_data(self, *args: Any, **load_kwargs: Any) -> Iterable[Document]:\n        \"\"\"Load data from the input directory lazily.\"\"\"\n        return self.documents\n\n\nclass ConfigurableComponent(Enum):\n    @classmethod\n    def from_component(cls, component: BaseComponent) -> \"ConfigurableComponent\":\n        component_class = type(component)\n        for component_type in cls:\n            if component_type.value.component_type == component_class:\n                return component_type\n        raise ValueError(\n            f\"Component {component} is not a supported data source component.\"\n        )\n\n    def build_configured_data_source(\n        self, component: BaseComponent, name: Optional[str] = None\n    ) -> \"ConfiguredDataSource\":\n        component_type = self.value.component_type\n        if not isinstance(component, component_type):\n            raise ValueError(\n                f\"The enum value {self} is not compatible with component of \"\n                f\"type {type(component)}\"\n            )\n        elif isinstance(component, BasePydanticReader):\n            reader_config = ReaderConfig(reader=component)\n            return ConfiguredDataSource[ReaderConfig](\n                component=reader_config\n            )  # type: ignore\n\n        if isinstance(component, DocumentGroup) and name is None:\n            # if the component is a DocumentGroup, we want to use the\n            # full file path as the name of the data source\n            component = cast(DocumentGroup, component)\n            name = component.file_path\n\n        if name is None:\n            suffix = uuid.uuid1()\n            name = self.value.name + f\" [{suffix}]]\"\n        return ConfiguredDataSource[component_type](  # type: ignore\n            component=component, name=name\n        )\n\n\ndef build_configurable_data_source_enum() -> ConfigurableComponent:\n    \"\"\"\n    Build an enum of configurable data sources.\n    But conditional on if the corresponding reader is available.\n    \"\"\"\n    enum_members = []\n\n    try:\n        from llama_index.readers.discord import DiscordReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"DISCORD\",\n                DataSource(\n                    name=\"Discord\",\n                    component_type=DiscordReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.elasticsearch import (\n            ElasticsearchReader,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"ELASTICSEARCH\",\n                DataSource(\n                    name=\"Elasticsearch\",\n                    component_type=ElasticsearchReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.notion import NotionPageReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"NOTION_PAGE\",\n                DataSource(\n                    name=\"Notion Page\",\n                    component_type=NotionPageReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.slack import SlackReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"SLACK\",\n                DataSource(\n                    name=\"Slack\",\n                    component_type=SlackReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.twitter import (\n            TwitterTweetReader,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"TWITTER\",\n                DataSource(\n                    name=\"Twitter\",\n                    component_type=TwitterTweetReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.web import SimpleWebPageReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"SIMPLE_WEB_PAGE\",\n                DataSource(\n                    name=\"Simple Web Page\",\n                    component_type=SimpleWebPageReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.web import TrafilaturaWebReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"TRAFILATURA_WEB_PAGE\",\n                DataSource(\n                    name=\"Trafilatura Web Page\",\n                    component_type=TrafilaturaWebReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.web import (\n            BeautifulSoupWebReader,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"BEAUTIFUL_SOUP_WEB_PAGE\",\n                DataSource(\n                    name=\"Beautiful Soup Web Page\",\n                    component_type=BeautifulSoupWebReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.web import RssReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"RSS\",\n                DataSource(\n                    name=\"RSS\",\n                    component_type=RssReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.wikipedia import WikipediaReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"WIKIPEDIA\",\n                DataSource(\n                    name=\"Wikipedia\",\n                    component_type=WikipediaReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.youtube_transcript import (\n            YoutubeTranscriptReader,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"YOUTUBE_TRANSCRIPT\",\n                DataSource(\n                    name=\"Youtube Transcript\",\n                    component_type=YoutubeTranscriptReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.google import GoogleDocsReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"GOOGLE_DOCS\",\n                DataSource(\n                    name=\"Google Docs\",\n                    component_type=GoogleDocsReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.google import GoogleSheetsReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"GOOGLE_SHEETS\",\n                DataSource(\n                    name=\"Google Sheets\",\n                    component_type=GoogleSheetsReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.s3 import S3Reader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"S3\",\n                DataSource(\n                    name=\"S3\",\n                    component_type=S3Reader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.azstorage_blob import (\n            AzStorageBlobReader,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"AZURE_STORAGE_BLOB\",\n                DataSource(\n                    name=\"Azure Storage Blob\",\n                    component_type=AzStorageBlobReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.gcs import GCSReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"GCS\",\n                DataSource(\n                    name=\"GCS\",\n                    component_type=GCSReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.google import GoogleDriveReader  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"GOOGLE_DRIVE\",\n                DataSource(\n                    name=\"Google Drive\",\n                    component_type=GoogleDriveReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.microsoft_onedrive import (\n            OneDriveReader,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"MICROSOFT_ONEDRIVE\",\n                DataSource(\n                    name=\"Microsoft OneDrive\",\n                    component_type=OneDriveReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.readers.microsoft_sharepoint import (\n            SharePointReader,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"MICROSOFT_SHAREPOINT\",\n                DataSource(\n                    name=\"Microsoft Sharepoint\",\n                    component_type=SharePointReader,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    enum_members.append(\n        (\n            \"READER\",\n            DataSource(\n                name=\"Reader\",\n                component_type=ReaderConfig,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"DOCUMENT_GROUP\",\n            DataSource(\n                name=\"Document Group\",\n                component_type=DocumentGroup,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"TEXT_NODE\",\n            DataSource(\n                name=\"Text Node\",\n                component_type=TextNode,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"DOCUMENT\",\n            DataSource(\n                name=\"Document\",\n                component_type=Document,\n            ),\n        )\n    )\n\n    return ConfigurableComponent(\"ConfigurableDataSources\", enum_members)  # type: ignore\n\n\nConfigurableDataSources = build_configurable_data_source_enum()\n\nT = TypeVar(\"T\", bound=BaseComponent)\n\n\nclass ConfiguredDataSource(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data source in a pipeline.\n    \"\"\"\n\n    name: str\n    component: T = Field(description=\"Component that implements the data source\")\n\n    @classmethod\n    def from_component(\n        cls, component: BaseComponent, name: Optional[str] = None\n    ) -> \"ConfiguredDataSource\":\n        \"\"\"\n        Build a ConfiguredDataSource from a component.\n\n        This should be the preferred way to build a ConfiguredDataSource\n        as it will ensure that the component is supported as indicated by having a\n        corresponding enum value in DataSources.\n\n        This has the added bonus that you don't need to specify the generic type\n        like ConfiguredDataSource[Document]. The return value of\n        this ConfiguredDataSource.from_component(document) will be\n        ConfiguredDataSource[Document] if document is\n        a Document object.\n        \"\"\"\n        return ConfigurableDataSources.from_component(\n            component\n        ).build_configured_data_source(component, name)\n\n    @property\n    def configurable_data_source_type(self) -> ConfigurableComponent:\n        return ConfigurableDataSources.from_component(self.component)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/transformations.py",
    "filename": "transformations.py",
    "relpath": "ingestion/transformations.py",
    "start_line": 1,
    "end_line": 378,
    "length": 378,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_component",
      "build_configured_transformation",
      "build_configurable_transformation_enum",
      "from_component",
      "configurable_transformation_type"
    ],
    "chunk_class_names": [
      "TransformationIOType",
      "TransformationIOTypes",
      "TransformationCategory",
      "TransformationCategories",
      "ConfigurableTransformation",
      "containing",
      "ConfigurableComponent",
      "ConfiguredTransformation",
      "containing"
    ],
    "document_function_names": [
      "from_component",
      "build_configured_transformation",
      "build_configurable_transformation_enum",
      "from_component",
      "configurable_transformation_type"
    ],
    "document_class_names": [
      "TransformationIOType",
      "TransformationIOTypes",
      "TransformationCategory",
      "TransformationCategories",
      "ConfigurableTransformation",
      "containing",
      "ConfigurableComponent",
      "ConfiguredTransformation",
      "containing"
    ],
    "content": "\"\"\"\nThis module maintains the list of transformations that are supported by the system.\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Generic, Sequence, Type, TypeVar\n\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    ValidationError,\n    SerializeAsAny,\n)\nfrom llama_index.core.node_parser import (\n    CodeSplitter,\n    HTMLNodeParser,\n    JSONNodeParser,\n    MarkdownNodeParser,\n    SentenceSplitter,\n    SimpleFileNodeParser,\n    TokenTextSplitter,\n    MarkdownElementNodeParser,\n)\nfrom llama_index.core.schema import BaseComponent, BaseNode, Document\n\n\n# Transform Input/Output Types\nclass TransformationIOType(BaseModel):\n    name: str = Field(description=\"Name of the input/output type\")\n    description: str = Field(description=\"Description of the input/output type\")\n    python_type: str = Field(description=\"Python type of the input/output type\")\n\n\nclass TransformationIOTypes(Enum):\n    DOCUMENTS = TransformationIOType(\n        name=\"Documents\",\n        description=\"A sequence of Documents\",\n        python_type=str(Sequence[Document]),\n    )\n    NODES = TransformationIOType(\n        name=\"Nodes\",\n        description=\"A sequence of Nodes from a sequence of Documents\",\n        python_type=str(Sequence[BaseNode]),\n    )\n\n\nclass TransformationCategory(BaseModel):\n    \"\"\"A description for a category of transformation within a pipeline.\"\"\"\n\n    name: str = Field(description=\"Unique name of the type of transformation\")\n    description: str = Field(description=\"Description for the type of transformation\")\n    input_type: TransformationIOType = Field(\n        description=\"Input type for the transformation type\"\n    )\n    output_type: TransformationIOType = Field(\n        description=\"Output type for the transformation type\"\n    )\n\n\nclass TransformationCategories(Enum):\n    \"\"\"Supported transformation categories.\"\"\"\n\n    NODE_PARSER = TransformationCategory(\n        name=\"NodeParser\",\n        description=\"Applies a function to parse nodes from documents\",\n        input_type=TransformationIOTypes.DOCUMENTS.value,\n        output_type=TransformationIOTypes.NODES.value,\n    )\n    EMBEDDING = TransformationCategory(\n        name=\"Embedding\",\n        description=\"Applies a function to embed nodes\",\n        input_type=TransformationIOTypes.NODES.value,\n        output_type=TransformationIOTypes.NODES.value,\n    )\n\n\nclass ConfigurableTransformation(BaseModel):\n    \"\"\"\n    A class containing metadata for a type of transformation that can be in a pipeline.\n    \"\"\"\n\n    name: str = Field(\n        description=\"Unique and human-readable name for the type of transformation\"\n    )\n    transformation_category: TransformationCategories = Field(\n        description=\"Type of transformation\"\n    )\n    component_type: Type[BaseComponent] = Field(\n        description=\"Type of component that implements the transformation\"\n    )\n\n\nclass ConfigurableComponent(Enum):\n    @classmethod\n    def from_component(cls, component: BaseComponent) -> \"ConfigurableComponent\":\n        component_class = type(component)\n        for component_type in cls:\n            if component_type.value.component_type == component_class:\n                return component_type\n        raise ValueError(\n            f\"Component {component} is not a supported transformation component.\"\n        )\n\n    def build_configured_transformation(\n        self, component: BaseComponent\n    ) -> \"ConfiguredTransformation\":\n        component_type = self.value.component_type\n        if not isinstance(component, component_type):\n            raise ValueError(\n                f\"The enum value {self} is not compatible with component of \"\n                f\"type {type(component)}\"\n            )\n        return ConfiguredTransformation[component_type](  # type: ignore\n            component=component, name=self.value.name\n        )\n\n\ndef build_configurable_transformation_enum() -> ConfigurableComponent:\n    \"\"\"\n    Build an enum of configurable transformations.\n    But conditional on if the corresponding component is available.\n    \"\"\"\n    enum_members = []\n\n    # Node parsers\n    enum_members.append(\n        (\n            \"CODE_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"Code Splitter\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=CodeSplitter,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"SENTENCE_AWARE_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"Sentence Splitter\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=SentenceSplitter,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"TOKEN_AWARE_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"Token Text Splitter\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=TokenTextSplitter,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"HTML_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"HTML Node Parser\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=HTMLNodeParser,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"MARKDOWN_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"Markdown Node Parser\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=MarkdownNodeParser,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"JSON_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"JSON Node Parser\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=JSONNodeParser,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"SIMPLE_FILE_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"Simple File Node Parser\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=SimpleFileNodeParser,\n            ),\n        )\n    )\n\n    enum_members.append(\n        (\n            \"MARKDOWN_ELEMENT_NODE_PARSER\",\n            ConfigurableTransformation(\n                name=\"Markdown Element Node Parser\",\n                transformation_category=TransformationCategories.NODE_PARSER,\n                component_type=MarkdownElementNodeParser,\n            ),\n        )\n    )\n\n    # Embeddings\n    try:\n        from llama_index.embeddings.openai import OpenAIEmbedding  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"OPENAI_EMBEDDING\",\n                ConfigurableTransformation(\n                    name=\"OpenAI Embedding\",\n                    transformation_category=TransformationCategories.EMBEDDING,\n                    component_type=OpenAIEmbedding,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.embeddings.azure_openai import (\n            AzureOpenAIEmbedding,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"AZURE_EMBEDDING\",\n                ConfigurableTransformation(\n                    name=\"Azure OpenAI Embedding\",\n                    transformation_category=TransformationCategories.EMBEDDING,\n                    component_type=AzureOpenAIEmbedding,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.embeddings.cohere import (\n            CohereEmbedding,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"COHERE_EMBEDDING\",\n                ConfigurableTransformation(\n                    name=\"Cohere Embedding\",\n                    transformation_category=TransformationCategories.EMBEDDING,\n                    component_type=CohereEmbedding,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.embeddings.bedrock import (\n            BedrockEmbedding,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"BEDROCK_EMBEDDING\",\n                ConfigurableTransformation(\n                    name=\"Bedrock Embedding\",\n                    transformation_category=TransformationCategories.EMBEDDING,\n                    component_type=BedrockEmbedding,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.embeddings.huggingface_api import (\n            HuggingFaceInferenceAPIEmbedding,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"HUGGINGFACE_API_EMBEDDING\",\n                ConfigurableTransformation(\n                    name=\"HuggingFace API Embedding\",\n                    transformation_category=TransformationCategories.EMBEDDING,\n                    component_type=HuggingFaceInferenceAPIEmbedding,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.embeddings.gemini import (\n            GeminiEmbedding,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"GEMINI_EMBEDDING\",\n                ConfigurableTransformation(\n                    name=\"Gemini Embedding\",\n                    transformation_category=TransformationCategories.EMBEDDING,\n                    component_type=GeminiEmbedding,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.embeddings.mistralai import (\n            MistralAIEmbedding,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"MISTRALAI_EMBEDDING\",\n                ConfigurableTransformation(\n                    name=\"MistralAI Embedding\",\n                    transformation_category=TransformationCategories.EMBEDDING,\n                    component_type=MistralAIEmbedding,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    return ConfigurableComponent(\"ConfigurableTransformations\", enum_members)  # type: ignore\n\n\nConfigurableTransformations = build_configurable_transformation_enum()\n\nT = TypeVar(\"T\", bound=BaseComponent)\n\n\nclass ConfiguredTransformation(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a transformation in a pipeline.\n    \"\"\"\n\n    name: str\n    component: SerializeAsAny[T] = Field(\n        description=\"Component that implements the transformation\"\n    )\n\n    @classmethod\n    def from_component(cls, component: BaseComponent) -> \"ConfiguredTransformation\":\n        \"\"\"\n        Build a ConfiguredTransformation from a component.\n\n        This should be the preferred way to build a ConfiguredTransformation\n        as it will ensure that the component is supported as indicated by having a\n        corresponding enum value in ConfigurableTransformations.\n\n        This has the added bonus that you don't need to specify the generic type\n        like ConfiguredTransformation[SentenceSplitter]. The return value of\n        this ConfiguredTransformation.from_component(simple_node_parser) will be\n        ConfiguredTransformation[SentenceSplitter] if simple_node_parser is\n        a SentenceSplitter.\n        \"\"\"\n        return ConfigurableTransformations.from_component(\n            component\n        ).build_configured_transformation(component)\n\n    @property\n    def configurable_transformation_type(self) -> ConfigurableComponent:\n        return ConfigurableTransformations.from_component(self.component)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/api_utils.py",
    "filename": "api_utils.py",
    "relpath": "ingestion/api_utils.py",
    "start_line": 1,
    "end_line": 49,
    "length": 49,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_client",
      "get_aclient"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_client",
      "get_aclient"
    ],
    "document_class_names": [],
    "content": "import os\nimport httpx\nfrom typing import Optional, TYPE_CHECKING\n\nfrom llama_index.core.constants import (\n    DEFAULT_APP_URL,\n    DEFAULT_BASE_URL,\n)\n\nif TYPE_CHECKING:\n    from llama_cloud.client import AsyncLlamaCloud, LlamaCloud\n\n\ndef get_client(\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    app_url: Optional[str] = None,\n    timeout: int = 60,\n    httpx_client: Optional[httpx.Client] = None,\n) -> \"LlamaCloud\":\n    \"\"\"Get the sync platform API client.\"\"\"\n    from llama_cloud.client import LlamaCloud\n\n    base_url = base_url or os.environ.get(\"LLAMA_CLOUD_BASE_URL\", DEFAULT_BASE_URL)\n    app_url = app_url or os.environ.get(\"LLAMA_CLOUD_APP_URL\", DEFAULT_APP_URL)\n    api_key = api_key or os.environ.get(\"LLAMA_CLOUD_API_KEY\", None)\n\n    return LlamaCloud(\n        base_url=base_url, token=api_key, timeout=timeout, httpx_client=httpx_client\n    )\n\n\ndef get_aclient(\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    app_url: Optional[str] = None,\n    timeout: int = 60,\n    httpx_client: Optional[httpx.AsyncClient] = None,\n) -> \"AsyncLlamaCloud\":\n    \"\"\"Get the async platform API client.\"\"\"\n    from llama_cloud.client import AsyncLlamaCloud\n\n    base_url = base_url or os.environ.get(\"LLAMA_CLOUD_BASE_URL\", DEFAULT_BASE_URL)\n    app_url = app_url or os.environ.get(\"LLAMA_CLOUD_APP_URL\", DEFAULT_APP_URL)\n    api_key = api_key or os.environ.get(\"LLAMA_CLOUD_API_KEY\", None)\n\n    return AsyncLlamaCloud(\n        base_url=base_url, token=api_key, timeout=timeout, httpx_client=httpx_client\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/pipeline.py",
    "filename": "pipeline.py",
    "relpath": "ingestion/pipeline.py",
    "start_line": 1,
    "end_line": 185,
    "length": 185,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "remove_unstable_values",
      "get_transformation_hash",
      "run_transformations",
      "arun_transformations",
      "arun_transformations_wrapper"
    ],
    "chunk_class_names": [
      "DocstoreStrategy"
    ],
    "document_function_names": [
      "remove_unstable_values",
      "get_transformation_hash",
      "run_transformations",
      "arun_transformations",
      "arun_transformations_wrapper",
      "__init__",
      "persist",
      "load",
      "_get_default_transformations",
      "_prepare_inputs",
      "_handle_duplicates",
      "_handle_upserts",
      "_node_batcher",
      "run",
      "_ahandle_duplicates",
      "_ahandle_upserts",
      "arun"
    ],
    "document_class_names": [
      "DocstoreStrategy",
      "IngestionPipeline"
    ],
    "content": "import asyncio\nimport multiprocessing\nimport os\nimport re\nimport warnings\nfrom concurrent.futures import ProcessPoolExecutor\nfrom enum import Enum\nfrom functools import partial, reduce\nfrom hashlib import sha256\nfrom itertools import repeat\nfrom pathlib import Path\nfrom typing import Any, Generator, List, Optional, Sequence, Union\n\nfrom fsspec import AbstractFileSystem\n\nfrom llama_index.core.constants import (\n    DEFAULT_PIPELINE_NAME,\n    DEFAULT_PROJECT_NAME,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.ingestion.cache import DEFAULT_CACHE_NAME, IngestionCache\nfrom llama_index.core.instrumentation import get_dispatcher\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.readers.base import ReaderConfig\nfrom llama_index.core.schema import (\n    BaseNode,\n    Document,\n    MetadataMode,\n    TransformComponent,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore import (\n    BaseDocumentStore,\n    SimpleDocumentStore,\n)\nfrom llama_index.core.storage.storage_context import DOCSTORE_FNAME\nfrom llama_index.core.utils import concat_dirs\nfrom llama_index.core.vector_stores.types import BasePydanticVectorStore\n\ndispatcher = get_dispatcher(__name__)\n\n\ndef remove_unstable_values(s: str) -> str:\n    \"\"\"\n    Remove unstable key/value pairs.\n\n    Examples include:\n    - <__main__.Test object at 0x7fb9f3793f50>\n    - <function test_fn at 0x7fb9f37a8900>\n    \"\"\"\n    pattern = r\"<[\\w\\s_\\. ]+ at 0x[a-z0-9]+>\"\n    return re.sub(pattern, \"\", s)\n\n\ndef get_transformation_hash(\n    nodes: Sequence[BaseNode], transformation: TransformComponent\n) -> str:\n    \"\"\"Get the hash of a transformation.\"\"\"\n    nodes_str = \"\".join(\n        [str(node.get_content(metadata_mode=MetadataMode.ALL)) for node in nodes]\n    )\n\n    transformation_dict = transformation.to_dict()\n    transform_string = remove_unstable_values(str(transformation_dict))\n\n    return sha256((nodes_str + transform_string).encode(\"utf-8\")).hexdigest()\n\n\ndef run_transformations(\n    nodes: Sequence[BaseNode],\n    transformations: Sequence[TransformComponent],\n    in_place: bool = True,\n    cache: Optional[IngestionCache] = None,\n    cache_collection: Optional[str] = None,\n    **kwargs: Any,\n) -> Sequence[BaseNode]:\n    \"\"\"\n    Run a series of transformations on a set of nodes.\n\n    Args:\n        nodes: The nodes to transform.\n        transformations: The transformations to apply to the nodes.\n\n    Returns:\n        The transformed nodes.\n    \"\"\"\n    if not in_place:\n        nodes = list(nodes)\n\n    for transform in transformations:\n        if cache is not None:\n            hash = get_transformation_hash(nodes, transform)\n            cached_nodes = cache.get(hash, collection=cache_collection)\n            if cached_nodes is not None:\n                nodes = cached_nodes\n            else:\n                nodes = transform(nodes, **kwargs)\n                cache.put(hash, nodes, collection=cache_collection)\n        else:\n            nodes = transform(nodes, **kwargs)\n\n    return nodes\n\n\nasync def arun_transformations(\n    nodes: Sequence[BaseNode],\n    transformations: Sequence[TransformComponent],\n    in_place: bool = True,\n    cache: Optional[IngestionCache] = None,\n    cache_collection: Optional[str] = None,\n    **kwargs: Any,\n) -> Sequence[BaseNode]:\n    \"\"\"\n    Run a series of transformations on a set of nodes.\n\n    Args:\n        nodes: The nodes to transform.\n        transformations: The transformations to apply to the nodes.\n\n    Returns:\n        The transformed nodes.\n    \"\"\"\n    if not in_place:\n        nodes = list(nodes)\n\n    for transform in transformations:\n        if cache is not None:\n            hash = get_transformation_hash(nodes, transform)\n\n            cached_nodes = cache.get(hash, collection=cache_collection)\n            if cached_nodes is not None:\n                nodes = cached_nodes\n            else:\n                nodes = await transform.acall(nodes, **kwargs)\n                cache.put(hash, nodes, collection=cache_collection)\n        else:\n            nodes = await transform.acall(nodes, **kwargs)\n\n    return nodes\n\n\ndef arun_transformations_wrapper(\n    nodes: Sequence[BaseNode],\n    transformations: Sequence[TransformComponent],\n    in_place: bool = True,\n    cache: Optional[IngestionCache] = None,\n    cache_collection: Optional[str] = None,\n    **kwargs: Any,\n) -> Sequence[BaseNode]:\n    \"\"\"\n    Wrapper for async run_transformation. To be used in loop.run_in_executor\n    within a ProcessPoolExecutor.\n    \"\"\"\n    loop = asyncio.new_event_loop()\n    nodes = loop.run_until_complete(\n        arun_transformations(\n            nodes=nodes,\n            transformations=transformations,\n            in_place=in_place,\n            cache=cache,\n            cache_collection=cache_collection,\n            **kwargs,\n        )\n    )\n    loop.close()\n    return nodes\n\n\nclass DocstoreStrategy(str, Enum):\n    \"\"\"\n    Document de-duplication de-deduplication strategies work by comparing the hashes or ids stored in the document store.\n       They require a document store to be set which must be persisted across pipeline runs.\n\n    Attributes:\n        UPSERTS:\n            ('upserts') Use upserts to handle duplicates. Checks if the a document is already in the doc store based on its id. If it is not, or if the hash of the document is updated, it will update the document in the doc store and run the transformations.\n        DUPLICATES_ONLY:\n            ('duplicates_only') Only handle duplicates. Checks if the hash of a document is already in the doc store. Only then it will add the document to the doc store and run the transformations\n        UPSERTS_AND_DELETE:\n            ('upserts_and_delete') Use upserts and delete to handle duplicates. Like the upsert strategy but it will also delete non-existing documents from the doc store\n    \"\"\"\n\n    UPSERTS = \"upserts\"\n    DUPLICATES_ONLY = \"duplicates_only\"\n    UPSERTS_AND_DELETE = \"upserts_and_delete\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/pipeline.py",
    "filename": "pipeline.py",
    "relpath": "ingestion/pipeline.py",
    "start_line": 185,
    "end_line": 188,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "IngestionPipeline"
    ],
    "document_function_names": [
      "remove_unstable_values",
      "get_transformation_hash",
      "run_transformations",
      "arun_transformations",
      "arun_transformations_wrapper",
      "__init__",
      "persist",
      "load",
      "_get_default_transformations",
      "_prepare_inputs",
      "_handle_duplicates",
      "_handle_upserts",
      "_node_batcher",
      "run",
      "_ahandle_duplicates",
      "_ahandle_upserts",
      "arun"
    ],
    "document_class_names": [
      "DocstoreStrategy",
      "IngestionPipeline"
    ],
    "content": "class IngestionPipeline(BaseModel):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/pipeline.py",
    "filename": "pipeline.py",
    "relpath": "ingestion/pipeline.py",
    "start_line": 188,
    "end_line": 584,
    "length": 397,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "persist",
      "load",
      "_get_default_transformations",
      "_prepare_inputs",
      "_handle_duplicates",
      "_handle_upserts",
      "_node_batcher",
      "run",
      "_ahandle_duplicates"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "remove_unstable_values",
      "get_transformation_hash",
      "run_transformations",
      "arun_transformations",
      "arun_transformations_wrapper",
      "__init__",
      "persist",
      "load",
      "_get_default_transformations",
      "_prepare_inputs",
      "_handle_duplicates",
      "_handle_upserts",
      "_node_batcher",
      "run",
      "_ahandle_duplicates",
      "_ahandle_upserts",
      "arun"
    ],
    "document_class_names": [
      "DocstoreStrategy",
      "IngestionPipeline"
    ],
    "content": "\"\"\"\n    An ingestion pipeline that can be applied to data.\n\n    Args:\n        name (str, optional):\n            Unique name of the ingestion pipeline. Defaults to DEFAULT_PIPELINE_NAME.\n        project_name (str, optional):\n            Unique name of the project. Defaults to DEFAULT_PROJECT_NAME.\n        transformations (List[TransformComponent], optional):\n            Transformations to apply to the data. Defaults to None.\n        documents (Optional[Sequence[Document]], optional):\n            Documents to ingest. Defaults to None.\n        readers (Optional[List[ReaderConfig]], optional):\n            Reader to use to read the data. Defaults to None.\n        vector_store (Optional[BasePydanticVectorStore], optional):\n            Vector store to use to store the data. Defaults to None.\n        cache (Optional[IngestionCache], optional):\n            Cache to use to store the data. Defaults to None.\n        docstore (Optional[BaseDocumentStore], optional):\n            Document store to use for de-duping with a vector store. Defaults to None.\n        docstore_strategy (DocstoreStrategy, optional):\n            Document de-dup strategy. Defaults to DocstoreStrategy.UPSERTS.\n        disable_cache (bool, optional):\n            Disable the cache. Defaults to False.\n        base_url (str, optional):\n            Base URL for the LlamaCloud API. Defaults to DEFAULT_BASE_URL.\n        app_url (str, optional):\n            Base URL for the LlamaCloud app. Defaults to DEFAULT_APP_URL.\n        api_key (Optional[str], optional):\n            LlamaCloud API key. Defaults to None.\n\n    Examples:\n        ```python\n        from llama_index.core.ingestion import IngestionPipeline\n        from llama_index.core.node_parser import SentenceSplitter\n        from llama_index.embeddings.openai import OpenAIEmbedding\n\n        pipeline = IngestionPipeline(\n            transformations=[\n                SentenceSplitter(chunk_size=512, chunk_overlap=20),\n                OpenAIEmbedding(),\n            ],\n        )\n\n        nodes = pipeline.run(documents=documents)\n        ```\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(\n        default=DEFAULT_PIPELINE_NAME,\n        description=\"Unique name of the ingestion pipeline\",\n    )\n    project_name: str = Field(\n        default=DEFAULT_PROJECT_NAME, description=\"Unique name of the project\"\n    )\n\n    transformations: List[TransformComponent] = Field(\n        description=\"Transformations to apply to the data\"\n    )\n\n    documents: Optional[Sequence[Document]] = Field(description=\"Documents to ingest\")\n    readers: Optional[List[ReaderConfig]] = Field(\n        description=\"Reader to use to read the data\"\n    )\n    vector_store: Optional[BasePydanticVectorStore] = Field(\n        description=\"Vector store to use to store the data\"\n    )\n    cache: IngestionCache = Field(\n        default_factory=IngestionCache,\n        description=\"Cache to use to store the data\",\n    )\n    docstore: Optional[BaseDocumentStore] = Field(\n        default=None,\n        description=\"Document store to use for de-duping with a vector store.\",\n    )\n    docstore_strategy: DocstoreStrategy = Field(\n        default=DocstoreStrategy.UPSERTS, description=\"Document de-dup strategy.\"\n    )\n    disable_cache: bool = Field(default=False, description=\"Disable the cache\")\n\n    def __init__(\n        self,\n        name: str = DEFAULT_PIPELINE_NAME,\n        project_name: str = DEFAULT_PROJECT_NAME,\n        transformations: Optional[List[TransformComponent]] = None,\n        readers: Optional[List[ReaderConfig]] = None,\n        documents: Optional[Sequence[Document]] = None,\n        vector_store: Optional[BasePydanticVectorStore] = None,\n        cache: Optional[IngestionCache] = None,\n        docstore: Optional[BaseDocumentStore] = None,\n        docstore_strategy: DocstoreStrategy = DocstoreStrategy.UPSERTS,\n        disable_cache: bool = False,\n    ) -> None:\n        if transformations is None:\n            transformations = self._get_default_transformations()\n\n        super().__init__(\n            name=name,\n            project_name=project_name,\n            transformations=transformations,\n            readers=readers,\n            documents=documents,\n            vector_store=vector_store,\n            cache=cache or IngestionCache(),\n            docstore=docstore,\n            docstore_strategy=docstore_strategy,\n            disable_cache=disable_cache,\n        )\n\n    def persist(\n        self,\n        persist_dir: str = \"./pipeline_storage\",\n        fs: Optional[AbstractFileSystem] = None,\n        cache_name: str = DEFAULT_CACHE_NAME,\n        docstore_name: str = DOCSTORE_FNAME,\n    ) -> None:\n        \"\"\"Persist the pipeline to disk.\"\"\"\n        if fs is not None:\n            persist_dir = str(persist_dir)  # NOTE: doesn't support Windows here\n            docstore_path = concat_dirs(persist_dir, docstore_name)\n            cache_path = concat_dirs(persist_dir, cache_name)\n\n        else:\n            persist_path = Path(persist_dir)\n            docstore_path = str(persist_path / docstore_name)\n            cache_path = str(persist_path / cache_name)\n\n        self.cache.persist(cache_path, fs=fs)\n        if self.docstore is not None:\n            self.docstore.persist(docstore_path, fs=fs)\n\n    def load(\n        self,\n        persist_dir: str = \"./pipeline_storage\",\n        fs: Optional[AbstractFileSystem] = None,\n        cache_name: str = DEFAULT_CACHE_NAME,\n        docstore_name: str = DOCSTORE_FNAME,\n    ) -> None:\n        \"\"\"Load the pipeline from disk.\"\"\"\n        if fs is not None:\n            self.cache = IngestionCache.from_persist_path(\n                concat_dirs(persist_dir, cache_name), fs=fs\n            )\n            persist_docstore_path = concat_dirs(persist_dir, docstore_name)\n            if os.path.exists(persist_docstore_path):\n                self.docstore = SimpleDocumentStore.from_persist_path(\n                    concat_dirs(persist_dir, docstore_name), fs=fs\n                )\n        else:\n            self.cache = IngestionCache.from_persist_path(\n                str(Path(persist_dir) / cache_name)\n            )\n            persist_docstore_path = str(Path(persist_dir) / docstore_name)\n            if os.path.exists(persist_docstore_path):\n                self.docstore = SimpleDocumentStore.from_persist_path(\n                    str(Path(persist_dir) / docstore_name)\n                )\n\n    def _get_default_transformations(self) -> List[TransformComponent]:\n        return [\n            SentenceSplitter(),\n            Settings.embed_model,\n        ]\n\n    def _prepare_inputs(\n        self,\n        documents: Optional[Sequence[Document]],\n        nodes: Optional[Sequence[BaseNode]],\n    ) -> Sequence[BaseNode]:\n        input_nodes: Sequence[BaseNode] = []\n\n        if documents is not None:\n            input_nodes += documents  # type: ignore\n\n        if nodes is not None:\n            input_nodes += nodes  # type: ignore\n\n        if self.documents is not None:\n            input_nodes += self.documents  # type: ignore\n\n        if self.readers is not None:\n            for reader in self.readers:\n                input_nodes += reader.read()  # type: ignore\n\n        return input_nodes\n\n    def _handle_duplicates(\n        self,\n        nodes: Sequence[BaseNode],\n        store_doc_text: bool = True,\n    ) -> Sequence[BaseNode]:\n        \"\"\"Handle docstore duplicates by checking all hashes.\"\"\"\n        assert self.docstore is not None\n\n        existing_hashes = self.docstore.get_all_document_hashes()\n        current_hashes = []\n        nodes_to_run = []\n        for node in nodes:\n            if node.hash not in existing_hashes and node.hash not in current_hashes:\n                self.docstore.set_document_hash(node.id_, node.hash)\n                nodes_to_run.append(node)\n                current_hashes.append(node.hash)\n\n        self.docstore.add_documents(nodes_to_run, store_text=store_doc_text)\n\n        return nodes_to_run\n\n    def _handle_upserts(\n        self,\n        nodes: Sequence[BaseNode],\n        store_doc_text: bool = True,\n    ) -> Sequence[BaseNode]:\n        \"\"\"Handle docstore upserts by checking hashes and ids.\"\"\"\n        assert self.docstore is not None\n\n        doc_ids_from_nodes = set()\n        deduped_nodes_to_run = {}\n        for node in nodes:\n            ref_doc_id = node.ref_doc_id if node.ref_doc_id else node.id_\n            doc_ids_from_nodes.add(ref_doc_id)\n            existing_hash = self.docstore.get_document_hash(ref_doc_id)\n            if not existing_hash:\n                # document doesn't exist, so add it\n                deduped_nodes_to_run[ref_doc_id] = node\n            elif existing_hash and existing_hash != node.hash:\n                self.docstore.delete_ref_doc(ref_doc_id, raise_error=False)\n\n                if self.vector_store is not None:\n                    self.vector_store.delete(ref_doc_id)\n\n                deduped_nodes_to_run[ref_doc_id] = node\n            else:\n                continue  # document exists and is unchanged, so skip it\n\n        if self.docstore_strategy == DocstoreStrategy.UPSERTS_AND_DELETE:\n            # Identify missing docs and delete them from docstore and vector store\n            existing_doc_ids_before = set(\n                self.docstore.get_all_document_hashes().values()\n            )\n            doc_ids_to_delete = existing_doc_ids_before - doc_ids_from_nodes\n            for ref_doc_id in doc_ids_to_delete:\n                self.docstore.delete_document(ref_doc_id)\n\n                if self.vector_store is not None:\n                    self.vector_store.delete(ref_doc_id)\n\n        nodes_to_run = list(deduped_nodes_to_run.values())\n        self.docstore.set_document_hashes({n.id_: n.hash for n in nodes_to_run})\n        self.docstore.add_documents(nodes_to_run, store_text=store_doc_text)\n\n        return nodes_to_run\n\n    @staticmethod\n    def _node_batcher(\n        num_batches: int, nodes: Union[Sequence[BaseNode], List[Document]]\n    ) -> Generator[Union[Sequence[BaseNode], List[Document]], Any, Any]:\n        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n        batch_size = max(1, int(len(nodes) / num_batches))\n        for i in range(0, len(nodes), batch_size):\n            yield nodes[i : i + batch_size]\n\n    @dispatcher.span\n    def run(\n        self,\n        show_progress: bool = False,\n        documents: Optional[List[Document]] = None,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        cache_collection: Optional[str] = None,\n        in_place: bool = True,\n        store_doc_text: bool = True,\n        num_workers: Optional[int] = None,\n        **kwargs: Any,\n    ) -> Sequence[BaseNode]:\n        \"\"\"\n        Run a series of transformations on a set of nodes.\n\n        If a vector store is provided, nodes with embeddings will be added to the vector store.\n\n        If a vector store + docstore are provided, the docstore will be used to de-duplicate documents.\n\n        Args:\n            show_progress (bool, optional): Shows execution progress bar(s). Defaults to False.\n            documents (Optional[List[Document]], optional): Set of documents to be transformed. Defaults to None.\n            nodes (Optional[Sequence[BaseNode]], optional): Set of nodes to be transformed. Defaults to None.\n            cache_collection (Optional[str], optional): Cache for transformations. Defaults to None.\n            in_place (bool, optional): Whether transformations creates a new list for transformed nodes or modifies the\n                array passed to `run_transformations`. Defaults to True.\n            num_workers (Optional[int], optional): The number of parallel processes to use.\n                If set to None, then sequential compute is used. Defaults to None.\n\n        Returns:\n            Sequence[BaseNode]: The set of transformed Nodes/Documents\n        \"\"\"\n        input_nodes = self._prepare_inputs(documents, nodes)\n\n        # check if we need to dedup\n        if self.docstore is not None and self.vector_store is not None:\n            if self.docstore_strategy in (\n                DocstoreStrategy.UPSERTS,\n                DocstoreStrategy.UPSERTS_AND_DELETE,\n            ):\n                nodes_to_run = self._handle_upserts(\n                    input_nodes, store_doc_text=store_doc_text\n                )\n            elif self.docstore_strategy == DocstoreStrategy.DUPLICATES_ONLY:\n                nodes_to_run = self._handle_duplicates(\n                    input_nodes, store_doc_text=store_doc_text\n                )\n            else:\n                raise ValueError(f\"Invalid docstore strategy: {self.docstore_strategy}\")\n        elif self.docstore is not None and self.vector_store is None:\n            if self.docstore_strategy == DocstoreStrategy.UPSERTS:\n                print(\n                    \"Docstore strategy set to upserts, but no vector store. \"\n                    \"Switching to duplicates_only strategy.\"\n                )\n                self.docstore_strategy = DocstoreStrategy.DUPLICATES_ONLY\n            elif self.docstore_strategy == DocstoreStrategy.UPSERTS_AND_DELETE:\n                print(\n                    \"Docstore strategy set to upserts and delete, but no vector store. \"\n                    \"Switching to duplicates_only strategy.\"\n                )\n                self.docstore_strategy = DocstoreStrategy.DUPLICATES_ONLY\n            nodes_to_run = self._handle_duplicates(\n                input_nodes, store_doc_text=store_doc_text\n            )\n\n        else:\n            nodes_to_run = input_nodes\n\n        if num_workers and num_workers > 1:\n            num_cpus = multiprocessing.cpu_count()\n            if num_workers > num_cpus:\n                warnings.warn(\n                    \"Specified num_workers exceed number of CPUs in the system. \"\n                    \"Setting `num_workers` down to the maximum CPU count.\"\n                )\n                num_workers = num_cpus\n\n            with multiprocessing.get_context(\"spawn\").Pool(num_workers) as p:\n                node_batches = self._node_batcher(\n                    num_batches=num_workers, nodes=nodes_to_run\n                )\n                nodes_parallel = p.starmap(\n                    run_transformations,\n                    zip(\n                        node_batches,\n                        repeat(self.transformations),\n                        repeat(in_place),\n                        repeat(self.cache if not self.disable_cache else None),\n                        repeat(cache_collection),\n                    ),\n                )\n                nodes = reduce(lambda x, y: x + y, nodes_parallel, [])  # type: ignore\n        else:\n            nodes = run_transformations(\n                nodes_to_run,\n                self.transformations,\n                show_progress=show_progress,\n                cache=self.cache if not self.disable_cache else None,\n                cache_collection=cache_collection,\n                in_place=in_place,\n                **kwargs,\n            )\n\n        nodes = nodes or []\n\n        if self.vector_store is not None:\n            nodes_with_embeddings = [n for n in nodes if n.embedding is not None]\n            if nodes_with_embeddings:\n                self.vector_store.add(nodes_with_embeddings)\n\n        return nodes\n\n    # ------ async methods ------\n    async def _ahandle_duplicates(\n        self,\n        nodes: Sequence[BaseNode],\n        store_doc_text: bool = True,\n    ) -> Sequence[BaseNode]:\n        \"\"\"Handle docstore duplicates by checking all hashes.\"\"\"\n        assert self.docstore is not None\n\n        existing_hashes = await self.docstore.aget_all_document_hashes()\n        current_hashes = []\n        nodes_to_run = []\n        for node in nodes:\n            if node.hash not in existing_hashes and node.hash not in current_hashes:\n                await self.docstore.aset_document_hash(node.id_, node.hash)\n                nodes_to_run.append(node)\n                current_hashes.append(node.hash)\n\n        await self.docstore.async_add_documents(nodes_to_run, store_text=store_doc_text)\n\n        return nodes_to_run"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/pipeline.py",
    "filename": "pipeline.py",
    "relpath": "ingestion/pipeline.py",
    "start_line": 584,
    "end_line": 749,
    "length": 166,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_ahandle_upserts",
      "arun"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "remove_unstable_values",
      "get_transformation_hash",
      "run_transformations",
      "arun_transformations",
      "arun_transformations_wrapper",
      "__init__",
      "persist",
      "load",
      "_get_default_transformations",
      "_prepare_inputs",
      "_handle_duplicates",
      "_handle_upserts",
      "_node_batcher",
      "run",
      "_ahandle_duplicates",
      "_ahandle_upserts",
      "arun"
    ],
    "document_class_names": [
      "DocstoreStrategy",
      "IngestionPipeline"
    ],
    "content": "async def _ahandle_upserts(\n        self,\n        nodes: Sequence[BaseNode],\n        store_doc_text: bool = True,\n    ) -> Sequence[BaseNode]:\n        \"\"\"Handle docstore upserts by checking hashes and ids.\"\"\"\n        assert self.docstore is not None\n\n        doc_ids_from_nodes = set()\n        deduped_nodes_to_run = {}\n        for node in nodes:\n            ref_doc_id = node.ref_doc_id if node.ref_doc_id else node.id_\n            doc_ids_from_nodes.add(ref_doc_id)\n            existing_hash = await self.docstore.aget_document_hash(ref_doc_id)\n            if not existing_hash:\n                # document doesn't exist, so add it\n                deduped_nodes_to_run[ref_doc_id] = node\n            elif existing_hash and existing_hash != node.hash:\n                await self.docstore.adelete_ref_doc(ref_doc_id, raise_error=False)\n\n                if self.vector_store is not None:\n                    await self.vector_store.adelete(ref_doc_id)\n\n                deduped_nodes_to_run[ref_doc_id] = node\n            else:\n                continue  # document exists and is unchanged, so skip it\n\n        if self.docstore_strategy == DocstoreStrategy.UPSERTS_AND_DELETE:\n            # Identify missing docs and delete them from docstore and vector store\n            existing_doc_ids_before = set(\n                (await self.docstore.aget_all_document_hashes()).values()\n            )\n            doc_ids_to_delete = existing_doc_ids_before - doc_ids_from_nodes\n            for ref_doc_id in doc_ids_to_delete:\n                await self.docstore.adelete_document(ref_doc_id)\n\n                if self.vector_store is not None:\n                    await self.vector_store.adelete(ref_doc_id)\n\n        nodes_to_run = list(deduped_nodes_to_run.values())\n        await self.docstore.async_add_documents(nodes_to_run, store_text=store_doc_text)\n        await self.docstore.aset_document_hashes({n.id_: n.hash for n in nodes_to_run})\n\n        return nodes_to_run\n\n    @dispatcher.span\n    async def arun(\n        self,\n        show_progress: bool = False,\n        documents: Optional[List[Document]] = None,\n        nodes: Optional[Sequence[BaseNode]] = None,\n        cache_collection: Optional[str] = None,\n        in_place: bool = True,\n        store_doc_text: bool = True,\n        num_workers: Optional[int] = None,\n        **kwargs: Any,\n    ) -> Sequence[BaseNode]:\n        \"\"\"\n        Run a series of transformations on a set of nodes.\n\n        If a vector store is provided, nodes with embeddings will be added to the vector store.\n\n        If a vector store + docstore are provided, the docstore will be used to de-duplicate documents.\n\n        Args:\n            show_progress (bool, optional): Shows execution progress bar(s). Defaults to False.\n            documents (Optional[List[Document]], optional): Set of documents to be transformed. Defaults to None.\n            nodes (Optional[Sequence[BaseNode]], optional): Set of nodes to be transformed. Defaults to None.\n            cache_collection (Optional[str], optional): Cache for transformations. Defaults to None.\n            in_place (bool, optional): Whether transformations creates a new list for transformed nodes or modifies the\n                array passed to `run_transformations`. Defaults to True.\n            num_workers (Optional[int], optional): The number of parallel processes to use.\n                If set to None, then sequential compute is used. Defaults to None.\n\n        Returns:\n            Sequence[BaseNode]: The set of transformed Nodes/Documents\n        \"\"\"\n        input_nodes = self._prepare_inputs(documents, nodes)\n\n        # check if we need to dedup\n        if self.docstore is not None and self.vector_store is not None:\n            if self.docstore_strategy in (\n                DocstoreStrategy.UPSERTS,\n                DocstoreStrategy.UPSERTS_AND_DELETE,\n            ):\n                nodes_to_run = await self._ahandle_upserts(\n                    input_nodes, store_doc_text=store_doc_text\n                )\n            elif self.docstore_strategy == DocstoreStrategy.DUPLICATES_ONLY:\n                nodes_to_run = await self._ahandle_duplicates(\n                    input_nodes, store_doc_text=store_doc_text\n                )\n            else:\n                raise ValueError(f\"Invalid docstore strategy: {self.docstore_strategy}\")\n        elif self.docstore is not None and self.vector_store is None:\n            if self.docstore_strategy == DocstoreStrategy.UPSERTS:\n                print(\n                    \"Docstore strategy set to upserts, but no vector store. \"\n                    \"Switching to duplicates_only strategy.\"\n                )\n                self.docstore_strategy = DocstoreStrategy.DUPLICATES_ONLY\n            elif self.docstore_strategy == DocstoreStrategy.UPSERTS_AND_DELETE:\n                print(\n                    \"Docstore strategy set to upserts and delete, but no vector store. \"\n                    \"Switching to duplicates_only strategy.\"\n                )\n                self.docstore_strategy = DocstoreStrategy.DUPLICATES_ONLY\n            nodes_to_run = await self._ahandle_duplicates(\n                input_nodes, store_doc_text=store_doc_text\n            )\n\n        else:\n            nodes_to_run = input_nodes\n\n        if num_workers and num_workers > 1:\n            num_cpus = multiprocessing.cpu_count()\n            if num_workers > num_cpus:\n                warnings.warn(\n                    \"Specified num_workers exceed number of CPUs in the system. \"\n                    \"Setting `num_workers` down to the maximum CPU count.\"\n                )\n                num_workers = num_cpus\n\n            loop = asyncio.get_event_loop()\n            with ProcessPoolExecutor(max_workers=num_workers) as p:\n                node_batches = self._node_batcher(\n                    num_batches=num_workers, nodes=nodes_to_run\n                )\n                tasks = [\n                    loop.run_in_executor(\n                        p,\n                        partial(\n                            arun_transformations_wrapper,\n                            transformations=self.transformations,\n                            in_place=in_place,\n                            cache=self.cache if not self.disable_cache else None,\n                            cache_collection=cache_collection,\n                        ),\n                        batch,\n                    )\n                    for batch in node_batches\n                ]\n                result: Sequence[Sequence[BaseNode]] = await asyncio.gather(*tasks)\n                nodes: Sequence[BaseNode] = reduce(lambda x, y: x + y, result, [])  # type: ignore\n        else:\n            nodes = await arun_transformations(  # type: ignore\n                nodes_to_run,\n                self.transformations,\n                show_progress=show_progress,\n                cache=self.cache if not self.disable_cache else None,\n                cache_collection=cache_collection,\n                in_place=in_place,\n                **kwargs,\n            )\n            nodes = nodes\n\n        nodes = nodes or []\n\n        if self.vector_store is not None:\n            nodes_with_embeddings = [n for n in nodes if n.embedding is not None]\n            if nodes_with_embeddings:\n                await self.vector_store.async_add(nodes_with_embeddings)\n\n        return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/data_sinks.py",
    "filename": "data_sinks.py",
    "relpath": "ingestion/data_sinks.py",
    "start_line": 1,
    "end_line": 180,
    "length": 180,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_component",
      "build_configured_data_sink",
      "build_conifurable_data_sink_enum",
      "from_component",
      "configurable_data_sink_type"
    ],
    "chunk_class_names": [
      "DataSink",
      "containing",
      "ConfigurableComponent",
      "ConfiguredDataSink",
      "containing"
    ],
    "document_function_names": [
      "from_component",
      "build_configured_data_sink",
      "build_conifurable_data_sink_enum",
      "from_component",
      "configurable_data_sink_type"
    ],
    "document_class_names": [
      "DataSink",
      "containing",
      "ConfigurableComponent",
      "ConfiguredDataSink",
      "containing"
    ],
    "content": "from enum import Enum\nfrom typing import Generic, Type, TypeVar\n\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    ValidationError,\n)\nfrom llama_index.core.vector_stores.types import BasePydanticVectorStore\n\n\nclass DataSink(BaseModel):\n    \"\"\"\n    A class containing metadata for a type of data sink.\n    \"\"\"\n\n    name: str = Field(\n        description=\"Unique and human-readable name for the type of data sink\"\n    )\n    component_type: Type[BasePydanticVectorStore] = Field(\n        description=\"Type of component that implements the data sink\"\n    )\n\n\nclass ConfigurableComponent(Enum):\n    @classmethod\n    def from_component(\n        cls, component: BasePydanticVectorStore\n    ) -> \"ConfigurableComponent\":\n        component_class = type(component)\n        for component_type in cls:\n            if component_type.value.component_type == component_class:\n                return component_type\n        raise ValueError(\n            f\"Component {component} is not a supported data sink component.\"\n        )\n\n    def build_configured_data_sink(\n        self, component: BasePydanticVectorStore\n    ) -> \"ConfiguredDataSink\":\n        component_type = self.value.component_type\n        if not isinstance(component, component_type):\n            raise ValueError(\n                f\"The enum value {self} is not compatible with component of \"\n                f\"type {type(component)}\"\n            )\n        return ConfiguredDataSink[component_type](  # type: ignore\n            component=component, name=self.value.name\n        )\n\n\ndef build_conifurable_data_sink_enum() -> ConfigurableComponent:\n    \"\"\"\n    Build an enum of configurable data sinks.\n    But conditional on if the corresponding vector store is available.\n    \"\"\"\n    enum_members = []\n\n    try:\n        from llama_index.vector_stores.chroma import (\n            ChromaVectorStore,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"CHROMA\",\n                DataSink(\n                    name=\"Chroma\",\n                    component_type=ChromaVectorStore,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.vector_stores.pinecone import (\n            PineconeVectorStore,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"PINECONE\",\n                DataSink(\n                    name=\"Pinecone\",\n                    component_type=PineconeVectorStore,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.vector_stores.postgres import (\n            PGVectorStore,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"POSTGRES\",\n                DataSink(\n                    name=\"PostgreSQL\",\n                    component_type=PGVectorStore,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.vector_stores.qdrant import (\n            QdrantVectorStore,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"QDRANT\",\n                DataSink(\n                    name=\"Qdrant\",\n                    component_type=QdrantVectorStore,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    try:\n        from llama_index.vector_stores.weaviate import (\n            WeaviateVectorStore,\n        )  # pants: no-infer-dep\n\n        enum_members.append(\n            (\n                \"WEAVIATE\",\n                DataSink(\n                    name=\"Weaviate\",\n                    component_type=WeaviateVectorStore,\n                ),\n            )\n        )\n    except (ImportError, ValidationError):\n        pass\n\n    return ConfigurableComponent(\"ConfigurableDataSinks\", enum_members)  # type: ignore\n\n\nConfigurableDataSinks = build_conifurable_data_sink_enum()\n\n\nT = TypeVar(\"T\", bound=BasePydanticVectorStore)\n\n\nclass ConfiguredDataSink(BaseModel, Generic[T]):\n    \"\"\"\n    A class containing metadata & implementation for a data sink in a pipeline.\n    \"\"\"\n\n    name: str\n    component: T = Field(description=\"Component that implements the data sink\")\n\n    @classmethod\n    def from_component(cls, component: BasePydanticVectorStore) -> \"ConfiguredDataSink\":\n        \"\"\"\n        Build a ConfiguredDataSink from a component.\n        This should be the preferred way to build a ConfiguredDataSink\n        as it will ensure that the component is supported as indicated by having a\n        corresponding enum value in DataSources.\n        This has the added bonus that you don't need to specify the generic type\n        like ConfiguredDataSink[Document]. The return value of\n        this ConfiguredDataSink.from_component(document) will be\n        ConfiguredDataSink[Document] if document is\n        a Document object.\n        \"\"\"\n        return ConfigurableDataSinks.from_component(\n            component\n        ).build_configured_data_sink(component)\n\n    @property\n    def configurable_data_sink_type(self) -> ConfigurableDataSinks:  # type: ignore\n        return ConfigurableDataSinks.from_component(self.component)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/cache.py",
    "filename": "cache.py",
    "relpath": "ingestion/cache.py",
    "start_line": 1,
    "end_line": 78,
    "length": 78,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "put",
      "get",
      "clear",
      "persist",
      "from_persist_path"
    ],
    "chunk_class_names": [
      "IngestionCache"
    ],
    "document_function_names": [
      "put",
      "get",
      "clear",
      "persist",
      "from_persist_path"
    ],
    "document_class_names": [
      "IngestionCache"
    ],
    "content": "from typing import Optional, Sequence\n\nimport fsspec\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.schema import BaseNode\nfrom llama_index.core.storage.docstore.utils import doc_to_json, json_to_doc\nfrom llama_index.core.storage.kvstore import (\n    SimpleKVStore as SimpleCache,\n)\nfrom llama_index.core.storage.kvstore.types import (\n    BaseKVStore as BaseCache,\n)\n\nDEFAULT_CACHE_NAME = \"llama_cache\"\n\n\nclass IngestionCache(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    nodes_key: str = \"nodes\"\n\n    collection: str = Field(\n        default=DEFAULT_CACHE_NAME, description=\"Collection name of the cache.\"\n    )\n    cache: BaseCache = Field(default_factory=SimpleCache, description=\"Cache to use.\")\n\n    # TODO: add async get/put methods?\n    def put(\n        self, key: str, nodes: Sequence[BaseNode], collection: Optional[str] = None\n    ) -> None:\n        \"\"\"Put a value into the cache.\"\"\"\n        collection = collection or self.collection\n\n        val = {self.nodes_key: [doc_to_json(node) for node in nodes]}\n        self.cache.put(key, val, collection=collection)\n\n    def get(\n        self, key: str, collection: Optional[str] = None\n    ) -> Optional[Sequence[BaseNode]]:\n        \"\"\"Get a value from the cache.\"\"\"\n        collection = collection or self.collection\n        node_dicts = self.cache.get(key, collection=collection)\n\n        if node_dicts is None:\n            return None\n\n        return [json_to_doc(node_dict) for node_dict in node_dicts[self.nodes_key]]\n\n    def clear(self, collection: Optional[str] = None) -> None:\n        \"\"\"Clear the cache.\"\"\"\n        collection = collection or self.collection\n        data = self.cache.get_all(collection=collection)\n        for key in data:\n            self.cache.delete(key, collection=collection)\n\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        \"\"\"Persist the cache to a directory, if possible.\"\"\"\n        if isinstance(self.cache, SimpleCache):\n            self.cache.persist(persist_path, fs=fs)\n        else:\n            print(\"Warning: skipping persist, only needed for SimpleCache.\")\n\n    @classmethod\n    def from_persist_path(\n        cls,\n        persist_path: str,\n        collection: str = DEFAULT_CACHE_NAME,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"IngestionCache\":\n        \"\"\"Create a IngestionCache from a persist directory.\"\"\"\n        return cls(\n            collection=collection,\n            cache=SimpleCache.from_persist_path(persist_path, fs=fs),\n        )\n\n\n__all__ = [\"SimpleCache\", \"BaseCache\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/ingestion/__init__.py",
    "filename": "__init__.py",
    "relpath": "ingestion/__init__.py",
    "start_line": 1,
    "end_line": 15,
    "length": 15,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.ingestion.cache import IngestionCache\nfrom llama_index.core.ingestion.pipeline import (\n    DocstoreStrategy,\n    IngestionPipeline,\n    arun_transformations,\n    run_transformations,\n)\n\n__all__ = [\n    \"DocstoreStrategy\",\n    \"IngestionCache\",\n    \"IngestionPipeline\",\n    \"run_transformations\",\n    \"arun_transformations\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/types.py",
    "filename": "types.py",
    "relpath": "tools/types.py",
    "start_line": 1,
    "end_line": 216,
    "length": 216,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_parameters_dict",
      "fn_schema_str",
      "get_name",
      "to_openai_function",
      "to_openai_tool",
      "__str__",
      "metadata",
      "__call__",
      "_process_langchain_tool_kwargs",
      "to_langchain_tool",
      "to_langchain_structured_tool",
      "__call__",
      "call",
      "acall",
      "__init__",
      "metadata",
      "call",
      "acall",
      "adapt_to_async_tool"
    ],
    "chunk_class_names": [
      "from",
      "DefaultToolFnSchema",
      "class",
      "ToolOutput",
      "BaseTool",
      "AsyncBaseTool",
      "that",
      "BaseToolAsyncAdapter",
      "that"
    ],
    "document_function_names": [
      "get_parameters_dict",
      "fn_schema_str",
      "get_name",
      "to_openai_function",
      "to_openai_tool",
      "__str__",
      "metadata",
      "__call__",
      "_process_langchain_tool_kwargs",
      "to_langchain_tool",
      "to_langchain_structured_tool",
      "__call__",
      "call",
      "acall",
      "__init__",
      "metadata",
      "call",
      "acall",
      "adapt_to_async_tool"
    ],
    "document_class_names": [
      "from",
      "DefaultToolFnSchema",
      "class",
      "ToolOutput",
      "BaseTool",
      "AsyncBaseTool",
      "that",
      "BaseToolAsyncAdapter",
      "that"
    ],
    "content": "import asyncio\nimport json\nfrom abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Type\n\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\n\nif TYPE_CHECKING:\n    from llama_index.core.bridge.langchain import StructuredTool, Tool\nfrom deprecated import deprecated\nfrom llama_index.core.bridge.pydantic import BaseModel\n\n\nclass DefaultToolFnSchema(BaseModel):\n    \"\"\"Default tool function Schema.\"\"\"\n\n    input: str\n\n\n@dataclass\nclass ToolMetadata:\n    description: str\n    name: Optional[str] = None\n    fn_schema: Optional[Type[BaseModel]] = DefaultToolFnSchema\n    return_direct: bool = False\n\n    def get_parameters_dict(self) -> dict:\n        if self.fn_schema is None:\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"input\": {\"title\": \"input query string\", \"type\": \"string\"},\n                },\n                \"required\": [\"input\"],\n            }\n        else:\n            parameters = self.fn_schema.model_json_schema()\n            parameters = {\n                k: v\n                for k, v in parameters.items()\n                if k in [\"type\", \"properties\", \"required\", \"definitions\", \"$defs\"]\n            }\n        return parameters\n\n    @property\n    def fn_schema_str(self) -> str:\n        \"\"\"Get fn schema as string.\"\"\"\n        if self.fn_schema is None:\n            raise ValueError(\"fn_schema is None.\")\n        parameters = self.get_parameters_dict()\n        return json.dumps(parameters, ensure_ascii=False)\n\n    def get_name(self) -> str:\n        \"\"\"Get name.\"\"\"\n        if self.name is None:\n            raise ValueError(\"name is None.\")\n        return self.name\n\n    @deprecated(\n        \"Deprecated in favor of `to_openai_tool`, which should be used instead.\"\n    )\n    def to_openai_function(self) -> Dict[str, Any]:\n        \"\"\"Deprecated and replaced by `to_openai_tool`.\n        The name and arguments of a function that should be called, as generated by the\n        model.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"description\": self.description,\n            \"parameters\": self.get_parameters_dict(),\n        }\n\n    def to_openai_tool(self, skip_length_check: bool = False) -> Dict[str, Any]:\n        \"\"\"To OpenAI tool.\"\"\"\n        if not skip_length_check and len(self.description) > 1024:\n            raise ValueError(\n                \"Tool description exceeds maximum length of 1024 characters. \"\n                \"Please shorten your description or move it to the prompt.\"\n            )\n        return {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": self.name,\n                \"description\": self.description,\n                \"parameters\": self.get_parameters_dict(),\n            },\n        }\n\n\nclass ToolOutput(BaseModel):\n    \"\"\"Tool output.\"\"\"\n\n    content: str\n    tool_name: str\n    raw_input: Dict[str, Any]\n    raw_output: Any\n    is_error: bool = False\n\n    def __str__(self) -> str:\n        \"\"\"String.\"\"\"\n        return str(self.content)\n\n\nclass BaseTool(DispatcherSpanMixin):\n    @property\n    @abstractmethod\n    def metadata(self) -> ToolMetadata:\n        pass\n\n    @abstractmethod\n    def __call__(self, input: Any) -> ToolOutput:\n        pass\n\n    def _process_langchain_tool_kwargs(\n        self,\n        langchain_tool_kwargs: Any,\n    ) -> Dict[str, Any]:\n        \"\"\"Process langchain tool kwargs.\"\"\"\n        if \"name\" not in langchain_tool_kwargs:\n            langchain_tool_kwargs[\"name\"] = self.metadata.name or \"\"\n        if \"description\" not in langchain_tool_kwargs:\n            langchain_tool_kwargs[\"description\"] = self.metadata.description\n        if \"fn_schema\" not in langchain_tool_kwargs:\n            langchain_tool_kwargs[\"args_schema\"] = self.metadata.fn_schema\n\n        # Callback dont exist on langchain\n        if \"_callback\" in langchain_tool_kwargs:\n            del langchain_tool_kwargs[\"_callback\"]\n        if \"_async_callback\" in langchain_tool_kwargs:\n            del langchain_tool_kwargs[\"_async_callback\"]\n\n        return langchain_tool_kwargs\n\n    def to_langchain_tool(\n        self,\n        **langchain_tool_kwargs: Any,\n    ) -> \"Tool\":\n        \"\"\"To langchain tool.\"\"\"\n        from llama_index.core.bridge.langchain import Tool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return Tool.from_function(\n            func=self.__call__,\n            **langchain_tool_kwargs,\n        )\n\n    def to_langchain_structured_tool(\n        self,\n        **langchain_tool_kwargs: Any,\n    ) -> \"StructuredTool\":\n        \"\"\"To langchain structured tool.\"\"\"\n        from llama_index.core.bridge.langchain import StructuredTool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return StructuredTool.from_function(\n            func=self.__call__,\n            **langchain_tool_kwargs,\n        )\n\n\nclass AsyncBaseTool(BaseTool):\n    \"\"\"\n    Base-level tool class that is backwards compatible with the old tool spec but also\n    supports async.\n    \"\"\"\n\n    def __call__(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        return self.call(*args, **kwargs)\n\n    @abstractmethod\n    def call(self, input: Any) -> ToolOutput:\n        \"\"\"\n        This is the method that should be implemented by the tool developer.\n        \"\"\"\n\n    @abstractmethod\n    async def acall(self, input: Any) -> ToolOutput:\n        \"\"\"\n        This is the async version of the call method.\n        Should also be implemented by the tool developer as an\n        async-compatible implementation.\n        \"\"\"\n\n\nclass BaseToolAsyncAdapter(AsyncBaseTool):\n    \"\"\"\n    Adapter class that allows a synchronous tool to be used as an async tool.\n    \"\"\"\n\n    def __init__(self, tool: BaseTool):\n        self.base_tool = tool\n\n    @property\n    def metadata(self) -> ToolMetadata:\n        return self.base_tool.metadata\n\n    def call(self, input: Any) -> ToolOutput:\n        return self.base_tool(input)\n\n    async def acall(self, input: Any) -> ToolOutput:\n        return await asyncio.to_thread(self.call, input)\n\n\ndef adapt_to_async_tool(tool: BaseTool) -> AsyncBaseTool:\n    \"\"\"\n    Converts a synchronous tool to an async tool.\n    \"\"\"\n    if isinstance(tool, AsyncBaseTool):\n        return tool\n    else:\n        return BaseToolAsyncAdapter(tool)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/query_plan.py",
    "filename": "query_plan.py",
    "relpath": "tools/query_plan.py",
    "start_line": 1,
    "end_line": 230,
    "length": 230,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "metadata",
      "metadata",
      "_execute_node",
      "_find_root_nodes",
      "__call__"
    ],
    "chunk_class_names": [
      "QueryNode",
      "QueryPlan",
      "QueryPlanTool"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "metadata",
      "metadata",
      "_execute_node",
      "_find_root_nodes",
      "__call__"
    ],
    "document_class_names": [
      "QueryNode",
      "QueryPlan",
      "QueryPlanTool"
    ],
    "content": "\"\"\"Query plan tool.\"\"\"\n\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.response_synthesizers import (\n    BaseSynthesizer,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import NodeWithScore, TextNode\nfrom llama_index.core.tools.types import BaseTool, ToolMetadata, ToolOutput\nfrom llama_index.core.utils import print_text\n\nDEFAULT_NAME = \"query_plan_tool\"\n\nQUERYNODE_QUERY_STR_DESC = \"\"\"\\\nQuestion we are asking. This is the query string that will be executed. \\\n\"\"\"\n\nQUERYNODE_TOOL_NAME_DESC = \"\"\"\\\nName of the tool to execute the `query_str`. \\\nShould NOT be specified if there are subquestions to be specified, in which \\\ncase child_nodes should be nonempty instead.\\\n\"\"\"\n\nQUERYNODE_DEPENDENCIES_DESC = \"\"\"\\\nList of sub-questions that need to be answered in order \\\nto answer the question given by `query_str`.\\\nShould be blank if there are no sub-questions to be specified, in which case \\\n`tool_name` is specified.\\\n\"\"\"\n\n\nclass QueryNode(BaseModel):\n    \"\"\"Query node.\n\n    A query node represents a query (query_str) that must be answered.\n    It can either be answered by a tool (tool_name), or by a list of child nodes\n    (child_nodes).\n    The tool_name and child_nodes fields are mutually exclusive.\n\n    \"\"\"\n\n    # NOTE: inspired from https://github.com/jxnl/openai_function_call/pull/3/files\n\n    id: int = Field(..., description=\"ID of the query node.\")\n    query_str: str = Field(..., description=QUERYNODE_QUERY_STR_DESC)\n    tool_name: Optional[str] = Field(\n        default=None, description=\"Name of the tool to execute the `query_str`.\"\n    )\n    dependencies: List[int] = Field(\n        default_factory=list, description=QUERYNODE_DEPENDENCIES_DESC\n    )\n\n\nclass QueryPlan(BaseModel):\n    \"\"\"Query plan.\n\n    Contains a list of QueryNode objects (which is a recursive object).\n    Out of the list of QueryNode objects, one of them must be the root node.\n    The root node is the one that isn't a dependency of any other node.\n\n    \"\"\"\n\n    nodes: List[QueryNode] = Field(\n        ...,\n        description=\"The original question we are asking.\",\n    )\n\n\nDEFAULT_DESCRIPTION_PREFIX = \"\"\"\\\nThis is a query plan tool that takes in a list of tools and executes a \\\nquery plan over these tools to answer a query. The query plan is a DAG of query nodes.\n\nGiven a list of tool names and the query plan schema, you \\\ncan choose to generate a query plan to answer a question.\n\nThe tool names and descriptions are as follows:\n\"\"\"\n\n\nclass QueryPlanTool(BaseTool):\n    \"\"\"Query plan tool.\n\n    A tool that takes in a list of tools and executes a query plan.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine_tools: List[BaseTool],\n        response_synthesizer: BaseSynthesizer,\n        name: str,\n        description_prefix: str,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        self._query_tools_dict = {t.metadata.name: t for t in query_engine_tools}\n        self._response_synthesizer = response_synthesizer\n        self._name = name\n        self._description_prefix = description_prefix\n        self._custom_metadata: Optional[ToolMetadata] = None\n\n    @classmethod\n    def from_defaults(\n        cls,\n        query_engine_tools: List[BaseTool],\n        response_synthesizer: Optional[BaseSynthesizer] = None,\n        name: Optional[str] = None,\n        description_prefix: Optional[str] = None,\n    ) -> \"QueryPlanTool\":\n        \"\"\"Initialize from defaults.\"\"\"\n        name = name or DEFAULT_NAME\n        description_prefix = description_prefix or DEFAULT_DESCRIPTION_PREFIX\n        response_synthesizer = response_synthesizer or get_response_synthesizer()\n\n        return cls(\n            query_engine_tools=query_engine_tools,\n            response_synthesizer=response_synthesizer,\n            name=name,\n            description_prefix=description_prefix,\n        )\n\n    @property\n    def metadata(self) -> ToolMetadata:\n        \"\"\"Metadata.\"\"\"\n        if self._custom_metadata is not None:\n            return self._custom_metadata\n\n        tools_description = \"\\n\\n\".join(\n            [\n                f\"Tool Name: {tool.metadata.name}\\n\"\n                + f\"Tool Description: {tool.metadata.description} \"\n                for tool in self._query_tools_dict.values()\n            ]\n        )\n        # TODO: fill in description with query engine tools.\n        description = f\"\"\"\\\n        {self._description_prefix}\\n\\n\n        {tools_description}\n        \"\"\"\n        return ToolMetadata(description, self._name, fn_schema=QueryPlan)\n\n    @metadata.setter\n    def metadata(self, value: ToolMetadata) -> None:\n        self._custom_metadata = value\n\n    def _execute_node(\n        self, node: QueryNode, nodes_dict: Dict[int, QueryNode]\n    ) -> ToolOutput:\n        \"\"\"Execute node.\"\"\"\n        print_text(f\"Executing node {node.model_dump_json()}\\n\", color=\"blue\")\n        if len(node.dependencies) > 0:\n            print_text(\n                f\"Executing {len(node.dependencies)} child nodes\\n\", color=\"pink\"\n            )\n            child_query_nodes: List[QueryNode] = [\n                nodes_dict[dep] for dep in node.dependencies\n            ]\n            # execute the child nodes first\n            child_responses: List[ToolOutput] = [\n                self._execute_node(child, nodes_dict) for child in child_query_nodes\n            ]\n            # form the child Node/NodeWithScore objects\n            child_nodes = []\n            for child_query_node, child_response in zip(\n                child_query_nodes, child_responses\n            ):\n                node_text = (\n                    f\"Query: {child_query_node.query_str}\\n\"\n                    f\"Response: {child_response!s}\\n\"\n                )\n                child_node = TextNode(text=node_text)\n                child_nodes.append(child_node)\n            # use response synthesizer to combine results\n            child_nodes_with_scores = [\n                NodeWithScore(node=n, score=1.0) for n in child_nodes\n            ]\n            response_obj = self._response_synthesizer.synthesize(\n                query=node.query_str,\n                nodes=child_nodes_with_scores,\n            )\n            response = ToolOutput(\n                content=str(response_obj),\n                tool_name=node.query_str,\n                raw_input={\"query\": node.query_str},\n                raw_output=response_obj,\n            )\n\n            if node.tool_name in self._query_tools_dict:\n                tool = self._query_tools_dict[node.tool_name]\n                print_text(f\"Selected Tool: {tool.metadata}\\n\", color=\"pink\")\n                response = tool(node.query_str)\n\n        else:\n            # this is a leaf request, execute the query string using the specified tool\n            tool = self._query_tools_dict[node.tool_name]\n            print_text(f\"Selected Tool: {tool.metadata}\\n\", color=\"pink\")\n            response = tool(node.query_str)\n        print_text(\n            \"Executed query, got response.\\n\"\n            f\"Query: {node.query_str}\\n\"\n            f\"Response: {response!s}\\n\",\n            color=\"blue\",\n        )\n        return response\n\n    def _find_root_nodes(self, nodes_dict: Dict[int, QueryNode]) -> List[QueryNode]:\n        \"\"\"Find root node.\"\"\"\n        # the root node is the one that isn't a dependency of any other node\n        node_counts = {node_id: 0 for node_id in nodes_dict}\n        for node in nodes_dict.values():\n            for dep in node.dependencies:\n                node_counts[dep] += 1\n        root_node_ids = [\n            node_id for node_id, count in node_counts.items() if count == 0\n        ]\n        return [nodes_dict[node_id] for node_id in root_node_ids]\n\n    def __call__(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        \"\"\"Call.\"\"\"\n        # the kwargs represented as a JSON object\n        # should be a QueryPlan object\n        query_plan = QueryPlan(**kwargs)\n\n        nodes_dict = {node.id: node for node in query_plan.nodes}\n        root_nodes = self._find_root_nodes(nodes_dict)\n        if len(root_nodes) > 1:\n            raise ValueError(\"Query plan should have exactly one root node.\")\n\n        return self._execute_node(root_nodes[0], nodes_dict)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/retriever_tool.py",
    "filename": "retriever_tool.py",
    "relpath": "tools/retriever_tool.py",
    "start_line": 1,
    "end_line": 133,
    "length": 133,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "retriever",
      "metadata",
      "call",
      "acall",
      "as_langchain_tool",
      "_apply_node_postprocessors"
    ],
    "chunk_class_names": [
      "RetrieverTool"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "retriever",
      "metadata",
      "call",
      "acall",
      "as_langchain_tool",
      "_apply_node_postprocessors"
    ],
    "document_class_names": [
      "RetrieverTool"
    ],
    "content": "\"\"\"Retriever tool.\"\"\"\n\nfrom typing import TYPE_CHECKING, Any, List, Optional\n\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\n\nif TYPE_CHECKING:\n    from llama_index.core.langchain_helpers.agents.tools import LlamaIndexTool\nfrom llama_index.core.schema import (\n    MetadataMode,\n    Node,\n    NodeWithScore,\n    QueryBundle,\n    TextNode,\n)\nfrom llama_index.core.tools.types import AsyncBaseTool, ToolMetadata, ToolOutput\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\n\nDEFAULT_NAME = \"retriever_tool\"\nDEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and retrieving a set of relevant documents.\n\"\"\"\n\n\nclass RetrieverTool(AsyncBaseTool):\n    \"\"\"Retriever tool.\n\n    A tool making use of a retriever.\n\n    Args:\n        retriever (BaseRetriever): A retriever.\n        metadata (ToolMetadata): The associated metadata of the query engine.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): A list of\n            node postprocessors.\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        metadata: ToolMetadata,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n    ) -> None:\n        self._retriever = retriever\n        self._metadata = metadata\n        self._node_postprocessors = node_postprocessors or []\n\n    @classmethod\n    def from_defaults(\n        cls,\n        retriever: BaseRetriever,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n    ) -> \"RetrieverTool\":\n        name = name or DEFAULT_NAME\n        description = description or DEFAULT_DESCRIPTION\n\n        metadata = ToolMetadata(name=name, description=description)\n        return cls(\n            retriever=retriever,\n            metadata=metadata,\n            node_postprocessors=node_postprocessors,\n        )\n\n    @property\n    def retriever(self) -> BaseRetriever:\n        return self._retriever\n\n    @property\n    def metadata(self) -> ToolMetadata:\n        return self._metadata\n\n    def call(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        query_str = \"\"\n        if args is not None:\n            query_str += \", \".join([str(arg) for arg in args]) + \"\\n\"\n        if kwargs is not None:\n            query_str += (\n                \", \".join([f\"{k!s} is {v!s}\" for k, v in kwargs.items()]) + \"\\n\"\n            )\n        if query_str == \"\":\n            raise ValueError(\"Cannot call query engine without inputs\")\n\n        docs = self._retriever.retrieve(query_str)\n        docs = self._apply_node_postprocessors(docs, QueryBundle(query_str))\n        content = \"\"\n        for doc in docs:\n            assert isinstance(doc.node, (Node, TextNode))\n            node_copy = doc.node.model_copy()\n            content += node_copy.get_content(MetadataMode.LLM) + \"\\n\\n\"\n        return ToolOutput(\n            content=content,\n            tool_name=self.metadata.name,\n            raw_input={\"input\": query_str},\n            raw_output=docs,\n        )\n\n    async def acall(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        query_str = \"\"\n        if args is not None:\n            query_str += \", \".join([str(arg) for arg in args]) + \"\\n\"\n        if kwargs is not None:\n            query_str += (\n                \", \".join([f\"{k!s} is {v!s}\" for k, v in kwargs.items()]) + \"\\n\"\n            )\n        if query_str == \"\":\n            raise ValueError(\"Cannot call query engine without inputs\")\n        docs = await self._retriever.aretrieve(query_str)\n        content = \"\"\n        docs = self._apply_node_postprocessors(docs, QueryBundle(query_str))\n        for doc in docs:\n            assert isinstance(doc.node, (Node, TextNode))\n            node_copy = doc.node.model_copy()\n            content += node_copy.get_content(MetadataMode.LLM) + \"\\n\\n\"\n        return ToolOutput(\n            content=content,\n            tool_name=self.metadata.name,\n            raw_input={\"input\": query_str},\n            raw_output=docs,\n        )\n\n    def as_langchain_tool(self) -> \"LlamaIndexTool\":\n        raise NotImplementedError(\"`as_langchain_tool` not implemented here.\")\n\n    def _apply_node_postprocessors(\n        self, nodes: List[NodeWithScore], query_bundle: QueryBundle\n    ) -> List[NodeWithScore]:\n        for node_postprocessor in self._node_postprocessors:\n            nodes = node_postprocessor.postprocess_nodes(\n                nodes, query_bundle=query_bundle\n            )\n        return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/calling.py",
    "filename": "calling.py",
    "relpath": "tools/calling.py",
    "start_line": 1,
    "end_line": 104,
    "length": 104,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "call_tool",
      "acall_tool",
      "call_tool_with_selection",
      "acall_tool_with_selection"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "call_tool",
      "acall_tool",
      "call_tool_with_selection",
      "acall_tool_with_selection"
    ],
    "document_class_names": [],
    "content": "from llama_index.core.tools.types import BaseTool, ToolOutput, adapt_to_async_tool\nfrom typing import TYPE_CHECKING, Sequence\nfrom llama_index.core.llms.llm import ToolSelection\nimport json\n\nif TYPE_CHECKING:\n    from llama_index.core.tools.types import BaseTool\n\n\ndef call_tool(tool: BaseTool, arguments: dict) -> ToolOutput:\n    \"\"\"Call a tool with arguments.\"\"\"\n    try:\n        if (\n            len(tool.metadata.get_parameters_dict()[\"properties\"]) == 1\n            and len(arguments) == 1\n        ):\n            try:\n                single_arg = arguments[next(iter(arguments))]\n                return tool(single_arg)\n            except Exception:\n                # some tools will REQUIRE kwargs, so try it\n                return tool(**arguments)\n        else:\n            return tool(**arguments)\n    except Exception as e:\n        return ToolOutput(\n            content=\"Encountered error: \" + str(e),\n            tool_name=tool.metadata.name,\n            raw_input=arguments,\n            raw_output=str(e),\n            is_error=True,\n        )\n\n\nasync def acall_tool(tool: BaseTool, arguments: dict) -> ToolOutput:\n    \"\"\"Call a tool with arguments asynchronously.\"\"\"\n    async_tool = adapt_to_async_tool(tool)\n    try:\n        if (\n            len(tool.metadata.get_parameters_dict()[\"properties\"]) == 1\n            and len(arguments) == 1\n        ):\n            try:\n                single_arg = arguments[next(iter(arguments))]\n                return await async_tool.acall(single_arg)\n            except Exception:\n                # some tools will REQUIRE kwargs, so try it\n                return await async_tool.acall(**arguments)\n        else:\n            return await async_tool.acall(**arguments)\n    except Exception as e:\n        return ToolOutput(\n            content=\"Encountered error: \" + str(e),\n            tool_name=tool.metadata.name,\n            raw_input=arguments,\n            raw_output=str(e),\n            is_error=True,\n        )\n\n\ndef call_tool_with_selection(\n    tool_call: ToolSelection,\n    tools: Sequence[\"BaseTool\"],\n    verbose: bool = False,\n) -> ToolOutput:\n    from llama_index.core.tools.calling import call_tool\n\n    tools_by_name = {tool.metadata.name: tool for tool in tools}\n    name = tool_call.tool_name\n    if verbose:\n        arguments_str = json.dumps(tool_call.tool_kwargs)\n        print(\"=== Calling Function ===\")\n        print(f\"Calling function: {name} with args: {arguments_str}\")\n    tool = tools_by_name[name]\n    output = call_tool(tool, tool_call.tool_kwargs)\n\n    if verbose:\n        print(\"=== Function Output ===\")\n        print(output.content)\n\n    return output\n\n\nasync def acall_tool_with_selection(\n    tool_call: ToolSelection,\n    tools: Sequence[\"BaseTool\"],\n    verbose: bool = False,\n) -> ToolOutput:\n    from llama_index.core.tools.calling import acall_tool\n\n    tools_by_name = {tool.metadata.name: tool for tool in tools}\n    name = tool_call.tool_name\n    if verbose:\n        arguments_str = json.dumps(tool_call.tool_kwargs)\n        print(\"=== Calling Function ===\")\n        print(f\"Calling function: {name} with args: {arguments_str}\")\n    tool = tools_by_name[name]\n    output = await acall_tool(tool, tool_call.tool_kwargs)\n\n    if verbose:\n        print(\"=== Function Output ===\")\n        print(output.content)\n\n    return output"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/__init__.py",
    "filename": "__init__.py",
    "relpath": "tools/__init__.py",
    "start_line": 1,
    "end_line": 35,
    "length": 35,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Tools.\"\"\"\n\nfrom llama_index.core.tools.download import download_tool\nfrom llama_index.core.tools.function_tool import FunctionTool\nfrom llama_index.core.tools.query_engine import QueryEngineTool\nfrom llama_index.core.tools.query_plan import QueryPlanTool\nfrom llama_index.core.tools.retriever_tool import RetrieverTool\nfrom llama_index.core.tools.types import (\n    AsyncBaseTool,\n    BaseTool,\n    ToolMetadata,\n    ToolOutput,\n    adapt_to_async_tool,\n)\nfrom llama_index.core.tools.calling import (\n    ToolSelection,\n    call_tool_with_selection,\n    acall_tool_with_selection,\n)\n\n__all__ = [\n    \"BaseTool\",\n    \"adapt_to_async_tool\",\n    \"AsyncBaseTool\",\n    \"QueryEngineTool\",\n    \"RetrieverTool\",\n    \"ToolMetadata\",\n    \"ToolOutput\",\n    \"FunctionTool\",\n    \"QueryPlanTool\",\n    \"download_tool\",\n    \"ToolSelection\",\n    \"call_tool_with_selection\",\n    \"acall_tool_with_selection\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/utils.py",
    "filename": "utils.py",
    "relpath": "tools/utils.py",
    "start_line": 1,
    "end_line": 80,
    "length": 80,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "create_schema_from_function"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "create_schema_from_function"
    ],
    "document_class_names": [],
    "content": "from inspect import signature\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    cast,\n    get_origin,\n    get_args,\n)\nimport typing\n\nfrom llama_index.core.bridge.pydantic import BaseModel, FieldInfo, create_model\n\n\ndef create_schema_from_function(\n    name: str,\n    func: Union[Callable[..., Any], Callable[..., Awaitable[Any]]],\n    additional_fields: Optional[\n        List[Union[Tuple[str, Type, Any], Tuple[str, Type]]]\n    ] = None,\n    ignore_fields: Optional[List[str]] = None,\n) -> Type[BaseModel]:\n    \"\"\"Create schema from function.\"\"\"\n    fields = {}\n    ignore_fields = ignore_fields or []\n    params = signature(func).parameters\n    for param_name in params:\n        if param_name in ignore_fields:\n            continue\n\n        param_type = params[param_name].annotation\n        param_default = params[param_name].default\n        description = None\n\n        if get_origin(param_type) is typing.Annotated:\n            args = get_args(param_type)\n            param_type = args[0]\n            if isinstance(args[1], str):\n                description = args[1]\n            elif isinstance(args[1], FieldInfo):\n                description = args[1].description\n\n        if param_type is params[param_name].empty:\n            param_type = Any\n\n        if param_default is params[param_name].empty:\n            # Required field\n            fields[param_name] = (param_type, FieldInfo(description=description))\n        elif isinstance(param_default, FieldInfo):\n            # Field with pydantic.Field as default value\n            fields[param_name] = (param_type, param_default)\n        else:\n            fields[param_name] = (\n                param_type,\n                FieldInfo(default=param_default, description=description),\n            )\n\n    additional_fields = additional_fields or []\n    for field_info in additional_fields:\n        if len(field_info) == 3:\n            field_info = cast(Tuple[str, Type, Any], field_info)\n            field_name, field_type, field_default = field_info\n            fields[field_name] = (field_type, FieldInfo(default=field_default))\n        elif len(field_info) == 2:\n            # Required field has no default value\n            field_info = cast(Tuple[str, Type], field_info)\n            field_name, field_type = field_info\n            fields[field_name] = (field_type, FieldInfo())\n        else:\n            raise ValueError(\n                f\"Invalid additional field info: {field_info}. \"\n                \"Must be a tuple of length 2 or 3.\"\n            )\n\n    return create_model(name, **fields)  # type: ignore"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/ondemand_loader_tool.py",
    "filename": "ondemand_loader_tool.py",
    "relpath": "tools/ondemand_loader_tool.py",
    "start_line": 1,
    "end_line": 168,
    "length": 168,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "metadata",
      "from_defaults",
      "from_tool",
      "_parse_args",
      "call",
      "acall"
    ],
    "chunk_class_names": [
      "OnDemandLoaderTool"
    ],
    "document_function_names": [
      "__init__",
      "metadata",
      "from_defaults",
      "from_tool",
      "_parse_args",
      "call",
      "acall"
    ],
    "document_class_names": [
      "OnDemandLoaderTool"
    ],
    "content": "\"\"\"Ad-hoc data loader tool.\n\nTool that wraps any data loader, and is able to load data on-demand.\n\n\"\"\"\n\n\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom llama_index.core.readers.base import BaseReader\nfrom llama_index.core.schema import Document\nfrom llama_index.core.tools.function_tool import FunctionTool\nfrom llama_index.core.tools.types import AsyncBaseTool, ToolMetadata, ToolOutput\nfrom llama_index.core.tools.utils import create_schema_from_function\n\n\nclass OnDemandLoaderTool(AsyncBaseTool):\n    \"\"\"On-demand data loader tool.\n\n    Loads data with by calling the provided loader function,\n    stores in index, and queries for relevant data with a\n    natural language query string.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        loader: Callable[..., List[Document]],\n        index_cls: Type[BaseIndex],\n        index_kwargs: Dict,\n        metadata: ToolMetadata,\n        use_query_str_in_loader: bool = False,\n        query_str_kwargs_key: str = \"query_str\",\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._loader = loader\n        self._index_cls = index_cls\n        self._index_kwargs = index_kwargs\n        self._use_query_str_in_loader = use_query_str_in_loader\n        self._metadata = metadata\n        self._query_str_kwargs_key = query_str_kwargs_key\n\n    @property\n    def metadata(self) -> ToolMetadata:\n        return self._metadata\n\n    @classmethod\n    def from_defaults(\n        cls,\n        reader: BaseReader,\n        index_cls: Optional[Type[BaseIndex]] = None,\n        index_kwargs: Optional[Dict] = None,\n        use_query_str_in_loader: bool = False,\n        query_str_kwargs_key: str = \"query_str\",\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        fn_schema: Optional[Type[BaseModel]] = None,\n    ) -> \"OnDemandLoaderTool\":\n        \"\"\"From defaults.\"\"\"\n        # NOTE: fn_schema should be specified if you want to use as langchain Tool\n\n        index_cls = index_cls or VectorStoreIndex\n        index_kwargs = index_kwargs or {}\n        if description is None:\n            description = f\"Tool to load data from {reader.__class__.__name__}\"\n        if fn_schema is None:\n            fn_schema = create_schema_from_function(\n                name or \"LoadData\",\n                reader.load_data,\n                [(query_str_kwargs_key, str, None)],\n            )\n\n        metadata = ToolMetadata(name=name, description=description, fn_schema=fn_schema)\n        return cls(\n            loader=reader.load_data,\n            index_cls=index_cls,\n            index_kwargs=index_kwargs,\n            use_query_str_in_loader=use_query_str_in_loader,\n            query_str_kwargs_key=query_str_kwargs_key,\n            metadata=metadata,\n        )\n\n    @classmethod\n    def from_tool(\n        cls,\n        tool: FunctionTool,\n        index_cls: Optional[Type[BaseIndex]] = None,\n        index_kwargs: Optional[Dict] = None,\n        use_query_str_in_loader: bool = False,\n        query_str_kwargs_key: str = \"query_str\",\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        fn_schema: Optional[Type[BaseModel]] = None,\n    ) -> \"OnDemandLoaderTool\":\n        \"\"\"From defaults.\"\"\"\n        # NOTE: fn_schema should be specified if you want to use as langchain Tool\n\n        index_cls = index_cls or VectorStoreIndex\n        index_kwargs = index_kwargs or {}\n        if description is None:\n            description = f\"Tool to load data from {tool.__class__.__name__}\"\n        if fn_schema is None:\n            fn_schema = create_schema_from_function(\n                name or \"LoadData\", tool._fn, [(query_str_kwargs_key, str, None)]\n            )\n        metadata = ToolMetadata(\n            name=name,\n            description=description,\n            fn_schema=fn_schema,\n            return_direct=return_direct,\n        )\n        return cls(\n            loader=tool._fn,\n            index_cls=index_cls,\n            index_kwargs=index_kwargs,\n            use_query_str_in_loader=use_query_str_in_loader,\n            query_str_kwargs_key=query_str_kwargs_key,\n            metadata=metadata,\n        )\n\n    def _parse_args(self, *args: Any, **kwargs: Any) -> Tuple[str, List[Document]]:\n        if self._query_str_kwargs_key not in kwargs:\n            raise ValueError(\n                \"Missing query_str in kwargs with parameter name: \"\n                f\"{self._query_str_kwargs_key}\"\n            )\n        if self._use_query_str_in_loader:\n            query_str = kwargs[self._query_str_kwargs_key]\n        else:\n            query_str = kwargs.pop(self._query_str_kwargs_key)\n\n        docs = self._loader(*args, **kwargs)\n\n        return query_str, docs\n\n    def call(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        \"\"\"Call.\"\"\"\n        query_str, docs = self._parse_args(*args, **kwargs)\n\n        index = self._index_cls.from_documents(docs, **self._index_kwargs)\n        # TODO: add query kwargs\n        query_engine = index.as_query_engine()\n        response = query_engine.query(query_str)\n        return ToolOutput(\n            content=str(response),\n            tool_name=self.metadata.name,\n            raw_input={\"query\": query_str},\n            raw_output=response,\n        )\n\n    async def acall(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        \"\"\"Async Call.\"\"\"\n        query_str, docs = self._parse_args(*args, **kwargs)\n\n        index = self._index_cls.from_documents(docs, **self._index_kwargs)\n        # TODO: add query kwargs\n        query_engine = index.as_query_engine()\n        response = await query_engine.aquery(query_str)\n        return ToolOutput(\n            content=str(response),\n            tool_name=self.metadata.name,\n            raw_input={\"query\": query_str},\n            raw_output=response,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/download.py",
    "filename": "download.py",
    "relpath": "tools/download.py",
    "start_line": 1,
    "end_line": 62,
    "length": 62,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "download_tool"
    ],
    "chunk_class_names": [
      "you",
      "in",
      "of"
    ],
    "document_function_names": [
      "download_tool"
    ],
    "document_class_names": [
      "you",
      "in",
      "of"
    ],
    "content": "\"\"\"Download tool from Llama Index. Deprecated.\"\"\"\n\nimport json\nimport os\nfrom typing import Optional, Type\n\nfrom deprecated import deprecated\n\nfrom llama_index.core.download.integration import download_integration\nfrom llama_index.core.tools.tool_spec.base import BaseToolSpec\n\n\n@deprecated(\n    \"`download_tool()` is deprecated. \"\n    \"Please install tool using pip install directly instead.\"\n)\ndef download_tool(\n    tool_class: str,\n    llama_hub_url: str = \"\",\n    refresh_cache: bool = False,\n    custom_path: Optional[str] = None,\n) -> Type[BaseToolSpec]:\n    \"\"\"Download a single tool from Llama Hub.\n\n    Args:\n        tool_class: The name of the tool class you want to download,\n            such as `GmailToolSpec`.\n        refresh_cache: If true, the local cache will be skipped and the\n            loader will be fetched directly from the remote repo.\n        custom_path: Custom dirpath to download loader into.\n\n    Returns:\n        A Loader.\n    \"\"\"\n    del llama_hub_url  # maintain during deprecation period\n    del refresh_cache\n    del custom_path\n    mappings_path = os.path.join(\n        os.path.abspath(\n            os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir)\n        ),\n        \"command_line/mappings.json\",\n    )\n    with open(mappings_path) as f:\n        mappings = json.load(f)\n\n    if tool_class in mappings:\n        new_import_parent = mappings[tool_class]\n        new_install_parent = new_import_parent.replace(\".\", \"-\").replace(\"_\", \"-\")\n    else:\n        raise ValueError(f\"Failed to find python package for class {tool_class}\")\n\n    tool_cls = download_integration(\n        module_str=new_install_parent,\n        module_import_str=new_import_parent,\n        cls_name=tool_class,\n    )\n\n    if not issubclass(tool_cls, BaseToolSpec):\n        raise ValueError(f\"Tool class {tool_class} must be a subclass of BaseToolSpec.\")\n\n    return tool_cls"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/query_engine.py",
    "filename": "query_engine.py",
    "relpath": "tools/query_engine.py",
    "start_line": 1,
    "end_line": 111,
    "length": 111,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "query_engine",
      "metadata",
      "call",
      "acall",
      "as_langchain_tool",
      "_get_query_str"
    ],
    "chunk_class_names": [
      "QueryEngineTool"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "query_engine",
      "metadata",
      "call",
      "acall",
      "as_langchain_tool",
      "_get_query_str"
    ],
    "document_class_names": [
      "QueryEngineTool"
    ],
    "content": "from typing import TYPE_CHECKING, Any, Optional\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\n\nif TYPE_CHECKING:\n    from llama_index.core.langchain_helpers.agents.tools import (\n        LlamaIndexTool,\n    )\nfrom llama_index.core.tools.types import AsyncBaseTool, ToolMetadata, ToolOutput\n\nDEFAULT_NAME = \"query_engine_tool\"\nDEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and get back a natural language response.\n\"\"\"\n\n\nclass QueryEngineTool(AsyncBaseTool):\n    \"\"\"Query engine tool.\n\n    A tool making use of a query engine.\n\n    Args:\n        query_engine (BaseQueryEngine): A query engine.\n        metadata (ToolMetadata): The associated metadata of the query engine.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        metadata: ToolMetadata,\n        resolve_input_errors: bool = True,\n    ) -> None:\n        self._query_engine = query_engine\n        self._metadata = metadata\n        self._resolve_input_errors = resolve_input_errors\n\n    @classmethod\n    def from_defaults(\n        cls,\n        query_engine: BaseQueryEngine,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        resolve_input_errors: bool = True,\n    ) -> \"QueryEngineTool\":\n        name = name or DEFAULT_NAME\n        description = description or DEFAULT_DESCRIPTION\n\n        metadata = ToolMetadata(\n            name=name, description=description, return_direct=return_direct\n        )\n        return cls(\n            query_engine=query_engine,\n            metadata=metadata,\n            resolve_input_errors=resolve_input_errors,\n        )\n\n    @property\n    def query_engine(self) -> BaseQueryEngine:\n        return self._query_engine\n\n    @property\n    def metadata(self) -> ToolMetadata:\n        return self._metadata\n\n    def call(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        query_str = self._get_query_str(*args, **kwargs)\n        response = self._query_engine.query(query_str)\n        return ToolOutput(\n            content=str(response),\n            tool_name=self.metadata.name,\n            raw_input={\"input\": query_str},\n            raw_output=response,\n        )\n\n    async def acall(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        query_str = self._get_query_str(*args, **kwargs)\n        response = await self._query_engine.aquery(query_str)\n        return ToolOutput(\n            content=str(response),\n            tool_name=self.metadata.name,\n            raw_input={\"input\": query_str},\n            raw_output=response,\n        )\n\n    def as_langchain_tool(self) -> \"LlamaIndexTool\":\n        from llama_index.core.langchain_helpers.agents.tools import (\n            IndexToolConfig,\n            LlamaIndexTool,\n        )\n\n        tool_config = IndexToolConfig(\n            query_engine=self.query_engine,\n            name=self.metadata.name,\n            description=self.metadata.description,\n        )\n        return LlamaIndexTool.from_tool_config(tool_config=tool_config)\n\n    def _get_query_str(self, *args: Any, **kwargs: Any) -> str:\n        if args is not None and len(args) > 0:\n            query_str = str(args[0])\n        elif kwargs is not None and \"input\" in kwargs:\n            # NOTE: this assumes our default function schema of `input`\n            query_str = kwargs[\"input\"]\n        elif kwargs is not None and self._resolve_input_errors:\n            query_str = str(kwargs)\n        else:\n            raise ValueError(\n                \"Cannot call query engine without specifying `input` parameter.\"\n            )\n        return query_str"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/eval_query_engine.py",
    "filename": "eval_query_engine.py",
    "relpath": "tools/eval_query_engine.py",
    "start_line": 1,
    "end_line": 95,
    "length": 95,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_process_tool_output",
      "from_defaults",
      "call",
      "acall"
    ],
    "chunk_class_names": [
      "EvalQueryEngineTool"
    ],
    "document_function_names": [
      "__init__",
      "_process_tool_output",
      "from_defaults",
      "call",
      "acall"
    ],
    "document_class_names": [
      "EvalQueryEngineTool"
    ],
    "content": "from typing import Any, Optional\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.evaluation import (\n    AnswerRelevancyEvaluator,\n    BaseEvaluator,\n    EvaluationResult,\n)\nfrom llama_index.core.tools import QueryEngineTool\nfrom llama_index.core.tools.types import ToolMetadata, ToolOutput\n\n\nDEFAULT_NAME = \"query_engine_tool\"\nDEFAULT_DESCRIPTION = \"\"\"Useful for running a natural language query\nagainst a knowledge base and get back a natural language response.\n\"\"\"\nFAILED_TOOL_OUTPUT_TEMPLATE = (\n    \"Could not use tool {tool_name} because it failed evaluation.\\n\" \"Reason: {reason}\"\n)\n\n\nclass EvalQueryEngineTool(QueryEngineTool):\n    \"\"\"Evaluating query engine tool.\n\n    A tool that makes use of a query engine and an evaluator, where the\n    evaluation of the query engine response will determine the tool output.\n\n    Args:\n        evaluator (BaseEvaluator): A query engine.\n        query_engine (BaseQueryEngine): A query engine.\n        metadata (ToolMetadata): The associated metadata of the query engine.\n    \"\"\"\n\n    _evaluator: BaseEvaluator\n    _failed_tool_output_template: str\n\n    def __init__(\n        self,\n        evaluator: BaseEvaluator,\n        *args: Any,\n        failed_tool_output_template: str = FAILED_TOOL_OUTPUT_TEMPLATE,\n        **kwargs: Any,\n    ):\n        super().__init__(*args, **kwargs)\n        self._evaluator = evaluator\n        self._failed_tool_output_template = failed_tool_output_template\n\n    def _process_tool_output(\n        self,\n        tool_output: ToolOutput,\n        evaluation_result: EvaluationResult,\n    ) -> ToolOutput:\n        if evaluation_result.passing:\n            return tool_output\n\n        tool_output.content = self._failed_tool_output_template.format(\n            tool_name=self.metadata.name,\n            reason=evaluation_result.feedback,\n        )\n        return tool_output\n\n    @classmethod\n    def from_defaults(\n        cls,\n        query_engine: BaseQueryEngine,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        resolve_input_errors: bool = True,\n        evaluator: Optional[BaseEvaluator] = None,\n    ) -> \"EvalQueryEngineTool\":\n        return cls(\n            evaluator=evaluator or AnswerRelevancyEvaluator(),\n            query_engine=query_engine,\n            metadata=ToolMetadata(\n                name=name or DEFAULT_NAME,\n                description=description or DEFAULT_DESCRIPTION,\n                return_direct=return_direct,\n            ),\n            resolve_input_errors=resolve_input_errors,\n        )\n\n    def call(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        tool_output = super().call(*args, **kwargs)\n        evaluation_results = self._evaluator.evaluate_response(\n            tool_output.raw_input[\"input\"], tool_output.raw_output\n        )\n        return self._process_tool_output(tool_output, evaluation_results)\n\n    async def acall(self, *args: Any, **kwargs: Any) -> ToolOutput:\n        tool_output = await super().acall(*args, **kwargs)\n        evaluation_results = await self._evaluator.aevaluate_response(\n            tool_output.raw_input[\"input\"], tool_output.raw_output\n        )\n        return self._process_tool_output(tool_output, evaluation_results)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/function_tool.py",
    "filename": "function_tool.py",
    "relpath": "tools/function_tool.py",
    "start_line": 1,
    "end_line": 295,
    "length": 295,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "sync_to_async",
      "_async_wrapped_fn",
      "async_to_sync",
      "_sync_wrapped_fn",
      "__init__",
      "_run_sync_callback",
      "_run_async_callback",
      "from_defaults",
      "metadata",
      "fn",
      "async_fn",
      "call",
      "acall",
      "to_langchain_tool",
      "to_langchain_structured_tool"
    ],
    "chunk_class_names": [
      "FunctionTool"
    ],
    "document_function_names": [
      "sync_to_async",
      "_async_wrapped_fn",
      "async_to_sync",
      "_sync_wrapped_fn",
      "__init__",
      "_run_sync_callback",
      "_run_async_callback",
      "from_defaults",
      "metadata",
      "fn",
      "async_fn",
      "call",
      "acall",
      "to_langchain_tool",
      "to_langchain_structured_tool"
    ],
    "document_class_names": [
      "FunctionTool"
    ],
    "content": "import asyncio\nimport inspect\nfrom typing import TYPE_CHECKING, Any, Awaitable, Callable, Optional, Type, Union\n\nif TYPE_CHECKING:\n    from llama_index.core.bridge.langchain import StructuredTool, Tool\n\nfrom llama_index.core.async_utils import asyncio_run\nfrom llama_index.core.bridge.pydantic import BaseModel, FieldInfo\nfrom llama_index.core.tools.types import AsyncBaseTool, ToolMetadata, ToolOutput\nfrom llama_index.core.tools.utils import create_schema_from_function\nfrom llama_index.core.workflow.context import Context\n\nAsyncCallable = Callable[..., Awaitable[Any]]\n\n\ndef sync_to_async(fn: Callable[..., Any]) -> AsyncCallable:\n    \"\"\"Sync to async.\"\"\"\n\n    async def _async_wrapped_fn(*args: Any, **kwargs: Any) -> Any:\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, lambda: fn(*args, **kwargs))\n\n    return _async_wrapped_fn\n\n\ndef async_to_sync(func_async: AsyncCallable) -> Callable:\n    \"\"\"Async to sync.\"\"\"\n\n    def _sync_wrapped_fn(*args: Any, **kwargs: Any) -> Any:\n        return asyncio_run(func_async(*args, **kwargs))  # type: ignore[arg-type]\n\n    return _sync_wrapped_fn\n\n\n# The type that the callback can return: either a ToolOutput instance or a string to override the content.\nCallbackReturn = Optional[Union[ToolOutput, str]]\n\n\nclass FunctionTool(AsyncBaseTool):\n    \"\"\"Function Tool.\n\n    A tool that takes in a function, optionally handles workflow context,\n    and allows the use of callbacks. The callback can return a new ToolOutput\n    to override the default one or a string that will be used as the final content.\n    \"\"\"\n\n    def __init__(\n        self,\n        fn: Optional[Callable[..., Any]] = None,\n        metadata: Optional[ToolMetadata] = None,\n        async_fn: Optional[Callable[..., Any]] = None,\n        callback: Optional[Callable[..., Any]] = None,\n        async_callback: Optional[Callable[..., Any]] = None,\n    ) -> None:\n        if fn is None and async_fn is None:\n            raise ValueError(\"fn or async_fn must be provided.\")\n\n        # Handle function (sync and async)\n        if async_fn is not None:\n            self._async_fn = async_fn\n            self._fn = fn or async_to_sync(async_fn)\n        else:\n            assert fn is not None\n            if inspect.iscoroutinefunction(fn):\n                self._async_fn = fn\n                self._fn = async_to_sync(fn)\n            else:\n                self._fn = fn\n                self._async_fn = sync_to_async(fn)\n\n        # Determine if the function requires context by inspecting its signature\n        fn_to_inspect = fn or async_fn\n        assert fn_to_inspect is not None\n        sig = inspect.signature(fn_to_inspect)\n        self.requires_context = any(\n            param.annotation == Context for param in sig.parameters.values()\n        )\n\n        if metadata is None:\n            raise ValueError(\"metadata must be provided\")\n        self._metadata = metadata\n\n        # Handle callback (sync and async)\n        self._callback = None\n        if callback is not None:\n            self._callback = callback\n        elif async_callback is not None:\n            self._callback = async_to_sync(async_callback)\n\n        self._async_callback = None\n        if async_callback is not None:\n            self._async_callback = async_callback\n        elif self._callback is not None:\n            self._async_callback = sync_to_async(self._callback)\n\n    def _run_sync_callback(self, result: Any) -> CallbackReturn:\n        \"\"\"Runs the sync callback, if provided, and returns either a ToolOutput\n        to override the default output or a string to override the content.\n        \"\"\"\n        if self._callback:\n            ret: CallbackReturn = self._callback(result)\n            return ret\n        return None\n\n    async def _run_async_callback(self, result: Any) -> CallbackReturn:\n        \"\"\"Runs the async callback, if provided, and returns either a ToolOutput\n        to override the default output or a string to override the content.\n        \"\"\"\n        if self._async_callback:\n            ret: CallbackReturn = await self._async_callback(result)\n            return ret\n        return None\n\n    @classmethod\n    def from_defaults(\n        cls,\n        fn: Optional[Callable[..., Any]] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        fn_schema: Optional[Type[BaseModel]] = None,\n        async_fn: Optional[AsyncCallable] = None,\n        tool_metadata: Optional[ToolMetadata] = None,\n        callback: Optional[Callable[[Any], Any]] = None,\n        async_callback: Optional[AsyncCallable] = None,\n    ) -> \"FunctionTool\":\n        if tool_metadata is None:\n            fn_to_parse = fn or async_fn\n            assert fn_to_parse is not None, \"fn must be provided\"\n            name = name or fn_to_parse.__name__\n            docstring = fn_to_parse.__doc__\n\n            # Get function signature\n            fn_sig = inspect.signature(fn_to_parse)\n\n            # Remove ctx parameter from schema if present\n            has_ctx = any(\n                param.annotation == Context for param in fn_sig.parameters.values()\n            )\n            ctx_param_name = None\n            if has_ctx:\n                ctx_param_name = next(\n                    param.name\n                    for param in fn_sig.parameters.values()\n                    if param.annotation == Context\n                )\n                fn_sig = fn_sig.replace(\n                    parameters=[\n                        param\n                        for param in fn_sig.parameters.values()\n                        if param.annotation != Context\n                    ]\n                )\n\n            # Handle FieldInfo defaults\n            fn_sig = fn_sig.replace(\n                parameters=[\n                    param.replace(default=inspect.Parameter.empty)\n                    if isinstance(param.default, FieldInfo)\n                    else param\n                    for param in fn_sig.parameters.values()\n                ]\n            )\n\n            description = description or f\"{name}{fn_sig}\\n{docstring}\"\n            if fn_schema is None:\n                fn_schema = create_schema_from_function(\n                    f\"{name}\",\n                    fn_to_parse,\n                    additional_fields=None,\n                    ignore_fields=[ctx_param_name]\n                    if ctx_param_name is not None\n                    else None,\n                )\n            tool_metadata = ToolMetadata(\n                name=name,\n                description=description,\n                fn_schema=fn_schema,\n                return_direct=return_direct,\n            )\n        return cls(\n            fn=fn,\n            metadata=tool_metadata,\n            async_fn=async_fn,\n            callback=callback,\n            async_callback=async_callback,\n        )\n\n    @property\n    def metadata(self) -> ToolMetadata:\n        \"\"\"Metadata.\"\"\"\n        return self._metadata\n\n    @property\n    def fn(self) -> Callable[..., Any]:\n        \"\"\"Function.\"\"\"\n        return self._fn\n\n    @property\n    def async_fn(self) -> AsyncCallable:\n        \"\"\"Async function.\"\"\"\n        return self._async_fn\n\n    def call(\n        self, *args: Any, ctx: Optional[Context] = None, **kwargs: Any\n    ) -> ToolOutput:\n        \"\"\"Sync Call.\"\"\"\n        if self.requires_context:\n            if ctx is None:\n                raise ValueError(\"Context is required for this tool\")\n            raw_output = self._fn(ctx, *args, **kwargs)\n        else:\n            raw_output = self._fn(*args, **kwargs)\n        # Default ToolOutput based on the raw output\n        default_output = ToolOutput(\n            content=str(raw_output),\n            tool_name=self.metadata.name,\n            raw_input={\"args\": args, \"kwargs\": kwargs},\n            raw_output=raw_output,\n        )\n        # Check for a sync callback override\n        callback_result = self._run_sync_callback(raw_output)\n        if callback_result is not None:\n            if isinstance(callback_result, ToolOutput):\n                return callback_result\n            else:\n                # Assume callback_result is a string to override the content.\n                return ToolOutput(\n                    content=str(callback_result),\n                    tool_name=self.metadata.name,\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output=raw_output,\n                )\n        return default_output\n\n    async def acall(\n        self, *args: Any, ctx: Optional[Context] = None, **kwargs: Any\n    ) -> ToolOutput:\n        \"\"\"Async Call.\"\"\"\n        if self.requires_context:\n            if ctx is None:\n                raise ValueError(\"Context is required for this tool\")\n            raw_output = await self._async_fn(ctx, *args, **kwargs)\n        else:\n            raw_output = await self._async_fn(*args, **kwargs)\n        # Default ToolOutput based on the raw output\n        default_output = ToolOutput(\n            content=str(raw_output),\n            tool_name=self.metadata.name,\n            raw_input={\"args\": args, \"kwargs\": kwargs},\n            raw_output=raw_output,\n        )\n        # Check for an async callback override\n        callback_result = await self._run_async_callback(raw_output)\n        if callback_result is not None:\n            if isinstance(callback_result, ToolOutput):\n                return callback_result\n            else:\n                # Assume callback_result is a string to override the content.\n                return ToolOutput(\n                    content=str(callback_result),\n                    tool_name=self.metadata.name,\n                    raw_input={\"args\": args, \"kwargs\": kwargs},\n                    raw_output=raw_output,\n                )\n        return default_output\n\n    def to_langchain_tool(self, **langchain_tool_kwargs: Any) -> \"Tool\":\n        \"\"\"To langchain tool.\"\"\"\n        from llama_index.core.bridge.langchain import Tool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return Tool.from_function(\n            func=self.fn,\n            coroutine=self.async_fn,\n            **langchain_tool_kwargs,\n        )\n\n    def to_langchain_structured_tool(\n        self, **langchain_tool_kwargs: Any\n    ) -> \"StructuredTool\":\n        \"\"\"To langchain structured tool.\"\"\"\n        from llama_index.core.bridge.langchain import StructuredTool\n\n        langchain_tool_kwargs = self._process_langchain_tool_kwargs(\n            langchain_tool_kwargs\n        )\n        return StructuredTool.from_function(\n            func=self.fn,\n            coroutine=self.async_fn,\n            **langchain_tool_kwargs,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/tool_spec/base.py",
    "filename": "base.py",
    "relpath": "tools/tool_spec/base.py",
    "start_line": 1,
    "end_line": 121,
    "length": 121,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_fn_schema_from_fn_name",
      "get_metadata_from_fn_name",
      "to_tool_list",
      "patch_sync",
      "patched_sync"
    ],
    "chunk_class_names": [
      "BaseToolSpec"
    ],
    "document_function_names": [
      "get_fn_schema_from_fn_name",
      "get_metadata_from_fn_name",
      "to_tool_list",
      "patch_sync",
      "patched_sync"
    ],
    "document_class_names": [
      "BaseToolSpec"
    ],
    "content": "\"\"\"Base tool spec class.\"\"\"\n\n\nimport asyncio\nfrom inspect import signature\nfrom typing import Any, Awaitable, Callable, Dict, List, Optional, Tuple, Type, Union\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.tools.function_tool import FunctionTool\nfrom llama_index.core.tools.types import ToolMetadata\nfrom llama_index.core.tools.utils import create_schema_from_function\n\nAsyncCallable = Callable[..., Awaitable[Any]]\n\n\n# TODO: deprecate the Tuple (there's no use for it)\nSPEC_FUNCTION_TYPE = Union[str, Tuple[str, str]]\n\n\nclass BaseToolSpec:\n    \"\"\"Base tool spec class.\"\"\"\n\n    # list of functions that you'd want to convert to spec\n    spec_functions: List[SPEC_FUNCTION_TYPE]\n\n    def get_fn_schema_from_fn_name(\n        self, fn_name: str, spec_functions: Optional[List[SPEC_FUNCTION_TYPE]] = None\n    ) -> Optional[Type[BaseModel]]:\n        \"\"\"Return map from function name.\n\n        Return type is Optional, meaning that the schema can be None.\n        In this case, it's up to the downstream tool implementation to infer the schema.\n\n        \"\"\"\n        spec_functions = spec_functions or self.spec_functions\n        for fn in spec_functions:\n            if fn == fn_name:\n                return create_schema_from_function(fn_name, getattr(self, fn_name))\n\n        raise ValueError(f\"Invalid function name: {fn_name}\")\n\n    def get_metadata_from_fn_name(\n        self, fn_name: str, spec_functions: Optional[List[SPEC_FUNCTION_TYPE]] = None\n    ) -> Optional[ToolMetadata]:\n        \"\"\"Return map from function name.\n\n        Return type is Optional, meaning that the schema can be None.\n        In this case, it's up to the downstream tool implementation to infer the schema.\n\n        \"\"\"\n        try:\n            func = getattr(self, fn_name)\n        except AttributeError:\n            return None\n        name = fn_name\n        docstring = func.__doc__ or \"\"\n        description = f\"{name}{signature(func)}\\n{docstring}\"\n        fn_schema = self.get_fn_schema_from_fn_name(\n            fn_name, spec_functions=spec_functions\n        )\n        return ToolMetadata(name=name, description=description, fn_schema=fn_schema)\n\n    def to_tool_list(\n        self,\n        spec_functions: Optional[List[SPEC_FUNCTION_TYPE]] = None,\n        func_to_metadata_mapping: Optional[Dict[str, ToolMetadata]] = None,\n    ) -> List[FunctionTool]:\n        \"\"\"Convert tool spec to list of tools.\"\"\"\n        spec_functions = spec_functions or self.spec_functions\n        func_to_metadata_mapping = func_to_metadata_mapping or {}\n        tool_list = []\n        for func_spec in spec_functions:\n            func_sync = None\n            func_async = None\n            if isinstance(func_spec, str):\n                func = getattr(self, func_spec)\n                if asyncio.iscoroutinefunction(func):\n                    func_async = func\n                else:\n                    func_sync = func\n                metadata = func_to_metadata_mapping.get(func_spec, None)\n                if metadata is None:\n                    metadata = self.get_metadata_from_fn_name(func_spec)\n            elif isinstance(func_spec, tuple) and len(func_spec) == 2:\n                func_sync = getattr(self, func_spec[0])\n                func_async = getattr(self, func_spec[1])\n                metadata = func_to_metadata_mapping.get(func_spec[0], None)\n                if metadata is None:\n                    metadata = func_to_metadata_mapping.get(func_spec[1], None)\n                    if metadata is None:\n                        metadata = self.get_metadata_from_fn_name(func_spec[0])\n            else:\n                raise ValueError(\n                    \"spec_functions must be of type: List[Union[str, Tuple[str, str]]]\"\n                )\n\n            if func_sync is None:\n                if func_async is not None:\n                    func_sync = patch_sync(func_async)\n                else:\n                    raise ValueError(\n                        f\"Could not retrieve a function for spec: {func_spec}\"\n                    )\n\n            tool = FunctionTool.from_defaults(\n                fn=func_sync,\n                async_fn=func_async,\n                tool_metadata=metadata,\n            )\n            tool_list.append(tool)\n        return tool_list\n\n\ndef patch_sync(func_async: AsyncCallable) -> Callable:\n    \"\"\"Patch sync function from async function.\"\"\"\n\n    def patched_sync(*args: Any, **kwargs: Any) -> Any:\n        loop = asyncio.get_event_loop()\n        return loop.run_until_complete(func_async(*args, **kwargs))\n\n    return patched_sync"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/tool_spec/__init__.py",
    "filename": "__init__.py",
    "relpath": "tools/tool_spec/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/tool_spec/load_and_search/base.py",
    "filename": "base.py",
    "relpath": "tools/tool_spec/load_and_search/base.py",
    "start_line": 1,
    "end_line": 159,
    "length": 159,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "metadata",
      "from_defaults",
      "to_tool_list",
      "load",
      "read"
    ],
    "chunk_class_names": [
      "LoadAndSearchToolSpec"
    ],
    "document_function_names": [
      "__init__",
      "metadata",
      "from_defaults",
      "to_tool_list",
      "load",
      "read"
    ],
    "document_class_names": [
      "LoadAndSearchToolSpec"
    ],
    "content": "\"\"\"Ad-hoc data loader tool.\n\nTool that wraps any data loader, and is able to load data on-demand.\n\n\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Type\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom llama_index.core.schema import Document\nfrom llama_index.core.tools.function_tool import FunctionTool\nfrom llama_index.core.tools.tool_spec.base import SPEC_FUNCTION_TYPE, BaseToolSpec\nfrom llama_index.core.tools.types import ToolMetadata\nfrom llama_index.core.tools.utils import create_schema_from_function\n\n\nclass LoadAndSearchToolSpec(BaseToolSpec):\n    \"\"\"Load and Search Tool.\n\n    This tool can be used with other tools that load large amounts of\n    information. Compared to OndemandLoaderTool this returns two tools,\n    one to retrieve data to an index and another to allow the Agent to search\n    the retrieved data with a natural language query string.\n\n    \"\"\"\n\n    loader_prompt = \"\"\"\n        Use this tool to load data from the following function. It must then be read from\n        the corresponding read_{} function.\n\n        {}\n    \"\"\"\n\n    # TODO, more general read prompt, not always natural language?\n    reader_prompt = \"\"\"\n        Once data has been loaded from {} it can then be read using a natural\n        language query from this function.\n\n        You are required to pass the natural language query argument when calling this endpoint\n\n        Args:\n            query (str): The natural language query used to retreieve information from the index\n    \"\"\"\n\n    def __init__(\n        self,\n        tool: FunctionTool,\n        index_cls: Type[BaseIndex],\n        index_kwargs: Dict,\n        metadata: ToolMetadata,\n        index: Optional[BaseIndex] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._index_cls = index_cls\n        self._index_kwargs = index_kwargs\n        self._index = index\n        self._metadata = metadata\n        self._tool = tool\n\n        if self._metadata.name is None:\n            raise ValueError(\"Tool name cannot be None\")\n        self.spec_functions = [\n            self._metadata.name,\n            f\"read_{self._metadata.name}\",\n        ]\n        self._tool_list = [\n            FunctionTool.from_defaults(\n                fn=self.load,\n                name=self._metadata.name,\n                description=self.loader_prompt.format(\n                    self._metadata.name, self._metadata.description\n                ),\n                fn_schema=self._metadata.fn_schema,\n            ),\n            FunctionTool.from_defaults(\n                fn=self.read,\n                name=str(f\"read_{self._metadata.name}\"),\n                description=self.reader_prompt.format(metadata.name),\n                fn_schema=create_schema_from_function(\"ReadData\", self.read),\n            ),\n        ]\n\n    @property\n    def metadata(self) -> ToolMetadata:\n        return self._metadata\n\n    @classmethod\n    def from_defaults(\n        cls,\n        tool: FunctionTool,\n        index_cls: Optional[Type[BaseIndex]] = None,\n        index_kwargs: Optional[Dict] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        fn_schema: Optional[Type[BaseModel]] = None,\n    ) -> \"LoadAndSearchToolSpec\":\n        \"\"\"From defaults.\"\"\"\n        index_cls = index_cls or VectorStoreIndex\n        index_kwargs = index_kwargs or {}\n        if name is None:\n            name = tool.metadata.name\n        if description is None:\n            description = tool.metadata.description\n        if fn_schema is None:\n            fn_schema = tool.metadata.fn_schema\n        metadata = ToolMetadata(name=name, description=description, fn_schema=fn_schema)\n        return cls(\n            tool=tool,\n            index_cls=index_cls,\n            index_kwargs=index_kwargs,\n            metadata=metadata,\n        )\n\n    def to_tool_list(\n        self,\n        spec_functions: Optional[List[SPEC_FUNCTION_TYPE]] = None,\n        func_to_metadata_mapping: Optional[Dict[str, ToolMetadata]] = None,\n    ) -> List[FunctionTool]:\n        return self._tool_list\n\n    def load(self, *args: Any, **kwargs: Any) -> Any:\n        # Call the wrapped tool and save the result in the index\n        docs = self._tool(*args, **kwargs).raw_output\n\n        # convert to Document if necessary\n        if isinstance(docs, list):\n            for i, doc in enumerate(docs):\n                if not isinstance(doc, Document):\n                    docs[i] = Document(text=str(doc))\n        elif isinstance(docs, str):\n            docs = [Document(text=docs)]\n        elif isinstance(docs, Document):\n            docs = [docs]\n        else:\n            docs = [Document(text=str(docs))]\n\n        if self._index:\n            for doc in docs:\n                self._index.insert(doc, **self._index_kwargs)\n        else:\n            self._index = self._index_cls.from_documents(docs, **self._index_kwargs)\n        return (\n            \"Content loaded! You can now search the information using read_{}\".format(\n                self._metadata.name\n            )\n        )\n\n    def read(self, query: str) -> Any:\n        # Query the index for the result\n        if not self._index:\n            return (\n                \"Error: No content has been loaded into the index. \"\n                f\"You must call {self._metadata.name} first\"\n            )\n        query_engine = self._index.as_query_engine()\n        response = query_engine.query(query)\n        return str(response)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/tools/tool_spec/load_and_search/__init__.py",
    "filename": "__init__.py",
    "relpath": "tools/tool_spec/load_and_search/__init__.py",
    "start_line": 1,
    "end_line": 5,
    "length": 5,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.tools.tool_spec.load_and_search.base import (\n    LoadAndSearchToolSpec,\n)\n\n__all__ = [\"LoadAndSearchToolSpec\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/base.py",
    "filename": "base.py",
    "relpath": "llama_dataset/base.py",
    "start_line": 1,
    "end_line": 344,
    "length": 344,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__str__",
      "__str__",
      "class_name",
      "class_name",
      "__getitem__",
      "to_pandas",
      "save_json",
      "from_json",
      "class_name",
      "__getitem__",
      "to_pandas",
      "save_json",
      "from_json",
      "_construct_prediction_dataset",
      "_predict_example",
      "make_predictions_with",
      "_apredict_example",
      "_batch_examples",
      "amake_predictions_with",
      "class_name"
    ],
    "chunk_class_names": [
      "CreatedByType",
      "CreatedBy",
      "BaseLlamaExamplePrediction",
      "BaseLlamaDataExample",
      "BaseLlamaPredictionDataset",
      "BaseLlamaDataset",
      "caches"
    ],
    "document_function_names": [
      "__str__",
      "__str__",
      "class_name",
      "class_name",
      "__getitem__",
      "to_pandas",
      "save_json",
      "from_json",
      "class_name",
      "__getitem__",
      "to_pandas",
      "save_json",
      "from_json",
      "_construct_prediction_dataset",
      "_predict_example",
      "make_predictions_with",
      "_apredict_example",
      "_batch_examples",
      "amake_predictions_with",
      "class_name"
    ],
    "document_class_names": [
      "CreatedByType",
      "CreatedBy",
      "BaseLlamaExamplePrediction",
      "BaseLlamaDataExample",
      "BaseLlamaPredictionDataset",
      "BaseLlamaDataset",
      "caches"
    ],
    "content": "\"\"\"Llama Dataset Class.\"\"\"\n\nimport json\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import (\n    Any,\n    ClassVar,\n    Generator,\n    Generic,\n    List,\n    Sequence,\n    Optional,\n    Type,\n    TypeVar,\n    Union,\n)\n\nimport tqdm\nfrom llama_index.core.async_utils import asyncio_module\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, PrivateAttr, ConfigDict\nfrom llama_index.core.evaluation import BaseEvaluator\n\n\nPredictorType = Union[BaseQueryEngine, BaseEvaluator, LLM]\nP = TypeVar(\"P\", bound=PredictorType)\n\n\nclass CreatedByType(str, Enum):\n    \"\"\"The kinds of rag data examples.\"\"\"\n\n    HUMAN = \"human\"\n    AI = \"ai\"\n\n    def __str__(self) -> str:\n        return self.value\n\n\nclass CreatedBy(BaseModel):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_name: Optional[str] = Field(\n        default_factory=str, description=\"When CreatedByType.AI, specify model name.\"\n    )\n    type: CreatedByType\n\n    def __str__(self) -> str:\n        if self.type == \"ai\":\n            return f\"{self.type!s} ({self.model_name})\"\n        else:\n            return str(self.type)\n\n\nclass BaseLlamaExamplePrediction(BaseModel):\n    \"\"\"Base llama dataset example class.\"\"\"\n\n    @property\n    @abstractmethod\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseLlamaPrediction\"\n\n\nclass BaseLlamaDataExample(BaseModel):\n    \"\"\"Base llama dataset example class.\"\"\"\n\n    @property\n    @abstractmethod\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseLlamaDataExample\"\n\n\nclass BaseLlamaPredictionDataset(BaseModel):\n    _prediction_type: ClassVar[Type[BaseLlamaExamplePrediction]]\n    predictions: List[BaseLlamaExamplePrediction] = Field(\n        default_factory=list, description=\"Predictions on train_examples.\"\n    )\n\n    def __getitem__(\n        self, val: Union[slice, int]\n    ) -> Union[Sequence[BaseLlamaExamplePrediction], BaseLlamaExamplePrediction]:\n        \"\"\"Enable slicing and indexing.\n\n        Returns the desired slice on `predictions`.\n        \"\"\"\n        return self.predictions[val]\n\n    @abstractmethod\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n\n    def save_json(self, path: str) -> None:\n        \"\"\"Save json.\"\"\"\n        with open(path, \"w\") as f:\n            predictions = None\n            if self.predictions:\n                predictions = [\n                    self._prediction_type.model_dump(el) for el in self.predictions\n                ]\n            data = {\n                \"predictions\": predictions,\n            }\n\n            json.dump(data, f, indent=4)\n\n    @classmethod\n    def from_json(cls, path: str) -> \"BaseLlamaPredictionDataset\":\n        \"\"\"Load json.\"\"\"\n        with open(path) as f:\n            data = json.load(f)\n\n        predictions = [\n            cls._prediction_type.model_validate(el) for el in data[\"predictions\"]\n        ]\n\n        return cls(\n            predictions=predictions,\n        )\n\n    @property\n    @abstractmethod\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseLlamaPredictionDataset\"\n\n\nclass BaseLlamaDataset(BaseModel, Generic[P]):\n    _example_type: ClassVar[Type[BaseLlamaDataExample]]\n    examples: List[BaseLlamaDataExample] = Field(\n        default=[], description=\"Data examples of this dataset.\"\n    )\n    _predictions_cache: List[BaseLlamaExamplePrediction] = PrivateAttr(\n        default_factory=list\n    )\n\n    def __getitem__(\n        self, val: Union[slice, int]\n    ) -> Union[Sequence[BaseLlamaDataExample], BaseLlamaDataExample]:\n        \"\"\"Enable slicing and indexing.\n\n        Returns the desired slice on `examples`.\n        \"\"\"\n        return self.examples[val]\n\n    @abstractmethod\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n\n    def save_json(self, path: str) -> None:\n        \"\"\"Save json.\"\"\"\n        with open(path, \"w\") as f:\n            examples = [self._example_type.model_dump(el) for el in self.examples]\n            data = {\n                \"examples\": examples,\n            }\n\n            json.dump(data, f, indent=4)\n\n    @classmethod\n    def from_json(cls, path: str) -> \"BaseLlamaDataset\":\n        \"\"\"Load json.\"\"\"\n        with open(path) as f:\n            data = json.load(f)\n\n        examples = [cls._example_type.model_validate(el) for el in data[\"examples\"]]\n\n        return cls(\n            examples=examples,\n        )\n\n    @abstractmethod\n    def _construct_prediction_dataset(\n        self, predictions: Sequence[BaseLlamaExamplePrediction]\n    ) -> BaseLlamaPredictionDataset:\n        \"\"\"Construct the specific prediction dataset.\n\n        Args:\n            predictions (List[BaseLlamaExamplePrediction]): the list of predictions.\n\n        Returns:\n            BaseLlamaPredictionDataset: A dataset of predictions.\n        \"\"\"\n\n    @abstractmethod\n    def _predict_example(\n        self,\n        predictor: P,\n        example: BaseLlamaDataExample,\n        sleep_time_in_seconds: int = 0,\n    ) -> BaseLlamaExamplePrediction:\n        \"\"\"Predict on a single example.\n\n        NOTE: Subclasses need to implement this.\n\n        Args:\n            predictor (PredictorType): The predictor to make the prediciton with.\n            example (BaseLlamaDataExample): The example to predict on.\n\n        Returns:\n            BaseLlamaExamplePrediction: The prediction.\n        \"\"\"\n\n    def make_predictions_with(\n        self,\n        predictor: P,\n        show_progress: bool = False,\n        batch_size: int = 20,\n        sleep_time_in_seconds: int = 0,\n    ) -> BaseLlamaPredictionDataset:\n        \"\"\"Predict with a given query engine.\n\n        Args:\n            predictor (PredictorType): The predictor to make predictions with.\n            show_progress (bool, optional): Show progress of making predictions.\n            batch_size (int): Used to batch async calls, especially to reduce chances\n                            of hitting RateLimitError from openai.\n            sleep_time_in_seconds (int): Amount of time to sleep between batch call\n                            to reduce chance of hitting RateLimitError from openai.\n\n        Returns:\n            BaseLlamaPredictionDataset: A dataset of predictions.\n        \"\"\"\n        if self._predictions_cache:\n            start_example_position = len(self._predictions_cache)\n        else:\n            start_example_position = 0\n\n        for batch in self._batch_examples(\n            batch_size=batch_size, start_position=start_example_position\n        ):\n            if show_progress:\n                example_iterator = tqdm.tqdm(batch)\n            else:\n                example_iterator = batch\n            for example in example_iterator:\n                self._predictions_cache.append(\n                    self._predict_example(predictor, example, sleep_time_in_seconds)\n                )\n\n        return self._construct_prediction_dataset(predictions=self._predictions_cache)\n\n    # async methods\n    @abstractmethod\n    async def _apredict_example(\n        self,\n        predictor: P,\n        example: BaseLlamaDataExample,\n        sleep_time_in_seconds: int,\n    ) -> BaseLlamaExamplePrediction:\n        \"\"\"Async predict on a single example.\n\n        NOTE: Subclasses need to implement this.\n\n        Args:\n            predictor (PredictorType): The predictor to make the prediciton with.\n            example (BaseLlamaDataExample): The example to predict on.\n\n        Returns:\n            BaseLlamaExamplePrediction: The prediction.\n        \"\"\"\n\n    def _batch_examples(\n        self,\n        batch_size: int = 20,\n        start_position: int = 0,\n    ) -> Generator[Sequence[BaseLlamaDataExample], None, None]:\n        \"\"\"Batches examples and predictions with a given batch_size.\"\"\"\n        num_examples = len(self.examples)\n        for ndx in range(start_position, num_examples, batch_size):\n            yield self.examples[ndx : min(ndx + batch_size, num_examples)]\n\n    async def amake_predictions_with(\n        self,\n        predictor: P,\n        show_progress: bool = False,\n        batch_size: int = 20,\n        sleep_time_in_seconds: int = 1,\n    ) -> BaseLlamaPredictionDataset:\n        \"\"\"Async predict with a given query engine.\n\n        Args:\n            predictor (PredictorType): The predictor to make predictions with.\n            show_progress (bool, optional): Show progress of making predictions.\n            batch_size (int): Used to batch async calls, especially to reduce chances\n                            of hitting RateLimitError from openai.\n            sleep_time_in_seconds (int): Amount of time to sleep between batch call\n                            to reduce chance of hitting RateLimitError from openai.\n\n        Returns:\n            BaseLlamaPredictionDataset: A dataset of predictions.\n        \"\"\"\n        if self._predictions_cache:\n            start_example_position = len(self._predictions_cache)\n        else:\n            start_example_position = 0\n\n        for batch in self._batch_examples(\n            batch_size=batch_size, start_position=start_example_position\n        ):\n            tasks = []\n            for example in batch:\n                tasks.append(\n                    self._apredict_example(predictor, example, sleep_time_in_seconds)\n                )\n            asyncio_mod = asyncio_module(show_progress=show_progress)\n\n            try:\n                if show_progress:\n                    batch_predictions = await asyncio_mod.gather(\n                        *tasks, desc=\"Batch processing of predictions\"\n                    )\n                else:\n                    batch_predictions = await asyncio_mod.gather(*tasks)\n            except Exception as err:\n                if show_progress:\n                    asyncio_mod.close()\n\n                if \"RateLimitError\" in str(err):\n                    raise ValueError(\n                        \"You've hit rate limits on your OpenAI subscription. This\"\n                        \" class caches previous predictions after each successful\"\n                        \" batch execution. Based off this cache, when executing this\"\n                        \" command again it will attempt to predict on only the examples \"\n                        \"that have not yet been predicted. Try reducing your batch_size.\"\n                    ) from err\n                else:\n                    raise err  # noqa: TRY201\n\n            self._predictions_cache += batch_predictions\n            # time.sleep(sleep_time_in_seconds)\n\n        prediction_dataset = self._construct_prediction_dataset(\n            predictions=self._predictions_cache\n        )\n        self._predictions_cache = []  # clear cache\n        return prediction_dataset\n\n    @property\n    @abstractmethod\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseLlamaDataset\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/generator.py",
    "filename": "generator.py",
    "relpath": "llama_dataset/generator.py",
    "start_line": 1,
    "end_line": 261,
    "length": 261,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_documents",
      "_agenerate_dataset",
      "agenerate_questions_from_nodes",
      "agenerate_dataset_from_nodes",
      "generate_questions_from_nodes",
      "generate_dataset_from_nodes",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts"
    ],
    "chunk_class_names": [
      "RagDatasetGenerator"
    ],
    "document_function_names": [
      "__init__",
      "from_documents",
      "_agenerate_dataset",
      "agenerate_questions_from_nodes",
      "agenerate_dataset_from_nodes",
      "generate_questions_from_nodes",
      "generate_dataset_from_nodes",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts"
    ],
    "document_class_names": [
      "RagDatasetGenerator"
    ],
    "content": "\"\"\"Dataset generation from documents.\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nimport warnings\nfrom typing import List, Sequence, Optional\n\nfrom llama_index.core import Document, SummaryIndex\nfrom llama_index.core.async_utils import DEFAULT_NUM_WORKERS, run_jobs, asyncio_run\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.ingestion import run_transformations\nfrom llama_index.core.llama_dataset import (\n    CreatedBy,\n    CreatedByType,\n    LabelledRagDataExample,\n    LabelledRagDataset,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.postprocessor.node import KeywordNodePostprocessor\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom llama_index.core.prompts.mixin import (\n    PromptDictType,\n    PromptMixin,\n    PromptMixinType,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    MetadataMode,\n    NodeWithScore,\n    TransformComponent,\n)\nfrom llama_index.core.settings import Settings\n\n\nDEFAULT_QUESTION_GENERATION_PROMPT = \"\"\"\\\nContext information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n{query_str}\n\"\"\"\n\n\nclass RagDatasetGenerator(PromptMixin):\n    \"\"\"Generate dataset (question/ question-answer pairs) \\\n    based on the given documents.\n\n    NOTE: this is a beta feature, subject to change!\n\n    Args:\n        nodes (List[Node]): List of nodes. (Optional)\n        num_questions_per_chunk: number of question to be \\\n        generated per chunk. Each document is chunked of size 512 words.\n        text_question_template: Question generation template.\n        question_gen_query: Question generation query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: List[BaseNode],\n        llm: Optional[LLM] = None,\n        num_questions_per_chunk: int = 3,\n        text_question_template: Optional[BasePromptTemplate] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        question_gen_query: Optional[str] = None,\n        metadata_mode: MetadataMode = MetadataMode.NONE,\n        show_progress: bool = False,\n        workers: int = DEFAULT_NUM_WORKERS,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n        self.num_questions_per_chunk = num_questions_per_chunk\n        self.text_question_template = text_question_template or PromptTemplate(\n            DEFAULT_QUESTION_GENERATION_PROMPT\n        )\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.question_gen_query = (\n            question_gen_query\n            or f\"You are a Teacher/Professor. Your task is to setup {num_questions_per_chunk} questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\n        )\n        self.nodes = nodes\n        self._metadata_mode = metadata_mode\n        self._show_progress = show_progress\n        self._workers = workers\n\n    @classmethod\n    def from_documents(\n        cls,\n        documents: Sequence[Document],\n        llm: Optional[LLM] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        num_questions_per_chunk: int = 3,\n        text_question_template: Optional[BasePromptTemplate] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        question_gen_query: Optional[str] = None,\n        required_keywords: Optional[List[str]] = None,\n        exclude_keywords: Optional[List[str]] = None,\n        show_progress: bool = False,\n        workers: int = DEFAULT_NUM_WORKERS,\n    ) -> RagDatasetGenerator:\n        \"\"\"Generate dataset from documents.\"\"\"\n        llm = llm or Settings.llm\n        transformations = transformations or Settings.transformations\n\n        nodes = run_transformations(\n            documents, transformations, show_progress=show_progress\n        )\n\n        # use node postprocessor to filter nodes\n        required_keywords = required_keywords or []\n        exclude_keywords = exclude_keywords or []\n        node_postprocessor = KeywordNodePostprocessor(\n            required_keywords=required_keywords,\n            exclude_keywords=exclude_keywords,\n        )\n        node_with_scores = [NodeWithScore(node=node) for node in nodes]\n        node_with_scores = node_postprocessor.postprocess_nodes(node_with_scores)\n        nodes = [node_with_score.node for node_with_score in node_with_scores]\n\n        return cls(\n            nodes=nodes,\n            llm=llm,\n            num_questions_per_chunk=num_questions_per_chunk,\n            text_question_template=text_question_template,\n            text_qa_template=text_qa_template,\n            question_gen_query=question_gen_query,\n            show_progress=show_progress,\n            workers=workers,\n        )\n\n    async def _agenerate_dataset(\n        self,\n        nodes: List[BaseNode],\n        labelled: bool = False,\n    ) -> LabelledRagDataset:\n        \"\"\"Node question generator.\"\"\"\n        query_tasks = []\n        examples: List[LabelledRagDataExample] = []\n        summary_indices: List[SummaryIndex] = []\n        for node in nodes:\n            index = SummaryIndex.from_documents(\n                [\n                    Document(\n                        text=node.get_content(metadata_mode=self._metadata_mode),\n                        metadata=node.metadata,  # type: ignore\n                        excluded_llm_metadata_keys=node.excluded_llm_metadata_keys,\n                        excluded_embed_metadata_keys=node.excluded_embed_metadata_keys,\n                        relationships=node.relationships,\n                    )\n                ],\n            )\n\n            query_engine = index.as_query_engine(\n                llm=self._llm,\n                text_qa_template=self.text_question_template,\n                use_async=True,\n            )\n            task = query_engine.aquery(\n                self.question_gen_query,\n            )\n            query_tasks.append(task)\n            summary_indices.append(index)\n\n        responses = await run_jobs(query_tasks, self._show_progress, self._workers)\n        for idx, response in enumerate(responses):\n            result = str(response).strip().split(\"\\n\")\n            cleaned_questions = [\n                re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n            ]\n            cleaned_questions = [\n                question for question in cleaned_questions if len(question) > 0\n            ][: self.num_questions_per_chunk]\n\n            num_questions_generated = len(cleaned_questions)\n            if num_questions_generated < self.num_questions_per_chunk:\n                warnings.warn(\n                    f\"Fewer questions generated ({num_questions_generated}) \"\n                    f\"than requested ({self.num_questions_per_chunk}).\"\n                )\n\n            index = summary_indices[idx]\n            reference_context = nodes[idx].get_content(metadata_mode=MetadataMode.NONE)\n            model_name = self._llm.metadata.model_name\n            created_by = CreatedBy(type=CreatedByType.AI, model_name=model_name)\n            if labelled:\n                index = summary_indices[idx]\n                qr_tasks = []\n                for query in cleaned_questions:\n                    # build summary index off of node (i.e. context)\n                    qa_query_engine = index.as_query_engine(\n                        llm=self._llm,\n                        text_qa_template=self.text_qa_template,\n                    )\n                    qr_task = qa_query_engine.aquery(query)\n                    qr_tasks.append(qr_task)\n                answer_responses: List[RESPONSE_TYPE] = await run_jobs(\n                    qr_tasks, self._show_progress, self._workers\n                )\n                for question, answer_response in zip(\n                    cleaned_questions, answer_responses\n                ):\n                    example = LabelledRagDataExample(\n                        query=question,\n                        reference_answer=str(answer_response),\n                        reference_contexts=[reference_context],\n                        reference_answer_by=created_by,\n                        query_by=created_by,\n                    )\n                    examples.append(example)\n            else:\n                for query in cleaned_questions:\n                    example = LabelledRagDataExample(\n                        query=query,\n                        reference_answer=\"\",\n                        reference_contexts=[reference_context],\n                        reference_answer_by=None,\n                        query_by=created_by,\n                    )\n                    examples.append(example)\n\n        # split train/test\n        return LabelledRagDataset(examples=examples)\n\n    async def agenerate_questions_from_nodes(self) -> LabelledRagDataset:\n        \"\"\"Generates questions but not the reference answers.\"\"\"\n        return await self._agenerate_dataset(self.nodes, labelled=False)\n\n    async def agenerate_dataset_from_nodes(self) -> LabelledRagDataset:\n        \"\"\"Generates questions for each document.\"\"\"\n        return await self._agenerate_dataset(self.nodes, labelled=True)\n\n    def generate_questions_from_nodes(self) -> LabelledRagDataset:\n        \"\"\"Generates questions but not the reference answers.\"\"\"\n        return asyncio_run(self.agenerate_questions_from_nodes())\n\n    def generate_dataset_from_nodes(self) -> LabelledRagDataset:\n        \"\"\"Generates questions for each document.\"\"\"\n        return asyncio_run(self.agenerate_dataset_from_nodes())\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"text_question_template\": self.text_question_template,\n            \"text_qa_template\": self.text_qa_template,\n        }\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"text_question_template\" in prompts:\n            self.text_question_template = prompts[\"text_question_template\"]\n        if \"text_qa_template\" in prompts:\n            self.text_qa_template = prompts[\"text_qa_template\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/evaluator_evaluation.py",
    "filename": "evaluator_evaluation.py",
    "relpath": "llama_dataset/evaluator_evaluation.py",
    "start_line": 1,
    "end_line": 351,
    "length": 351,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "EvaluatorExamplePrediction",
      "name",
      "LabelledEvaluatorDataExample",
      "contains",
      "name",
      "EvaluatorPredictionDataset",
      "LabelledEvaluatorDataset",
      "PairwiseEvaluatorExamplePrediction",
      "name",
      "PairwiseEvaluatorPredictionDataset",
      "LabelledPairwiseEvaluatorDataExample",
      "name"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "class_name",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name"
    ],
    "document_class_names": [
      "EvaluatorExamplePrediction",
      "name",
      "LabelledEvaluatorDataExample",
      "contains",
      "name",
      "EvaluatorPredictionDataset",
      "LabelledEvaluatorDataset",
      "PairwiseEvaluatorExamplePrediction",
      "name",
      "PairwiseEvaluatorPredictionDataset",
      "LabelledPairwiseEvaluatorDataExample",
      "name",
      "LabelledPairwiseEvaluatorDataset"
    ],
    "content": "\"\"\"Labelled Evaluation Class.\"\"\"\n\nimport asyncio\nimport time\nfrom typing import Any, Dict, List, Sequence, Optional\n\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.evaluation import (\n    BaseEvaluator,\n    EvaluationResult,\n)\nfrom llama_index.core.evaluation.pairwise import EvaluationSource\nfrom llama_index.core.llama_dataset.base import (\n    BaseLlamaDataExample,\n    BaseLlamaDataset,\n    BaseLlamaExamplePrediction,\n    BaseLlamaPredictionDataset,\n    CreatedBy,\n)\n\n\nclass EvaluatorExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"Evaluation example prediction class.\n\n    Args:\n        feedback (Optional[str]): The evaluator's feedback.\n        score (Optional[float]): The evaluator's score.\n    \"\"\"\n\n    feedback: str = Field(\n        default_factory=str,\n        description=\"The generated (predicted) response that can be compared to a reference (ground-truth) answer.\",\n    )\n    score: Optional[float] = Field(\n        default=None,\n        description=\"The generated (predicted) response that can be compared to a reference (ground-truth) answer.\",\n    )\n    invalid_prediction: bool = Field(\n        default=False, description=\"Whether or not the prediction is a valid one.\"\n    )\n    invalid_reason: Optional[str] = Field(\n        default=None, description=\"Reason as to why prediction is invalid.\"\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"EvaluatorExamplePrediction\"\n\n\nclass LabelledEvaluatorDataExample(BaseLlamaDataExample):\n    \"\"\"Evaluation example class.\n\n    This data class contains the ingredients to perform a new \"prediction\" i.e.,\n    evaluation. Here an evaluator is meant to evaluate a response against an\n    associated query as well as optionally contexts.\n\n    Args:\n        query (str): The user query\n        query_by (CreatedBy): Query generated by human or ai (model-name)\n        contexts (Optional[List[str]]): The contexts used for response\n        answer (str): Answer to the query that is to be evaluated.\n        answer_by: The reference answer generated by human or ai (model-name).\n        ground_truth_answer (Optional[str]):\n        ground_truth_answer_by (Optional[CreatedBy]):\n        reference_feedback (str): The reference feedback evaluation.\n        reference_score (float): The reference score evaluation.\n        reference_evaluation_by (CreatedBy): Evaluation generated by human or ai (model-name)\n    \"\"\"\n\n    query: str = Field(\n        default_factory=str, description=\"The user query for the example.\"\n    )\n    query_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the query.\"\n    )\n    contexts: Optional[List[str]] = Field(\n        default=None,\n        description=\"The contexts used to generate the answer.\",\n    )\n    answer: str = Field(\n        default_factory=str,\n        description=\"The provided answer to the example that is to be evaluated.\",\n    )\n    answer_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the answer.\"\n    )\n    ground_truth_answer: Optional[str] = Field(\n        default=None,\n        description=\"The ground truth answer to the example that is used to evaluate the provided `answer`.\",\n    )\n    ground_truth_answer_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the ground-truth answer.\"\n    )\n    reference_feedback: Optional[str] = Field(\n        default=None,\n        description=\"The reference feedback (ground-truth).\",\n    )\n    reference_score: float = Field(\n        default_factory=float, description=\"The reference score (ground-truth).\"\n    )\n    reference_evaluation_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the evaluation (feedback and score).\"\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"LabelledEvaluatorDataExample\"\n\n\nclass EvaluatorPredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"Evaluation Prediction Dataset Class.\"\"\"\n\n    _prediction_type = EvaluatorExamplePrediction\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List] = {\n            \"feedback\": [],\n            \"score\": [],\n        }\n        for pred in self.predictions:\n            if not isinstance(pred, EvaluatorExamplePrediction):\n                raise ValueError(\n                    \"EvaluatorPredictionDataset can only contain EvaluatorExamplePrediction instances.\"\n                )\n            data[\"feedback\"].append(pred.feedback)\n            data[\"score\"].append(pred.score)\n\n        return pd.DataFrame(data)\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"EvaluatorPredictionDataset\"\n\n\nclass LabelledEvaluatorDataset(BaseLlamaDataset[BaseEvaluator]):\n    \"\"\"LabelledEvalationDataset class.\"\"\"\n\n    _example_type = LabelledEvaluatorDataExample\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List] = {\n            \"query\": [],\n            \"answer\": [],\n            \"contexts\": [],\n            \"ground_truth_answer\": [],\n            \"query_by\": [],\n            \"answer_by\": [],\n            \"ground_truth_answer_by\": [],\n            \"reference_feedback\": [],\n            \"reference_score\": [],\n            \"reference_evaluation_by\": [],\n        }\n\n        for example in self.examples:\n            if not isinstance(example, LabelledEvaluatorDataExample):\n                raise ValueError(\n                    \"LabelledEvaluatorDataset can only contain LabelledEvaluatorDataExample instances.\"\n                )\n            data[\"query\"].append(example.query)\n            data[\"answer\"].append(example.answer)\n            data[\"contexts\"].append(example.contexts)\n            data[\"ground_truth_answer\"].append(example.ground_truth_answer)\n            data[\"query_by\"].append(str(example.query_by))\n            data[\"answer_by\"].append(str(example.answer_by))\n            data[\"ground_truth_answer_by\"].append(str(example.ground_truth_answer_by))\n            data[\"reference_feedback\"].append(example.reference_feedback)\n            data[\"reference_score\"].append(example.reference_score)\n            data[\"reference_evaluation_by\"].append(str(example.reference_evaluation_by))\n\n        return pd.DataFrame(data)\n\n    async def _apredict_example(  # type: ignore\n        self,\n        predictor: BaseEvaluator,\n        example: LabelledEvaluatorDataExample,\n        sleep_time_in_seconds: int,\n    ) -> EvaluatorExamplePrediction:\n        \"\"\"Async predict RAG example with a query engine.\"\"\"\n        await asyncio.sleep(sleep_time_in_seconds)\n        try:\n            eval_result: EvaluationResult = await predictor.aevaluate(\n                query=example.query,\n                response=example.answer,\n                contexts=example.contexts,\n                reference=example.ground_truth_answer,\n                sleep_time_in_seconds=sleep_time_in_seconds,\n            )\n        except Exception as err:\n            # TODO: raise warning here as well\n            return EvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=f\"Caught error {err!s}\"\n            )\n\n        if not eval_result.invalid_result:\n            return EvaluatorExamplePrediction(\n                feedback=eval_result.feedback or \"\", score=eval_result.score\n            )\n        else:\n            return EvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=eval_result.invalid_reason\n            )\n\n    def _predict_example(  # type: ignore\n        self,\n        predictor: BaseEvaluator,\n        example: LabelledEvaluatorDataExample,\n        sleep_time_in_seconds: int = 0,\n    ) -> EvaluatorExamplePrediction:\n        \"\"\"Predict RAG example with a query engine.\"\"\"\n        time.sleep(sleep_time_in_seconds)\n        try:\n            eval_result: EvaluationResult = predictor.evaluate(\n                query=example.query,\n                response=example.answer,\n                contexts=example.contexts,\n                reference=example.ground_truth_answer,\n                sleep_time_in_seconds=sleep_time_in_seconds,\n            )\n        except Exception as err:\n            # TODO: raise warning here as well\n            return EvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=f\"Caught error {err!s}\"\n            )\n\n        if not eval_result.invalid_result:\n            return EvaluatorExamplePrediction(\n                feedback=eval_result.feedback or \"\", score=eval_result.score\n            )\n        else:\n            return EvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=eval_result.invalid_reason\n            )\n\n    def _construct_prediction_dataset(  # type: ignore\n        self, predictions: Sequence[EvaluatorExamplePrediction]\n    ) -> EvaluatorPredictionDataset:\n        \"\"\"Construct prediction dataset.\"\"\"\n        return EvaluatorPredictionDataset(predictions=predictions)\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LabelledEvaluatorDataset\"\n\n\nclass PairwiseEvaluatorExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"Pairwise evaluation example prediction class.\n\n    Args:\n        feedback (Optional[str]): The evaluator's feedback.\n        score (Optional[float]): The evaluator's score.\n        evaluation_source (EvaluationSource): If the evaluation came from original order or flipped; or inconclusive.\n    \"\"\"\n\n    feedback: str = Field(\n        default_factory=str,\n        description=\"The generated (predicted) response that can be compared to a reference (ground-truth) answer.\",\n    )\n    score: Optional[float] = Field(\n        default=None,\n        description=\"The generated (predicted) response that can be compared to a reference (ground-truth) answer.\",\n    )\n    evaluation_source: Optional[EvaluationSource] = Field(\n        default=None,\n        description=(\n            \"Whether the evaluation comes from original, or flipped ordering. Can also be neither here indicating inconclusive judgement.\"\n        ),\n    )\n    invalid_prediction: bool = Field(\n        default=False, description=\"Whether or not the prediction is a valid one.\"\n    )\n    invalid_reason: Optional[str] = Field(\n        default=None, description=\"Reason as to why prediction is invalid.\"\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"PairwiseEvaluatorExamplePrediction\"\n\n\nclass PairwiseEvaluatorPredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"Pairwise evaluation predictions dataset class.\"\"\"\n\n    _prediction_type = PairwiseEvaluatorExamplePrediction\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List] = {\n            \"feedback\": [],\n            \"score\": [],\n            \"ordering\": [],\n        }\n        for prediction in self.predictions:\n            if not isinstance(prediction, PairwiseEvaluatorExamplePrediction):\n                raise ValueError(\n                    \"PairwiseEvaluatorPredictionDataset can only contain PairwiseEvaluatorExamplePrediction instances.\"\n                )\n            data[\"feedback\"].append(prediction.feedback)\n            data[\"score\"].append(prediction.score)\n            data[\"ordering\"].append(str(prediction.evaluation_source))\n\n        return pd.DataFrame(data)\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"PairwiseEvaluatorPredictionDataset\"\n\n\nclass LabelledPairwiseEvaluatorDataExample(LabelledEvaluatorDataExample):\n    \"\"\"Labelled pairwise evaluation data example class.\"\"\"\n\n    second_answer: str = Field(\n        default_factory=str,\n        description=\"The second answer to the example that is to be evaluated along versus `answer`.\",\n    )\n    second_answer_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the second answer.\"\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"LabelledPairwiseEvaluatorDataExample\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/evaluator_evaluation.py",
    "filename": "evaluator_evaluation.py",
    "relpath": "llama_dataset/evaluator_evaluation.py",
    "start_line": 351,
    "end_line": 491,
    "length": 141,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name"
    ],
    "chunk_class_names": [
      "LabelledPairwiseEvaluatorDataset"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "class_name",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name"
    ],
    "document_class_names": [
      "EvaluatorExamplePrediction",
      "name",
      "LabelledEvaluatorDataExample",
      "contains",
      "name",
      "EvaluatorPredictionDataset",
      "LabelledEvaluatorDataset",
      "PairwiseEvaluatorExamplePrediction",
      "name",
      "PairwiseEvaluatorPredictionDataset",
      "LabelledPairwiseEvaluatorDataExample",
      "name",
      "LabelledPairwiseEvaluatorDataset"
    ],
    "content": "class LabelledPairwiseEvaluatorDataset(BaseLlamaDataset[BaseEvaluator]):\n    \"\"\"Labelled pairwise evaluation dataset. For evaluating the evaluator in\n    performing pairwise evaluations.\n\n    Args:\n        BaseLlamaDataset (_type_): _description_\n    \"\"\"\n\n    _example_type = LabelledPairwiseEvaluatorDataExample\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List] = {\n            \"query\": [],\n            \"answer\": [],\n            \"second_answer\": [],\n            \"contexts\": [],\n            \"ground_truth_answer\": [],\n            \"query_by\": [],\n            \"answer_by\": [],\n            \"second_answer_by\": [],\n            \"ground_truth_answer_by\": [],\n            \"reference_feedback\": [],\n            \"reference_score\": [],\n            \"reference_evaluation_by\": [],\n        }\n        for example in self.examples:\n            if not isinstance(example, LabelledPairwiseEvaluatorDataExample):\n                raise ValueError(\n                    \"LabelledPairwiseEvaluatorDataset can only contain LabelledPairwiseEvaluatorDataExample instances.\"\n                )\n            data[\"query\"].append(example.query)\n            data[\"answer\"].append(example.answer)\n            data[\"second_answer\"].append(example.second_answer)\n            data[\"contexts\"].append(example.contexts)\n            data[\"ground_truth_answer\"].append(example.ground_truth_answer)\n            data[\"query_by\"].append(str(example.query_by))\n            data[\"answer_by\"].append(str(example.answer_by))\n            data[\"second_answer_by\"].append(str(example.second_answer_by))\n            data[\"ground_truth_answer_by\"].append(str(example.ground_truth_answer_by))\n            data[\"reference_feedback\"].append(example.reference_feedback)\n            data[\"reference_score\"].append(example.reference_score)\n            data[\"reference_evaluation_by\"].append(str(example.reference_evaluation_by))\n\n        return pd.DataFrame(data)\n\n    async def _apredict_example(  # type: ignore\n        self,\n        predictor: BaseEvaluator,\n        example: LabelledPairwiseEvaluatorDataExample,\n        sleep_time_in_seconds: int,\n    ) -> PairwiseEvaluatorExamplePrediction:\n        \"\"\"Async predict evaluation example with an Evaluator.\"\"\"\n        await asyncio.sleep(sleep_time_in_seconds)\n        try:\n            eval_result: EvaluationResult = await predictor.aevaluate(\n                query=example.query,\n                response=example.answer,\n                second_response=example.second_answer,\n                contexts=example.contexts,\n                reference=example.ground_truth_answer,\n                sleep_time_in_seconds=sleep_time_in_seconds,\n            )\n        except Exception as err:\n            # TODO: raise warning here as well\n            return PairwiseEvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=f\"Caught error {err!s}\"\n            )\n\n        if not eval_result.invalid_result:\n            return PairwiseEvaluatorExamplePrediction(\n                feedback=eval_result.feedback or \"\",\n                score=eval_result.score,\n                evaluation_source=EvaluationSource(eval_result.pairwise_source),\n            )\n        else:\n            return PairwiseEvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=eval_result.invalid_reason\n            )\n\n    def _predict_example(  # type: ignore\n        self,\n        predictor: BaseEvaluator,\n        example: LabelledPairwiseEvaluatorDataExample,\n        sleep_time_in_seconds: int = 0,\n    ) -> PairwiseEvaluatorExamplePrediction:\n        \"\"\"Predict RAG example with a query engine.\"\"\"\n        time.sleep(sleep_time_in_seconds)\n        try:\n            eval_result: EvaluationResult = predictor.evaluate(\n                query=example.query,\n                response=example.answer,\n                second_response=example.second_answer,\n                contexts=example.contexts,\n                reference=example.ground_truth_answer,\n                sleep_time_in_seconds=sleep_time_in_seconds,\n            )\n        except Exception as err:\n            # TODO: raise warning here as well\n            return PairwiseEvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=f\"Caught error {err!s}\"\n            )\n\n        if not eval_result.invalid_result:\n            return PairwiseEvaluatorExamplePrediction(\n                feedback=eval_result.feedback or \"\",\n                score=eval_result.score,\n                evaluation_source=EvaluationSource(eval_result.pairwise_source),\n            )\n        else:\n            return PairwiseEvaluatorExamplePrediction(\n                invalid_prediction=True, invalid_reason=eval_result.invalid_reason\n            )\n\n    def _construct_prediction_dataset(  # type: ignore\n        self, predictions: Sequence[PairwiseEvaluatorExamplePrediction]\n    ) -> PairwiseEvaluatorPredictionDataset:\n        \"\"\"Construct prediction dataset.\"\"\"\n        return PairwiseEvaluatorPredictionDataset(predictions=predictions)\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LabelledPairwiseEvaluatorDataset\"\n\n\n# British English + American English\nLabeledEvaluatorDataExample = LabelledEvaluatorDataExample\nLabeledEvaluatorDataset = LabelledEvaluatorDataset\nLabeledPairwiseEvaluatorDataExample = LabelledPairwiseEvaluatorDataExample\nLabeledPairwiseEvaluatorDataset = LabelledPairwiseEvaluatorDataset"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/__init__.py",
    "filename": "__init__.py",
    "relpath": "llama_dataset/__init__.py",
    "start_line": 1,
    "end_line": 61,
    "length": 61,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\" Dataset Module.\"\"\"\n\nfrom llama_index.core.llama_dataset.base import (\n    BaseLlamaDataExample,\n    BaseLlamaDataset,\n    BaseLlamaExamplePrediction,\n    BaseLlamaPredictionDataset,\n    CreatedBy,\n    CreatedByType,\n)\nfrom llama_index.core.llama_dataset.download import download_llama_dataset\nfrom llama_index.core.llama_dataset.evaluator_evaluation import (\n    EvaluatorExamplePrediction,\n    EvaluatorPredictionDataset,\n    LabeledEvaluatorDataExample,\n    LabeledEvaluatorDataset,\n    LabeledPairwiseEvaluatorDataExample,\n    LabeledPairwiseEvaluatorDataset,\n    LabelledEvaluatorDataExample,\n    LabelledEvaluatorDataset,\n    LabelledPairwiseEvaluatorDataExample,\n    LabelledPairwiseEvaluatorDataset,\n    PairwiseEvaluatorExamplePrediction,\n    PairwiseEvaluatorPredictionDataset,\n)\nfrom llama_index.core.llama_dataset.rag import (\n    LabeledRagDataExample,\n    LabeledRagDataset,\n    LabelledRagDataExample,\n    LabelledRagDataset,\n    RagExamplePrediction,\n    RagPredictionDataset,\n)\n\n__all__ = [\n    \"BaseLlamaDataset\",\n    \"BaseLlamaDataExample\",\n    \"BaseLlamaExamplePrediction\",\n    \"BaseLlamaPredictionDataset\",\n    \"LabelledRagDataExample\",\n    \"LabelledRagDataset\",\n    \"LabeledRagDataExample\",\n    \"LabeledRagDataset\",\n    \"RagExamplePrediction\",\n    \"RagPredictionDataset\",\n    \"CreatedByType\",\n    \"CreatedBy\",\n    \"download_llama_dataset\",\n    \"EvaluatorExamplePrediction\",\n    \"EvaluatorPredictionDataset\",\n    \"LabeledEvaluatorDataset\",\n    \"LabelledEvaluatorDataset\",\n    \"LabelledEvaluatorDataExample\",\n    \"LabeledEvaluatorDataExample\",\n    \"LabelledPairwiseEvaluatorDataExample\",\n    \"LabelledPairwiseEvaluatorDataset\",\n    \"LabeledPairwiseEvaluatorDataExample\",\n    \"LabeledPairwiseEvaluatorDataset\",\n    \"PairwiseEvaluatorExamplePrediction\",\n    \"PairwiseEvaluatorPredictionDataset\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/rag.py",
    "filename": "rag.py",
    "relpath": "llama_dataset/rag.py",
    "start_line": 1,
    "end_line": 188,
    "length": 188,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name"
    ],
    "chunk_class_names": [
      "RagExamplePrediction",
      "name",
      "LabelledRagDataExample",
      "name",
      "RagPredictionDataset",
      "LabelledRagDataset"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "to_pandas",
      "class_name",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "_construct_prediction_dataset",
      "class_name"
    ],
    "document_class_names": [
      "RagExamplePrediction",
      "name",
      "LabelledRagDataExample",
      "name",
      "RagPredictionDataset",
      "LabelledRagDataset"
    ],
    "content": "\"\"\"Llama Dataset Class.\"\"\"\n\nimport asyncio\nimport time\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.llama_dataset.base import (\n    BaseLlamaDataExample,\n    BaseLlamaDataset,\n    BaseLlamaExamplePrediction,\n    BaseLlamaPredictionDataset,\n    CreatedBy,\n)\n\n\nclass RagExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"RAG example prediction class.\n\n    Args:\n        response (str): The response generated by the LLM.\n        contexts (Optional[List[str]]): The retrieved context (text) for generating\n                                        response.\n    \"\"\"\n\n    response: str = Field(\n        default=\"\",\n        description=\"The generated (predicted) response that can be compared to a reference (ground-truth) answer.\",\n    )\n    contexts: Optional[List[str]] = Field(\n        default=None,\n        description=\"The contexts in raw text form used to generate the response.\",\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"RagExamplePrediction\"\n\n\nclass LabelledRagDataExample(BaseLlamaDataExample):\n    \"\"\"RAG example class. Analogous to traditional ML datasets, this dataset contains\n    the \"features\" (i.e., query + context) to make a prediction and the \"label\" (i.e., response)\n    to evaluate the prediction.\n\n    Args:\n        query (str): The user query\n        query_by (CreatedBy): Query generated by human or ai (model-name)\n        reference_contexts (Optional[List[str]]): The contexts used for response\n        reference_answer ([str]): Reference answer to the query. An answer\n                                    that would receive full marks upon evaluation.\n        reference_answer_by: The reference answer generated by human or ai (model-name).\n    \"\"\"\n\n    query: str = Field(\n        default_factory=str, description=\"The user query for the example.\"\n    )\n    query_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the query.\"\n    )\n    reference_contexts: Optional[List[str]] = Field(\n        default=None,\n        description=\"The contexts used to generate the reference answer.\",\n    )\n    reference_answer: str = Field(\n        default_factory=str,\n        description=\"The reference (ground-truth) answer to the example.\",\n    )\n    reference_answer_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the reference answer.\"\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"LabelledRagDataExample\"\n\n\nclass RagPredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"RagDataset class.\"\"\"\n\n    _prediction_type = RagExamplePrediction\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List] = {\n            \"response\": [],\n            \"contexts\": [],\n        }\n        for pred in self.predictions:\n            if not isinstance(pred, RagExamplePrediction):\n                raise ValueError(\n                    \"All predictions in the dataset must be of type RagExamplePrediction.\"\n                )\n            data[\"response\"].append(pred.response)\n            data[\"contexts\"].append(pred.contexts)\n\n        return pd.DataFrame(data)\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"RagPredictionDataset\"\n\n\nclass LabelledRagDataset(BaseLlamaDataset[BaseQueryEngine]):\n    \"\"\"RagDataset class.\"\"\"\n\n    _example_type = LabelledRagDataExample\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List] = {\n            \"query\": [],\n            \"reference_contexts\": [],\n            \"reference_answer\": [],\n            \"reference_answer_by\": [],\n            \"query_by\": [],\n        }\n        for example in self.examples:\n            if not isinstance(example, LabelledRagDataExample):\n                raise ValueError(\n                    \"All examples in the dataset must be of type LabelledRagDataExample.\"\n                )\n            data[\"query\"].append(example.query)\n            data[\"reference_contexts\"].append(example.reference_contexts)\n            data[\"reference_answer\"].append(example.reference_answer)\n            data[\"reference_answer_by\"].append(str(example.reference_answer_by))\n            data[\"query_by\"].append(str(example.query_by))\n\n        return pd.DataFrame(data)\n\n    async def _apredict_example(  # type: ignore\n        self,\n        predictor: BaseQueryEngine,\n        example: LabelledRagDataExample,\n        sleep_time_in_seconds: int,\n    ) -> RagExamplePrediction:\n        \"\"\"Async predict RAG example with a query engine.\"\"\"\n        await asyncio.sleep(sleep_time_in_seconds)\n        response = await predictor.aquery(example.query)\n        return RagExamplePrediction(\n            response=str(response), contexts=[s.text for s in response.source_nodes]\n        )\n\n    def _predict_example(  # type: ignore\n        self,\n        predictor: BaseQueryEngine,\n        example: LabelledRagDataExample,\n        sleep_time_in_seconds: int = 0,\n    ) -> RagExamplePrediction:\n        \"\"\"Predict RAG example with a query engine.\"\"\"\n        time.sleep(sleep_time_in_seconds)\n        response = predictor.query(example.query)\n        return RagExamplePrediction(\n            response=str(response), contexts=[s.text for s in response.source_nodes]\n        )\n\n    def _construct_prediction_dataset(  # type: ignore\n        self, predictions: Sequence[RagExamplePrediction]\n    ) -> RagPredictionDataset:\n        \"\"\"Construct prediction dataset.\"\"\"\n        return RagPredictionDataset(predictions=predictions)\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LabelledRagDataset\"\n\n\n# British English + American English\nLabeledRagDataExample = LabelledRagDataExample\nLabeledRagDataset = LabelledRagDataset"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/download.py",
    "filename": "download.py",
    "relpath": "llama_dataset/download.py",
    "start_line": 1,
    "end_line": 93,
    "length": 93,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_resolve_dataset_class",
      "download_llama_dataset"
    ],
    "chunk_class_names": [
      "based",
      "you"
    ],
    "document_function_names": [
      "_resolve_dataset_class",
      "download_llama_dataset"
    ],
    "document_class_names": [
      "based",
      "you"
    ],
    "content": "from typing import List, Tuple, Type\n\nfrom llama_index.core.download.dataset import (\n    LLAMA_DATASETS_LFS_URL,\n    LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL,\n    LLAMA_DATASETS_URL,\n)\nfrom llama_index.core.download.dataset import download_llama_dataset as download\nfrom llama_index.core.download.module import (\n    MODULE_TYPE,\n    track_download,\n)\nfrom llama_index.core.llama_dataset.base import BaseLlamaDataset\nfrom llama_index.core.llama_dataset.evaluator_evaluation import (\n    LabelledEvaluatorDataset,\n    LabelledPairwiseEvaluatorDataset,\n)\nfrom llama_index.core.llama_dataset.rag import LabelledRagDataset\nfrom llama_index.core.readers import SimpleDirectoryReader\nfrom llama_index.core.schema import Document\n\n\ndef _resolve_dataset_class(filename: str) -> Type[BaseLlamaDataset]:\n    \"\"\"Resolve appropriate llama dataset class based on file name.\"\"\"\n    if \"rag_dataset.json\" in filename:\n        return LabelledRagDataset\n    elif \"pairwise_evaluator_dataset.json\" in filename:\n        return LabelledPairwiseEvaluatorDataset\n    elif \"evaluator_dataset.json\" in filename:\n        return LabelledEvaluatorDataset\n    else:\n        raise ValueError(\"Unknown filename.\")\n\n\ndef download_llama_dataset(\n    llama_dataset_class: str,\n    download_dir: str,\n    llama_datasets_url: str = LLAMA_DATASETS_URL,\n    llama_datasets_lfs_url: str = LLAMA_DATASETS_LFS_URL,\n    llama_datasets_source_files_tree_url: str = LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL,\n    show_progress: bool = False,\n    load_documents: bool = True,\n) -> Tuple[BaseLlamaDataset, List[Document]]:\n    \"\"\"Download dataset from datasets-LFS and llamahub.\n\n    Args:\n        dataset_class: The name of the llamadataset class you want to download,\n            such as `PaulGrahamEssayDataset`.\n        custom_dir: Custom dir name to download loader into (under parent folder).\n        custom_path: Custom dirpath to download loader into.\n        llama_datasets_url: Url for getting ordinary files from llama_datasets repo\n        llama_datasets_lfs_url: Url for lfs-traced files llama_datasets repo\n        llama_datasets_source_files_tree_url: Url for listing source_files contents\n        refresh_cache: If true, the local cache will be skipped and the\n            loader will be fetched directly from the remote repo.\n        source_files_dirpath: The directory for storing source files\n        library_path: File name of the library file.\n        base_file_name: The rag dataset json file\n        disable_library_cache: Boolean to control library cache\n        override_path: Boolean to control overriding path\n        show_progress: Boolean for showing progress on downloading source files\n        load_documents: Boolean for whether or not source_files for LabelledRagDataset should\n                        be loaded.\n\n    Returns:\n        a `BaseLlamaDataset` and a `List[Document]`\n    \"\"\"\n    filenames: Tuple[str, str] = download(\n        llama_dataset_class,\n        llama_datasets_url=llama_datasets_url,\n        llama_datasets_lfs_url=llama_datasets_lfs_url,\n        llama_datasets_source_files_tree_url=llama_datasets_source_files_tree_url,\n        refresh_cache=True,\n        custom_path=download_dir,\n        library_path=\"library.json\",\n        disable_library_cache=True,\n        override_path=True,\n        show_progress=show_progress,\n    )\n    dataset_filename, source_files_dir = filenames\n    track_download(llama_dataset_class, MODULE_TYPE.DATASETS)\n\n    dataset = _resolve_dataset_class(dataset_filename).from_json(dataset_filename)\n    documents = []\n\n    # for now only rag datasets need to provide the documents\n    # in order to build an index over them\n    if \"rag_dataset.json\" in dataset_filename and load_documents:\n        documents = SimpleDirectoryReader(input_dir=source_files_dir).load_data(\n            show_progress=show_progress\n        )\n\n    return (dataset, documents)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/simple.py",
    "filename": "simple.py",
    "relpath": "llama_dataset/simple.py",
    "start_line": 1,
    "end_line": 137,
    "length": 137,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "to_pandas",
      "class_name",
      "class_name",
      "_construct_prediction_dataset",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "class_name"
    ],
    "chunk_class_names": [
      "SimpleExamplePrediction",
      "name",
      "SimplePredictionDataset",
      "LabelledSimpleDataExample",
      "name",
      "LabelledSimpleDataset",
      "name"
    ],
    "document_function_names": [
      "class_name",
      "to_pandas",
      "class_name",
      "class_name",
      "_construct_prediction_dataset",
      "to_pandas",
      "_apredict_example",
      "_predict_example",
      "class_name"
    ],
    "document_class_names": [
      "SimpleExamplePrediction",
      "name",
      "SimplePredictionDataset",
      "LabelledSimpleDataExample",
      "name",
      "LabelledSimpleDataset",
      "name"
    ],
    "content": "from typing import Any, Dict, List, Sequence, Optional\nfrom llama_index.core.llama_dataset.base import (\n    BaseLlamaDataExample,\n    BaseLlamaDataset,\n    CreatedBy,\n    BaseLlamaExamplePrediction,\n    BaseLlamaPredictionDataset,\n)\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.bridge.pydantic import Field\n\n\nclass SimpleExamplePrediction(BaseLlamaExamplePrediction):\n    \"\"\"RAG example prediction class.\n\n    Args:\n        response (str): The response generated by the LLM.\n        contexts (Optional[List[str]]): The retrieved context (text) for generating\n                                        response.\n    \"\"\"\n\n    label: str = Field(\n        default_factory=str,\n        description=\"The generated (predicted) label that can be compared to a reference (ground-truth) label.\",\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"SimpleExamplePrediction\"\n\n\nclass SimplePredictionDataset(BaseLlamaPredictionDataset):\n    \"\"\"RagDataset class.\"\"\"\n\n    _prediction_type = SimpleExamplePrediction\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List[str]] = {\"label\": []}\n        if self.predictions:\n            for t in self.predictions:\n                assert isinstance(t, self._prediction_type)\n\n                data[\"label\"].append(t.label)\n\n        return pd.DataFrame(data)\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SimplePredictionDataset\"\n\n\nclass LabelledSimpleDataExample(BaseLlamaDataExample):\n    reference_label: str = Field(default_factory=str, description=\"Class label\")\n    text: str = Field(default_factory=str, description=\"Text body of example\")\n    text_by: Optional[CreatedBy] = Field(\n        default=None, description=\"What generated the query.\"\n    )\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"LabelledSimpleDataExample\"\n\n\nclass LabelledSimpleDataset(BaseLlamaDataset[LLM]):\n    _example_type = LabelledSimpleDataExample\n\n    def _construct_prediction_dataset(  # type: ignore\n        self, predictions: Sequence[SimpleExamplePrediction]\n    ) -> SimplePredictionDataset:\n        \"\"\"Construct the specific prediction dataset.\n\n        Args:\n            predictions (List[BaseLlamaExamplePrediction]): the list of predictions.\n\n        Returns:\n            BaseLlamaPredictionDataset: A dataset of predictions.\n        \"\"\"\n        return SimplePredictionDataset(predictions=predictions)\n\n    def to_pandas(self) -> Any:\n        \"\"\"Create pandas dataframe.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        data: Dict[str, List[str]] = {\n            \"reference_label\": [],\n            \"text\": [],\n            \"text_by\": [],\n        }\n        for example in self.examples:\n            if not isinstance(example, self._example_type):\n                raise ValueError(\n                    f\"Expected example of type {LabelledSimpleDataExample}, got {type(example)}\"\n                )\n\n            data[\"reference_label\"].append(example.reference_label)\n            data[\"text\"].append(example.text)\n            data[\"text_by\"].append(str(example.text_by))\n\n        return pd.DataFrame(data)\n\n    async def _apredict_example(\n        self,\n        predictor: LLM,\n        example: BaseLlamaDataExample,\n        sleep_time_in_seconds: int,\n    ) -> BaseLlamaExamplePrediction:\n        \"\"\"Async predict RAG example with a query engine.\"\"\"\n        raise NotImplementedError(\"This method has not yet been implemented.\")\n\n    def _predict_example(\n        self,\n        predictor: LLM,\n        example: BaseLlamaDataExample,\n        sleep_time_in_seconds: int = 0,\n    ) -> BaseLlamaExamplePrediction:\n        raise NotImplementedError(\"This method has not yet been implemented.\")\n\n    @property\n    def class_name(self) -> str:\n        \"\"\"Data example class name.\"\"\"\n        return \"LabelledSimpleDataset\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py",
    "filename": "embedding.py",
    "relpath": "llama_dataset/legacy/embedding.py",
    "start_line": 1,
    "end_line": 115,
    "length": 115,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "query_docid_pairs",
      "save_json",
      "from_json",
      "generate_qa_embedding_pairs"
    ],
    "chunk_class_names": [
      "EmbeddingQAFinetuneDataset"
    ],
    "document_function_names": [
      "query_docid_pairs",
      "save_json",
      "from_json",
      "generate_qa_embedding_pairs"
    ],
    "document_class_names": [
      "EmbeddingQAFinetuneDataset"
    ],
    "content": "\"\"\"Common utils for embeddings.\"\"\"\n\nimport json\nimport re\nimport uuid\nimport warnings\nfrom typing import Dict, List, Optional, Tuple\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.llms.utils import LLM\nfrom llama_index.core.schema import MetadataMode, TextNode\nfrom llama_index.core.settings import Settings\nfrom tqdm import tqdm\n\n\nclass EmbeddingQAFinetuneDataset(BaseModel):\n    \"\"\"Embedding QA Finetuning Dataset.\n\n    Args:\n        queries (Dict[str, str]): Dict id -> query.\n        corpus (Dict[str, str]): Dict id -> string.\n        relevant_docs (Dict[str, List[str]]): Dict query id -> list of doc ids.\n\n    \"\"\"\n\n    queries: Dict[str, str]  # dict id -> query\n    corpus: Dict[str, str]  # dict id -> string\n    relevant_docs: Dict[str, List[str]]  # query id -> list of doc ids\n    mode: str = \"text\"\n\n    @property\n    def query_docid_pairs(self) -> List[Tuple[str, List[str]]]:\n        \"\"\"Get query, relevant doc ids.\"\"\"\n        return [\n            (query, self.relevant_docs[query_id])\n            for query_id, query in self.queries.items()\n        ]\n\n    def save_json(self, path: str) -> None:\n        \"\"\"Save json.\"\"\"\n        with open(path, \"w\") as f:\n            json.dump(self.model_dump(), f, indent=4)\n\n    @classmethod\n    def from_json(cls, path: str) -> \"EmbeddingQAFinetuneDataset\":\n        \"\"\"Load json.\"\"\"\n        with open(path) as f:\n            data = json.load(f)\n        return cls(**data)\n\n\nDEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Teacher/ Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. Restrict the questions to the \\\ncontext information provided.\"\n\"\"\"\n\n\n# generate queries as a convenience function\ndef generate_qa_embedding_pairs(\n    nodes: List[TextNode],\n    llm: Optional[LLM] = None,\n    qa_generate_prompt_tmpl: str = DEFAULT_QA_GENERATE_PROMPT_TMPL,\n    num_questions_per_chunk: int = 2,\n) -> EmbeddingQAFinetuneDataset:\n    \"\"\"Generate examples given a set of nodes.\"\"\"\n    llm = llm or Settings.llm\n    node_dict = {\n        node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)\n        for node in nodes\n    }\n\n    queries = {}\n    relevant_docs = {}\n    for node_id, text in tqdm(node_dict.items()):\n        query = qa_generate_prompt_tmpl.format(\n            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n        )\n        response = llm.complete(query)\n\n        result = str(response).strip().split(\"\\n\")\n        questions = [\n            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n        ]\n        questions = [question for question in questions if len(question) > 0][\n            :num_questions_per_chunk\n        ]\n\n        num_questions_generated = len(questions)\n        if num_questions_generated < num_questions_per_chunk:\n            warnings.warn(\n                f\"Fewer questions generated ({num_questions_generated}) \"\n                f\"than requested ({num_questions_per_chunk}).\"\n            )\n\n        for question in questions:\n            question_id = str(uuid.uuid4())\n            queries[question_id] = question\n            relevant_docs[question_id] = [node_id]\n\n    # construct dataset\n    return EmbeddingQAFinetuneDataset(\n        queries=queries, corpus=node_dict, relevant_docs=relevant_docs\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/loading.py",
    "filename": "loading.py",
    "relpath": "node_parser/loading.py",
    "start_line": 1,
    "end_line": 43,
    "length": 43,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_parser"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "load_parser"
    ],
    "document_class_names": [],
    "content": "from typing import Dict, Type\n\nfrom llama_index.core.node_parser.file.html import HTMLNodeParser\nfrom llama_index.core.node_parser.file.json import JSONNodeParser\nfrom llama_index.core.node_parser.file.markdown import MarkdownNodeParser\nfrom llama_index.core.node_parser.file.simple_file import SimpleFileNodeParser\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.relational.hierarchical import (\n    HierarchicalNodeParser,\n)\nfrom llama_index.core.node_parser.text.code import CodeSplitter\nfrom llama_index.core.node_parser.text.sentence import SentenceSplitter\nfrom llama_index.core.node_parser.text.sentence_window import (\n    SentenceWindowNodeParser,\n)\nfrom llama_index.core.node_parser.text.token import TokenTextSplitter\n\nall_node_parsers: Dict[str, Type[NodeParser]] = {\n    HTMLNodeParser.class_name(): HTMLNodeParser,\n    JSONNodeParser.class_name(): JSONNodeParser,\n    MarkdownNodeParser.class_name(): MarkdownNodeParser,\n    SimpleFileNodeParser.class_name(): SimpleFileNodeParser,\n    HierarchicalNodeParser.class_name(): HierarchicalNodeParser,\n    CodeSplitter.class_name(): CodeSplitter,\n    SentenceSplitter.class_name(): SentenceSplitter,\n    TokenTextSplitter.class_name(): TokenTextSplitter,\n    SentenceWindowNodeParser.class_name(): SentenceWindowNodeParser,\n}\n\n\ndef load_parser(\n    data: dict,\n) -> NodeParser:\n    if isinstance(data, NodeParser):\n        return data\n    parser_name = data.get(\"class_name\", None)\n    if parser_name is None:\n        raise ValueError(\"Parser loading requires a class_name\")\n\n    if parser_name not in all_node_parsers:\n        raise ValueError(f\"Invalid parser name: {parser_name}\")\n    else:\n        return all_node_parsers[parser_name].from_dict(data)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/interface.py",
    "filename": "interface.py",
    "relpath": "node_parser/interface.py",
    "start_line": 1,
    "end_line": 268,
    "length": 268,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_validate_id_func",
      "_serialize_id_func",
      "_parse_nodes",
      "_aparse_nodes",
      "_postprocess_parsed_nodes",
      "get_nodes_from_documents",
      "aget_nodes_from_documents",
      "__call__",
      "acall",
      "split_text",
      "split_texts",
      "_parse_nodes",
      "split_text_metadata_aware",
      "split_texts_metadata_aware",
      "_get_metadata_str",
      "_parse_nodes"
    ],
    "chunk_class_names": [
      "NodeParser",
      "TextSplitter",
      "MetadataAwareTextSplitter"
    ],
    "document_function_names": [
      "_validate_id_func",
      "_serialize_id_func",
      "_parse_nodes",
      "_aparse_nodes",
      "_postprocess_parsed_nodes",
      "get_nodes_from_documents",
      "aget_nodes_from_documents",
      "__call__",
      "acall",
      "split_text",
      "split_texts",
      "_parse_nodes",
      "split_text_metadata_aware",
      "split_texts_metadata_aware",
      "_get_metadata_str",
      "_parse_nodes"
    ],
    "document_class_names": [
      "NodeParser",
      "TextSplitter",
      "MetadataAwareTextSplitter"
    ],
    "content": "\"\"\"Node parser interface.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Callable, Dict, List, Sequence, Optional\nfrom typing_extensions import Annotated\n\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    WithJsonSchema,\n    BeforeValidator,\n    ConfigDict,\n    PlainSerializer,\n)\nfrom llama_index.core.callbacks import CallbackManager, CBEventType, EventPayload\nfrom llama_index.core.node_parser.node_utils import (\n    build_nodes_from_splits,\n    default_id_func,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    Document,\n    MetadataMode,\n    NodeRelationship,\n    TextNode,\n    TransformComponent,\n)\nfrom llama_index.core.utils import get_tqdm_iterable\n\n\ndef _validate_id_func(v: Any) -> Any:\n    if v is None:\n        return default_id_func\n    return v\n\n\ndef _serialize_id_func(f: Callable) -> Any:\n    return {\"id_func_name\": f\"{f.__name__}\", \"title\": \"id_func\"}\n\n\nIdFuncCallable = Annotated[\n    Callable,\n    Field(validate_default=True),\n    BeforeValidator(_validate_id_func),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n    PlainSerializer(_serialize_id_func),\n]\n\n\nclass NodeParser(TransformComponent, ABC):\n    \"\"\"Base interface for node parser.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    include_metadata: bool = Field(\n        default=True, description=\"Whether or not to consider metadata when splitting.\"\n    )\n    include_prev_next_rel: bool = Field(\n        default=True, description=\"Include prev/next node relationships.\"\n    )\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n    id_func: Optional[IdFuncCallable] = Field(\n        default=None,\n        description=\"Function to generate node IDs.\",\n    )\n\n    @abstractmethod\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        ...\n\n    async def _aparse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        return self._parse_nodes(nodes, show_progress=show_progress, **kwargs)\n\n    def _postprocess_parsed_nodes(\n        self, nodes: List[BaseNode], parent_doc_map: Dict[str, Document]\n    ) -> List[BaseNode]:\n        for i, node in enumerate(nodes):\n            parent_doc = parent_doc_map.get(node.ref_doc_id or \"\", None)\n            parent_node = node.source_node\n\n            if parent_doc is not None:\n                if parent_doc.source_node is not None:\n                    node.relationships.update(\n                        {\n                            NodeRelationship.SOURCE: parent_doc.source_node,\n                        }\n                    )\n                start_char_idx = parent_doc.text.find(\n                    node.get_content(metadata_mode=MetadataMode.NONE)\n                )\n\n                # update start/end char idx\n                if start_char_idx >= 0 and isinstance(node, TextNode):\n                    node.start_char_idx = start_char_idx\n                    node.end_char_idx = start_char_idx + len(\n                        node.get_content(metadata_mode=MetadataMode.NONE)\n                    )\n\n                # update metadata\n                if self.include_metadata:\n                    # Merge parent_doc.metadata into nodes.metadata, giving preference to node's values\n                    node.metadata = {**parent_doc.metadata, **node.metadata}\n\n            if parent_node is not None:\n                if self.include_metadata:\n                    parent_metadata = parent_node.metadata\n\n                    combined_metadata = {**parent_metadata, **node.metadata}\n\n                    # Merge parent_node.metadata into nodes.metadata, giving preference to node's values\n                    node.metadata.update(combined_metadata)\n\n            if self.include_prev_next_rel:\n                # establish prev/next relationships if nodes share the same source_node\n                if (\n                    i > 0\n                    and node.source_node\n                    and nodes[i - 1].source_node\n                    and nodes[i - 1].source_node.node_id == node.source_node.node_id  # type: ignore\n                ):\n                    node.relationships[NodeRelationship.PREVIOUS] = nodes[\n                        i - 1\n                    ].as_related_node_info()\n                if (\n                    i < len(nodes) - 1\n                    and node.source_node\n                    and nodes[i + 1].source_node\n                    and nodes[i + 1].source_node.node_id == node.source_node.node_id  # type: ignore\n                ):\n                    node.relationships[NodeRelationship.NEXT] = nodes[\n                        i + 1\n                    ].as_related_node_info()\n\n        return nodes\n\n    def get_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Parse documents into nodes.\n\n        Args:\n            documents (Sequence[Document]): documents to parse\n            show_progress (bool): whether to show progress bar\n\n        \"\"\"\n        doc_id_to_document = {doc.id_: doc for doc in documents}\n\n        with self.callback_manager.event(\n            CBEventType.NODE_PARSING, payload={EventPayload.DOCUMENTS: documents}\n        ) as event:\n            nodes = self._parse_nodes(documents, show_progress=show_progress, **kwargs)\n            nodes = self._postprocess_parsed_nodes(nodes, doc_id_to_document)\n\n            event.on_end({EventPayload.NODES: nodes})\n\n        return nodes\n\n    async def aget_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        doc_id_to_document = {doc.id_: doc for doc in documents}\n\n        with self.callback_manager.event(\n            CBEventType.NODE_PARSING, payload={EventPayload.DOCUMENTS: documents}\n        ) as event:\n            nodes = await self._aparse_nodes(\n                documents, show_progress=show_progress, **kwargs\n            )\n            nodes = self._postprocess_parsed_nodes(nodes, doc_id_to_document)\n\n            event.on_end({EventPayload.NODES: nodes})\n\n        return nodes\n\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        return self.get_nodes_from_documents(nodes, **kwargs)  # type: ignore\n\n    async def acall(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        return await self.aget_nodes_from_documents(nodes, **kwargs)  # type: ignore\n\n\nclass TextSplitter(NodeParser):\n    @abstractmethod\n    def split_text(self, text: str) -> List[str]:\n        ...\n\n    def split_texts(self, texts: List[str]) -> List[str]:\n        nested_texts = [self.split_text(text) for text in texts]\n        return [item for sublist in nested_texts for item in sublist]\n\n    def _parse_nodes(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n        for node in nodes_with_progress:\n            splits = self.split_text(node.get_content())\n\n            all_nodes.extend(\n                build_nodes_from_splits(splits, node, id_func=self.id_func)\n            )\n\n        return all_nodes\n\n\nclass MetadataAwareTextSplitter(TextSplitter):\n    @abstractmethod\n    def split_text_metadata_aware(self, text: str, metadata_str: str) -> List[str]:\n        ...\n\n    def split_texts_metadata_aware(\n        self, texts: List[str], metadata_strs: List[str]\n    ) -> List[str]:\n        if len(texts) != len(metadata_strs):\n            raise ValueError(\"Texts and metadata_strs must have the same length\")\n        nested_texts = [\n            self.split_text_metadata_aware(text, metadata)\n            for text, metadata in zip(texts, metadata_strs)\n        ]\n        return [item for sublist in nested_texts for item in sublist]\n\n    def _get_metadata_str(self, node: BaseNode) -> str:\n        \"\"\"Helper function to get the proper metadata str for splitting.\"\"\"\n        embed_metadata_str = node.get_metadata_str(mode=MetadataMode.EMBED)\n        llm_metadata_str = node.get_metadata_str(mode=MetadataMode.LLM)\n\n        # use the longest metadata str for splitting\n        if len(embed_metadata_str) > len(llm_metadata_str):\n            metadata_str = embed_metadata_str\n        else:\n            metadata_str = llm_metadata_str\n\n        return metadata_str\n\n    def _parse_nodes(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            metadata_str = self._get_metadata_str(node)\n            splits = self.split_text_metadata_aware(\n                node.get_content(metadata_mode=MetadataMode.NONE),\n                metadata_str=metadata_str,\n            )\n            all_nodes.extend(\n                build_nodes_from_splits(splits, node, id_func=self.id_func)\n            )\n\n        return all_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/__init__.py",
    "filename": "__init__.py",
    "relpath": "node_parser/__init__.py",
    "start_line": 1,
    "end_line": 72,
    "length": 72,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Node parsers.\"\"\"\n\nfrom llama_index.core.node_parser.file.html import HTMLNodeParser\nfrom llama_index.core.node_parser.file.json import JSONNodeParser\nfrom llama_index.core.node_parser.file.markdown import MarkdownNodeParser\nfrom llama_index.core.node_parser.file.simple_file import SimpleFileNodeParser\nfrom llama_index.core.node_parser.interface import (\n    MetadataAwareTextSplitter,\n    NodeParser,\n    TextSplitter,\n)\nfrom llama_index.core.node_parser.relational.hierarchical import (\n    HierarchicalNodeParser,\n    get_leaf_nodes,\n    get_root_nodes,\n    get_child_nodes,\n    get_deeper_nodes,\n)\nfrom llama_index.core.node_parser.relational.markdown_element import (\n    MarkdownElementNodeParser,\n)\nfrom llama_index.core.node_parser.relational.unstructured_element import (\n    UnstructuredElementNodeParser,\n)\nfrom llama_index.core.node_parser.relational.llama_parse_json_element import (\n    LlamaParseJsonNodeParser,\n)\nfrom llama_index.core.node_parser.text.code import CodeSplitter\nfrom llama_index.core.node_parser.text.langchain import LangchainNodeParser\nfrom llama_index.core.node_parser.text.semantic_splitter import (\n    SemanticSplitterNodeParser,\n)\nfrom llama_index.core.node_parser.text.semantic_double_merging_splitter import (\n    SemanticDoubleMergingSplitterNodeParser,\n    LanguageConfig,\n)\nfrom llama_index.core.node_parser.text.sentence import SentenceSplitter\nfrom llama_index.core.node_parser.text.sentence_window import (\n    SentenceWindowNodeParser,\n)\nfrom llama_index.core.node_parser.text.token import TokenTextSplitter\n\n# deprecated, for backwards compatibility\nSimpleNodeParser = SentenceSplitter\n\n__all__ = [\n    \"TokenTextSplitter\",\n    \"SentenceSplitter\",\n    \"CodeSplitter\",\n    \"SimpleFileNodeParser\",\n    \"HTMLNodeParser\",\n    \"MarkdownNodeParser\",\n    \"JSONNodeParser\",\n    \"SentenceWindowNodeParser\",\n    \"SemanticSplitterNodeParser\",\n    \"SemanticDoubleMergingSplitterNodeParser\",\n    \"LanguageConfig\",\n    \"NodeParser\",\n    \"HierarchicalNodeParser\",\n    \"TextSplitter\",\n    \"MarkdownElementNodeParser\",\n    \"MetadataAwareTextSplitter\",\n    \"LangchainNodeParser\",\n    \"UnstructuredElementNodeParser\",\n    \"get_leaf_nodes\",\n    \"get_root_nodes\",\n    \"get_child_nodes\",\n    \"get_deeper_nodes\",\n    \"LlamaParseJsonNodeParser\",\n    # deprecated, for backwards compatibility\n    \"SimpleNodeParser\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/node_utils.py",
    "filename": "node_utils.py",
    "relpath": "node_parser/node_utils.py",
    "start_line": 1,
    "end_line": 92,
    "length": 92,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__call__",
      "default_id_func",
      "build_nodes_from_splits"
    ],
    "chunk_class_names": [
      "IdFuncCallable"
    ],
    "document_function_names": [
      "__call__",
      "default_id_func",
      "build_nodes_from_splits"
    ],
    "document_class_names": [
      "IdFuncCallable"
    ],
    "content": "\"\"\"General node utils.\"\"\"\n\nimport logging\nimport uuid\nfrom typing import List, Optional, Protocol, runtime_checkable\n\nfrom llama_index.core.schema import (\n    BaseNode,\n    Document,\n    ImageDocument,\n    ImageNode,\n    NodeRelationship,\n    TextNode,\n)\nfrom llama_index.core.utils import truncate_text\n\nlogger = logging.getLogger(__name__)\n\n\n@runtime_checkable\nclass IdFuncCallable(Protocol):\n    def __call__(self, i: int, doc: BaseNode) -> str:\n        ...\n\n\ndef default_id_func(i: int, doc: BaseNode) -> str:\n    return str(uuid.uuid4())\n\n\ndef build_nodes_from_splits(\n    text_splits: List[str],\n    document: BaseNode,\n    ref_doc: Optional[BaseNode] = None,\n    id_func: Optional[IdFuncCallable] = None,\n) -> List[TextNode]:\n    \"\"\"Build nodes from splits.\"\"\"\n    ref_doc = ref_doc or document\n    id_func = id_func or default_id_func\n    nodes: List[TextNode] = []\n    \"\"\"Calling as_related_node_info() on a document recomputes the hash for the whole text and metadata\"\"\"\n    \"\"\"It is not that bad, when creating relationships between the nodes, but is terrible when adding a relationship\"\"\"\n    \"\"\"between the node and a document, hence we create the relationship only once here and pass it to the nodes\"\"\"\n    relationships = {NodeRelationship.SOURCE: ref_doc.as_related_node_info()}\n    for i, text_chunk in enumerate(text_splits):\n        logger.debug(f\"> Adding chunk: {truncate_text(text_chunk, 50)}\")\n\n        if isinstance(document, ImageDocument):\n            image_node = ImageNode(\n                id_=id_func(i, document),\n                text=text_chunk,\n                embedding=document.embedding,\n                image=document.image,\n                image_path=document.image_path,\n                image_url=document.image_url,\n                excluded_embed_metadata_keys=document.excluded_embed_metadata_keys,\n                excluded_llm_metadata_keys=document.excluded_llm_metadata_keys,\n                metadata_seperator=document.metadata_separator,\n                metadata_template=document.metadata_template,\n                text_template=document.text_template,\n                relationships=relationships,\n            )\n            nodes.append(image_node)  # type: ignore\n        elif isinstance(document, Document):\n            node = TextNode(\n                id_=id_func(i, document),\n                text=text_chunk,\n                embedding=document.embedding,\n                excluded_embed_metadata_keys=document.excluded_embed_metadata_keys,\n                excluded_llm_metadata_keys=document.excluded_llm_metadata_keys,\n                metadata_seperator=document.metadata_separator,\n                metadata_template=document.metadata_template,\n                text_template=document.text_template,\n                relationships=relationships,\n            )\n            nodes.append(node)\n        elif isinstance(document, TextNode):\n            node = TextNode(\n                id_=id_func(i, document),\n                text=text_chunk,\n                embedding=document.embedding,\n                excluded_embed_metadata_keys=document.excluded_embed_metadata_keys,\n                excluded_llm_metadata_keys=document.excluded_llm_metadata_keys,\n                metadata_seperator=document.metadata_seperator,\n                metadata_template=document.metadata_template,\n                text_template=document.text_template,\n                relationships=relationships,\n            )\n            nodes.append(node)\n        else:\n            raise ValueError(f\"Unknown document type: {type(document)}\")\n\n    return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_double_merging_splitter.py",
    "filename": "semantic_double_merging_splitter.py",
    "relpath": "node_parser/text/semantic_double_merging_splitter.py",
    "start_line": 1,
    "end_line": 369,
    "length": 369,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "load_model",
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "build_semantic_nodes_from_documents",
      "_create_initial_chunks",
      "_merge_initial_chunks",
      "_clean_text_advanced"
    ],
    "chunk_class_names": [
      "LanguageConfig",
      "SemanticDoubleMergingSplitterNodeParser"
    ],
    "document_function_names": [
      "__init__",
      "load_model",
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "build_semantic_nodes_from_documents",
      "_create_initial_chunks",
      "_merge_initial_chunks",
      "_clean_text_advanced"
    ],
    "document_class_names": [
      "LanguageConfig",
      "SemanticDoubleMergingSplitterNodeParser"
    ],
    "content": "import re\nimport string\nfrom typing import Any, Callable, Dict, List, Optional, Sequence\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser import NodeParser\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.node_utils import (\n    build_nodes_from_splits,\n    default_id_func,\n)\nfrom llama_index.core.node_parser.text.utils import split_by_sentence_tokenizer\nfrom llama_index.core.schema import BaseNode, Document\nfrom llama_index.core.utils import get_tqdm_iterable\n\nDEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\n\n# TODO test more languages\nLANGUAGES: List[str] = [\"english\", \"german\", \"spanish\"]\nLANGUAGE_MODELS: Dict[str, List[str]] = {\n    \"english\": [\"en_core_web_md\", \"en_core_web_lg\"],\n    \"german\": [\"de_core_news_md\", \"de_core_news_lg\"],\n    \"spanish\": [\"es_core_news_md\", \"es_core_news_lg\"],\n}\n\n\nclass LanguageConfig:\n    def __init__(\n        self,\n        language: str = \"english\",\n        spacy_model: str = \"en_core_web_md\",\n        model_validation: bool = True,\n    ):\n        if language not in LANGUAGES:\n            raise ValueError(\n                f\"{language} language is not supported yet! Available languages: {LANGUAGES}\"\n            )\n\n        if spacy_model not in LANGUAGE_MODELS[language] and model_validation:\n            raise ValueError(\n                f\"{spacy_model} model is not matching your language: {language}\"\n            )\n\n        self.language = language\n        self.spacy_model = spacy_model\n        self.nlp = None\n        self.stopwords: List[str] = []\n\n    def load_model(self) -> None:\n        try:\n            import spacy\n        except ImportError:\n            raise ImportError(\n                \"Spacy is not installed, please install it with `pip install spacy`.\"\n            )\n        self.nlp = spacy.load(self.spacy_model)  # type: ignore\n        self.stopwords = set(stopwords.words(self.language))  # type: ignore\n\n\nclass SemanticDoubleMergingSplitterNodeParser(NodeParser):\n    \"\"\"Semantic double merging text splitter.\n\n    Splits a document into Nodes, with each node being a group of semantically related sentences.\n\n    Args:\n        language_config (LanguageConfig): chooses language and spacy language model to be used\n        initial_threshold (float): sets threshold for initializing new chunk\n        appending_threshold (float): sets threshold for appending new sentences to chunk\n        merging_threshold (float): sets threshold for merging whole chunks\n        max_chunk_size (int): maximum size of chunk (in characters)\n        merging_range (int): How many chunks 'ahead' beyond the nearest neighbor to be merged if similar (1 or 2 available)\n        sentence_splitter (Optional[Callable]): splits text into sentences\n    \"\"\"\n\n    language_config: LanguageConfig = Field(\n        default=LanguageConfig(),\n        description=\"Config that selects language and spacy model for chunking\",\n    )\n\n    initial_threshold: float = Field(\n        default=0.6,\n        description=(\n            \"The value of semantic similarity that must be exceeded between two\"\n            \"sentences to create a new chunk.  The bigger this \"\n            \"value is, the more nodes will be generated. Range is from 0 to 1.\"\n        ),\n    )\n\n    appending_threshold: float = Field(\n        default=0.8,\n        description=(\n            \"The value of semantic similarity that must be exceeded between a \"\n            \"chunk and new sentence to add this sentence to existing chunk.  The bigger this \"\n            \"value is, the more nodes will be generated. Range is from 0 to 1.\"\n        ),\n    )\n\n    merging_threshold: float = Field(\n        default=0.8,\n        description=(\n            \"The value of semantic similarity that must be exceeded between two chunks \"\n            \"to form a bigger chunk.  The bigger this value is,\"\n            \"the more nodes will be generated. Range is from 0 to 1.\"\n        ),\n    )\n\n    max_chunk_size: int = Field(\n        default=1000,\n        description=\"Maximum length of chunk that can be subjected to verification (number of characters)\",\n    )\n\n    merging_range: int = Field(\n        default=1,\n        description=(\n            \"How many chunks 'ahead' beyond the nearest neighbor\"\n            \"should the algorithm check during the second pass\"\n            \"(possible options are 1 or 2\"\n        ),\n    )\n\n    sentence_splitter: Callable[[str], List[str]] = Field(\n        default_factory=split_by_sentence_tokenizer,\n        description=\"The text splitter to use when splitting documents.\",\n        exclude=True,\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SemanticDoubleMergingSplitterNodeParser\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        language_config: Optional[LanguageConfig] = LanguageConfig(),\n        initial_threshold: Optional[float] = 0.6,\n        appending_threshold: Optional[float] = 0.8,\n        merging_threshold: Optional[float] = 0.8,\n        max_chunk_size: Optional[int] = 1000,\n        merging_range: Optional[int] = 1,\n        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\n        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        id_func: Optional[Callable[[int, Document], str]] = None,\n    ) -> \"SemanticDoubleMergingSplitterNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        sentence_splitter = sentence_splitter or split_by_sentence_tokenizer()\n\n        id_func = id_func or default_id_func\n\n        return cls(\n            language_config=language_config,\n            initial_threshold=initial_threshold,\n            appending_threshold=appending_threshold,\n            merging_threshold=merging_threshold,\n            max_chunk_size=max_chunk_size,\n            merging_range=merging_range,\n            sentence_splitter=sentence_splitter,\n            original_text_metadata_key=original_text_metadata_key,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            callback_manager=callback_manager,\n            id_func=id_func,\n        )\n\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Parse document into nodes.\"\"\"\n        # Load model\n        self.language_config.load_model()\n\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = self.build_semantic_nodes_from_documents([node])\n            all_nodes.extend(nodes)\n        return all_nodes\n\n    def build_semantic_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n    ) -> List[BaseNode]:\n        \"\"\"Build window nodes from documents.\"\"\"\n        all_nodes: List[BaseNode] = []\n\n        for doc in documents:\n            text = doc.text\n            sentences = self.sentence_splitter(text)\n            sentences = [s.strip() for s in sentences]\n            initial_chunks = self._create_initial_chunks(sentences)\n            chunks = self._merge_initial_chunks(initial_chunks)\n\n            nodes = build_nodes_from_splits(\n                chunks,\n                doc,\n                id_func=self.id_func,\n            )\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    def _create_initial_chunks(self, sentences: List[str]) -> List[str]:\n        initial_chunks: List[str] = []\n        chunk = sentences[0]  # \"\"\n        new = True\n\n        assert self.language_config.nlp is not None\n\n        for sentence in sentences[1:]:\n            if new:\n                # check if 2 sentences got anything in common\n\n                if (\n                    self.language_config.nlp(\n                        self._clean_text_advanced(chunk)\n                    ).similarity(\n                        self.language_config.nlp(self._clean_text_advanced(sentence))\n                    )\n                    < self.initial_threshold\n                    and len(chunk) + len(sentence) + 1 <= self.max_chunk_size\n                ):\n                    # if not then leave first sentence as separate chunk\n                    initial_chunks.append(chunk)\n                    chunk = sentence\n                    continue\n\n                chunk_sentences = [chunk]\n                if len(chunk) + len(sentence) + 1 <= self.max_chunk_size:\n                    chunk_sentences.append(sentence)\n                    chunk = \" \".join(chunk_sentences)\n                    new = False\n                else:\n                    new = True\n                    initial_chunks.append(chunk)\n                    chunk = sentence\n                    continue\n                last_sentences = \" \".join(chunk_sentences[-2:])\n                # new = False\n\n            elif (\n                self.language_config.nlp(\n                    self._clean_text_advanced(last_sentences)\n                ).similarity(\n                    self.language_config.nlp(self._clean_text_advanced(sentence))\n                )\n                > self.appending_threshold\n                and len(last_sentences) + len(sentence) + 1 <= self.max_chunk_size\n                # and not len(chunk) > self.max_chunk_size\n            ):\n                # elif nlp(last_sentences).similarity(nlp(sentence)) > self.threshold:\n                chunk_sentences.append(sentence)\n                last_sentences = \" \".join(chunk_sentences[-2:])\n                chunk += \" \" + sentence\n            else:\n                initial_chunks.append(chunk)\n                chunk = sentence  # \"\"\n                new = True\n        initial_chunks.append(chunk)\n\n        return initial_chunks\n\n    def _merge_initial_chunks(self, initial_chunks: List[str]) -> List[str]:\n        chunks: List[str] = []\n        skip = 0\n        current = initial_chunks[0]\n\n        assert self.language_config.nlp is not None\n\n        # TODO avoid connecting 1st chunk with 3rd if 2nd one is above some value, or if its length is above some value\n\n        for i in range(1, len(initial_chunks)):\n            # avoid connecting same chunk multiple times\n            if skip > 0:\n                skip -= 1\n                continue\n\n            current_nlp = self.language_config.nlp(self._clean_text_advanced(current))\n\n            if len(current) >= self.max_chunk_size:\n                chunks.append(current)\n                current = initial_chunks[i]\n\n            # check if 1st and 2nd chunk should be connected\n            elif (\n                current_nlp.similarity(\n                    self.language_config.nlp(\n                        self._clean_text_advanced(initial_chunks[i])\n                    )\n                )\n                > self.merging_threshold\n                and len(current) + len(initial_chunks[i]) + 1 <= self.max_chunk_size\n            ):\n                current += \" \" + initial_chunks[i]\n\n            # check if 1st and 3rd chunk are similar, if yes then merge 1st, 2nd, 3rd together\n            elif (\n                i <= len(initial_chunks) - 2\n                and current_nlp.similarity(\n                    self.language_config.nlp(\n                        self._clean_text_advanced(initial_chunks[i + 1])\n                    )\n                )\n                > self.merging_threshold\n                and len(current)\n                + len(initial_chunks[i])\n                + len(initial_chunks[i + 1])\n                + 2\n                <= self.max_chunk_size\n            ):\n                current += \" \" + initial_chunks[i] + \" \" + initial_chunks[i + 1]\n                skip = 1\n\n            # check if 1st and 4th chunk are smilar, if yes then merge 1st, 2nd, 3rd and 4th together\n            elif (\n                i < len(initial_chunks) - 2\n                and current_nlp.similarity(\n                    self.language_config.nlp(\n                        self._clean_text_advanced(initial_chunks[i + 2])\n                    )\n                )\n                > self.merging_threshold\n                and self.merging_range == 2\n                and len(current)\n                + len(initial_chunks[i])\n                + len(initial_chunks[i + 1])\n                + len(initial_chunks[i + 2])\n                + 3\n                <= self.max_chunk_size\n            ):\n                current += (\n                    \" \"\n                    + initial_chunks[i]\n                    + \" \"\n                    + initial_chunks[i + 1]\n                    + \" \"\n                    + initial_chunks[i + 2]\n                )\n                skip = 2\n\n            else:\n                chunks.append(current)\n                current = initial_chunks[i]\n\n        chunks.append(current)\n        return chunks\n\n    def _clean_text_advanced(self, text: str) -> str:\n        text = text.lower()\n        # Remove urls\n        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n        # Remove punctuations\n        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n        # Remove stopwords\n        tokens = word_tokenize(text)\n        filtered_words = [w for w in tokens if w not in self.language_config.stopwords]\n\n        return \" \".join(filtered_words)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/sentence_window.py",
    "filename": "sentence_window.py",
    "relpath": "node_parser/text/sentence_window.py",
    "start_line": 1,
    "end_line": 138,
    "length": 138,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "build_window_nodes_from_documents"
    ],
    "chunk_class_names": [
      "SentenceWindowNodeParser"
    ],
    "document_function_names": [
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "build_window_nodes_from_documents"
    ],
    "document_class_names": [
      "SentenceWindowNodeParser"
    ],
    "content": "\"\"\"Simple node parser.\"\"\"\n\nfrom typing import Any, Callable, List, Optional, Sequence\n\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.node_utils import (\n    build_nodes_from_splits,\n    default_id_func,\n)\nfrom llama_index.core.node_parser.text.utils import split_by_sentence_tokenizer\nfrom llama_index.core.schema import BaseNode, Document\nfrom llama_index.core.utils import get_tqdm_iterable\n\nDEFAULT_WINDOW_SIZE = 3\nDEFAULT_WINDOW_METADATA_KEY = \"window\"\nDEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\n\n\nclass SentenceWindowNodeParser(NodeParser):\n    \"\"\"Sentence window node parser.\n\n    Splits a document into Nodes, with each node being a sentence.\n    Each node contains a window from the surrounding sentences in the metadata.\n\n    Args:\n        sentence_splitter (Optional[Callable]): splits text into sentences\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n    \"\"\"\n\n    sentence_splitter: Callable[[str], List[str]] = Field(\n        default_factory=split_by_sentence_tokenizer,\n        description=\"The text splitter to use when splitting documents.\",\n        exclude=True,\n    )\n    window_size: int = Field(\n        default=DEFAULT_WINDOW_SIZE,\n        description=\"The number of sentences on each side of a sentence to capture.\",\n        gt=0,\n    )\n    window_metadata_key: str = Field(\n        default=DEFAULT_WINDOW_METADATA_KEY,\n        description=\"The metadata key to store the sentence window under.\",\n    )\n    original_text_metadata_key: str = Field(\n        default=DEFAULT_OG_TEXT_METADATA_KEY,\n        description=\"The metadata key to store the original sentence in.\",\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SentenceWindowNodeParser\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\n        window_size: int = DEFAULT_WINDOW_SIZE,\n        window_metadata_key: str = DEFAULT_WINDOW_METADATA_KEY,\n        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        id_func: Optional[Callable[[int, Document], str]] = None,\n    ) -> \"SentenceWindowNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        sentence_splitter = sentence_splitter or split_by_sentence_tokenizer()\n\n        id_func = id_func or default_id_func\n\n        return cls(\n            sentence_splitter=sentence_splitter,\n            window_size=window_size,\n            window_metadata_key=window_metadata_key,\n            original_text_metadata_key=original_text_metadata_key,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            callback_manager=callback_manager,\n            id_func=id_func,\n        )\n\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Parse document into nodes.\"\"\"\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = self.build_window_nodes_from_documents([node])\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    def build_window_nodes_from_documents(\n        self, documents: Sequence[Document]\n    ) -> List[BaseNode]:\n        \"\"\"Build window nodes from documents.\"\"\"\n        all_nodes: List[BaseNode] = []\n        for doc in documents:\n            text = doc.text\n            text_splits = self.sentence_splitter(text)\n            nodes = build_nodes_from_splits(\n                text_splits,\n                doc,\n                id_func=self.id_func,\n            )\n\n            # add window to each node\n            for i, node in enumerate(nodes):\n                window_nodes = nodes[\n                    max(0, i - self.window_size) : min(\n                        i + self.window_size + 1, len(nodes)\n                    )\n                ]\n\n                node.metadata[self.window_metadata_key] = \" \".join(\n                    [n.text for n in window_nodes]\n                )\n                node.metadata[self.original_text_metadata_key] = node.text\n\n                # exclude window metadata from embed and llm\n                node.excluded_embed_metadata_keys.extend(\n                    [self.window_metadata_key, self.original_text_metadata_key]\n                )\n                node.excluded_llm_metadata_keys.extend(\n                    [self.window_metadata_key, self.original_text_metadata_key]\n                )\n\n            all_nodes.extend(nodes)\n\n        return all_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/semantic_splitter.py",
    "filename": "semantic_splitter.py",
    "relpath": "node_parser/text/semantic_splitter.py",
    "start_line": 1,
    "end_line": 309,
    "length": 309,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "_aparse_nodes",
      "build_semantic_nodes_from_documents",
      "abuild_semantic_nodes_from_documents",
      "_build_sentence_groups",
      "_calculate_distances_between_sentence_groups",
      "_build_node_chunks"
    ],
    "chunk_class_names": [
      "SentenceCombination",
      "SemanticSplitterNodeParser"
    ],
    "document_function_names": [
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "_aparse_nodes",
      "build_semantic_nodes_from_documents",
      "abuild_semantic_nodes_from_documents",
      "_build_sentence_groups",
      "_calculate_distances_between_sentence_groups",
      "_build_node_chunks"
    ],
    "document_class_names": [
      "SentenceCombination",
      "SemanticSplitterNodeParser"
    ],
    "content": "from typing import Any, Callable, List, Optional, Sequence, TypedDict\nfrom typing_extensions import Annotated\n\nimport numpy as np\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.bridge.pydantic import Field, SerializeAsAny, WithJsonSchema\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser import NodeParser\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.node_utils import (\n    build_nodes_from_splits,\n    default_id_func,\n)\nfrom llama_index.core.node_parser.text.utils import split_by_sentence_tokenizer\nfrom llama_index.core.schema import BaseNode, Document\nfrom llama_index.core.utils import get_tqdm_iterable\n\nDEFAULT_OG_TEXT_METADATA_KEY = \"original_text\"\n\n\nclass SentenceCombination(TypedDict):\n    sentence: str\n    index: int\n    combined_sentence: str\n    combined_sentence_embedding: List[float]\n\n\nSentenceSplitterCallable = Annotated[\n    Callable[[str], List[str]],\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n]\n\n\nclass SemanticSplitterNodeParser(NodeParser):\n    \"\"\"Semantic node parser.\n\n    Splits a document into Nodes, with each node being a group of semantically related sentences.\n\n    Args:\n        buffer_size (int): number of sentences to group together when evaluating semantic similarity\n        embed_model: (BaseEmbedding): embedding model to use\n        sentence_splitter (Optional[Callable]): splits text into sentences\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n    \"\"\"\n\n    sentence_splitter: SentenceSplitterCallable = Field(\n        default_factory=split_by_sentence_tokenizer,\n        description=\"The text splitter to use when splitting documents.\",\n        exclude=True,\n    )\n\n    embed_model: SerializeAsAny[BaseEmbedding] = Field(\n        description=\"The embedding model to use to for semantic comparison\",\n    )\n\n    buffer_size: int = Field(\n        default=1,\n        description=(\n            \"The number of sentences to group together when evaluating semantic similarity. \"\n            \"Set to 1 to consider each sentence individually. \"\n            \"Set to >1 to group sentences together.\"\n        ),\n    )\n\n    breakpoint_percentile_threshold: int = Field(\n        default=95,\n        description=(\n            \"The percentile of cosine dissimilarity that must be exceeded between a \"\n            \"group of sentences and the next to form a node.  The smaller this \"\n            \"number is, the more nodes will be generated\"\n        ),\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SemanticSplitterNodeParser\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        embed_model: Optional[BaseEmbedding] = None,\n        breakpoint_percentile_threshold: Optional[int] = 95,\n        buffer_size: Optional[int] = 1,\n        sentence_splitter: Optional[Callable[[str], List[str]]] = None,\n        original_text_metadata_key: str = DEFAULT_OG_TEXT_METADATA_KEY,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        id_func: Optional[Callable[[int, Document], str]] = None,\n    ) -> \"SemanticSplitterNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        sentence_splitter = sentence_splitter or split_by_sentence_tokenizer()\n        if embed_model is None:\n            try:\n                from llama_index.embeddings.openai import (\n                    OpenAIEmbedding,\n                )  # pants: no-infer-dep\n\n                embed_model = embed_model or OpenAIEmbedding()\n            except ImportError:\n                raise ImportError(\n                    \"`llama-index-embeddings-openai` package not found, \"\n                    \"please run `pip install llama-index-embeddings-openai`\"\n                )\n\n        id_func = id_func or default_id_func\n\n        return cls(\n            embed_model=embed_model,\n            breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n            buffer_size=buffer_size,\n            sentence_splitter=sentence_splitter,\n            original_text_metadata_key=original_text_metadata_key,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            callback_manager=callback_manager,\n            id_func=id_func,\n        )\n\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Parse document into nodes.\"\"\"\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = self.build_semantic_nodes_from_documents([node], show_progress)\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    async def _aparse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Asynchronously parse document into nodes.\"\"\"\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = await self.abuild_semantic_nodes_from_documents(\n                [node], show_progress\n            )\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    def build_semantic_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n        show_progress: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"Build window nodes from documents.\"\"\"\n        all_nodes: List[BaseNode] = []\n        for doc in documents:\n            text = doc.text\n            text_splits = self.sentence_splitter(text)\n\n            sentences = self._build_sentence_groups(text_splits)\n\n            combined_sentence_embeddings = self.embed_model.get_text_embedding_batch(\n                [s[\"combined_sentence\"] for s in sentences],\n                show_progress=show_progress,\n            )\n\n            for i, embedding in enumerate(combined_sentence_embeddings):\n                sentences[i][\"combined_sentence_embedding\"] = embedding\n\n            distances = self._calculate_distances_between_sentence_groups(sentences)\n\n            chunks = self._build_node_chunks(sentences, distances)\n\n            nodes = build_nodes_from_splits(\n                chunks,\n                doc,\n                id_func=self.id_func,\n            )\n\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    async def abuild_semantic_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n        show_progress: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"Asynchronously build window nodes from documents.\"\"\"\n        all_nodes: List[BaseNode] = []\n        for doc in documents:\n            text = doc.text\n            text_splits = self.sentence_splitter(text)\n\n            sentences = self._build_sentence_groups(text_splits)\n\n            combined_sentence_embeddings = (\n                await self.embed_model.aget_text_embedding_batch(\n                    [s[\"combined_sentence\"] for s in sentences],\n                    show_progress=show_progress,\n                )\n            )\n\n            for i, embedding in enumerate(combined_sentence_embeddings):\n                sentences[i][\"combined_sentence_embedding\"] = embedding\n\n            distances = self._calculate_distances_between_sentence_groups(sentences)\n\n            chunks = self._build_node_chunks(sentences, distances)\n\n            nodes = build_nodes_from_splits(\n                chunks,\n                doc,\n                id_func=self.id_func,\n            )\n\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    def _build_sentence_groups(\n        self, text_splits: List[str]\n    ) -> List[SentenceCombination]:\n        sentences: List[SentenceCombination] = [\n            {\n                \"sentence\": x,\n                \"index\": i,\n                \"combined_sentence\": \"\",\n                \"combined_sentence_embedding\": [],\n            }\n            for i, x in enumerate(text_splits)\n        ]\n\n        # Group sentences and calculate embeddings for sentence groups\n        for i in range(len(sentences)):\n            combined_sentence = \"\"\n\n            for j in range(i - self.buffer_size, i):\n                if j >= 0:\n                    combined_sentence += sentences[j][\"sentence\"]\n\n            combined_sentence += sentences[i][\"sentence\"]\n\n            for j in range(i + 1, i + 1 + self.buffer_size):\n                if j < len(sentences):\n                    combined_sentence += sentences[j][\"sentence\"]\n\n            sentences[i][\"combined_sentence\"] = combined_sentence\n\n        return sentences\n\n    def _calculate_distances_between_sentence_groups(\n        self, sentences: List[SentenceCombination]\n    ) -> List[float]:\n        distances = []\n        for i in range(len(sentences) - 1):\n            embedding_current = sentences[i][\"combined_sentence_embedding\"]\n            embedding_next = sentences[i + 1][\"combined_sentence_embedding\"]\n\n            similarity = self.embed_model.similarity(embedding_current, embedding_next)\n\n            distance = 1 - similarity\n\n            distances.append(distance)\n\n        return distances\n\n    def _build_node_chunks(\n        self, sentences: List[SentenceCombination], distances: List[float]\n    ) -> List[str]:\n        chunks = []\n        if len(distances) > 0:\n            breakpoint_distance_threshold = np.percentile(\n                distances, self.breakpoint_percentile_threshold\n            )\n\n            indices_above_threshold = [\n                i for i, x in enumerate(distances) if x > breakpoint_distance_threshold\n            ]\n\n            # Chunk sentences into semantic groups based on percentile breakpoints\n            start_index = 0\n\n            for index in indices_above_threshold:\n                group = sentences[start_index : index + 1]\n                combined_text = \"\".join([d[\"sentence\"] for d in group])\n                chunks.append(combined_text)\n\n                start_index = index + 1\n\n            if start_index < len(sentences):\n                combined_text = \"\".join(\n                    [d[\"sentence\"] for d in sentences[start_index:]]\n                )\n                chunks.append(combined_text)\n        else:\n            # If, for some reason we didn't get any distances (i.e. very, very small documents) just\n            # treat the whole document as a single node\n            chunks = [\" \".join([s[\"sentence\"] for s in sentences])]\n\n        return chunks"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/code.py",
    "filename": "code.py",
    "relpath": "node_parser/text/code.py",
    "start_line": 1,
    "end_line": 164,
    "length": 164,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "class_name",
      "_chunk_node",
      "split_text"
    ],
    "chunk_class_names": [
      "CodeSplitter"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "class_name",
      "_chunk_node",
      "split_text"
    ],
    "document_class_names": [
      "CodeSplitter"
    ],
    "content": "\"\"\"Code splitter.\"\"\"\n\nfrom typing import Any, Callable, List, Optional\n\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.node_parser.interface import TextSplitter\nfrom llama_index.core.node_parser.node_utils import default_id_func\nfrom llama_index.core.schema import Document\n\nDEFAULT_CHUNK_LINES = 40\nDEFAULT_LINES_OVERLAP = 15\nDEFAULT_MAX_CHARS = 1500\n\n\nclass CodeSplitter(TextSplitter):\n    \"\"\"Split code using a AST parser.\n\n    Thank you to Kevin Lu / SweepAI for suggesting this elegant code splitting solution.\n    https://docs.sweep.dev/blogs/chunking-2m-files\n    \"\"\"\n\n    language: str = Field(\n        description=\"The programming language of the code being split.\"\n    )\n    chunk_lines: int = Field(\n        default=DEFAULT_CHUNK_LINES,\n        description=\"The number of lines to include in each chunk.\",\n        gt=0,\n    )\n    chunk_lines_overlap: int = Field(\n        default=DEFAULT_LINES_OVERLAP,\n        description=\"How many lines of code each chunk overlaps with.\",\n        gt=0,\n    )\n    max_chars: int = Field(\n        default=DEFAULT_MAX_CHARS,\n        description=\"Maximum number of characters per chunk.\",\n        gt=0,\n    )\n    _parser: Any = PrivateAttr()\n\n    def __init__(\n        self,\n        language: str,\n        chunk_lines: int = DEFAULT_CHUNK_LINES,\n        chunk_lines_overlap: int = DEFAULT_LINES_OVERLAP,\n        max_chars: int = DEFAULT_MAX_CHARS,\n        parser: Any = None,\n        callback_manager: Optional[CallbackManager] = None,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        id_func: Optional[Callable[[int, Document], str]] = None,\n    ) -> None:\n        \"\"\"Initialize a CodeSplitter.\"\"\"\n        from tree_sitter import Parser  # pants: no-infer-dep\n\n        callback_manager = callback_manager or CallbackManager([])\n        id_func = id_func or default_id_func\n\n        super().__init__(\n            language=language,\n            chunk_lines=chunk_lines,\n            chunk_lines_overlap=chunk_lines_overlap,\n            max_chars=max_chars,\n            callback_manager=callback_manager,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            id_func=id_func,\n        )\n\n        if parser is None:\n            try:\n                import tree_sitter_languages  # pants: no-infer-dep\n\n                parser = tree_sitter_languages.get_parser(language)\n            except ImportError:\n                raise ImportError(\n                    \"Please install tree_sitter_languages to use CodeSplitter.\"\n                    \"Or pass in a parser object.\"\n                )\n            except Exception:\n                print(\n                    f\"Could not get parser for language {language}. Check \"\n                    \"https://github.com/grantjenks/py-tree-sitter-languages#license \"\n                    \"for a list of valid languages.\"\n                )\n                raise\n        if not isinstance(parser, Parser):\n            raise ValueError(\"Parser must be a tree-sitter Parser object.\")\n\n        self._parser = parser\n\n    @classmethod\n    def from_defaults(\n        cls,\n        language: str,\n        chunk_lines: int = DEFAULT_CHUNK_LINES,\n        chunk_lines_overlap: int = DEFAULT_LINES_OVERLAP,\n        max_chars: int = DEFAULT_MAX_CHARS,\n        callback_manager: Optional[CallbackManager] = None,\n        parser: Any = None,\n    ) -> \"CodeSplitter\":\n        \"\"\"Create a CodeSplitter with default values.\"\"\"\n        return cls(\n            language=language,\n            chunk_lines=chunk_lines,\n            chunk_lines_overlap=chunk_lines_overlap,\n            max_chars=max_chars,\n            callback_manager=callback_manager,\n            parser=parser,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"CodeSplitter\"\n\n    def _chunk_node(self, node: Any, text: str, last_end: int = 0) -> List[str]:\n        new_chunks = []\n        current_chunk = \"\"\n        for child in node.children:\n            if child.end_byte - child.start_byte > self.max_chars:\n                # Child is too big, recursively chunk the child\n                if len(current_chunk) > 0:\n                    new_chunks.append(current_chunk)\n                current_chunk = \"\"\n                new_chunks.extend(self._chunk_node(child, text, last_end))\n            elif (\n                len(current_chunk) + child.end_byte - child.start_byte > self.max_chars\n            ):\n                # Child would make the current chunk too big, so start a new chunk\n                new_chunks.append(current_chunk)\n                current_chunk = text[last_end : child.end_byte]\n            else:\n                current_chunk += text[last_end : child.end_byte]\n            last_end = child.end_byte\n        if len(current_chunk) > 0:\n            new_chunks.append(current_chunk)\n        return new_chunks\n\n    def split_text(self, text: str) -> List[str]:\n        \"\"\"Split incoming code and return chunks using the AST.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: [text]}\n        ) as event:\n            tree = self._parser.parse(bytes(text, \"utf-8\"))\n\n            if (\n                not tree.root_node.children\n                or tree.root_node.children[0].type != \"ERROR\"\n            ):\n                chunks = [\n                    chunk.strip() for chunk in self._chunk_node(tree.root_node, text)\n                ]\n                event.on_end(\n                    payload={EventPayload.CHUNKS: chunks},\n                )\n\n                return chunks\n            else:\n                raise ValueError(f\"Could not parse code with language {self.language}.\")\n\n        # TODO: set up auto-language detection using something like https://github.com/yoeo/guesslang."
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/langchain.py",
    "filename": "langchain.py",
    "relpath": "node_parser/text/langchain.py",
    "start_line": 1,
    "end_line": 45,
    "length": 45,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "split_text"
    ],
    "chunk_class_names": [
      "LangchainNodeParser"
    ],
    "document_function_names": [
      "__init__",
      "split_text"
    ],
    "document_class_names": [
      "LangchainNodeParser"
    ],
    "content": "from typing import TYPE_CHECKING, Callable, List, Optional\n\nfrom llama_index.core.bridge.pydantic import PrivateAttr\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.node_parser.interface import TextSplitter\nfrom llama_index.core.node_parser.node_utils import default_id_func\nfrom llama_index.core.schema import Document\n\nif TYPE_CHECKING:\n    from langchain.text_splitter import (\n        TextSplitter as LC_TextSplitter,\n    )  # pants: no-infer-dep\n\n\nclass LangchainNodeParser(TextSplitter):\n    \"\"\"\n    Basic wrapper around langchain's text splitter.\n\n    TODO: Figure out how to make this metadata aware.\n    \"\"\"\n\n    _lc_splitter: \"LC_TextSplitter\" = PrivateAttr()\n\n    def __init__(\n        self,\n        lc_splitter: \"LC_TextSplitter\",\n        callback_manager: Optional[CallbackManager] = None,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        id_func: Optional[Callable[[int, Document], str]] = None,\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        id_func = id_func or default_id_func\n\n        super().__init__(\n            callback_manager=callback_manager or CallbackManager(),\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            id_func=id_func,\n        )\n        self._lc_splitter = lc_splitter\n\n    def split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into sentences.\"\"\"\n        return self._lc_splitter.split_text(text)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/__init__.py",
    "filename": "__init__.py",
    "relpath": "node_parser/text/__init__.py",
    "start_line": 1,
    "end_line": 26,
    "length": 26,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.node_parser.text.code import CodeSplitter\nfrom llama_index.core.node_parser.text.langchain import LangchainNodeParser\nfrom llama_index.core.node_parser.text.semantic_splitter import (\n    SemanticSplitterNodeParser,\n)\nfrom llama_index.core.node_parser.text.sentence import SentenceSplitter\nfrom llama_index.core.node_parser.text.sentence_window import (\n    SentenceWindowNodeParser,\n)\nfrom llama_index.core.node_parser.text.semantic_double_merging_splitter import (\n    SemanticDoubleMergingSplitterNodeParser,\n    LanguageConfig,\n)\nfrom llama_index.core.node_parser.text.token import TokenTextSplitter\n\n\n__all__ = [\n    \"CodeSplitter\",\n    \"LangchainNodeParser\",\n    \"SemanticSplitterNodeParser\",\n    \"SentenceSplitter\",\n    \"SentenceWindowNodeParser\",\n    \"TokenTextSplitter\",\n    \"SemanticDoubleMergingSplitterNodeParser\",\n    \"LanguageConfig\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/utils.py",
    "filename": "utils.py",
    "relpath": "node_parser/text/utils.py",
    "start_line": 1,
    "end_line": 77,
    "length": 77,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "truncate_text",
      "split_text_keep_separator",
      "split_by_sep",
      "split_by_char",
      "split_by_sentence_tokenizer_internal",
      "split_by_sentence_tokenizer",
      "split_by_regex",
      "split_by_phrase_regex"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "truncate_text",
      "split_text_keep_separator",
      "split_by_sep",
      "split_by_char",
      "split_by_sentence_tokenizer_internal",
      "split_by_sentence_tokenizer",
      "split_by_regex",
      "split_by_phrase_regex"
    ],
    "document_class_names": [],
    "content": "import logging\nfrom typing import Any, Callable, List\n\nfrom llama_index.core.node_parser.interface import TextSplitter\n\nlogger = logging.getLogger(__name__)\n\n\ndef truncate_text(text: str, text_splitter: TextSplitter) -> str:\n    \"\"\"Truncate text to fit within the chunk size.\"\"\"\n    chunks = text_splitter.split_text(text)\n    return chunks[0]\n\n\ndef split_text_keep_separator(text: str, separator: str) -> List[str]:\n    \"\"\"Split text with separator and keep the separator at the end of each split.\"\"\"\n    parts = text.split(separator)\n    result = [separator + s if i > 0 else s for i, s in enumerate(parts)]\n    return [s for s in result if s]\n\n\ndef split_by_sep(sep: str, keep_sep: bool = True) -> Callable[[str], List[str]]:\n    \"\"\"Split text by separator.\"\"\"\n    if keep_sep:\n        return lambda text: split_text_keep_separator(text, sep)\n    else:\n        return lambda text: text.split(sep)\n\n\ndef split_by_char() -> Callable[[str], List[str]]:\n    \"\"\"Split text by character.\"\"\"\n    return lambda text: list(text)\n\n\ndef split_by_sentence_tokenizer_internal(text: str, tokenizer: Any) -> List[str]:\n    \"\"\"Get the spans and then return the sentences.\n\n    Using the start index of each span\n    Instead of using end, use the start of the next span if available\n    \"\"\"\n    spans = list(tokenizer.span_tokenize(text))\n    sentences = []\n    for i, span in enumerate(spans):\n        start = span[0]\n        if i < len(spans) - 1:\n            end = spans[i + 1][0]\n        else:\n            end = len(text)\n        sentences.append(text[start:end])\n    return sentences\n\n\ndef split_by_sentence_tokenizer() -> Callable[[str], List[str]]:\n    import nltk\n\n    tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n    return lambda text: split_by_sentence_tokenizer_internal(text, tokenizer)\n\n\ndef split_by_regex(regex: str) -> Callable[[str], List[str]]:\n    \"\"\"Split text by regex.\"\"\"\n    import re\n\n    return lambda text: re.findall(regex, text)\n\n\ndef split_by_phrase_regex() -> Callable[[str], List[str]]:\n    \"\"\"Split text by phrase regex.\n\n    This regular expression will split the sentences into phrases,\n    where each phrase is a sequence of one or more non-comma,\n    non-period, and non-semicolon characters, followed by an optional comma,\n    period, or semicolon. The regular expression will also capture the\n    delimiters themselves as separate items in the list of phrases.\n    \"\"\"\n    regex = \"[^,.;\u3002]+[,.;\u3002]?\"\n    return split_by_regex(regex)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/token.py",
    "filename": "token.py",
    "relpath": "node_parser/text/token.py",
    "start_line": 1,
    "end_line": 239,
    "length": 239,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "class_name",
      "split_text_metadata_aware",
      "split_text",
      "_split_text",
      "_split",
      "_merge"
    ],
    "chunk_class_names": [
      "TokenTextSplitter"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "class_name",
      "split_text_metadata_aware",
      "split_text",
      "_split_text",
      "_split",
      "_merge"
    ],
    "document_class_names": [
      "TokenTextSplitter"
    ],
    "content": "\"\"\"Token splitter.\"\"\"\n\nimport logging\nfrom typing import Callable, List, Optional\n\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.constants import DEFAULT_CHUNK_OVERLAP, DEFAULT_CHUNK_SIZE\nfrom llama_index.core.node_parser.interface import MetadataAwareTextSplitter\nfrom llama_index.core.node_parser.node_utils import default_id_func\nfrom llama_index.core.node_parser.text.utils import split_by_char, split_by_sep\nfrom llama_index.core.schema import Document\nfrom llama_index.core.utils import get_tokenizer\n\n_logger = logging.getLogger(__name__)\n\n# NOTE: this is the number of tokens we reserve for metadata formatting\nDEFAULT_METADATA_FORMAT_LEN = 2\n\n\nclass TokenTextSplitter(MetadataAwareTextSplitter):\n    \"\"\"Implementation of splitting text that looks at word tokens.\"\"\"\n\n    chunk_size: int = Field(\n        default=DEFAULT_CHUNK_SIZE,\n        description=\"The token chunk size for each chunk.\",\n        gt=0,\n    )\n    chunk_overlap: int = Field(\n        default=DEFAULT_CHUNK_OVERLAP,\n        description=\"The token overlap of each chunk when splitting.\",\n        ge=0,\n    )\n    separator: str = Field(\n        default=\" \", description=\"Default separator for splitting into words\"\n    )\n    backup_separators: List = Field(\n        default_factory=list, description=\"Additional separators for splitting.\"\n    )\n\n    keep_whitespaces: bool = Field(\n        default=False,\n        description=\"Whether to keep leading/trailing whitespaces in the chunk.\",\n    )\n\n    _tokenizer: Callable = PrivateAttr()\n    _split_fns: List[Callable] = PrivateAttr()\n\n    def __init__(\n        self,\n        chunk_size: int = DEFAULT_CHUNK_SIZE,\n        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n        tokenizer: Optional[Callable] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        separator: str = \" \",\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n        keep_whitespaces: bool = False,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        id_func: Optional[Callable[[int, Document], str]] = None,\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        callback_manager = callback_manager or CallbackManager([])\n        id_func = id_func or default_id_func\n        super().__init__(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            separator=separator,\n            backup_separators=backup_separators,\n            keep_whitespaces=keep_whitespaces,\n            callback_manager=callback_manager,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            id_func=id_func,\n        )\n        self._tokenizer = tokenizer or get_tokenizer()\n        all_seps = [separator] + (backup_separators or [])\n        self._split_fns = [split_by_sep(sep) for sep in all_seps] + [split_by_char()]\n\n    @classmethod\n    def from_defaults(\n        cls,\n        chunk_size: int = DEFAULT_CHUNK_SIZE,\n        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n        separator: str = \" \",\n        backup_separators: Optional[List[str]] = [\"\\n\"],\n        callback_manager: Optional[CallbackManager] = None,\n        keep_whitespaces: bool = False,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        id_func: Optional[Callable[[int, Document], str]] = None,\n    ) -> \"TokenTextSplitter\":\n        \"\"\"Initialize with default parameters.\"\"\"\n        callback_manager = callback_manager or CallbackManager([])\n        return cls(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            separator=separator,\n            backup_separators=backup_separators,\n            keep_whitespaces=keep_whitespaces,\n            callback_manager=callback_manager,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            id_func=id_func,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TokenTextSplitter\"\n\n    def split_text_metadata_aware(self, text: str, metadata_str: str) -> List[str]:\n        \"\"\"Split text into chunks, reserving space required for metadata str.\"\"\"\n        metadata_len = len(self._tokenizer(metadata_str)) + DEFAULT_METADATA_FORMAT_LEN\n        effective_chunk_size = self.chunk_size - metadata_len\n        if effective_chunk_size <= 0:\n            raise ValueError(\n                f\"Metadata length ({metadata_len}) is longer than chunk size \"\n                f\"({self.chunk_size}). Consider increasing the chunk size or \"\n                \"decreasing the size of your metadata to avoid this.\"\n            )\n        elif effective_chunk_size < 50:\n            print(\n                f\"Metadata length ({metadata_len}) is close to chunk size \"\n                f\"({self.chunk_size}). Resulting chunks are less than 50 tokens. \"\n                \"Consider increasing the chunk size or decreasing the size of \"\n                \"your metadata to avoid this.\",\n                flush=True,\n            )\n\n        return self._split_text(text, chunk_size=effective_chunk_size)\n\n    def split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into chunks.\"\"\"\n        return self._split_text(text, chunk_size=self.chunk_size)\n\n    def _split_text(self, text: str, chunk_size: int) -> List[str]:\n        \"\"\"Split text into chunks up to chunk_size.\"\"\"\n        if text == \"\":\n            return [text]\n\n        with self.callback_manager.event(\n            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: [text]}\n        ) as event:\n            splits = self._split(text, chunk_size)\n            chunks = self._merge(splits, chunk_size)\n\n            event.on_end(\n                payload={EventPayload.CHUNKS: chunks},\n            )\n\n        return chunks\n\n    def _split(self, text: str, chunk_size: int) -> List[str]:\n        \"\"\"Break text into splits that are smaller than chunk size.\n\n        The order of splitting is:\n        1. split by separator\n        2. split by backup separators (if any)\n        3. split by characters\n\n        NOTE: the splits contain the separators.\n        \"\"\"\n        if len(self._tokenizer(text)) <= chunk_size:\n            return [text]\n\n        for split_fn in self._split_fns:\n            splits = split_fn(text)\n            if len(splits) > 1:\n                break\n\n        new_splits = []\n        for split in splits:\n            split_len = len(self._tokenizer(split))\n            if split_len <= chunk_size:\n                new_splits.append(split)\n            else:\n                # recursively split\n                new_splits.extend(self._split(split, chunk_size=chunk_size))\n        return new_splits\n\n    def _merge(self, splits: List[str], chunk_size: int) -> List[str]:\n        \"\"\"Merge splits into chunks.\n\n        The high-level idea is to keep adding splits to a chunk until we\n        exceed the chunk size, then we start a new chunk with overlap.\n\n        When we start a new chunk, we pop off the first element of the previous\n        chunk until the total length is less than the chunk size.\n        \"\"\"\n        chunks: List[str] = []\n\n        cur_chunk: List[str] = []\n        cur_len = 0\n        for split in splits:\n            split_len = len(self._tokenizer(split))\n            if split_len > chunk_size:\n                _logger.warning(\n                    f\"Got a split of size {split_len}, \",\n                    f\"larger than chunk size {chunk_size}.\",\n                )\n\n            # if we exceed the chunk size after adding the new split, then\n            # we need to end the current chunk and start a new one\n            if cur_len + split_len > chunk_size:\n                # end the previous chunk\n                chunk = (\n                    \"\".join(cur_chunk)\n                    if self.keep_whitespaces\n                    else \"\".join(cur_chunk).strip()\n                )\n                if chunk:\n                    chunks.append(chunk)\n\n                # start a new chunk with overlap\n                # keep popping off the first element of the previous chunk until:\n                #   1. the current chunk length is less than chunk overlap\n                #   2. the total length is less than chunk size\n                while cur_len > self.chunk_overlap or cur_len + split_len > chunk_size:\n                    # pop off the first element\n                    first_chunk = cur_chunk.pop(0)\n                    cur_len -= len(self._tokenizer(first_chunk))\n\n            cur_chunk.append(split)\n            cur_len += split_len\n\n        # handle the last chunk\n        chunk = (\n            \"\".join(cur_chunk) if self.keep_whitespaces else \"\".join(cur_chunk).strip()\n        )\n        if chunk:\n            chunks.append(chunk)\n\n        return chunks"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/text/sentence.py",
    "filename": "sentence.py",
    "relpath": "node_parser/text/sentence.py",
    "start_line": 1,
    "end_line": 321,
    "length": 321,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "class_name",
      "split_text_metadata_aware",
      "split_text",
      "_split_text",
      "_split",
      "_merge",
      "close_chunk",
      "_postprocess_chunks",
      "_token_size",
      "_get_splits_by_fns"
    ],
    "chunk_class_names": [
      "from",
      "class",
      "SentenceSplitter",
      "tries"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "class_name",
      "split_text_metadata_aware",
      "split_text",
      "_split_text",
      "_split",
      "_merge",
      "close_chunk",
      "_postprocess_chunks",
      "_token_size",
      "_get_splits_by_fns"
    ],
    "document_class_names": [
      "from",
      "class",
      "SentenceSplitter",
      "tries"
    ],
    "content": "\"\"\"Sentence splitter.\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Optional, Tuple\n\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.constants import DEFAULT_CHUNK_SIZE\nfrom llama_index.core.node_parser.interface import (\n    MetadataAwareTextSplitter,\n)\nfrom llama_index.core.node_parser.node_utils import default_id_func\nfrom llama_index.core.node_parser.text.utils import (\n    split_by_char,\n    split_by_regex,\n    split_by_sentence_tokenizer,\n    split_by_sep,\n)\nfrom llama_index.core.utils import get_tokenizer\n\nSENTENCE_CHUNK_OVERLAP = 200\nCHUNKING_REGEX = \"[^,.;\u3002\uff1f\uff01]+[,.;\u3002\uff1f\uff01]?\"\nDEFAULT_PARAGRAPH_SEP = \"\\n\\n\\n\"\n\n\n@dataclass\nclass _Split:\n    text: str  # the split text\n    is_sentence: bool  # save whether this is a full sentence\n    token_size: int  # token length of split text\n\n\nclass SentenceSplitter(MetadataAwareTextSplitter):\n    \"\"\"Parse text with a preference for complete sentences.\n\n    In general, this class tries to keep sentences and paragraphs together. Therefore\n    compared to the original TokenTextSplitter, there are less likely to be\n    hanging sentences or parts of sentences at the end of the node chunk.\n    \"\"\"\n\n    chunk_size: int = Field(\n        default=DEFAULT_CHUNK_SIZE,\n        description=\"The token chunk size for each chunk.\",\n        gt=0,\n    )\n    chunk_overlap: int = Field(\n        default=SENTENCE_CHUNK_OVERLAP,\n        description=\"The token overlap of each chunk when splitting.\",\n        ge=0,\n    )\n    separator: str = Field(\n        default=\" \", description=\"Default separator for splitting into words\"\n    )\n    paragraph_separator: str = Field(\n        default=DEFAULT_PARAGRAPH_SEP, description=\"Separator between paragraphs.\"\n    )\n    secondary_chunking_regex: Optional[str] = Field(\n        default=CHUNKING_REGEX, description=\"Backup regex for splitting into sentences.\"\n    )\n\n    _chunking_tokenizer_fn: Callable[[str], List[str]] = PrivateAttr()\n    _tokenizer: Callable = PrivateAttr()\n    _split_fns: List[Callable] = PrivateAttr()\n    _sub_sentence_split_fns: List[Callable] = PrivateAttr()\n\n    def __init__(\n        self,\n        separator: str = \" \",\n        chunk_size: int = DEFAULT_CHUNK_SIZE,\n        chunk_overlap: int = SENTENCE_CHUNK_OVERLAP,\n        tokenizer: Optional[Callable] = None,\n        paragraph_separator: str = DEFAULT_PARAGRAPH_SEP,\n        chunking_tokenizer_fn: Optional[Callable[[str], List[str]]] = None,\n        secondary_chunking_regex: Optional[str] = CHUNKING_REGEX,\n        callback_manager: Optional[CallbackManager] = None,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        id_func: Optional[Callable] = None,\n    ):\n        \"\"\"Initialize with parameters.\"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        id_func = id_func or default_id_func\n        callback_manager = callback_manager or CallbackManager([])\n        super().__init__(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            secondary_chunking_regex=secondary_chunking_regex,\n            separator=separator,\n            paragraph_separator=paragraph_separator,\n            callback_manager=callback_manager,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            id_func=id_func,\n        )\n        self._chunking_tokenizer_fn = (\n            chunking_tokenizer_fn or split_by_sentence_tokenizer()\n        )\n        self._tokenizer = tokenizer or get_tokenizer()\n\n        self._split_fns = [\n            split_by_sep(paragraph_separator),\n            self._chunking_tokenizer_fn,\n        ]\n\n        if secondary_chunking_regex:\n            self._sub_sentence_split_fns = [\n                split_by_regex(secondary_chunking_regex),\n                split_by_sep(separator),\n                split_by_char(),\n            ]\n        else:\n            self._sub_sentence_split_fns = [\n                split_by_sep(separator),\n                split_by_char(),\n            ]\n\n    @classmethod\n    def from_defaults(\n        cls,\n        separator: str = \" \",\n        chunk_size: int = DEFAULT_CHUNK_SIZE,\n        chunk_overlap: int = SENTENCE_CHUNK_OVERLAP,\n        tokenizer: Optional[Callable] = None,\n        paragraph_separator: str = DEFAULT_PARAGRAPH_SEP,\n        chunking_tokenizer_fn: Optional[Callable[[str], List[str]]] = None,\n        secondary_chunking_regex: str = CHUNKING_REGEX,\n        callback_manager: Optional[CallbackManager] = None,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n    ) -> \"SentenceSplitter\":\n        \"\"\"Initialize with parameters.\"\"\"\n        callback_manager = callback_manager or CallbackManager([])\n        return cls(\n            separator=separator,\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            tokenizer=tokenizer,\n            paragraph_separator=paragraph_separator,\n            chunking_tokenizer_fn=chunking_tokenizer_fn,\n            secondary_chunking_regex=secondary_chunking_regex,\n            callback_manager=callback_manager,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SentenceSplitter\"\n\n    def split_text_metadata_aware(self, text: str, metadata_str: str) -> List[str]:\n        metadata_len = len(self._tokenizer(metadata_str))\n        effective_chunk_size = self.chunk_size - metadata_len\n        if effective_chunk_size <= 0:\n            raise ValueError(\n                f\"Metadata length ({metadata_len}) is longer than chunk size \"\n                f\"({self.chunk_size}). Consider increasing the chunk size or \"\n                \"decreasing the size of your metadata to avoid this.\"\n            )\n        elif effective_chunk_size < 50:\n            print(\n                f\"Metadata length ({metadata_len}) is close to chunk size \"\n                f\"({self.chunk_size}). Resulting chunks are less than 50 tokens. \"\n                \"Consider increasing the chunk size or decreasing the size of \"\n                \"your metadata to avoid this.\",\n                flush=True,\n            )\n\n        return self._split_text(text, chunk_size=effective_chunk_size)\n\n    def split_text(self, text: str) -> List[str]:\n        return self._split_text(text, chunk_size=self.chunk_size)\n\n    def _split_text(self, text: str, chunk_size: int) -> List[str]:\n        \"\"\"\n        _Split incoming text and return chunks with overlap size.\n\n        Has a preference for complete sentences, phrases, and minimal overlap.\n        \"\"\"\n        if text == \"\":\n            return [text]\n\n        with self.callback_manager.event(\n            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: [text]}\n        ) as event:\n            splits = self._split(text, chunk_size)\n            chunks = self._merge(splits, chunk_size)\n\n            event.on_end(payload={EventPayload.CHUNKS: chunks})\n\n        return chunks\n\n    def _split(self, text: str, chunk_size: int) -> List[_Split]:\n        r\"\"\"Break text into splits that are smaller than chunk size.\n\n        The order of splitting is:\n        1. split by paragraph separator\n        2. split by chunking tokenizer (default is nltk sentence tokenizer)\n        3. split by second chunking regex (default is \"[^,\\.;]+[,\\.;]?\")\n        4. split by default separator (\" \")\n\n        \"\"\"\n        token_size = self._token_size(text)\n        if token_size <= chunk_size:\n            return [_Split(text, is_sentence=True, token_size=token_size)]\n\n        text_splits_by_fns, is_sentence = self._get_splits_by_fns(text)\n\n        text_splits = []\n        for text_split_by_fns in text_splits_by_fns:\n            token_size = self._token_size(text_split_by_fns)\n            if token_size <= chunk_size:\n                text_splits.append(\n                    _Split(\n                        text_split_by_fns,\n                        is_sentence=is_sentence,\n                        token_size=token_size,\n                    )\n                )\n            else:\n                recursive_text_splits = self._split(\n                    text_split_by_fns, chunk_size=chunk_size\n                )\n                text_splits.extend(recursive_text_splits)\n        return text_splits\n\n    def _merge(self, splits: List[_Split], chunk_size: int) -> List[str]:\n        \"\"\"Merge splits into chunks.\"\"\"\n        chunks: List[str] = []\n        cur_chunk: List[Tuple[str, int]] = []  # list of (text, length)\n        last_chunk: List[Tuple[str, int]] = []\n        cur_chunk_len = 0\n        new_chunk = True\n\n        def close_chunk() -> None:\n            nonlocal chunks, cur_chunk, last_chunk, cur_chunk_len, new_chunk\n\n            chunks.append(\"\".join([text for text, length in cur_chunk]))\n            last_chunk = cur_chunk\n            cur_chunk = []\n            cur_chunk_len = 0\n            new_chunk = True\n\n            # add overlap to the next chunk using the last one first\n            # there is a small issue with this logic. If the chunk directly after\n            # the overlap is really big, then we could go over the chunk_size, and\n            # in theory the correct thing to do would be to remove some/all of the\n            # overlap. However, it would complicate the logic further without\n            # much real world benefit, so it's not implemented now.\n            if len(last_chunk) > 0:\n                last_index = len(last_chunk) - 1\n                while (\n                    last_index >= 0\n                    and cur_chunk_len + last_chunk[last_index][1] <= self.chunk_overlap\n                ):\n                    text, length = last_chunk[last_index]\n                    cur_chunk_len += length\n                    cur_chunk.insert(0, (text, length))\n                    last_index -= 1\n\n        while len(splits) > 0:\n            cur_split = splits[0]\n            if cur_split.token_size > chunk_size:\n                raise ValueError(\"Single token exceeded chunk size\")\n            if cur_chunk_len + cur_split.token_size > chunk_size and not new_chunk:\n                # if adding split to current chunk exceeds chunk size: close out chunk\n                close_chunk()\n            else:\n                if (\n                    cur_split.is_sentence\n                    or cur_chunk_len + cur_split.token_size <= chunk_size\n                    or new_chunk  # new chunk, always add at least one split\n                ):\n                    # add split to chunk\n                    cur_chunk_len += cur_split.token_size\n                    cur_chunk.append((cur_split.text, cur_split.token_size))\n                    splits.pop(0)\n                    new_chunk = False\n                else:\n                    # close out chunk\n                    close_chunk()\n\n        # handle the last chunk\n        if not new_chunk:\n            chunk = \"\".join([text for text, length in cur_chunk])\n            chunks.append(chunk)\n\n        # run postprocessing to remove blank spaces\n        return self._postprocess_chunks(chunks)\n\n    def _postprocess_chunks(self, chunks: List[str]) -> List[str]:\n        \"\"\"Post-process chunks.\n        Remove whitespace only chunks and remove leading and trailing whitespace.\n        \"\"\"\n        new_chunks = []\n        for chunk in chunks:\n            stripped_chunk = chunk.strip()\n            if stripped_chunk == \"\":\n                continue\n            new_chunks.append(stripped_chunk)\n        return new_chunks\n\n    def _token_size(self, text: str) -> int:\n        return len(self._tokenizer(text))\n\n    def _get_splits_by_fns(self, text: str) -> Tuple[List[str], bool]:\n        for split_fn in self._split_fns:\n            splits = split_fn(text)\n            if len(splits) > 1:\n                return splits, True\n\n        for split_fn in self._sub_sentence_split_fns:\n            splits = split_fn(text)\n            if len(splits) > 1:\n                break\n\n        return splits, False"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/file/markdown.py",
    "filename": "markdown.py",
    "relpath": "node_parser/file/markdown.py",
    "start_line": 1,
    "end_line": 140,
    "length": 140,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_defaults",
      "get_nodes_from_node",
      "_build_node_from_split",
      "_parse_nodes"
    ],
    "chunk_class_names": [
      "MarkdownNodeParser"
    ],
    "document_function_names": [
      "from_defaults",
      "get_nodes_from_node",
      "_build_node_from_split",
      "_parse_nodes"
    ],
    "document_class_names": [
      "MarkdownNodeParser"
    ],
    "content": "\"\"\"Markdown node parser.\"\"\"\nimport re\nfrom typing import Any, List, Optional, Sequence\n\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.node_utils import build_nodes_from_splits\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\nfrom llama_index.core.utils import get_tqdm_iterable\n\n\nclass MarkdownNodeParser(NodeParser):\n    \"\"\"Markdown node parser.\n\n    Splits a document into Nodes using Markdown header-based splitting logic.\n    Each node contains its text content and the path of headers leading to it.\n\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n        header_path_separator (str): separator char used for section header path metadata\n    \"\"\"\n\n    header_path_separator: str = Field(\n        default=\"/\", description=\"Separator char used for section header path metadata.\"\n    )\n\n    @classmethod\n    def from_defaults(\n        cls,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        header_path_separator: str = \"/\",\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> \"MarkdownNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n        return cls(\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            header_path_separator=header_path_separator,\n            callback_manager=callback_manager,\n        )\n\n    def get_nodes_from_node(self, node: BaseNode) -> List[TextNode]:\n        \"\"\"Get nodes from document by splitting on headers.\"\"\"\n        text = node.get_content(metadata_mode=MetadataMode.NONE)\n        markdown_nodes = []\n        lines = text.split(\"\\n\")\n        current_section = \"\"\n        # Keep track of (markdown level, text) for headers\n        header_stack: List[tuple[int, str]] = []\n        code_block = False\n\n        for line in lines:\n            # Track if we're inside a code block to avoid parsing headers in code\n            if line.lstrip().startswith(\"```\"):\n                code_block = not code_block\n                current_section += line + \"\\n\"\n                continue\n\n            # Only parse headers if we're not in a code block\n            if not code_block:\n                header_match = re.match(r\"^(#+)\\s(.*)\", line)\n                if header_match:\n                    # Save the previous section before starting a new one\n                    if current_section.strip():\n                        markdown_nodes.append(\n                            self._build_node_from_split(\n                                current_section.strip(),\n                                node,\n                                self.header_path_separator.join(\n                                    h[1] for h in header_stack[:-1]\n                                ),\n                            )\n                        )\n\n                    header_level = len(header_match.group(1))\n                    header_text = header_match.group(2)\n\n                    # Compare against top-of-stack item\u2019s markdown level.\n                    # Pop headers of equal or higher markdown level; not necessarily current stack size / depth.\n                    # Hierarchy depth gets deeper one level at a time, but markdown headers can jump from H1 to H3, for example.\n                    while header_stack and header_stack[-1][0] >= header_level:\n                        header_stack.pop()\n\n                    # Add the new header\n                    header_stack.append((header_level, header_text))\n                    current_section = \"#\" * header_level + f\" {header_text}\\n\"\n                    continue\n\n            current_section += line + \"\\n\"\n\n        # Add the final section\n        if current_section.strip():\n            markdown_nodes.append(\n                self._build_node_from_split(\n                    current_section.strip(),\n                    node,\n                    self.header_path_separator.join(h[1] for h in header_stack[:-1]),\n                )\n            )\n\n        return markdown_nodes\n\n    def _build_node_from_split(\n        self,\n        text_split: str,\n        node: BaseNode,\n        header_path: str,\n    ) -> TextNode:\n        \"\"\"Build node from single text split.\"\"\"\n        node = build_nodes_from_splits([text_split], node, id_func=self.id_func)[0]\n\n        if self.include_metadata:\n            separator = self.header_path_separator\n            node.metadata[\"header_path\"] = (\n                # ex: \"/header1/header2/\" || \"/\"\n                separator + header_path + separator\n                if header_path\n                else separator\n            )\n\n        return node\n\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Parse nodes.\"\"\"\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = self.get_nodes_from_node(node)\n            all_nodes.extend(nodes)\n\n        return all_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/file/json.py",
    "filename": "json.py",
    "relpath": "node_parser/file/json.py",
    "start_line": 1,
    "end_line": 104,
    "length": 104,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_defaults",
      "class_name",
      "_parse_nodes",
      "get_nodes_from_node",
      "_depth_first_yield"
    ],
    "chunk_class_names": [
      "JSONNodeParser",
      "name"
    ],
    "document_function_names": [
      "from_defaults",
      "class_name",
      "_parse_nodes",
      "get_nodes_from_node",
      "_depth_first_yield"
    ],
    "document_class_names": [
      "JSONNodeParser",
      "name"
    ],
    "content": "\"\"\"JSON node parser.\"\"\"\nimport json\nfrom typing import Any, Dict, Generator, List, Optional, Sequence\n\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.node_utils import build_nodes_from_splits\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\nfrom llama_index.core.utils import get_tqdm_iterable\n\n\nclass JSONNodeParser(NodeParser):\n    \"\"\"JSON node parser.\n\n    Splits a document into Nodes using custom JSON splitting logic.\n\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n\n    \"\"\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> \"JSONNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        return cls(\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            callback_manager=callback_manager,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"JSONNodeParser\"\n\n    def _parse_nodes(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = self.get_nodes_from_node(node)\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    def get_nodes_from_node(self, node: BaseNode) -> List[TextNode]:\n        \"\"\"Get nodes from document.\"\"\"\n        text = node.get_content(metadata_mode=MetadataMode.NONE)\n        try:\n            data = json.loads(text)\n        except json.JSONDecodeError:\n            # Handle invalid JSON input here\n            return []\n\n        json_nodes = []\n        if isinstance(data, dict):\n            lines = [*self._depth_first_yield(data, 0, [])]\n            json_nodes.extend(\n                build_nodes_from_splits([\"\\n\".join(lines)], node, id_func=self.id_func)\n            )\n        elif isinstance(data, list):\n            for json_object in data:\n                lines = [*self._depth_first_yield(json_object, 0, [])]\n                json_nodes.extend(\n                    build_nodes_from_splits(\n                        [\"\\n\".join(lines)], node, id_func=self.id_func\n                    )\n                )\n        else:\n            raise ValueError(\"JSON is invalid\")\n\n        return json_nodes\n\n    def _depth_first_yield(\n        self, json_data: Dict, levels_back: int, path: List[str]\n    ) -> Generator[str, None, None]:\n        \"\"\"Do depth first yield of all of the leaf nodes of a JSON.\n\n        Combines keys in the JSON tree using spaces.\n\n        If levels_back is set to 0, prints all levels.\n\n        \"\"\"\n        if isinstance(json_data, dict):\n            for key, value in json_data.items():\n                new_path = path[:]\n                new_path.append(key)\n                yield from self._depth_first_yield(value, levels_back, new_path)\n        elif isinstance(json_data, list):\n            for _, value in enumerate(json_data):\n                yield from self._depth_first_yield(value, levels_back, path)\n        else:\n            new_path = path[-levels_back:]\n            new_path.append(str(json_data))\n            yield \" \".join(new_path)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/file/html.py",
    "filename": "html.py",
    "relpath": "node_parser/file/html.py",
    "start_line": 1,
    "end_line": 141,
    "length": 141,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_defaults",
      "class_name",
      "_parse_nodes",
      "get_nodes_from_node",
      "_extract_text_from_tag",
      "_build_node_from_split"
    ],
    "chunk_class_names": [
      "HTMLNodeParser",
      "name"
    ],
    "document_function_names": [
      "from_defaults",
      "class_name",
      "_parse_nodes",
      "get_nodes_from_node",
      "_extract_text_from_tag",
      "_build_node_from_split"
    ],
    "document_class_names": [
      "HTMLNodeParser",
      "name"
    ],
    "content": "\"\"\"HTML node parser.\"\"\"\nfrom typing import TYPE_CHECKING, Any, List, Optional, Sequence, Union\n\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.node_utils import build_nodes_from_splits\nfrom llama_index.core.schema import BaseNode, MetadataMode, TextNode\nfrom llama_index.core.utils import get_tqdm_iterable\n\nif TYPE_CHECKING:\n    from bs4 import Tag, PageElement, NavigableString\n\nDEFAULT_TAGS = [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\n\n\nclass HTMLNodeParser(NodeParser):\n    \"\"\"HTML node parser.\n\n    Splits a document into Nodes using custom HTML splitting logic.\n\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n\n    \"\"\"\n\n    tags: List[str] = Field(\n        default=DEFAULT_TAGS, description=\"HTML tags to extract text from.\"\n    )\n\n    @classmethod\n    def from_defaults(\n        cls,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        tags: Optional[List[str]] = DEFAULT_TAGS,\n    ) -> \"HTMLNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        return cls(\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            callback_manager=callback_manager,\n            tags=tags,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"HTMLNodeParser\"\n\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = self.get_nodes_from_node(node)\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    def get_nodes_from_node(self, node: BaseNode) -> List[TextNode]:\n        \"\"\"Get nodes from document.\"\"\"\n        try:\n            from bs4 import BeautifulSoup, Tag\n        except ImportError:\n            raise ImportError(\"bs4 is required to read HTML files.\")\n\n        text = node.get_content(metadata_mode=MetadataMode.NONE)\n        soup = BeautifulSoup(text, \"html.parser\")\n        html_nodes = []\n        last_tag = None\n        current_section = \"\"\n\n        tags = soup.find_all(self.tags)\n        for tag in tags:\n            tag_text = self._extract_text_from_tag(tag)\n            if isinstance(tag, Tag) and (tag.name == last_tag or last_tag is None):\n                last_tag = tag.name\n                current_section += f\"{tag_text.strip()}\\n\"\n            else:\n                html_nodes.append(\n                    self._build_node_from_split(\n                        current_section.strip(), node, {\"tag\": last_tag}\n                    )\n                )\n                if isinstance(tag, Tag):\n                    last_tag = tag.name\n                current_section = f\"{tag_text}\\n\"\n\n        if current_section:\n            html_nodes.append(\n                self._build_node_from_split(\n                    current_section.strip(), node, {\"tag\": last_tag}\n                )\n            )\n\n        return html_nodes\n\n    def _extract_text_from_tag(\n        self, tag: Union[\"Tag\", \"NavigableString\", \"PageElement\"]\n    ) -> str:\n        from bs4 import NavigableString, Tag, PageElement\n\n        texts = []\n        if isinstance(tag, Tag):\n            for elem in tag.children:\n                if isinstance(elem, NavigableString):\n                    if elem.strip():\n                        texts.append(elem.strip())\n                elif isinstance(elem, Tag):\n                    if elem.name in self.tags:\n                        continue\n                    else:\n                        texts.append(elem.get_text().strip())\n                elif isinstance(elem, PageElement):\n                    texts.append(elem.get_text().strip())\n        else:\n            texts.append(tag.get_text().strip())\n        return \"\\n\".join(texts)\n\n    def _build_node_from_split(\n        self,\n        text_split: str,\n        node: BaseNode,\n        metadata: dict,\n    ) -> TextNode:\n        \"\"\"Build node from single text split.\"\"\"\n        node = build_nodes_from_splits([text_split], node, id_func=self.id_func)[0]\n\n        if self.include_metadata:\n            node.metadata = {**node.metadata, **metadata}\n\n        return node"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/file/__init__.py",
    "filename": "__init__.py",
    "relpath": "node_parser/file/__init__.py",
    "start_line": 1,
    "end_line": 11,
    "length": 11,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.node_parser.file.html import HTMLNodeParser\nfrom llama_index.core.node_parser.file.json import JSONNodeParser\nfrom llama_index.core.node_parser.file.markdown import MarkdownNodeParser\nfrom llama_index.core.node_parser.file.simple_file import SimpleFileNodeParser\n\n__all__ = [\n    \"SimpleFileNodeParser\",\n    \"HTMLNodeParser\",\n    \"MarkdownNodeParser\",\n    \"JSONNodeParser\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/file/simple_file.py",
    "filename": "simple_file.py",
    "relpath": "node_parser/file/simple_file.py",
    "start_line": 1,
    "end_line": 90,
    "length": 90,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_defaults",
      "class_name",
      "_parse_nodes"
    ],
    "chunk_class_names": [
      "SimpleFileNodeParser",
      "name"
    ],
    "document_function_names": [
      "from_defaults",
      "class_name",
      "_parse_nodes"
    ],
    "document_class_names": [
      "SimpleFileNodeParser",
      "name"
    ],
    "content": "\"\"\"Simple file node parser.\"\"\"\nfrom typing import Any, Dict, List, Optional, Sequence, Type\n\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser.node_utils import build_nodes_from_splits\nfrom llama_index.core.node_parser.file.html import HTMLNodeParser\nfrom llama_index.core.node_parser.file.json import JSONNodeParser\nfrom llama_index.core.node_parser.file.markdown import MarkdownNodeParser\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.schema import BaseNode, MetadataMode\nfrom llama_index.core.utils import get_tqdm_iterable\n\nFILE_NODE_PARSERS: Dict[str, Type[NodeParser]] = {\n    \".md\": MarkdownNodeParser,\n    \".html\": HTMLNodeParser,\n    \".json\": JSONNodeParser,\n}\n\n\nclass SimpleFileNodeParser(NodeParser):\n    \"\"\"Simple file node parser.\n\n    Splits a document loaded from a file into Nodes using logic based on the file type\n    automatically detects the NodeParser to use based on file type\n\n    Args:\n        include_metadata (bool): whether to include metadata in nodes\n        include_prev_next_rel (bool): whether to include prev/next relationships\n\n    \"\"\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> \"SimpleFileNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        return cls(\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            callback_manager=callback_manager,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"SimpleFileNodeParser\"\n\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Parse document into nodes.\n\n        Args:\n            nodes (Sequence[BaseNode]): nodes to parse\n        \"\"\"\n        all_nodes: List[BaseNode] = []\n        documents_with_progress = get_tqdm_iterable(\n            nodes, show_progress, \"Parsing documents into nodes\"\n        )\n\n        for document in documents_with_progress:\n            ext = document.metadata.get(\"extension\", \"None\")\n            if ext in FILE_NODE_PARSERS:\n                parser = FILE_NODE_PARSERS[ext](\n                    include_metadata=self.include_metadata,\n                    include_prev_next_rel=self.include_prev_next_rel,\n                    callback_manager=self.callback_manager,\n                )\n\n                nodes = parser.get_nodes_from_documents([document], show_progress)\n                all_nodes.extend(nodes)\n            else:\n                # What to do when file type isn't supported yet?\n                all_nodes.extend(\n                    # build node from document\n                    build_nodes_from_splits(\n                        [document.get_content(metadata_mode=MetadataMode.NONE)],\n                        document,\n                        id_func=self.id_func,\n                    )\n                )\n\n        return all_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/llama_parse_json_element.py",
    "filename": "llama_parse_json_element.py",
    "relpath": "node_parser/relational/llama_parse_json_element.py",
    "start_line": 1,
    "end_line": 302,
    "length": 302,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "filter_table"
    ],
    "chunk_class_names": [
      "LlamaParseJsonNodeParser"
    ],
    "document_function_names": [
      "class_name",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "filter_table"
    ],
    "document_class_names": [
      "LlamaParseJsonNodeParser"
    ],
    "content": "from typing import Any, Callable, List, Optional, Dict\n\nfrom llama_index.core.node_parser.relational.base_element import (\n    BaseElementNodeParser,\n    Element,\n)\nfrom llama_index.core.schema import BaseNode, TextNode\nfrom llama_index.core.node_parser.relational.utils import md_to_df\n\n\nclass LlamaParseJsonNodeParser(BaseElementNodeParser):\n    \"\"\"Llama Parse Json format element node parser.\n\n    Splits a json format document from LlamaParse into Text Nodes and Index Nodes\n    corresponding to embedded objects (e.g. tables).\n\n    \"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"LlamaParseJsonNodeParser\"\n\n    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        elements = self.extract_elements(\n            node.get_content(),\n            table_filters=[self.filter_table],\n            node_id=node.id_,\n            node_metadata=node.metadata,\n        )\n        table_elements = self.get_table_elements(elements)\n        # extract summaries over table elements\n        self.extract_table_summaries(table_elements)\n        # convert into nodes\n        # will return a list of Nodes and Index Nodes\n        return self.get_nodes_from_elements(\n            elements, node, ref_doc_text=node.get_content()\n        )\n\n    async def aget_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        elements = self.extract_elements(\n            node.get_content(),\n            table_filters=[self.filter_table],\n            node_id=node.id_,\n            node_metadata=node.metadata,\n        )\n        table_elements = self.get_table_elements(elements)\n        # extract summaries over table elements\n        await self.aextract_table_summaries(table_elements)\n        # convert into nodes\n        # will return a list of Nodes and Index Nodes\n        return self.get_nodes_from_elements(\n            elements, node, ref_doc_text=node.get_content()\n        )\n\n    def extract_elements(\n        self,\n        text: str,\n        mode: Optional[str] = \"json\",\n        node_id: Optional[str] = None,\n        node_metadata: Optional[Dict[str, Any]] = None,\n        table_filters: Optional[List[Callable]] = None,\n        **kwargs: Any,\n    ) -> List[Element]:\n        # get node id for each node so that we can avoid using the same id for different nodes\n        \"\"\"Extract elements from json based nodes.\n\n        Args:\n            text: node's text content\n            mode: different modes for returning different types of elements based on the selected mode\n            node_id: unique id for the node\n            node_metadata: metadata for the node. the json output for the nodes contains a lot of fields for elements\n\n        \"\"\"\n        elements: List[Element] = []\n        currentElement = None\n        page_number = node_metadata.get(\"page\") if node_metadata is not None else 0\n\n        if mode == \"json\" and node_metadata is not None:\n            json_items = node_metadata.get(\"items\") or []\n            for element_idx, json_item in enumerate(json_items):\n                ele_type = json_item.get(\"type\")\n                if ele_type == \"heading\":\n                    elements.append(\n                        Element(\n                            id=f\"id_page_{page_number}_heading_{element_idx}\",\n                            type=\"heading\",\n                            title_level=json_item.get(\"lvl\"),\n                            element=json_item.get(\"value\"),\n                            markdown=json_item.get(\"md\"),\n                            page_number=page_number,\n                        )\n                    )\n                elif ele_type == \"text\":\n                    elements.append(\n                        Element(\n                            id=f\"id_page_{page_number}_text_{element_idx}\",\n                            type=\"text\",\n                            element=json_item.get(\"value\"),\n                            markdown=json_item.get(\"md\"),\n                            page_number=page_number,\n                        )\n                    )\n                elif ele_type == \"table\":\n                    elements.append(\n                        Element(\n                            id=f\"id_page_{page_number}_table_{element_idx}\",\n                            type=\"table\",\n                            element=json_item.get(\"rows\"),\n                            markdown=json_item.get(\"md\"),\n                            page_number=page_number,\n                        )\n                    )\n        elif mode == \"images\" and node_metadata is not None:\n            # only get images from json metadata\n            images = node_metadata.get(\"images\") or []\n            for idx, image in enumerate(images):\n                elements.append(\n                    Element(\n                        id=f\"id_page_{page_number}_image_{idx}\",\n                        type=\"image\",\n                        element=image,\n                    )\n                )\n        else:\n            lines = text.split(\"\\n\")\n            # Then parse the lines from raw text of json\n            for line in lines:\n                if line.startswith(\"```\"):\n                    # check if this is the end of a code block\n                    if currentElement is not None and currentElement.type == \"code\":\n                        elements.append(currentElement)\n                        currentElement = None\n                        # if there is some text after the ``` create a text element with it\n                        if len(line) > 3:\n                            elements.append(\n                                Element(\n                                    id=f\"id_{len(elements)}\",\n                                    type=\"text\",\n                                    element=line.lstrip(\"```\"),\n                                )\n                            )\n\n                    elif line.count(\"```\") == 2 and line[-3] != \"`\":\n                        # check if inline code block (aka have a second ``` in line but not at the end)\n                        if currentElement is not None:\n                            elements.append(currentElement)\n                        currentElement = Element(\n                            id=f\"id_{len(elements)}\",\n                            type=\"code\",\n                            element=line.lstrip(\"```\"),\n                        )\n                    elif currentElement is not None and currentElement.type == \"text\":\n                        currentElement.element += \"\\n\" + line\n                    else:\n                        if currentElement is not None:\n                            elements.append(currentElement)\n                        currentElement = Element(\n                            id=f\"id_{len(elements)}\", type=\"text\", element=line\n                        )\n\n                elif currentElement is not None and currentElement.type == \"code\":\n                    currentElement.element += \"\\n\" + line\n\n                elif line.startswith(\"|\"):\n                    if currentElement is not None and currentElement.type != \"table\":\n                        if currentElement is not None:\n                            elements.append(currentElement)\n                        currentElement = Element(\n                            id=f\"id_{len(elements)}\", type=\"table\", element=line\n                        )\n                    elif currentElement is not None:\n                        currentElement.element += \"\\n\" + line\n                    else:\n                        currentElement = Element(\n                            id=f\"id_{len(elements)}\", type=\"table\", element=line\n                        )\n                elif line.startswith(\"#\"):\n                    if currentElement is not None:\n                        elements.append(currentElement)\n                    currentElement = Element(\n                        id=f\"id_{len(elements)}\",\n                        type=\"title\",\n                        element=line.lstrip(\"#\"),\n                        title_level=len(line) - len(line.lstrip(\"#\")),\n                    )\n                else:\n                    if currentElement is not None and currentElement.type != \"text\":\n                        elements.append(currentElement)\n                        currentElement = Element(\n                            id=f\"id_{len(elements)}\", type=\"text\", element=line\n                        )\n                    elif currentElement is not None:\n                        currentElement.element += \"\\n\" + line\n                    else:\n                        currentElement = Element(\n                            id=f\"id_{len(elements)}\", type=\"text\", element=line\n                        )\n        if currentElement is not None:\n            elements.append(currentElement)\n\n        for idx, element in enumerate(elements):\n            if element.type == \"table\":\n                assert element.markdown is not None\n\n                should_keep = True\n                perfect_table = True\n\n                # verify that the table (markdown) have the same number of columns on each rows\n                table_lines = element.markdown.split(\"\\n\")\n                table_columns = [len(line.split(\"|\")) for line in table_lines]\n                if len(set(table_columns)) > 1:\n                    # if the table have different number of columns on each rows, it's not a perfect table\n                    # we will store the raw text for such tables instead of converting them to a dataframe\n                    perfect_table = False\n\n                # verify that the table (markdown) have at least 2 rows\n                if len(table_lines) < 2:\n                    should_keep = False\n\n                # apply the table filter, now only filter empty tables\n                if should_keep and perfect_table and table_filters is not None:\n                    should_keep = all(tf(element) for tf in table_filters)\n\n                # if the element is a table, convert it to a dataframe\n                if should_keep:\n                    if perfect_table:\n                        assert element.markdown is not None\n                        table = md_to_df(element.markdown)\n\n                        elements[idx] = Element(\n                            id=(\n                                f\"id_page_{page_number}_{node_id}_{idx}\"\n                                if node_id\n                                else f\"id_{idx}\"\n                            ),\n                            type=\"table\",\n                            element=element,\n                            table=table,\n                        )\n                    else:\n                        # for non-perfect tables, we will store the raw text\n                        # and give it a different type to differentiate it from perfect tables\n                        elements[idx] = Element(\n                            id=(\n                                f\"id_page_{page_number}_{node_id}_{idx}\"\n                                if node_id\n                                else f\"id_{idx}\"\n                            ),\n                            type=\"table_text\",\n                            element=element.element,\n                            # table=table\n                        )\n                else:\n                    elements[idx] = Element(\n                        id=(\n                            f\"id_page_{page_number}_{node_id}_{idx}\"\n                            if node_id\n                            else f\"id_page_{page_number}_{idx}\"\n                        ),\n                        type=\"text\",\n                        element=element.element,\n                    )\n            else:\n                # if the element is not a table, keep it as to text\n                elements[idx] = Element(\n                    id=(\n                        f\"id_page_{page_number}_{node_id}_{idx}\"\n                        if node_id\n                        else f\"id_page_{page_number}_{idx}\"\n                    ),\n                    type=\"text\",\n                    element=element.element,\n                )\n\n        # merge consecutive text elements together for now\n        merged_elements: List[Element] = []\n        for element in elements:\n            if (\n                len(merged_elements) > 0\n                and element.type == \"text\"\n                and merged_elements[-1].type == \"text\"\n            ):\n                if isinstance(element.element, list):\n                    merged_elements[-1].element += \"\\n\" + \" \".join(\n                        str(e) for e in element.element\n                    )\n                else:\n                    merged_elements[-1].element += \"\\n\" + element.element\n            else:\n                merged_elements.append(element)\n        elements = merged_elements\n        return merged_elements\n\n    def filter_table(self, table_element: Any) -> bool:\n        \"\"\"Filter tables.\"\"\"\n        # convert markdown of the table to df\n        table_df = md_to_df(table_element.markdown)\n\n        # check if table_df is not None, has more than one row, and more than one column\n        return table_df is not None and not table_df.empty and len(table_df.columns) > 1"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/base_element.py",
    "filename": "base_element.py",
    "relpath": "node_parser/relational/base_element.py",
    "start_line": 1,
    "end_line": 68,
    "length": 68,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__str__"
    ],
    "chunk_class_names": [
      "TableColumnOutput",
      "TableOutput",
      "Element"
    ],
    "document_function_names": [
      "__str__",
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "_aparse_nodes",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "get_table_elements",
      "get_text_elements",
      "extract_table_summaries",
      "_get_table_output",
      "aextract_table_summaries",
      "_get_table_output",
      "get_base_nodes_and_mappings",
      "get_nodes_and_objects",
      "_get_nodes_from_buffer",
      "get_nodes_from_elements",
      "__call__",
      "acall"
    ],
    "document_class_names": [
      "TableColumnOutput",
      "TableOutput",
      "Element",
      "BaseElementNodeParser"
    ],
    "content": "import uuid\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, cast\nfrom tqdm import tqdm\n\nfrom llama_index.core.async_utils import DEFAULT_NUM_WORKERS, run_jobs, asyncio_run\nfrom llama_index.core.base.response.schema import PydanticResponse\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    ValidationError,\n    ConfigDict,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.schema import (\n    BaseNode,\n    Document,\n    IndexNode,\n    MetadataMode,\n    TextNode,\n)\nfrom llama_index.core.utils import get_tqdm_iterable\n\nDEFAULT_SUMMARY_QUERY_STR = \"\"\"\\\nWhat is this table about? Give a very concise summary (imagine you are adding a new caption and summary for this table), \\\nand output the real/existing table title/caption if context provided.\\\nand output the real/existing table id if context provided.\\\nand also output whether or not the table should be kept.\\\n\"\"\"\n\n\nclass TableColumnOutput(BaseModel):\n    \"\"\"Output from analyzing a table column.\"\"\"\n\n    col_name: str\n    col_type: str\n    summary: Optional[str] = None\n\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        return (\n            f\"Column: {self.col_name}\\nType: {self.col_type}\\nSummary: {self.summary}\"\n        )\n\n\nclass TableOutput(BaseModel):\n    \"\"\"Output from analyzing a table.\"\"\"\n\n    summary: str\n    table_title: Optional[str] = None\n    table_id: Optional[str] = None\n    columns: List[TableColumnOutput]\n\n\nclass Element(BaseModel):\n    \"\"\"Element object.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    id: str\n    type: str\n    element: Any\n    title_level: Optional[int] = None\n    table_output: Optional[TableOutput] = None\n    table: Optional[Any] = None\n    markdown: Optional[str] = None\n    page_number: Optional[int] = None"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/base_element.py",
    "filename": "base_element.py",
    "relpath": "node_parser/relational/base_element.py",
    "start_line": 68,
    "end_line": 468,
    "length": 401,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "_aparse_nodes",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "get_table_elements",
      "get_text_elements",
      "extract_table_summaries",
      "_get_table_output",
      "aextract_table_summaries",
      "_get_table_output",
      "get_base_nodes_and_mappings",
      "get_nodes_and_objects",
      "_get_nodes_from_buffer",
      "get_nodes_from_elements",
      "__call__",
      "acall"
    ],
    "chunk_class_names": [
      "BaseElementNodeParser"
    ],
    "document_function_names": [
      "__str__",
      "class_name",
      "from_defaults",
      "_parse_nodes",
      "_aparse_nodes",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "get_table_elements",
      "get_text_elements",
      "extract_table_summaries",
      "_get_table_output",
      "aextract_table_summaries",
      "_get_table_output",
      "get_base_nodes_and_mappings",
      "get_nodes_and_objects",
      "_get_nodes_from_buffer",
      "get_nodes_from_elements",
      "__call__",
      "acall"
    ],
    "document_class_names": [
      "TableColumnOutput",
      "TableOutput",
      "Element",
      "BaseElementNodeParser"
    ],
    "content": "class BaseElementNodeParser(NodeParser):\n    \"\"\"\n    Splits a document into Text Nodes and Index Nodes corresponding to embedded objects.\n\n    Supports text and tables currently.\n    \"\"\"\n\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n    llm: Optional[LLM] = Field(\n        default=None, description=\"LLM model to use for summarization.\"\n    )\n    summary_query_str: str = Field(\n        default=DEFAULT_SUMMARY_QUERY_STR,\n        description=\"Query string to use for summarization.\",\n    )\n    num_workers: int = Field(\n        default=DEFAULT_NUM_WORKERS,\n        description=\"Num of workers for async jobs.\",\n    )\n\n    show_progress: bool = Field(default=True, description=\"Whether to show progress.\")\n\n    nested_node_parser: Optional[NodeParser] = Field(\n        default=None,\n        description=\"Other types of node parsers to handle some types of nodes.\",\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"BaseElementNodeParser\"\n\n    @classmethod\n    def from_defaults(\n        cls,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> \"BaseElementNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        return cls(\n            callback_manager=callback_manager,\n            **kwargs,\n        )\n\n    def _parse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = self.get_nodes_from_node(node)\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    async def _aparse_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        all_nodes: List[BaseNode] = []\n        nodes_with_progress = get_tqdm_iterable(nodes, show_progress, \"Parsing nodes\")\n\n        for node in nodes_with_progress:\n            nodes = await self.aget_nodes_from_node(node)\n            all_nodes.extend(nodes)\n\n        return all_nodes\n\n    @abstractmethod\n    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        ...\n\n    async def aget_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        return self.get_nodes_from_node(node)\n\n    @abstractmethod\n    def extract_elements(self, text: str, **kwargs: Any) -> List[Element]:\n        \"\"\"Extract elements from text.\"\"\"\n\n    def get_table_elements(self, elements: List[Element]) -> List[Element]:\n        \"\"\"Get table elements.\"\"\"\n        return [e for e in elements if e.type == \"table\" or e.type == \"table_text\"]\n\n    def get_text_elements(self, elements: List[Element]) -> List[Element]:\n        \"\"\"Get text elements.\"\"\"\n        # TODO: There we should maybe do something with titles\n        # and other elements in the future?\n        return [e for e in elements if e.type != \"table\"]\n\n    def extract_table_summaries(self, elements: List[Element]) -> None:\n        \"\"\"Go through elements, extract out summaries that are tables.\"\"\"\n        from llama_index.core.indices.list.base import SummaryIndex\n        from llama_index.core.settings import Settings\n\n        llm = self.llm or Settings.llm\n\n        table_context_list = []\n        for idx, element in tqdm(enumerate(elements)):\n            if element.type not in (\"table\", \"table_text\"):\n                continue\n            table_context = str(element.element)\n            if idx > 0 and str(elements[idx - 1].element).lower().strip().startswith(\n                \"table\"\n            ):\n                table_context = str(elements[idx - 1].element) + \"\\n\" + table_context\n            if idx < len(elements) + 1 and str(\n                elements[idx - 1].element\n            ).lower().strip().startswith(\"table\"):\n                table_context += \"\\n\" + str(elements[idx + 1].element)\n\n            table_context_list.append(table_context)\n\n        async def _get_table_output(table_context: str, summary_query_str: str) -> Any:\n            index = SummaryIndex.from_documents(\n                [Document(text=table_context)],\n            )\n            query_engine = index.as_query_engine(llm=llm, output_cls=TableOutput)\n            try:\n                response = await query_engine.aquery(summary_query_str)\n                if isinstance(response, PydanticResponse):\n                    return response.response\n                else:\n                    raise ValueError(f\"Expected PydanticResponse, got {type(response)}\")\n            except (ValidationError, ValueError):\n                # There was a pydantic validation error, so we will run with text completion\n                # fill in the summary and leave other fields blank\n                query_engine = index.as_query_engine(llm=llm)\n                response_txt = await query_engine.aquery(summary_query_str)\n                return TableOutput(summary=str(response_txt), columns=[])\n\n        summary_jobs = [\n            _get_table_output(table_context, self.summary_query_str)\n            for table_context in table_context_list\n        ]\n        summary_co = run_jobs(summary_jobs, workers=self.num_workers)\n        summary_outputs = asyncio_run(summary_co)\n        for element, summary_output in zip(elements, summary_outputs):\n            element.table_output = summary_output\n\n    async def aextract_table_summaries(self, elements: List[Element]) -> None:\n        \"\"\"Go through elements, extract out summaries that are tables.\"\"\"\n        from llama_index.core.indices.list.base import SummaryIndex\n        from llama_index.core.settings import Settings\n\n        llm = self.llm or Settings.llm\n\n        table_context_list = []\n        for idx, element in tqdm(enumerate(elements)):\n            if element.type not in (\"table\", \"table_text\"):\n                continue\n            table_context = str(element.element)\n            if idx > 0 and str(elements[idx - 1].element).lower().strip().startswith(\n                \"table\"\n            ):\n                table_context = str(elements[idx - 1].element) + \"\\n\" + table_context\n            if idx < len(elements) + 1 and str(\n                elements[idx - 1].element\n            ).lower().strip().startswith(\"table\"):\n                table_context += \"\\n\" + str(elements[idx + 1].element)\n\n            table_context_list.append(table_context)\n\n        async def _get_table_output(table_context: str, summary_query_str: str) -> Any:\n            index = SummaryIndex.from_documents(\n                [Document(text=table_context)],\n            )\n            query_engine = index.as_query_engine(llm=llm, output_cls=TableOutput)\n            try:\n                response = await query_engine.aquery(summary_query_str)\n                return cast(PydanticResponse, response).response\n            except (ValidationError, ValueError):\n                # There was a pydantic validation error, so we will run with text completion\n                # fill in the summary and leave other fields blank\n                query_engine = index.as_query_engine(llm=llm)\n                response_txt = await query_engine.aquery(summary_query_str)\n                return TableOutput(summary=str(response_txt), columns=[])\n\n        summary_jobs = [\n            _get_table_output(table_context, self.summary_query_str)\n            for table_context in table_context_list\n        ]\n        summary_outputs = await run_jobs(summary_jobs, workers=self.num_workers)\n        for element, summary_output in zip(elements, summary_outputs):\n            element.table_output = summary_output\n\n    def get_base_nodes_and_mappings(\n        self, nodes: List[BaseNode]\n    ) -> Tuple[List[BaseNode], Dict]:\n        \"\"\"Get base nodes and mappings.\n\n        Given a list of nodes and IndexNode objects, return the base nodes and a mapping\n        from index id to child nodes (which are excluded from the base nodes).\n\n        \"\"\"\n        node_dict = {node.node_id: node for node in nodes}\n\n        node_mappings = {}\n        base_nodes = []\n\n        # first map index nodes to their child nodes\n        nonbase_node_ids = set()\n        for node in nodes:\n            if isinstance(node, IndexNode):\n                node_mappings[node.index_id] = node_dict[node.index_id]\n                nonbase_node_ids.add(node.index_id)\n            else:\n                pass\n\n        # then add all nodes that are not children of index nodes\n        for node in nodes:\n            if node.node_id not in nonbase_node_ids:\n                base_nodes.append(node)\n\n        return base_nodes, node_mappings\n\n    def get_nodes_and_objects(\n        self, nodes: List[BaseNode]\n    ) -> Tuple[List[BaseNode], List[IndexNode]]:\n        base_nodes, node_mappings = self.get_base_nodes_and_mappings(nodes)\n\n        nodes = []\n        objects = []\n        for node in base_nodes:\n            if isinstance(node, IndexNode):\n                node.obj = node_mappings[node.index_id]\n                objects.append(node)\n            else:\n                nodes.append(node)\n\n        return nodes, objects\n\n    def _get_nodes_from_buffer(\n        self, buffer: List[str], node_parser: NodeParser\n    ) -> List[BaseNode]:\n        \"\"\"Get nodes from buffer.\"\"\"\n        doc = Document(text=\"\\n\\n\".join(list(buffer)))\n        return node_parser.get_nodes_from_documents([doc])\n\n    def get_nodes_from_elements(\n        self,\n        elements: List[Element],\n        node_inherited: Optional[TextNode] = None,\n        ref_doc_text: Optional[str] = None,\n    ) -> List[BaseNode]:\n        \"\"\"Get nodes and mappings.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        from llama_index.core.node_parser import SentenceSplitter\n\n        node_parser = self.nested_node_parser or SentenceSplitter()\n\n        nodes: List[BaseNode] = []\n        cur_text_el_buffer: List[str] = []\n        for element in elements:\n            if element.type == \"table\" or element.type == \"table_text\":\n                # flush text buffer for table\n                if len(cur_text_el_buffer) > 0:\n                    cur_text_nodes = self._get_nodes_from_buffer(\n                        cur_text_el_buffer, node_parser\n                    )\n                    nodes.extend(cur_text_nodes)\n                    cur_text_el_buffer = []\n\n                table_output = cast(TableOutput, element.table_output)\n                table_md = \"\"\n                if element.type == \"table\":\n                    table_df = cast(pd.DataFrame, element.table)\n                    # We serialize the table as markdown as it allow better accuracy\n                    # We do not use the table_df.to_markdown() method as it generate\n                    # a table with a token hungry format.\n                    table_md = \"|\"\n                    for col_name, col in table_df.items():\n                        table_md += f\"{col_name}|\"\n                    table_md += \"\\n|\"\n                    for col_name, col in table_df.items():\n                        table_md += f\"---|\"\n                    table_md += \"\\n\"\n                    for row in table_df.itertuples():\n                        table_md += \"|\"\n                        for col in row[1:]:\n                            table_md += f\"{col}|\"\n                        table_md += \"\\n\"\n                elif element.type == \"table_text\":\n                    # if the table is non-perfect table, we still want to keep the original text of table\n                    table_md = str(element.element)\n\n                col_schema = \"\\n\\n\".join([str(col) for col in table_output.columns])\n\n                # We build a summary of the table containing the extracted summary, and a description of the columns\n                table_summary = str(table_output.summary)\n                if table_output.table_title:\n                    table_summary += \",\\nwith the following table title:\\n\"\n                    table_summary += str(table_output.table_title)\n\n                table_summary += \",\\nwith the following columns:\\n\"\n\n                for col in table_output.columns:\n                    table_summary += f\"- {col.col_name}: {col.summary}\\n\"\n\n                # attempt to find start_char_idx for table\n                # raw table string regardless if perfect or not is stored in element.element\n\n                if ref_doc_text:\n                    start_char_idx = ref_doc_text.find(str(element.element))\n                    if start_char_idx >= 0:\n                        end_char_idx = start_char_idx + len(str(element.element))\n                    else:\n                        start_char_idx = None  # type: ignore\n                        end_char_idx = None  # type: ignore\n                else:\n                    start_char_idx = None  # type: ignore\n                    end_char_idx = None  # type: ignore\n\n                # shared index_id and node_id\n                node_id = str(uuid.uuid4())\n                index_node = IndexNode(\n                    text=table_summary,\n                    metadata={\n                        \"col_schema\": col_schema,\n                    },\n                    excluded_embed_metadata_keys=[\"col_schema\"],\n                    index_id=node_id,\n                    start_char_idx=start_char_idx,\n                    end_char_idx=end_char_idx,\n                )\n\n                table_str = table_summary + \"\\n\" + table_md\n\n                text_node = TextNode(\n                    id_=node_id,\n                    text=table_str,\n                    metadata={\n                        # serialize the table as a dictionary string for dataframe of perfect table\n                        \"table_df\": (\n                            str(table_df.to_dict())\n                            if element.type == \"table\"\n                            else table_md\n                        ),\n                        # add table summary for retrieval purposes\n                        \"table_summary\": table_summary,\n                    },\n                    excluded_embed_metadata_keys=[\"table_df\", \"table_summary\"],\n                    excluded_llm_metadata_keys=[\"table_df\", \"table_summary\"],\n                    start_char_idx=start_char_idx,\n                    end_char_idx=end_char_idx,\n                )\n                nodes.extend([index_node, text_node])\n            else:\n                cur_text_el_buffer.append(str(element.element))\n\n        # flush text buffer for the last batch\n        if len(cur_text_el_buffer) > 0:\n            cur_text_nodes = self._get_nodes_from_buffer(\n                cur_text_el_buffer, node_parser\n            )\n            nodes.extend(cur_text_nodes)\n            cur_text_el_buffer = []\n\n        # remove empty nodes and keep node original metadata inherited from parent nodes\n        for node in nodes:\n            if node_inherited and node_inherited.metadata:\n                node.metadata.update(node_inherited.metadata)\n                node.excluded_embed_metadata_keys = (\n                    node_inherited.excluded_embed_metadata_keys\n                )\n                node.excluded_llm_metadata_keys = (\n                    node_inherited.excluded_llm_metadata_keys\n                )\n        return [\n            node\n            for node in nodes\n            if len(node.get_content(metadata_mode=MetadataMode.NONE)) > 0\n        ]\n\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        nodes = self.get_nodes_from_documents(nodes, **kwargs)  # type: ignore\n        nodes, objects = self.get_nodes_and_objects(nodes)\n        return nodes + objects  # type: ignore\n\n    async def acall(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        nodes = await self.aget_nodes_from_documents(nodes, **kwargs)  # type: ignore\n        nodes, objects = self.get_nodes_and_objects(nodes)\n        return nodes + objects  # type: ignore"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/hierarchical.py",
    "filename": "hierarchical.py",
    "relpath": "node_parser/relational/hierarchical.py",
    "start_line": 1,
    "end_line": 234,
    "length": 234,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_add_parent_child_relationship",
      "get_leaf_nodes",
      "get_root_nodes",
      "get_child_nodes",
      "get_deeper_nodes",
      "from_defaults",
      "class_name",
      "_recursively_get_nodes_from_nodes",
      "get_nodes_from_documents",
      "_parse_nodes"
    ],
    "chunk_class_names": [
      "HierarchicalNodeParser"
    ],
    "document_function_names": [
      "_add_parent_child_relationship",
      "get_leaf_nodes",
      "get_root_nodes",
      "get_child_nodes",
      "get_deeper_nodes",
      "from_defaults",
      "class_name",
      "_recursively_get_nodes_from_nodes",
      "get_nodes_from_documents",
      "_parse_nodes"
    ],
    "document_class_names": [
      "HierarchicalNodeParser"
    ],
    "content": "\"\"\"Hierarchical node parser.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.node_parser.interface import NodeParser\nfrom llama_index.core.node_parser.text.sentence import SentenceSplitter\nfrom llama_index.core.schema import BaseNode, Document, NodeRelationship\nfrom llama_index.core.utils import get_tqdm_iterable\n\n\ndef _add_parent_child_relationship(parent_node: BaseNode, child_node: BaseNode) -> None:\n    \"\"\"Add parent/child relationship between nodes.\"\"\"\n    child_list = parent_node.child_nodes or []\n    child_list.append(child_node.as_related_node_info())\n    parent_node.relationships[NodeRelationship.CHILD] = child_list\n\n    child_node.relationships[\n        NodeRelationship.PARENT\n    ] = parent_node.as_related_node_info()\n\n\ndef get_leaf_nodes(nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get leaf nodes.\"\"\"\n    leaf_nodes = []\n    for node in nodes:\n        if NodeRelationship.CHILD not in node.relationships:\n            leaf_nodes.append(node)\n    return leaf_nodes\n\n\ndef get_root_nodes(nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get root nodes.\"\"\"\n    root_nodes = []\n    for node in nodes:\n        if NodeRelationship.PARENT not in node.relationships:\n            root_nodes.append(node)\n    return root_nodes\n\n\ndef get_child_nodes(nodes: List[BaseNode], all_nodes: List[BaseNode]) -> List[BaseNode]:\n    \"\"\"Get child nodes of nodes from given all_nodes.\"\"\"\n    children_ids = []\n    for node in nodes:\n        if NodeRelationship.CHILD not in node.relationships:\n            continue\n\n        children_ids.extend([r.node_id for r in (node.child_nodes or [])])\n\n    child_nodes = []\n    for candidate_node in all_nodes:\n        if candidate_node.node_id not in children_ids:\n            continue\n        child_nodes.append(candidate_node)\n\n    return child_nodes\n\n\ndef get_deeper_nodes(nodes: List[BaseNode], depth: int = 1) -> List[BaseNode]:\n    \"\"\"Get children of root nodes in given nodes that have given depth.\"\"\"\n    if depth < 0:\n        raise ValueError(\"Depth cannot be a negative number!\")\n    root_nodes = get_root_nodes(nodes)\n    if not root_nodes:\n        raise ValueError(\"There is no root nodes in given nodes!\")\n\n    deeper_nodes = root_nodes\n    for _ in range(depth):\n        deeper_nodes = get_child_nodes(deeper_nodes, nodes)\n\n    return deeper_nodes\n\n\nclass HierarchicalNodeParser(NodeParser):\n    \"\"\"Hierarchical node parser.\n\n    Splits a document into a recursive hierarchy Nodes using a NodeParser.\n\n    NOTE: this will return a hierarchy of nodes in a flat list, where there will be\n    overlap between parent nodes (e.g. with a bigger chunk size), and child nodes\n    per parent (e.g. with a smaller chunk size).\n\n    For instance, this may return a list of nodes like:\n\n    - list of top-level nodes with chunk size 2048\n    - list of second-level nodes, where each node is a child of a top-level node,\n      chunk size 512\n    - list of third-level nodes, where each node is a child of a second-level node,\n      chunk size 128\n    \"\"\"\n\n    chunk_sizes: Optional[List[int]] = Field(\n        default=None,\n        description=(\n            \"The chunk sizes to use when splitting documents, in order of level.\"\n        ),\n    )\n    node_parser_ids: List[str] = Field(\n        default_factory=list,\n        description=(\n            \"List of ids for the node parsers to use when splitting documents, \"\n            + \"in order of level (first id used for first level, etc.).\"\n        ),\n    )\n    node_parser_map: Dict[str, NodeParser] = Field(\n        description=\"Map of node parser id to node parser.\",\n    )\n\n    @classmethod\n    def from_defaults(\n        cls,\n        chunk_sizes: Optional[List[int]] = None,\n        chunk_overlap: int = 20,\n        node_parser_ids: Optional[List[str]] = None,\n        node_parser_map: Optional[Dict[str, NodeParser]] = None,\n        include_metadata: bool = True,\n        include_prev_next_rel: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> \"HierarchicalNodeParser\":\n        callback_manager = callback_manager or CallbackManager([])\n\n        if node_parser_ids is None:\n            if chunk_sizes is None:\n                chunk_sizes = [2048, 512, 128]\n\n            node_parser_ids = [f\"chunk_size_{chunk_size}\" for chunk_size in chunk_sizes]\n            node_parser_map = {}\n            for chunk_size, node_parser_id in zip(chunk_sizes, node_parser_ids):\n                node_parser_map[node_parser_id] = SentenceSplitter(\n                    chunk_size=chunk_size,\n                    callback_manager=callback_manager,\n                    chunk_overlap=chunk_overlap,\n                    include_metadata=include_metadata,\n                    include_prev_next_rel=include_prev_next_rel,\n                )\n        else:\n            if chunk_sizes is not None:\n                raise ValueError(\"Cannot specify both node_parser_ids and chunk_sizes.\")\n            if node_parser_map is None:\n                raise ValueError(\n                    \"Must specify node_parser_map if using node_parser_ids.\"\n                )\n\n        return cls(\n            chunk_sizes=chunk_sizes,\n            node_parser_ids=node_parser_ids,\n            node_parser_map=node_parser_map,\n            include_metadata=include_metadata,\n            include_prev_next_rel=include_prev_next_rel,\n            callback_manager=callback_manager,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"HierarchicalNodeParser\"\n\n    def _recursively_get_nodes_from_nodes(\n        self,\n        nodes: List[BaseNode],\n        level: int,\n        show_progress: bool = False,\n    ) -> List[BaseNode]:\n        \"\"\"Recursively get nodes from nodes.\"\"\"\n        if level >= len(self.node_parser_ids):\n            raise ValueError(\n                f\"Level {level} is greater than number of text \"\n                f\"splitters ({len(self.node_parser_ids)}).\"\n            )\n\n        # first split current nodes into sub-nodes\n        nodes_with_progress = get_tqdm_iterable(\n            nodes, show_progress, \"Parsing documents into nodes\"\n        )\n        sub_nodes = []\n        for node in nodes_with_progress:\n            cur_sub_nodes = self.node_parser_map[\n                self.node_parser_ids[level]\n            ].get_nodes_from_documents([node])\n            # add parent relationship from sub node to parent node\n            # add child relationship from parent node to sub node\n            # NOTE: Only add relationships if level > 0, since we don't want to add\n            # relationships for the top-level document objects that we are splitting\n            if level > 0:\n                for sub_node in cur_sub_nodes:\n                    _add_parent_child_relationship(\n                        parent_node=node,\n                        child_node=sub_node,\n                    )\n\n            sub_nodes.extend(cur_sub_nodes)\n\n        # now for each sub-node, recursively split into sub-sub-nodes, and add\n        if level < len(self.node_parser_ids) - 1:\n            sub_sub_nodes = self._recursively_get_nodes_from_nodes(\n                sub_nodes,\n                level + 1,\n                show_progress=show_progress,\n            )\n        else:\n            sub_sub_nodes = []\n\n        return sub_nodes + sub_sub_nodes\n\n    def get_nodes_from_documents(\n        self,\n        documents: Sequence[Document],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Parse document into nodes.\"\"\"\n        with self.callback_manager.event(\n            CBEventType.NODE_PARSING, payload={EventPayload.DOCUMENTS: documents}\n        ) as event:\n            all_nodes: List[BaseNode] = []\n            documents_with_progress = get_tqdm_iterable(\n                documents, show_progress, \"Parsing documents into nodes\"\n            )\n\n            # TODO: a bit of a hack rn for tqdm\n            for doc in documents_with_progress:\n                nodes_from_doc = self._recursively_get_nodes_from_nodes([doc], 0)\n                all_nodes.extend(nodes_from_doc)\n\n            event.on_end(payload={EventPayload.NODES: all_nodes})\n\n        return all_nodes\n\n    # Unused abstract method\n    def _parse_nodes(\n        self, nodes: Sequence[BaseNode], show_progress: bool = False, **kwargs: Any\n    ) -> List[BaseNode]:\n        return list(nodes)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/__init__.py",
    "filename": "__init__.py",
    "relpath": "node_parser/relational/__init__.py",
    "start_line": 1,
    "end_line": 19,
    "length": 19,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.node_parser.relational.hierarchical import (\n    HierarchicalNodeParser,\n)\nfrom llama_index.core.node_parser.relational.markdown_element import (\n    MarkdownElementNodeParser,\n)\nfrom llama_index.core.node_parser.relational.unstructured_element import (\n    UnstructuredElementNodeParser,\n)\nfrom llama_index.core.node_parser.relational.llama_parse_json_element import (\n    LlamaParseJsonNodeParser,\n)\n\n__all__ = [\n    \"HierarchicalNodeParser\",\n    \"MarkdownElementNodeParser\",\n    \"UnstructuredElementNodeParser\",\n    \"LlamaParseJsonNodeParser\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/utils.py",
    "filename": "utils.py",
    "relpath": "node_parser/relational/utils.py",
    "start_line": 1,
    "end_line": 74,
    "length": 74,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "md_to_df",
      "html_to_df"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "md_to_df",
      "html_to_df"
    ],
    "document_class_names": [],
    "content": "from typing import Any\n\nfrom io import StringIO\n\n\ndef md_to_df(md_str: str) -> Any:\n    \"\"\"Convert Markdown to dataframe.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"You must install the `pandas` package to use this node parser.\"\n        )\n\n    # Replace \" by \"\" in md_str\n    md_str = md_str.replace('\"', '\"\"')\n\n    # Replace markdown pipe tables with commas\n    md_str = md_str.replace(\"|\", '\",\"')\n\n    # Remove the second line (table header separator)\n    lines = md_str.split(\"\\n\")\n    md_str = \"\\n\".join(lines[:1] + lines[2:])\n\n    # Remove the first and last second char of the line (the pipes, transformed to \",\")\n    lines = md_str.split(\"\\n\")\n    md_str = \"\\n\".join([line[2:-2] for line in lines])\n\n    # Check if the table is empty\n    if len(md_str) == 0:\n        return None\n\n    # Use pandas to read the CSV string into a DataFrame\n    try:\n        return pd.read_csv(StringIO(md_str))\n    except pd.errors.ParserError:\n        return None\n\n\ndef html_to_df(html_str: str) -> Any:\n    \"\"\"Convert HTML to dataframe.\"\"\"\n    try:\n        from lxml import html\n    except ImportError:\n        raise ImportError(\n            \"You must install the `lxml` package to use this node parser.\"\n        )\n\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"You must install the `pandas` package to use this node parser.\"\n        )\n\n    tree = html.fromstring(html_str)\n    table_element = tree.xpath(\"//table\")[0]\n    rows = table_element.xpath(\".//tr\")\n\n    data = []\n    for row in rows:\n        cols = row.xpath(\".//td\")\n        cols = [c.text.strip() if c.text is not None else \"\" for c in cols]\n        data.append(cols)\n\n    # Check if the table is empty\n    if len(data) == 0:\n        return None\n\n    # Check if the all rows have the same number of columns\n    if not all(len(row) == len(data[0]) for row in data):\n        return None\n\n    return pd.DataFrame(data[1:], columns=data[0])"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/unstructured_element.py",
    "filename": "unstructured_element.py",
    "relpath": "node_parser/relational/unstructured_element.py",
    "start_line": 1,
    "end_line": 140,
    "length": 140,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "filter_table"
    ],
    "chunk_class_names": [
      "UnstructuredElementNodeParser"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "filter_table"
    ],
    "document_class_names": [
      "UnstructuredElementNodeParser"
    ],
    "content": "\"\"\"Unstructured element node parser.\"\"\"\n\nfrom typing import Any, Callable, List, Optional, Dict\n\n\nfrom llama_index.core.bridge.pydantic import Field\n\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.node_parser.relational.base_element import (\n    DEFAULT_SUMMARY_QUERY_STR,\n    BaseElementNodeParser,\n    Element,\n)\nfrom llama_index.core.schema import BaseNode, NodeRelationship, TextNode\nfrom llama_index.core.node_parser.relational.utils import html_to_df\n\n\nclass UnstructuredElementNodeParser(BaseElementNodeParser):\n    \"\"\"Unstructured element node parser.\n\n    Splits a document into Text Nodes and Index Nodes corresponding to embedded objects\n    (e.g. tables).\n\n    \"\"\"\n\n    partitioning_parameters: Optional[Dict[str, Any]] = Field(\n        default={},\n        description=\"Extra dictionary representing parameters of the partitioning process.\",\n    )\n\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager] = None,\n        llm: Optional[Any] = None,\n        summary_query_str: str = DEFAULT_SUMMARY_QUERY_STR,\n        partitioning_parameters: Optional[Dict[str, Any]] = {},\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        try:\n            import lxml  # noqa  # pants: no-infer-dep\n            import unstructured  # noqa  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"You must install the `unstructured` and `lxml` \"\n                \"package to use this node parser.\"\n            )\n        callback_manager = callback_manager or CallbackManager([])\n\n        return super().__init__(\n            callback_manager=callback_manager,\n            llm=llm,\n            summary_query_str=summary_query_str,\n            partitioning_parameters=partitioning_parameters,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"UnstructuredElementNodeParser\"\n\n    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        elements = self.extract_elements(\n            node.get_content(), table_filters=[self.filter_table]\n        )\n        table_elements = self.get_table_elements(elements)\n        # extract summaries over table elements\n        self.extract_table_summaries(table_elements)\n        # convert into nodes\n        # will return a list of Nodes and Index Nodes\n        nodes = self.get_nodes_from_elements(\n            elements, node, ref_doc_text=node.get_content()\n        )\n\n        source_document = node.source_node or node.as_related_node_info()\n        for n in nodes:\n            n.relationships[NodeRelationship.SOURCE] = source_document\n            n.metadata.update(node.metadata)\n        return nodes\n\n    async def aget_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        elements = self.extract_elements(\n            node.get_content(), table_filters=[self.filter_table]\n        )\n        table_elements = self.get_table_elements(elements)\n        # extract summaries over table elements\n        await self.aextract_table_summaries(table_elements)\n        # convert into nodes\n        # will return a list of Nodes and Index Nodes\n        nodes = self.get_nodes_from_elements(\n            elements, node, ref_doc_text=node.get_content()\n        )\n\n        source_document = node.source_node or node.as_related_node_info()\n        for n in nodes:\n            n.relationships[NodeRelationship.SOURCE] = source_document\n            n.metadata.update(node.metadata)\n        return nodes\n\n    def extract_elements(\n        self, text: str, table_filters: Optional[List[Callable]] = None, **kwargs: Any\n    ) -> List[Element]:\n        \"\"\"Extract elements from text.\"\"\"\n        from unstructured.partition.html import partition_html  # pants: no-infer-dep\n\n        table_filters = table_filters or []\n        partitioning_parameters = self.partitioning_parameters or {}\n        elements = partition_html(text=text, **partitioning_parameters)\n        output_els = []\n        for idx, element in enumerate(elements):\n            if \"unstructured.documents.elements.Table\" in str(type(element)):\n                should_keep = all(tf(element) for tf in table_filters)\n                if should_keep:\n                    table_df = html_to_df(str(element.metadata.text_as_html))\n                    output_els.append(\n                        Element(\n                            id=f\"id_{idx}\",\n                            type=\"table\",\n                            element=element,\n                            table=table_df,\n                        )\n                    )\n                else:\n                    # if not a table, keep it as Text as we don't want to lose context\n                    from unstructured.documents.elements import Text\n\n                    new_element = Text(str(element))\n                    output_els.append(\n                        Element(id=f\"id_{idx}\", type=\"text\", element=new_element)\n                    )\n            else:\n                output_els.append(Element(id=f\"id_{idx}\", type=\"text\", element=element))\n        return output_els\n\n    def filter_table(self, table_element: Any) -> bool:\n        \"\"\"Filter tables.\"\"\"\n        table_df = html_to_df(table_element.metadata.text_as_html)\n\n        # check if table_df is not None, has more than one row, and more than one column\n        return table_df is not None and not table_df.empty and len(table_df.columns) > 1"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/node_parser/relational/markdown_element.py",
    "filename": "markdown_element.py",
    "relpath": "node_parser/relational/markdown_element.py",
    "start_line": 1,
    "end_line": 223,
    "length": 223,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "filter_table"
    ],
    "chunk_class_names": [
      "MarkdownElementNodeParser"
    ],
    "document_function_names": [
      "class_name",
      "get_nodes_from_node",
      "aget_nodes_from_node",
      "extract_elements",
      "filter_table"
    ],
    "document_class_names": [
      "MarkdownElementNodeParser"
    ],
    "content": "from typing import Any, Callable, List, Optional\n\nfrom llama_index.core.node_parser.relational.base_element import (\n    BaseElementNodeParser,\n    Element,\n)\nfrom llama_index.core.schema import BaseNode, TextNode, NodeRelationship\nfrom llama_index.core.node_parser.relational.utils import md_to_df\n\n\nclass MarkdownElementNodeParser(BaseElementNodeParser):\n    \"\"\"Markdown element node parser.\n\n    Splits a markdown document into Text Nodes and Index Nodes corresponding to embedded objects\n    (e.g. tables).\n\n    \"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MarkdownElementNodeParser\"\n\n    def get_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        elements = self.extract_elements(\n            node.get_content(), table_filters=[self.filter_table], node_id=node.node_id\n        )\n        table_elements = self.get_table_elements(elements)\n        # extract summaries over table elements\n        self.extract_table_summaries(table_elements)\n        # convert into nodes\n        # will return a list of Nodes and Index Nodes\n        nodes = self.get_nodes_from_elements(\n            elements, node, ref_doc_text=node.get_content()\n        )\n        source_document = node.source_node or node.as_related_node_info()\n        for n in nodes:\n            n.relationships[NodeRelationship.SOURCE] = source_document\n            n.metadata.update(node.metadata)\n        return nodes\n\n    async def aget_nodes_from_node(self, node: TextNode) -> List[BaseNode]:\n        \"\"\"Get nodes from node.\"\"\"\n        elements = self.extract_elements(\n            node.get_content(), table_filters=[self.filter_table], node_id=node.node_id\n        )\n        table_elements = self.get_table_elements(elements)\n        # extract summaries over table elements\n        await self.aextract_table_summaries(table_elements)\n        # convert into nodes\n        # will return a list of Nodes and Index Nodes\n        nodes = self.get_nodes_from_elements(\n            elements, node, ref_doc_text=node.get_content()\n        )\n        source_document = node.source_node or node.as_related_node_info()\n        for n in nodes:\n            n.relationships[NodeRelationship.SOURCE] = source_document\n            n.metadata.update(node.metadata)\n        return nodes\n\n    def extract_elements(\n        self,\n        text: str,\n        node_id: Optional[str] = None,\n        table_filters: Optional[List[Callable]] = None,\n        **kwargs: Any,\n    ) -> List[Element]:\n        # get node id for each node so that we can avoid using the same id for different nodes\n        \"\"\"Extract elements from text.\"\"\"\n        lines = text.split(\"\\n\")\n        currentElement = None\n\n        elements: List[Element] = []\n        # Then parse the lines\n        for line in lines:\n            if line.startswith(\"```\"):\n                # check if this is the end of a code block\n                if currentElement is not None and currentElement.type == \"code\":\n                    elements.append(currentElement)\n                    currentElement = None\n                    # if there is some text after the ``` create a text element with it\n                    if len(line) > 3:\n                        elements.append(\n                            Element(\n                                id=f\"id_{len(elements)}\",\n                                type=\"text\",\n                                element=line.lstrip(\"```\"),\n                            )\n                        )\n\n                elif line.count(\"```\") == 2 and line[-3] != \"`\":\n                    # check if inline code block (aka have a second ``` in line but not at the end)\n                    if currentElement is not None:\n                        elements.append(currentElement)\n                    currentElement = Element(\n                        id=f\"id_{len(elements)}\",\n                        type=\"code\",\n                        element=line.lstrip(\"```\"),\n                    )\n                elif currentElement is not None and currentElement.type == \"text\":\n                    currentElement.element += \"\\n\" + line\n                else:\n                    if currentElement is not None:\n                        elements.append(currentElement)\n                    currentElement = Element(\n                        id=f\"id_{len(elements)}\", type=\"text\", element=line\n                    )\n\n            elif currentElement is not None and currentElement.type == \"code\":\n                currentElement.element += \"\\n\" + line\n\n            elif line.startswith(\"|\"):\n                if currentElement is not None and currentElement.type != \"table\":\n                    if currentElement is not None:\n                        elements.append(currentElement)\n                    currentElement = Element(\n                        id=f\"id_{len(elements)}\", type=\"table\", element=line\n                    )\n                elif currentElement is not None:\n                    currentElement.element += \"\\n\" + line\n                else:\n                    currentElement = Element(\n                        id=f\"id_{len(elements)}\", type=\"table\", element=line\n                    )\n            elif line.startswith(\"#\"):\n                if currentElement is not None:\n                    elements.append(currentElement)\n                currentElement = Element(\n                    id=f\"id_{len(elements)}\",\n                    type=\"title\",\n                    element=line.lstrip(\"#\"),\n                    title_level=len(line) - len(line.lstrip(\"#\")),\n                )\n            else:\n                if currentElement is not None and currentElement.type != \"text\":\n                    elements.append(currentElement)\n                    currentElement = Element(\n                        id=f\"id_{len(elements)}\", type=\"text\", element=line\n                    )\n                elif currentElement is not None:\n                    currentElement.element += \"\\n\" + line\n                else:\n                    currentElement = Element(\n                        id=f\"id_{len(elements)}\", type=\"text\", element=line\n                    )\n        if currentElement is not None:\n            elements.append(currentElement)\n\n        for idx, element in enumerate(elements):\n            if element.type == \"table\":\n                should_keep = True\n                perfect_table = True\n\n                # verify that the table (markdown) have the same number of columns on each rows\n                table_lines = element.element.split(\"\\n\")\n                table_columns = [len(line.split(\"|\")) for line in table_lines]\n                if len(set(table_columns)) > 1:\n                    # if the table have different number of columns on each rows, it's not a perfect table\n                    # we will store the raw text for such tables instead of converting them to a dataframe\n                    perfect_table = False\n\n                # verify that the table (markdown) have at least 2 rows\n                if len(table_lines) < 2:\n                    should_keep = False\n\n                # apply the table filter, now only filter empty tables\n                if should_keep and perfect_table and table_filters is not None:\n                    should_keep = all(tf(element) for tf in table_filters)\n\n                # if the element is a table, convert it to a dataframe\n                if should_keep:\n                    if perfect_table:\n                        table = md_to_df(element.element)\n\n                        elements[idx] = Element(\n                            id=f\"id_{node_id}_{idx}\" if node_id else f\"id_{idx}\",\n                            type=\"table\",\n                            element=element.element,\n                            table=table,\n                        )\n                    else:\n                        # for non-perfect tables, we will store the raw text\n                        # and give it a different type to differentiate it from perfect tables\n                        elements[idx] = Element(\n                            id=f\"id_{node_id}_{idx}\" if node_id else f\"id_{idx}\",\n                            type=\"table_text\",\n                            element=element.element,\n                            # table=table\n                        )\n                else:\n                    elements[idx] = Element(\n                        id=f\"id_{node_id}_{idx}\" if node_id else f\"id_{idx}\",\n                        type=\"text\",\n                        element=element.element,\n                    )\n            else:\n                # if the element is not a table, keep it as to text\n                elements[idx] = Element(\n                    id=f\"id_{node_id}_{idx}\" if node_id else f\"id_{idx}\",\n                    type=\"text\",\n                    element=element.element,\n                )\n\n        # merge consecutive text elements together for now\n        merged_elements: List[Element] = []\n        for element in elements:\n            if (\n                len(merged_elements) > 0\n                and element.type == \"text\"\n                and merged_elements[-1].type == \"text\"\n            ):\n                merged_elements[-1].element += \"\\n\" + element.element\n            else:\n                merged_elements.append(element)\n        elements = merged_elements\n        return merged_elements\n\n    def filter_table(self, table_element: Any) -> bool:\n        \"\"\"Filter tables.\"\"\"\n        table_df = md_to_df(table_element.element)\n\n        # check if table_df is not None, has more than one row, and more than one column\n        return table_df is not None and not table_df.empty and len(table_df.columns) > 1"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/dispatcher.py",
    "filename": "dispatcher.py",
    "relpath": "instrumentation/dispatcher.py",
    "start_line": 1,
    "end_line": 407,
    "length": 407,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "instrument_tags",
      "__call__",
      "__init__",
      "parent",
      "root",
      "add_event_handler",
      "add_span_handler",
      "event",
      "get_dispatch_event",
      "span_enter",
      "span_drop",
      "span_exit",
      "span",
      "wrapper",
      "handle_future_result",
      "async_wrapper",
      "log_name",
      "__init__",
      "add_dispatcher"
    ],
    "chunk_class_names": [
      "EventDispatcher",
      "Dispatcher",
      "Manager"
    ],
    "document_function_names": [
      "instrument_tags",
      "__call__",
      "__init__",
      "parent",
      "root",
      "add_event_handler",
      "add_span_handler",
      "event",
      "get_dispatch_event",
      "span_enter",
      "span_drop",
      "span_exit",
      "span",
      "wrapper",
      "handle_future_result",
      "async_wrapper",
      "log_name",
      "__init__",
      "add_dispatcher"
    ],
    "document_class_names": [
      "EventDispatcher",
      "Dispatcher",
      "Manager"
    ],
    "content": "import asyncio\nfrom functools import partial\nfrom contextlib import contextmanager\nfrom contextvars import Context, ContextVar, Token, copy_context\nfrom typing import Any, Callable, Generator, List, Optional, Dict, Protocol, TypeVar\nimport inspect\nimport logging\nimport uuid\nfrom deprecated import deprecated\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.instrumentation.event_handlers import BaseEventHandler\nfrom llama_index.core.instrumentation.span import active_span_id\nfrom llama_index.core.instrumentation.span_handlers import (\n    BaseSpanHandler,\n    NullSpanHandler,\n)\nfrom llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.instrumentation.events.span import SpanDropEvent\nimport wrapt\n\nDISPATCHER_SPAN_DECORATED_ATTR = \"__dispatcher_span_decorated__\"\n\n_logger = logging.getLogger(__name__)\n\n# ContextVar for managing active instrument tags\nactive_instrument_tags: ContextVar[Dict[str, Any]] = ContextVar(\n    \"instrument_tags\", default={}\n)\n_R = TypeVar(\"_R\")\n\n\n@contextmanager\ndef instrument_tags(new_tags: Dict[str, Any]) -> Generator[None, None, None]:\n    token = active_instrument_tags.set(new_tags)\n    try:\n        yield\n    finally:\n        active_instrument_tags.reset(token)\n\n\n# Keep for backwards compatibility\nclass EventDispatcher(Protocol):\n    def __call__(self, event: BaseEvent, **kwargs: Any) -> None:\n        ...\n\n\nclass Dispatcher(BaseModel):\n    \"\"\"Dispatcher class.\n\n    Responsible for dispatching BaseEvent (and its subclasses) as well as\n    sending signals to enter/exit/drop a BaseSpan. It does so by sending\n    event and span signals to its attached BaseEventHandler as well as\n    BaseSpanHandler.\n\n    Concurrency:\n        - Dispatcher is async-task and thread safe in the sense that\n        spans of async coros will maintain its hieararchy or trace-trees and\n        spans which emanate from various threads will also maintain its\n        hierarchy.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    name: str = Field(default_factory=str, description=\"Name of dispatcher\")\n    event_handlers: List[BaseEventHandler] = Field(\n        default=[], description=\"List of attached handlers\"\n    )\n    span_handlers: List[BaseSpanHandler] = Field(\n        default=[NullSpanHandler()], description=\"Span handler.\"\n    )\n    parent_name: str = Field(\n        default_factory=str, description=\"Name of parent Dispatcher.\"\n    )\n    manager: Optional[\"Manager\"] = Field(\n        default=None, description=\"Dispatcher manager.\"\n    )\n    root_name: str = Field(default=\"root\", description=\"Root of the Dispatcher tree.\")\n    propagate: bool = Field(\n        default=True,\n        description=\"Whether to propagate the event to parent dispatchers and their handlers\",\n    )\n    current_span_ids: Optional[Dict[Any, str]] = Field(\n        default_factory=dict,  # type: ignore\n        description=\"Id of current enclosing span. Used for creating `dispatch_event` partials.\",\n    )\n\n    def __init__(\n        self,\n        name: str = \"\",\n        event_handlers: List[BaseEventHandler] = [],\n        span_handlers: List[BaseSpanHandler] = [],\n        parent_name: str = \"\",\n        manager: Optional[\"Manager\"] = None,\n        root_name: str = \"root\",\n        propagate: bool = True,\n    ):\n        super().__init__(\n            name=name,\n            event_handlers=event_handlers,\n            span_handlers=span_handlers,\n            parent_name=parent_name,\n            manager=manager,\n            root_name=root_name,\n            propagate=propagate,\n        )\n\n    @property\n    def parent(self) -> \"Dispatcher\":\n        assert self.manager is not None\n        return self.manager.dispatchers[self.parent_name]\n\n    @property\n    def root(self) -> \"Dispatcher\":\n        assert self.manager is not None\n        return self.manager.dispatchers[self.root_name]\n\n    def add_event_handler(self, handler: BaseEventHandler) -> None:\n        \"\"\"Add handler to set of handlers.\"\"\"\n        self.event_handlers += [handler]\n\n    def add_span_handler(self, handler: BaseSpanHandler) -> None:\n        \"\"\"Add handler to set of handlers.\"\"\"\n        self.span_handlers += [handler]\n\n    def event(self, event: BaseEvent, **kwargs: Any) -> None:\n        \"\"\"Dispatch event to all registered handlers.\"\"\"\n        c: Optional[\"Dispatcher\"] = self\n\n        # Attach tags from the active context\n        event.tags.update(active_instrument_tags.get())\n\n        while c:\n            for h in c.event_handlers:\n                try:\n                    h.handle(event, **kwargs)\n                except BaseException:\n                    pass\n            if not c.propagate:\n                c = None\n            else:\n                c = c.parent\n\n    @deprecated(\n        version=\"0.10.41\",\n        reason=(\n            \"`get_dispatch_event()` has been deprecated in favor of using `event()` directly.\"\n            \" If running into this warning through an integration package, then please \"\n            \"update your integration to the latest version.\"\n        ),\n    )\n    def get_dispatch_event(self) -> EventDispatcher:\n        \"\"\"Keep for backwards compatibility.\n\n        In llama-index-core v0.10.41, we removed this method and made changes to\n        integrations or packs that relied on this method. Adding back this method\n        in case any integrations or apps have not been upgraded. That is, they\n        still rely on this method.\n        \"\"\"\n        return self.event\n\n    def span_enter(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        parent_id: Optional[str] = None,\n        tags: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Send notice to handlers that a span with id_ has started.\"\"\"\n        c: Optional[\"Dispatcher\"] = self\n        while c:\n            for h in c.span_handlers:\n                try:\n                    h.span_enter(\n                        id_=id_,\n                        bound_args=bound_args,\n                        instance=instance,\n                        parent_id=parent_id,\n                        tags=tags,\n                        **kwargs,\n                    )\n                except BaseException:\n                    pass\n            if not c.propagate:\n                c = None\n            else:\n                c = c.parent\n\n    def span_drop(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        err: Optional[BaseException] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Send notice to handlers that a span with id_ is being dropped.\"\"\"\n        c: Optional[\"Dispatcher\"] = self\n        while c:\n            for h in c.span_handlers:\n                try:\n                    h.span_drop(\n                        id_=id_,\n                        bound_args=bound_args,\n                        instance=instance,\n                        err=err,\n                        **kwargs,\n                    )\n                except BaseException:\n                    pass\n            if not c.propagate:\n                c = None\n            else:\n                c = c.parent\n\n    def span_exit(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        result: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Send notice to handlers that a span with id_ is exiting.\"\"\"\n        c: Optional[\"Dispatcher\"] = self\n        while c:\n            for h in c.span_handlers:\n                try:\n                    h.span_exit(\n                        id_=id_,\n                        bound_args=bound_args,\n                        instance=instance,\n                        result=result,\n                        **kwargs,\n                    )\n                except BaseException:\n                    pass\n            if not c.propagate:\n                c = None\n            else:\n                c = c.parent\n\n    def span(self, func: Callable[..., _R]) -> Callable[..., _R]:\n        # The `span` decorator should be idempotent.\n        try:\n            if hasattr(func, DISPATCHER_SPAN_DECORATED_ATTR):\n                return func\n            setattr(func, DISPATCHER_SPAN_DECORATED_ATTR, True)\n        except AttributeError:\n            # instance methods can fail with:\n            # AttributeError: 'method' object has no attribute '__dispatcher_span_decorated__'\n            pass\n\n        @wrapt.decorator\n        def wrapper(func: Callable, instance: Any, args: list, kwargs: dict) -> Any:\n            bound_args = inspect.signature(func).bind(*args, **kwargs)\n            id_ = f\"{func.__qualname__}-{uuid.uuid4()}\"\n            tags = active_instrument_tags.get()\n            result = None\n\n            # Copy the current context\n            context = copy_context()\n\n            token = active_span_id.set(id_)\n            parent_id = None if token.old_value is Token.MISSING else token.old_value\n            self.span_enter(\n                id_=id_,\n                bound_args=bound_args,\n                instance=instance,\n                parent_id=parent_id,\n                tags=tags,\n            )\n\n            def handle_future_result(\n                future: asyncio.Future,\n                span_id: str,\n                bound_args: inspect.BoundArguments,\n                instance: Any,\n                context: Context,\n            ) -> None:\n                from llama_index.core.workflow.errors import WorkflowCancelledByUser\n\n                try:\n                    exception = future.exception()\n                    if exception is not None:\n                        raise exception\n\n                    result = future.result()\n                    self.span_exit(\n                        id_=span_id,\n                        bound_args=bound_args,\n                        instance=instance,\n                        result=result,\n                    )\n                    return result\n                except WorkflowCancelledByUser:\n                    self.span_exit(\n                        id_=span_id,\n                        bound_args=bound_args,\n                        instance=instance,\n                        result=None,\n                    )\n                    return None\n                except BaseException as e:\n                    self.event(SpanDropEvent(span_id=span_id, err_str=str(e)))\n                    self.span_drop(\n                        id_=span_id, bound_args=bound_args, instance=instance, err=e\n                    )\n                    raise\n                finally:\n                    try:\n                        context.run(active_span_id.reset, token)\n                    except ValueError as e:\n                        # TODO: Since the context is created in a sync context no in async task,\n                        # detaching the token raises an ValueError saying \"token was created\n                        # in a different Context. We should figure out how to handle active spans\n                        # correctly, but for now just suppressing the error so it won't be\n                        # surfaced to the user.\n                        _logger.debug(f\"Failed to reset active_span_id: {e}\")\n\n            try:\n                result = func(*args, **kwargs)\n                if isinstance(result, asyncio.Future):\n                    # If the result is a Future, wrap it\n                    new_future = asyncio.ensure_future(result)\n                    new_future.add_done_callback(\n                        partial(\n                            handle_future_result,\n                            span_id=id_,\n                            bound_args=bound_args,\n                            instance=instance,\n                            context=context,\n                        )\n                    )\n                    return new_future\n                else:\n                    # For non-Future results, proceed as before\n                    self.span_exit(\n                        id_=id_, bound_args=bound_args, instance=instance, result=result\n                    )\n                    return result\n            except BaseException as e:\n                self.event(SpanDropEvent(span_id=id_, err_str=str(e)))\n                self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)\n                raise\n            finally:\n                if not isinstance(result, asyncio.Future):\n                    active_span_id.reset(token)\n\n        @wrapt.decorator\n        async def async_wrapper(\n            func: Callable, instance: Any, args: list, kwargs: dict\n        ) -> Any:\n            bound_args = inspect.signature(func).bind(*args, **kwargs)\n            id_ = f\"{func.__qualname__}-{uuid.uuid4()}\"\n            tags = active_instrument_tags.get()\n\n            token = active_span_id.set(id_)\n            parent_id = None if token.old_value is Token.MISSING else token.old_value\n            self.span_enter(\n                id_=id_,\n                bound_args=bound_args,\n                instance=instance,\n                parent_id=parent_id,\n                tags=tags,\n            )\n            try:\n                result = await func(*args, **kwargs)\n            except BaseException as e:\n                self.event(SpanDropEvent(span_id=id_, err_str=str(e)))\n                self.span_drop(id_=id_, bound_args=bound_args, instance=instance, err=e)\n                raise\n            else:\n                self.span_exit(\n                    id_=id_, bound_args=bound_args, instance=instance, result=result\n                )\n                return result\n            finally:\n                # clean up\n                active_span_id.reset(token)\n\n        if inspect.iscoroutinefunction(func):\n            return async_wrapper(func)\n        else:\n            return wrapper(func)\n\n    @property\n    def log_name(self) -> str:\n        \"\"\"Name to be used in logging.\"\"\"\n        if self.parent:\n            return f\"{self.parent.name}.{self.name}\"\n        else:\n            return self.name\n\n\nclass Manager:\n    def __init__(self, root: Dispatcher) -> None:\n        self.dispatchers: Dict[str, Dispatcher] = {root.name: root}\n\n    def add_dispatcher(self, d: Dispatcher) -> None:\n        if d.name in self.dispatchers:\n            pass\n        else:\n            self.dispatchers[d.name] = d\n\n\nDispatcher.model_rebuild()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/__init__.py",
    "filename": "__init__.py",
    "relpath": "instrumentation/__init__.py",
    "start_line": 1,
    "end_line": 83,
    "length": 83,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_dispatcher",
      "__init_subclass__"
    ],
    "chunk_class_names": [
      "DispatcherSpanMixin"
    ],
    "document_function_names": [
      "get_dispatcher",
      "__init_subclass__"
    ],
    "document_class_names": [
      "DispatcherSpanMixin"
    ],
    "content": "import inspect\nfrom abc import ABC\nfrom typing import Any, List\n\nfrom llama_index.core.instrumentation.dispatcher import (\n    Dispatcher,\n    Manager,\n    DISPATCHER_SPAN_DECORATED_ATTR,\n)\nfrom llama_index.core.instrumentation.event_handlers import NullEventHandler\nfrom llama_index.core.instrumentation.span_handlers import NullSpanHandler\n\nroot_dispatcher: Dispatcher = Dispatcher(\n    name=\"root\",\n    event_handlers=[NullEventHandler()],\n    span_handlers=[NullSpanHandler()],\n    propagate=False,\n)\n\nroot_manager: Manager = Manager(root_dispatcher)\n\n\ndef get_dispatcher(name: str = \"root\") -> Dispatcher:\n    \"\"\"Module method that should be used for creating a new Dispatcher.\"\"\"\n    if name in root_manager.dispatchers:\n        return root_manager.dispatchers[name]\n\n    candidate_parent_name = \".\".join(name.split(\".\")[:-1])\n    if candidate_parent_name in root_manager.dispatchers:\n        parent_name = candidate_parent_name\n    else:\n        parent_name = \"root\"\n\n    new_dispatcher = Dispatcher(\n        name=name,\n        root_name=root_dispatcher.name,\n        parent_name=parent_name,\n        manager=root_manager,\n    )\n    root_manager.add_dispatcher(new_dispatcher)\n    return new_dispatcher\n\n\nclass DispatcherSpanMixin(ABC):\n    \"\"\"\n    Apply the `dispatcher.span` decorator to implementations of abstract methods, as well\n    as any methods previously decorated (in any base class) that are being overridden by\n    a subclass. For example, if class `A` has abstract method `f`, and class `B` inherits\n    from `A` and provides an implementation of `f`, then `B.f` will be decorated by the mixin.\n    Furthermore, if `B` has a non-abstract method `g` that is decorated by `dispatcher.span`\n    and new class `C` inherits from `B` and overrides `g`, then `C.g` will also be decorated\n    by the mixin. Note that users can still manually apply `dispatcher.span` to the methods\n    in their custom subclasses without creating duplicate spans because the `dispatcher.span`\n    decorator should be idempotent.\n    \"\"\"\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        super().__init_subclass__(**kwargs)\n        abstract_methods: List[str] = []\n        decorated_methods: List[str] = []\n        for base_cls in inspect.getmro(cls):\n            if base_cls is cls:\n                continue\n            for attr, method in base_cls.__dict__.items():\n                if not callable(method):\n                    continue\n                if (\n                    hasattr(method, \"__isabstractmethod__\")\n                    and method.__isabstractmethod__\n                ):\n                    abstract_methods.append(attr)\n                elif hasattr(method, DISPATCHER_SPAN_DECORATED_ATTR):\n                    decorated_methods.append(attr)\n        dispatcher = get_dispatcher(cls.__module__)\n        for attr, method in cls.__dict__.items():\n            if (\n                not callable(method)\n                or hasattr(method, \"__isabstractmethod__\")\n                and method.__isabstractmethod__\n            ):\n                continue\n            if attr in abstract_methods or attr in decorated_methods:\n                setattr(cls, attr, dispatcher.span(method))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/base_handler.py",
    "filename": "base_handler.py",
    "relpath": "instrumentation/base_handler.py",
    "start_line": 1,
    "end_line": 8,
    "length": 8,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "init"
    ],
    "chunk_class_names": [
      "BaseInstrumentationHandler"
    ],
    "document_function_names": [
      "init"
    ],
    "document_class_names": [
      "BaseInstrumentationHandler"
    ],
    "content": "from abc import ABC, abstractmethod\n\n\nclass BaseInstrumentationHandler(ABC):\n    @classmethod\n    @abstractmethod\n    def init(cls) -> None:\n        \"\"\"Initialize the instrumentation handler.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/chat_engine.py",
    "filename": "chat_engine.py",
    "relpath": "instrumentation/events/chat_engine.py",
    "start_line": 1,
    "end_line": 57,
    "length": 57,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "StreamChatStartEvent",
      "StreamChatEndEvent",
      "StreamChatErrorEvent",
      "StreamChatDeltaReceivedEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "StreamChatStartEvent",
      "StreamChatEndEvent",
      "StreamChatErrorEvent",
      "StreamChatDeltaReceivedEvent"
    ],
    "content": "from llama_index.core.instrumentation.events.base import BaseEvent\n\n\nclass StreamChatStartEvent(BaseEvent):\n    \"\"\"StreamChatStartEvent.\n\n    Fired at the start of writing to the stream chat-engine queue.\n    \"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"StreamChatStartEvent\"\n\n\nclass StreamChatEndEvent(BaseEvent):\n    \"\"\"StreamChatEndEvent.\n\n    Fired at the end of writing to the stream chat-engine queue.\n    \"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"StreamChatEndEvent\"\n\n\nclass StreamChatErrorEvent(BaseEvent):\n    \"\"\"StreamChatErrorEvent.\n\n    Fired when an exception is raised during the stream chat-engine operation.\n\n    Args:\n        exception (Exception): Exception raised during the stream chat operation.\n    \"\"\"\n\n    exception: Exception\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"StreamChatErrorEvent\"\n\n\nclass StreamChatDeltaReceivedEvent(BaseEvent):\n    \"\"\"StreamChatDeltaReceivedEvent.\n\n    Args:\n        delta (str): Delta received from the stream chat.\n    \"\"\"\n\n    delta: str\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"StreamChatDeltaReceivedEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/base.py",
    "filename": "base.py",
    "relpath": "instrumentation/events/base.py",
    "start_line": 1,
    "end_line": 31,
    "length": 31,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "dict",
      "model_dump"
    ],
    "chunk_class_names": [
      "BaseEvent",
      "name"
    ],
    "document_function_names": [
      "class_name",
      "dict",
      "model_dump"
    ],
    "document_class_names": [
      "BaseEvent",
      "name"
    ],
    "content": "from typing import Any, Dict, Optional\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom uuid import uuid4\nfrom datetime import datetime\n\nfrom llama_index.core.instrumentation.span import active_span_id\n\n\nclass BaseEvent(BaseModel):\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        # copy_on_model_validation = \"deep\"  # not supported in Pydantic V2...\n    )\n    timestamp: datetime = Field(default_factory=lambda: datetime.now())\n    id_: str = Field(default_factory=lambda: str(uuid4()))\n    span_id: Optional[str] = Field(default_factory=active_span_id.get)  # type: ignore\n    tags: Dict[str, Any] = Field(default={})\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Return class name.\"\"\"\n        return \"BaseEvent\"\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Keep for backwards compatibility.\"\"\"\n        return self.model_dump(**kwargs)\n\n    def model_dump(self, **kwargs: Any) -> Dict[str, Any]:\n        data = super().model_dump(**kwargs)\n        data[\"class_name\"] = self.class_name()\n        return data"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/agent.py",
    "filename": "agent.py",
    "relpath": "instrumentation/events/agent.py",
    "start_line": 1,
    "end_line": 122,
    "length": 122,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "validate_response",
      "validate_response_type",
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "AgentRunStepStartEvent",
      "AgentRunStepEndEvent",
      "AgentChatWithStepStartEvent",
      "AgentChatWithStepEndEvent",
      "AgentToolCallEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "validate_response",
      "validate_response_type",
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "AgentRunStepStartEvent",
      "AgentRunStepEndEvent",
      "AgentChatWithStepStartEvent",
      "AgentChatWithStepEndEvent",
      "AgentToolCallEvent"
    ],
    "content": "from typing import Any, Optional\n\nfrom llama_index.core.base.agent.types import TaskStepOutput, TaskStep\nfrom llama_index.core.bridge.pydantic import model_validator, field_validator\nfrom llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n    AgentChatResponse,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.tools.types import ToolMetadata\n\n\nclass AgentRunStepStartEvent(BaseEvent):\n    \"\"\"AgentRunStepStartEvent.\n\n    Args:\n        task_id (str): Task ID.\n        step (Optional[TaskStep]): Task step.\n        input (Optional[str]): Optional input.\n    \"\"\"\n\n    task_id: str\n    step: Optional[TaskStep]\n    input: Optional[str]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"AgentRunStepStartEvent\"\n\n\nclass AgentRunStepEndEvent(BaseEvent):\n    \"\"\"AgentRunStepEndEvent.\n\n    Args:\n        step_output (TaskStepOutput): Task step output.\n    \"\"\"\n\n    step_output: TaskStepOutput\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"AgentRunStepEndEvent\"\n\n\nclass AgentChatWithStepStartEvent(BaseEvent):\n    \"\"\"AgentChatWithStepStartEvent.\n\n    Args:\n        user_msg (str): User input message.\n    \"\"\"\n\n    user_msg: str\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"AgentChatWithStepStartEvent\"\n\n\nclass AgentChatWithStepEndEvent(BaseEvent):\n    \"\"\"AgentChatWithStepEndEvent.\n\n    Args:\n        response (Optional[AGENT_CHAT_RESPONSE_TYPE]): Agent chat response.\n    \"\"\"\n\n    response: Optional[AGENT_CHAT_RESPONSE_TYPE]\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_response(cls: Any, values: Any) -> Any:\n        \"\"\"Validate response.\"\"\"\n        response = values.get(\"response\")\n        if response is None:\n            pass\n        elif not isinstance(response, AgentChatResponse) and not isinstance(\n            response, StreamingAgentChatResponse\n        ):\n            raise ValueError(\n                \"response must be of type AgentChatResponse or StreamingAgentChatResponse\"\n            )\n\n        return values\n\n    @field_validator(\"response\", mode=\"before\")\n    @classmethod\n    def validate_response_type(cls: Any, response: Any) -> Any:\n        \"\"\"Validate response type.\"\"\"\n        if response is None:\n            return response\n        if not isinstance(response, AgentChatResponse) and not isinstance(\n            response, StreamingAgentChatResponse\n        ):\n            raise ValueError(\n                \"response must be of type AgentChatResponse or StreamingAgentChatResponse\"\n            )\n        return response\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"AgentChatWithStepEndEvent\"\n\n\nclass AgentToolCallEvent(BaseEvent):\n    \"\"\"AgentToolCallEvent.\n\n    Args:\n        arguments (str): Arguments.\n        tool (ToolMetadata): Tool metadata.\n    \"\"\"\n\n    arguments: str\n    tool: ToolMetadata\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"AgentToolCallEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/__init__.py",
    "filename": "__init__.py",
    "relpath": "instrumentation/events/__init__.py",
    "start_line": 1,
    "end_line": 5,
    "length": 5,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.instrumentation.events.base import BaseEvent\n\n__all__ = [\n    \"BaseEvent\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/embedding.py",
    "filename": "embedding.py",
    "relpath": "instrumentation/events/embedding.py",
    "start_line": 1,
    "end_line": 71,
    "length": 71,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "EmbeddingStartEvent",
      "EmbeddingEndEvent",
      "SparseEmbeddingStartEvent",
      "SparseEmbeddingEndEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "EmbeddingStartEvent",
      "EmbeddingEndEvent",
      "SparseEmbeddingStartEvent",
      "SparseEmbeddingEndEvent"
    ],
    "content": "from typing import Dict, List\n\nfrom llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.bridge.pydantic import ConfigDict\n\n\nclass EmbeddingStartEvent(BaseEvent):\n    \"\"\"EmbeddingStartEvent.\n\n    Args:\n        model_dict (dict): Model dictionary containing details about the embedding model.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_dict: dict\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"EmbeddingStartEvent\"\n\n\nclass EmbeddingEndEvent(BaseEvent):\n    \"\"\"EmbeddingEndEvent.\n\n    Args:\n        chunks (List[str]): List of chunks.\n        embeddings (List[List[float]]): List of embeddings.\n\n    \"\"\"\n\n    chunks: List[str]\n    embeddings: List[List[float]]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"EmbeddingEndEvent\"\n\n\nclass SparseEmbeddingStartEvent(BaseEvent):\n    \"\"\"EmbeddingStartEvent.\n\n    Args:\n        model_dict (dict): Model dictionary containing details about the embedding model.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    model_dict: dict\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SparseEmbeddingStartEvent\"\n\n\nclass SparseEmbeddingEndEvent(BaseEvent):\n    \"\"\"EmbeddingEndEvent.\n\n    Args:\n        chunks (List[str]): List of chunks.\n        embeddings (List[List[float]]): List of embeddings.\n    \"\"\"\n\n    chunks: List[str]\n    embeddings: List[Dict[int, float]]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SparseEmbeddingEndEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/query.py",
    "filename": "query.py",
    "relpath": "instrumentation/events/query.py",
    "start_line": 1,
    "end_line": 35,
    "length": 35,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "QueryStartEvent",
      "QueryEndEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "QueryStartEvent",
      "QueryEndEvent"
    ],
    "content": "from llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.schema import QueryType\n\n\nclass QueryStartEvent(BaseEvent):\n    \"\"\"QueryStartEvent.\n\n    Args:\n        query (QueryType): Query as a string or query bundle.\n    \"\"\"\n\n    query: QueryType\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"QueryStartEvent\"\n\n\nclass QueryEndEvent(BaseEvent):\n    \"\"\"QueryEndEvent.\n\n    Args:\n        query (QueryType): Query as a string or query bundle.\n        response (RESPONSE_TYPE): Response.\n    \"\"\"\n\n    query: QueryType\n    response: RESPONSE_TYPE\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"QueryEndEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/synthesis.py",
    "filename": "synthesis.py",
    "relpath": "instrumentation/events/synthesis.py",
    "start_line": 1,
    "end_line": 66,
    "length": 66,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "SynthesizeStartEvent",
      "SynthesizeEndEvent",
      "GetResponseStartEvent",
      "GetResponseEndEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "SynthesizeStartEvent",
      "SynthesizeEndEvent",
      "GetResponseStartEvent",
      "GetResponseEndEvent"
    ],
    "content": "from typing import List\n\nfrom llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.schema import QueryType\n\n\nclass SynthesizeStartEvent(BaseEvent):\n    \"\"\"SynthesizeStartEvent.\n\n    Args:\n        query (QueryType): Query as a string or query bundle.\n    \"\"\"\n\n    query: QueryType\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SynthesizeStartEvent\"\n\n\nclass SynthesizeEndEvent(BaseEvent):\n    \"\"\"SynthesizeEndEvent.\n\n    Args:\n        query (QueryType): Query as a string or query bundle.\n        response (RESPONSE_TYPE): Response.\n    \"\"\"\n\n    query: QueryType\n    response: RESPONSE_TYPE\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SynthesizeEndEvent\"\n\n\nclass GetResponseStartEvent(BaseEvent):\n    \"\"\"GetResponseStartEvent.\n\n    Args:\n        query_str (str): Query string.\n        text_chunks (List[str]): List of text chunks.\n    \"\"\"\n\n    query_str: str\n    text_chunks: List[str]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"GetResponseStartEvent\"\n\n\nclass GetResponseEndEvent(BaseEvent):\n    \"\"\"GetResponseEndEvent.\"\"\"\n\n    # TODO: consumes the first chunk of generators??\n    # response: RESPONSE_TEXT_TYPE\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"GetResponseEndEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/retrieval.py",
    "filename": "retrieval.py",
    "relpath": "instrumentation/events/retrieval.py",
    "start_line": 1,
    "end_line": 35,
    "length": 35,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "RetrievalStartEvent",
      "RetrievalEndEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "RetrievalStartEvent",
      "RetrievalEndEvent"
    ],
    "content": "from typing import List\nfrom llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.schema import QueryType, NodeWithScore\n\n\nclass RetrievalStartEvent(BaseEvent):\n    \"\"\"RetrievalStartEvent.\n\n    Args:\n        str_or_query_bundle (QueryType): Query bundle.\n    \"\"\"\n\n    str_or_query_bundle: QueryType\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"RetrievalStartEvent\"\n\n\nclass RetrievalEndEvent(BaseEvent):\n    \"\"\"RetrievalEndEvent.\n\n    Args:\n        str_or_query_bundle (QueryType): Query bundle.\n        nodes (List[NodeWithScore]): List of nodes with scores.\n    \"\"\"\n\n    str_or_query_bundle: QueryType\n    nodes: List[NodeWithScore]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"RetrievalEndEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/exception.py",
    "filename": "exception.py",
    "relpath": "instrumentation/events/exception.py",
    "start_line": 1,
    "end_line": 16,
    "length": 16,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name"
    ],
    "chunk_class_names": [
      "ExceptionEvent"
    ],
    "document_function_names": [
      "class_name"
    ],
    "document_class_names": [
      "ExceptionEvent"
    ],
    "content": "from llama_index.core.instrumentation.events import BaseEvent\n\n\nclass ExceptionEvent(BaseEvent):\n    \"\"\"ExceptionEvent.\n\n    Args:\n        exception (BaseException): exception.\n    \"\"\"\n\n    exception: BaseException\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"ExceptionEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/span.py",
    "filename": "span.py",
    "relpath": "instrumentation/events/span.py",
    "start_line": 1,
    "end_line": 16,
    "length": 16,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name"
    ],
    "chunk_class_names": [
      "SpanDropEvent"
    ],
    "document_function_names": [
      "class_name"
    ],
    "document_class_names": [
      "SpanDropEvent"
    ],
    "content": "from llama_index.core.instrumentation.events.base import BaseEvent\n\n\nclass SpanDropEvent(BaseEvent):\n    \"\"\"SpanDropEvent.\n\n    Args:\n        err_str (str): Error string.\n    \"\"\"\n\n    err_str: str\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SpanDropEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/rerank.py",
    "filename": "rerank.py",
    "relpath": "instrumentation/events/rerank.py",
    "start_line": 1,
    "end_line": 42,
    "length": 42,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "ReRankStartEvent",
      "ReRankEndEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "ReRankStartEvent",
      "ReRankEndEvent"
    ],
    "content": "from typing import List, Optional\n\nfrom llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.schema import NodeWithScore, QueryType\nfrom llama_index.core.bridge.pydantic import ConfigDict\n\n\nclass ReRankStartEvent(BaseEvent):\n    \"\"\"ReRankStartEvent.\n\n    Args:\n        query (QueryType): Query as a string or query bundle.\n        nodes (List[NodeWithScore]): List of nodes with scores.\n        top_n (int): Number of nodes to return after rerank.\n        model_name (str): Name of the model used for reranking.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    query: Optional[QueryType]\n    nodes: List[NodeWithScore]\n    top_n: int\n    model_name: str\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"ReRankStartEvent\"\n\n\nclass ReRankEndEvent(BaseEvent):\n    \"\"\"ReRankEndEvent.\n\n    Args:\n        nodes (List[NodeWithScore]): List of returned nodes after rerank.\n    \"\"\"\n\n    nodes: List[NodeWithScore]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"ReRankEndEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/events/llm.py",
    "filename": "llm.py",
    "relpath": "instrumentation/events/llm.py",
    "start_line": 1,
    "end_line": 200,
    "length": 200,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "chunk_class_names": [
      "LLMPredictStartEvent",
      "LLMPredictEndEvent",
      "LLMStructuredPredictStartEvent",
      "to",
      "LLMStructuredPredictEndEvent",
      "LLMStructuredPredictInProgressEvent",
      "LLMCompletionStartEvent",
      "LLMCompletionInProgressEvent",
      "LLMCompletionEndEvent",
      "LLMChatStartEvent",
      "LLMChatInProgressEvent",
      "LLMChatEndEvent"
    ],
    "document_function_names": [
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name",
      "class_name"
    ],
    "document_class_names": [
      "LLMPredictStartEvent",
      "LLMPredictEndEvent",
      "LLMStructuredPredictStartEvent",
      "to",
      "LLMStructuredPredictEndEvent",
      "LLMStructuredPredictInProgressEvent",
      "LLMCompletionStartEvent",
      "LLMCompletionInProgressEvent",
      "LLMCompletionEndEvent",
      "LLMChatStartEvent",
      "LLMChatInProgressEvent",
      "LLMChatEndEvent"
    ],
    "content": "from typing import Any, List, Optional\nfrom llama_index.core.bridge.pydantic import SerializeAsAny, ConfigDict\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    CompletionResponse,\n)\nfrom llama_index.core.instrumentation.events.base import BaseEvent\nfrom llama_index.core.prompts import BasePromptTemplate\n\n\nclass LLMPredictStartEvent(BaseEvent):\n    \"\"\"LLMPredictStartEvent.\n\n    Args:\n        template (BasePromptTemplate): Prompt template.\n        template_args (Optional[dict]): Prompt template arguments.\n    \"\"\"\n\n    template: SerializeAsAny[BasePromptTemplate]\n    template_args: Optional[dict]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMPredictStartEvent\"\n\n\nclass LLMPredictEndEvent(BaseEvent):\n    \"\"\"LLMPredictEndEvent.\n\n    The result of an llm.predict() call.\n\n    Args:\n        output (str): Output.\n    \"\"\"\n\n    output: str\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMPredictEndEvent\"\n\n\nclass LLMStructuredPredictStartEvent(BaseEvent):\n    \"\"\"LLMStructuredPredictStartEvent.\n\n    Args:\n        output_cls (Any): Output class to predict.\n        template (BasePromptTemplate): Prompt template.\n        template_args (Optional[dict]): Prompt template arguments.\n    \"\"\"\n\n    output_cls: Any\n    template: SerializeAsAny[BasePromptTemplate]\n    template_args: Optional[dict]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMStructuredPredictStartEvent\"\n\n\nclass LLMStructuredPredictEndEvent(BaseEvent):\n    \"\"\"LLMStructuredPredictEndEvent.\n\n    Args:\n        output (BaseModel): Predicted output class.\n    \"\"\"\n\n    output: SerializeAsAny[Any]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMStructuredPredictEndEvent\"\n\n\nclass LLMStructuredPredictInProgressEvent(BaseEvent):\n    \"\"\"LLMStructuredPredictInProgressEvent.\n\n    Args:\n        output (BaseModel): Predicted output class.\n    \"\"\"\n\n    output: SerializeAsAny[Any]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMStructuredPredictInProgressEvent\"\n\n\nclass LLMCompletionStartEvent(BaseEvent):\n    \"\"\"LLMCompletionStartEvent.\n\n    Args:\n        prompt (str): The prompt to be completed.\n        additional_kwargs (dict): Additional keyword arguments.\n        model_dict (dict): Model dictionary.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    prompt: str\n    additional_kwargs: dict\n    model_dict: dict\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMCompletionStartEvent\"\n\n\nclass LLMCompletionInProgressEvent(BaseEvent):\n    \"\"\"LLMCompletionInProgressEvent.\n\n    Args:\n        prompt (str): The prompt to be completed.\n        response (CompletionResponse): Completion response.\n    \"\"\"\n\n    prompt: str\n    response: CompletionResponse\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMCompletionInProgressEvent\"\n\n\nclass LLMCompletionEndEvent(BaseEvent):\n    \"\"\"LLMCompletionEndEvent.\n\n    Args:\n        prompt (str): The prompt to be completed.\n        response (CompletionResponse): Completion response.\n    \"\"\"\n\n    prompt: str\n    response: CompletionResponse\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMCompletionEndEvent\"\n\n\nclass LLMChatStartEvent(BaseEvent):\n    \"\"\"LLMChatStartEvent.\n\n    Args:\n        messages (List[ChatMessage]): List of chat messages.\n        additional_kwargs (dict): Additional keyword arguments.\n        model_dict (dict): Model dictionary.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    messages: List[ChatMessage]\n    additional_kwargs: dict\n    model_dict: dict\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMChatStartEvent\"\n\n\nclass LLMChatInProgressEvent(BaseEvent):\n    \"\"\"LLMChatInProgressEvent.\n\n    Args:\n        messages (List[ChatMessage]): List of chat messages.\n        response (ChatResponse): Chat response currently being streamed.\n    \"\"\"\n\n    messages: List[ChatMessage]\n    response: ChatResponse\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMChatInProgressEvent\"\n\n\nclass LLMChatEndEvent(BaseEvent):\n    \"\"\"LLMChatEndEvent.\n\n    Args:\n        messages (List[ChatMessage]): List of chat messages.\n        response (Optional[ChatResponse]): Last chat response.\n    \"\"\"\n\n    messages: List[ChatMessage]\n    response: Optional[ChatResponse]\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"LLMChatEndEvent\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/span_handlers/base.py",
    "filename": "base.py",
    "relpath": "instrumentation/span_handlers/base.py",
    "start_line": 1,
    "end_line": 158,
    "length": 158,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "lock",
      "span_enter",
      "span_exit",
      "span_drop",
      "new_span",
      "prepare_to_exit_span",
      "prepare_to_drop_span"
    ],
    "chunk_class_names": [
      "BaseSpanHandler"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "lock",
      "span_enter",
      "span_exit",
      "span_drop",
      "new_span",
      "prepare_to_exit_span",
      "prepare_to_drop_span"
    ],
    "document_class_names": [
      "BaseSpanHandler"
    ],
    "content": "import inspect\nimport threading\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Generic, Optional, TypeVar\n\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, PrivateAttr, ConfigDict\nfrom llama_index.core.instrumentation.span.base import BaseSpan\n\nT = TypeVar(\"T\", bound=BaseSpan)\n\n\nclass BaseSpanHandler(BaseModel, Generic[T]):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    open_spans: Dict[str, T] = Field(\n        default_factory=dict, description=\"Dictionary of open spans.\"\n    )\n    completed_spans: List[T] = Field(\n        default_factory=list, description=\"List of completed spans.\"\n    )\n    dropped_spans: List[T] = Field(\n        default_factory=list, description=\"List of completed spans.\"\n    )\n    current_span_ids: Dict[Any, Optional[str]] = Field(\n        default={}, description=\"Id of current spans in a given thread.\"\n    )\n    _lock: Optional[threading.Lock] = PrivateAttr()\n\n    def __init__(\n        self,\n        open_spans: Dict[str, T] = {},\n        completed_spans: List[T] = [],\n        dropped_spans: List[T] = [],\n        current_span_ids: Dict[Any, str] = {},\n    ):\n        super().__init__(\n            open_spans=open_spans,\n            completed_spans=completed_spans,\n            dropped_spans=dropped_spans,\n            current_span_ids=current_span_ids,\n        )\n        self._lock = None\n\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseSpanHandler\"\n\n    @property\n    def lock(self) -> threading.Lock:\n        if self._lock is None:\n            self._lock = threading.Lock()\n        return self._lock\n\n    def span_enter(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        parent_id: Optional[str] = None,\n        tags: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Logic for entering a span.\"\"\"\n        if id_ in self.open_spans:\n            pass  # should probably raise an error here\n        else:\n            span = self.new_span(\n                id_=id_,\n                bound_args=bound_args,\n                instance=instance,\n                parent_span_id=parent_id,\n                tags=tags,\n            )\n            if span:\n                with self.lock:\n                    self.open_spans[id_] = span\n\n    def span_exit(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        result: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Logic for exiting a span.\"\"\"\n        span = self.prepare_to_exit_span(\n            id_=id_, bound_args=bound_args, instance=instance, result=result\n        )\n        if span:\n            with self.lock:\n                del self.open_spans[id_]\n\n    def span_drop(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        err: Optional[BaseException] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Logic for dropping a span i.e. early exit.\"\"\"\n        span = self.prepare_to_drop_span(\n            id_=id_, bound_args=bound_args, instance=instance, err=err\n        )\n        if span:\n            with self.lock:\n                del self.open_spans[id_]\n\n    @abstractmethod\n    def new_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        parent_span_id: Optional[str] = None,\n        tags: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Optional[T]:\n        \"\"\"Create a span.\n\n        Subclasses of BaseSpanHandler should create the respective span type T\n        and return it. Only NullSpanHandler should return a None here.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def prepare_to_exit_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        result: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> Optional[T]:\n        \"\"\"Logic for preparing to exit a span.\n\n        Subclasses of BaseSpanHandler should return back the specific span T\n        that is to be exited. If None is returned, then the span won't actually\n        be exited.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def prepare_to_drop_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        err: Optional[BaseException] = None,\n        **kwargs: Any,\n    ) -> Optional[T]:\n        \"\"\"Logic for preparing to drop a span.\n\n        Subclasses of BaseSpanHandler should return back the specific span T\n        that is to be dropped. If None is returned, then the span won't actually\n        be dropped.\n        \"\"\"\n        ..."
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/span_handlers/__init__.py",
    "filename": "__init__.py",
    "relpath": "instrumentation/span_handlers/__init__.py",
    "start_line": 1,
    "end_line": 10,
    "length": 10,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.instrumentation.span_handlers.base import BaseSpanHandler\nfrom llama_index.core.instrumentation.span_handlers.null import NullSpanHandler\nfrom llama_index.core.instrumentation.span_handlers.simple import SimpleSpanHandler\n\n\n__all__ = [\n    \"BaseSpanHandler\",\n    \"NullSpanHandler\",\n    \"SimpleSpanHandler\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/span_handlers/null.py",
    "filename": "null.py",
    "relpath": "instrumentation/span_handlers/null.py",
    "start_line": 1,
    "end_line": 68,
    "length": 68,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "span_enter",
      "span_exit",
      "new_span",
      "prepare_to_exit_span",
      "prepare_to_drop_span"
    ],
    "chunk_class_names": [
      "NullSpanHandler"
    ],
    "document_function_names": [
      "class_name",
      "span_enter",
      "span_exit",
      "new_span",
      "prepare_to_exit_span",
      "prepare_to_drop_span"
    ],
    "document_class_names": [
      "NullSpanHandler"
    ],
    "content": "import inspect\nfrom typing import Dict, Optional, Any\nfrom llama_index.core.instrumentation.span_handlers.base import BaseSpanHandler\nfrom llama_index.core.instrumentation.span.base import BaseSpan\n\n\nclass NullSpanHandler(BaseSpanHandler[BaseSpan]):\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"NullSpanHandler\"\n\n    def span_enter(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        parent_id: Optional[str] = None,\n        tags: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Logic for entering a span.\"\"\"\n        return\n\n    def span_exit(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        result: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Logic for exiting a span.\"\"\"\n        return\n\n    def new_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        parent_span_id: Optional[str] = None,\n        tags: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a span.\"\"\"\n        return\n\n    def prepare_to_exit_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        result: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Logic for exiting a span.\"\"\"\n        return\n\n    def prepare_to_drop_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        err: Optional[BaseException] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Logic for droppping a span.\"\"\"\n        return"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/span_handlers/simple.py",
    "filename": "simple.py",
    "relpath": "instrumentation/span_handlers/simple.py",
    "start_line": 1,
    "end_line": 147,
    "length": 147,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "new_span",
      "prepare_to_exit_span",
      "prepare_to_drop_span",
      "_get_parents",
      "_build_tree_by_parent",
      "_get_trace_trees",
      "print_trace_trees"
    ],
    "chunk_class_names": [
      "SimpleSpanHandler"
    ],
    "document_function_names": [
      "class_name",
      "new_span",
      "prepare_to_exit_span",
      "prepare_to_drop_span",
      "_get_parents",
      "_build_tree_by_parent",
      "_get_trace_trees",
      "print_trace_trees"
    ],
    "document_class_names": [
      "SimpleSpanHandler"
    ],
    "content": "import inspect\nfrom typing import Any, Dict, cast, List, Optional, TYPE_CHECKING\nfrom llama_index.core.instrumentation.span.simple import SimpleSpan\nfrom llama_index.core.instrumentation.span_handlers.base import BaseSpanHandler\nfrom datetime import datetime\nfrom functools import reduce\nimport warnings\n\nif TYPE_CHECKING:\n    from treelib import Tree\n\n\nclass SimpleSpanHandler(BaseSpanHandler[SimpleSpan]):\n    \"\"\"Span Handler that manages SimpleSpan's.\"\"\"\n\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SimpleSpanHandler\"\n\n    def new_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        parent_span_id: Optional[str] = None,\n        tags: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> SimpleSpan:\n        \"\"\"Create a span.\"\"\"\n        return SimpleSpan(id_=id_, parent_id=parent_span_id, tags=tags or {})\n\n    def prepare_to_exit_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        result: Optional[Any] = None,\n        **kwargs: Any,\n    ) -> SimpleSpan:\n        \"\"\"Logic for preparing to drop a span.\"\"\"\n        span = self.open_spans[id_]\n        span = cast(SimpleSpan, span)\n        span.end_time = datetime.now()\n        span.duration = (span.end_time - span.start_time).total_seconds()\n        with self.lock:\n            self.completed_spans += [span]\n        return span\n\n    def prepare_to_drop_span(\n        self,\n        id_: str,\n        bound_args: inspect.BoundArguments,\n        instance: Optional[Any] = None,\n        err: Optional[BaseException] = None,\n        **kwargs: Any,\n    ) -> Optional[SimpleSpan]:\n        \"\"\"Logic for droppping a span.\"\"\"\n        if id_ in self.open_spans:\n            with self.lock:\n                span = self.open_spans[id_]\n                span.metadata = {\"error\": str(err)}\n                self.dropped_spans += [span]\n            return span\n\n        return None\n\n    def _get_parents(self) -> List[SimpleSpan]:\n        \"\"\"Helper method to get all parent/root spans.\"\"\"\n        all_spans = self.completed_spans + self.dropped_spans\n        return [s for s in all_spans if s.parent_id is None]\n\n    def _build_tree_by_parent(\n        self, parent: SimpleSpan, acc: List[SimpleSpan], spans: List[SimpleSpan]\n    ) -> List[SimpleSpan]:\n        \"\"\"Builds the tree by parent root.\"\"\"\n        if not spans:\n            return acc\n\n        children = [s for s in spans if s.parent_id == parent.id_]\n        if not children:\n            return acc\n        updated_spans = [s for s in spans if s not in children]\n\n        children_trees = [\n            self._build_tree_by_parent(\n                parent=c, acc=[c], spans=[s for s in updated_spans if c != s]\n            )\n            for c in children\n        ]\n\n        return acc + reduce(lambda x, y: x + y, children_trees)\n\n    def _get_trace_trees(self) -> List[\"Tree\"]:\n        \"\"\"Method for getting trace trees.\"\"\"\n        try:\n            from treelib import Tree\n        except ImportError as e:\n            raise ImportError(\n                \"`treelib` package is missing. Please install it by using \"\n                \"`pip install treelib`.\"\n            )\n\n        all_spans = self.completed_spans + self.dropped_spans\n        for s in all_spans:\n            if s.parent_id is None:\n                continue\n            if not any(ns.id_ == s.parent_id for ns in all_spans):\n                warnings.warn(f\"Parent with id {s.parent_id} missing from spans\")\n                s.parent_id += \"-MISSING\"\n                all_spans.append(SimpleSpan(id_=s.parent_id, parent_id=None))\n\n        parents = self._get_parents()\n        span_groups = []\n        for p in parents:\n            this_span_group = self._build_tree_by_parent(\n                parent=p, acc=[p], spans=[s for s in all_spans if s != p]\n            )\n            sorted_span_group = sorted(this_span_group, key=lambda x: x.start_time)\n            span_groups.append(sorted_span_group)\n\n        trees = []\n        tree = Tree()\n        for grp in span_groups:\n            for span in grp:\n                if span.parent_id is None:\n                    # complete old tree unless its empty (i.e., start of loop)\n                    if tree.all_nodes():\n                        trees.append(tree)\n                        # start new tree\n                        tree = Tree()\n\n                tree.create_node(\n                    tag=f\"{span.id_} ({span.duration})\",\n                    identifier=span.id_,\n                    parent=span.parent_id,\n                    data=span.start_time,\n                )\n\n        trees.append(tree)\n        return trees\n\n    def print_trace_trees(self) -> None:\n        \"\"\"Method for viewing trace trees.\"\"\"\n        trees = self._get_trace_trees()\n        for tree in trees:\n            print(tree.show(stdout=False, sorting=True, key=lambda node: node.data))\n            print(\"\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/span/base.py",
    "filename": "base.py",
    "relpath": "instrumentation/span/base.py",
    "start_line": 1,
    "end_line": 12,
    "length": 12,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "BaseSpan",
      "representing"
    ],
    "document_function_names": [],
    "document_class_names": [
      "BaseSpan",
      "representing"
    ],
    "content": "from typing import Any, Dict, Optional\nfrom uuid import uuid4\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\n\n\nclass BaseSpan(BaseModel):\n    \"\"\"Base data class representing a span.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    id_: str = Field(default_factory=lambda: str(uuid4()), description=\"Id of span.\")\n    parent_id: Optional[str] = Field(default=None, description=\"Id of parent span.\")\n    tags: Dict[str, Any] = Field(default={})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/span/__init__.py",
    "filename": "__init__.py",
    "relpath": "instrumentation/span/__init__.py",
    "start_line": 1,
    "end_line": 11,
    "length": 11,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from contextvars import ContextVar\nfrom typing import Optional\n\nfrom llama_index.core.instrumentation.span.base import BaseSpan\nfrom llama_index.core.instrumentation.span.simple import SimpleSpan\n\n# ContextVar for managing active spans\nactive_span_id: ContextVar[Optional[str]] = ContextVar(\"active_span_id\", default=None)\nactive_span_id.set(None)\n\n__all__ = [\"BaseSpan\", \"SimpleSpan\", \"active_span_id\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/span/simple.py",
    "filename": "simple.py",
    "relpath": "instrumentation/span/simple.py",
    "start_line": 1,
    "end_line": 13,
    "length": 13,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "SimpleSpan"
    ],
    "document_function_names": [],
    "document_class_names": [
      "SimpleSpan"
    ],
    "content": "from typing import Dict, Optional\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.instrumentation.span.base import BaseSpan\nfrom datetime import datetime\n\n\nclass SimpleSpan(BaseSpan):\n    \"\"\"Simple span class.\"\"\"\n\n    start_time: datetime = Field(default_factory=lambda: datetime.now())\n    end_time: Optional[datetime] = Field(default=None)\n    duration: float = Field(default=0.0, description=\"Duration of span in seconds.\")\n    metadata: Optional[Dict] = Field(default=None)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/event_handlers/base.py",
    "filename": "base.py",
    "relpath": "instrumentation/event_handlers/base.py",
    "start_line": 1,
    "end_line": 19,
    "length": 19,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "handle"
    ],
    "chunk_class_names": [
      "BaseEventHandler"
    ],
    "document_function_names": [
      "class_name",
      "handle"
    ],
    "document_class_names": [
      "BaseEventHandler"
    ],
    "content": "from typing import Any\nfrom abc import abstractmethod\nfrom llama_index.core.bridge.pydantic import BaseModel, ConfigDict\nfrom llama_index.core.instrumentation.events.base import BaseEvent\n\n\nclass BaseEventHandler(BaseModel):\n    \"\"\"Base callback handler that can be used to track event starts and ends.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"BaseEventHandler\"\n\n    @abstractmethod\n    def handle(self, event: BaseEvent, **kwargs: Any) -> Any:\n        \"\"\"Logic for handling event.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/event_handlers/__init__.py",
    "filename": "__init__.py",
    "relpath": "instrumentation/event_handlers/__init__.py",
    "start_line": 1,
    "end_line": 5,
    "length": 5,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.instrumentation.event_handlers.base import BaseEventHandler\nfrom llama_index.core.instrumentation.event_handlers.null import NullEventHandler\n\n\n__all__ = [\"BaseEventHandler\", \"NullEventHandler\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/instrumentation/event_handlers/null.py",
    "filename": "null.py",
    "relpath": "instrumentation/event_handlers/null.py",
    "start_line": 1,
    "end_line": 14,
    "length": 14,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "handle"
    ],
    "chunk_class_names": [
      "NullEventHandler"
    ],
    "document_function_names": [
      "class_name",
      "handle"
    ],
    "document_class_names": [
      "NullEventHandler"
    ],
    "content": "from typing import Any\nfrom llama_index.core.instrumentation.event_handlers.base import BaseEventHandler\nfrom llama_index.core.instrumentation.events.base import BaseEvent\n\n\nclass NullEventHandler(BaseEventHandler):\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"NullEventHandler\"\n\n    def handle(self, event: BaseEvent, **kwargs: Any) -> Any:\n        \"\"\"Handle logic - null handler does nothing.\"\"\"\n        return"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/service_context_elements/llm_predictor.py",
    "filename": "llm_predictor.py",
    "relpath": "service_context_elements/llm_predictor.py",
    "start_line": 1,
    "end_line": 82,
    "length": 82,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "model_dump",
      "dict",
      "to_dict",
      "llm",
      "callback_manager",
      "metadata",
      "predict",
      "stream",
      "apredict",
      "astream",
      "__init__"
    ],
    "chunk_class_names": [
      "BaseLLMPredictor",
      "LLMPredictor",
      "directly",
      "is",
      "directly"
    ],
    "document_function_names": [
      "model_dump",
      "dict",
      "to_dict",
      "llm",
      "callback_manager",
      "metadata",
      "predict",
      "stream",
      "apredict",
      "astream",
      "__init__"
    ],
    "document_class_names": [
      "BaseLLMPredictor",
      "LLMPredictor",
      "directly",
      "is",
      "directly"
    ],
    "content": "\"\"\"Wrapper functions around an LLM chain.\"\"\"\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict\n\nfrom llama_index.core.base.llms.types import LLMMetadata\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate\nfrom llama_index.core.schema import BaseComponent\nfrom llama_index.core.types import TokenAsyncGen, TokenGen\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseLLMPredictor(BaseComponent, DispatcherSpanMixin, ABC):\n    \"\"\"Base LLM Predictor.\"\"\"\n\n    def model_dump(self, **kwargs: Any) -> Dict[str, Any]:\n        print(\"here\", flush=True)\n        data = super().model_dump(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()\n        return data\n\n    def dict(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Keep for backwards compatibility.\"\"\"\n        return self.model_dump(**kwargs)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        data = super().to_dict(**kwargs)\n        data[\"llm\"] = self.llm.to_dict()\n        return data\n\n    @property\n    @abstractmethod\n    def llm(self) -> LLM:\n        \"\"\"Get LLM.\"\"\"\n\n    @property\n    @abstractmethod\n    def callback_manager(self) -> CallbackManager:\n        \"\"\"Get callback manager.\"\"\"\n\n    @property\n    @abstractmethod\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n\n    @abstractmethod\n    def predict(self, prompt: BasePromptTemplate, **prompt_args: Any) -> str:\n        \"\"\"Predict the answer to a query.\"\"\"\n\n    @abstractmethod\n    def stream(self, prompt: BasePromptTemplate, **prompt_args: Any) -> TokenGen:\n        \"\"\"Stream the answer to a query.\"\"\"\n\n    @abstractmethod\n    async def apredict(self, prompt: BasePromptTemplate, **prompt_args: Any) -> str:\n        \"\"\"Async predict the answer to a query.\"\"\"\n\n    @abstractmethod\n    async def astream(\n        self, prompt: BasePromptTemplate, **prompt_args: Any\n    ) -> TokenAsyncGen:\n        \"\"\"Async predict the answer to a query.\"\"\"\n\n\nclass LLMPredictor(BaseLLMPredictor):\n    \"\"\"LLM predictor class.\n\n    NOTE: Deprecated. Use any LLM class directly.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        raise ValueError(\"This class is deprecated. Use any LLM class directly.\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/service_context_elements/llama_logger.py",
    "filename": "llama_logger.py",
    "relpath": "service_context_elements/llama_logger.py",
    "start_line": 1,
    "end_line": 39,
    "length": 39,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "reset",
      "set_metadata",
      "unset_metadata",
      "get_metadata",
      "add_log",
      "get_logs"
    ],
    "chunk_class_names": [
      "LlamaLogger"
    ],
    "document_function_names": [
      "__init__",
      "reset",
      "set_metadata",
      "unset_metadata",
      "get_metadata",
      "add_log",
      "get_logs"
    ],
    "document_class_names": [
      "LlamaLogger"
    ],
    "content": "\"\"\"Logger class.\"\"\"\n\nfrom typing import Any, Dict, List, Set\n\n\nclass LlamaLogger:\n    \"\"\"Logger class.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Init params.\"\"\"\n        self._logs: List[Dict] = []\n        self._metadata: Dict[str, Any] = {}\n\n    def reset(self) -> None:\n        \"\"\"Reset logs.\"\"\"\n        self._logs = []\n\n    def set_metadata(self, metadata: Dict) -> None:\n        \"\"\"Set metadata.\"\"\"\n        self._metadata.update(metadata)\n\n    def unset_metadata(self, metadata_keys: Set) -> None:\n        \"\"\"Unset metadata.\"\"\"\n        for key in metadata_keys:\n            self._metadata.pop(key, None)\n\n    def get_metadata(self) -> Dict:\n        \"\"\"Get metadata.\"\"\"\n        return self._metadata\n\n    def add_log(self, log: Dict) -> None:\n        \"\"\"Add log.\"\"\"\n        updated_log = {**self._metadata, **log}\n        # TODO: figure out better abstraction\n        self._logs.append(updated_log)\n\n    def get_logs(self) -> List[Dict]:\n        \"\"\"Get logs.\"\"\"\n        return self._logs"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/__init__.py",
    "filename": "__init__.py",
    "relpath": "storage/__init__.py",
    "start_line": 1,
    "end_line": 7,
    "length": 7,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Storage classes.\"\"\"\n\nfrom llama_index.core.storage.storage_context import StorageContext\n\n__all__ = [\n    \"StorageContext\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/storage_context.py",
    "filename": "storage_context.py",
    "relpath": "storage/storage_context.py",
    "start_line": 1,
    "end_line": 273,
    "length": 273,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_defaults",
      "persist",
      "to_dict",
      "from_dict",
      "vector_store",
      "add_vector_store"
    ],
    "chunk_class_names": [
      "from",
      "class"
    ],
    "document_function_names": [
      "from_defaults",
      "persist",
      "to_dict",
      "from_dict",
      "vector_store",
      "add_vector_store"
    ],
    "document_class_names": [
      "from",
      "class"
    ],
    "content": "import os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Optional, Union\n\nimport fsspec\nfrom llama_index.core.constants import (\n    DOC_STORE_KEY,\n    GRAPH_STORE_KEY,\n    INDEX_STORE_KEY,\n    VECTOR_STORE_KEY,\n    PG_STORE_KEY,\n)\nfrom llama_index.core.graph_stores.simple import (\n    DEFAULT_PERSIST_FNAME as GRAPH_STORE_FNAME,\n)\nfrom llama_index.core.graph_stores.simple import SimpleGraphStore\nfrom llama_index.core.graph_stores.simple_labelled import SimplePropertyGraphStore\nfrom llama_index.core.graph_stores.types import DEFUALT_PG_PERSIST_FNAME as PG_FNAME\nfrom llama_index.core.graph_stores.types import GraphStore, PropertyGraphStore\nfrom llama_index.core.storage.docstore.simple_docstore import SimpleDocumentStore\nfrom llama_index.core.storage.docstore.types import (\n    DEFAULT_PERSIST_FNAME as DOCSTORE_FNAME,\n)\nfrom llama_index.core.storage.docstore.types import BaseDocumentStore\nfrom llama_index.core.storage.index_store.simple_index_store import (\n    SimpleIndexStore,\n)\nfrom llama_index.core.storage.index_store.types import (\n    DEFAULT_PERSIST_FNAME as INDEX_STORE_FNAME,\n)\nfrom llama_index.core.storage.index_store.types import BaseIndexStore\nfrom llama_index.core.utils import concat_dirs\nfrom llama_index.core.vector_stores.simple import (\n    DEFAULT_PERSIST_FNAME as VECTOR_STORE_FNAME,\n)\nfrom llama_index.core.vector_stores.simple import (\n    DEFAULT_VECTOR_STORE,\n    NAMESPACE_SEP,\n    SimpleVectorStore,\n)\nfrom llama_index.core.vector_stores.types import (\n    BasePydanticVectorStore,\n)\nfrom llama_index.core.bridge.pydantic import SerializeAsAny\n\nDEFAULT_PERSIST_DIR = \"./storage\"\nIMAGE_STORE_FNAME = \"image_store.json\"\nIMAGE_VECTOR_STORE_NAMESPACE = \"image\"\n\n\n@dataclass\nclass StorageContext:\n    \"\"\"Storage context.\n\n    The storage context container is a utility container for storing nodes,\n    indices, and vectors. It contains the following:\n    - docstore: BaseDocumentStore\n    - index_store: BaseIndexStore\n    - vector_store: BasePydanticVectorStore\n    - graph_store: GraphStore\n    - property_graph_store: PropertyGraphStore (lazily initialized)\n\n    \"\"\"\n\n    docstore: BaseDocumentStore\n    index_store: BaseIndexStore\n    vector_stores: Dict[str, SerializeAsAny[BasePydanticVectorStore]]\n    graph_store: GraphStore\n    property_graph_store: Optional[PropertyGraphStore] = None\n\n    @classmethod\n    def from_defaults(\n        cls,\n        docstore: Optional[BaseDocumentStore] = None,\n        index_store: Optional[BaseIndexStore] = None,\n        vector_store: Optional[BasePydanticVectorStore] = None,\n        image_store: Optional[BasePydanticVectorStore] = None,\n        vector_stores: Optional[Dict[str, BasePydanticVectorStore]] = None,\n        graph_store: Optional[GraphStore] = None,\n        property_graph_store: Optional[PropertyGraphStore] = None,\n        persist_dir: Optional[str] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"StorageContext\":\n        \"\"\"Create a StorageContext from defaults.\n\n        Args:\n            docstore (Optional[BaseDocumentStore]): document store\n            index_store (Optional[BaseIndexStore]): index store\n            vector_store (Optional[BasePydanticVectorStore]): vector store\n            graph_store (Optional[GraphStore]): graph store\n            image_store (Optional[BasePydanticVectorStore]): image store\n\n        \"\"\"\n        if persist_dir is None:\n            docstore = docstore or SimpleDocumentStore()\n            index_store = index_store or SimpleIndexStore()\n            graph_store = graph_store or SimpleGraphStore()\n            image_store = image_store or SimpleVectorStore()\n\n            if vector_store:\n                vector_stores = {DEFAULT_VECTOR_STORE: vector_store}\n            else:\n                vector_stores = vector_stores or {\n                    DEFAULT_VECTOR_STORE: SimpleVectorStore()\n                }\n            if image_store:\n                # append image store to vector stores\n                vector_stores[IMAGE_VECTOR_STORE_NAMESPACE] = image_store\n        else:\n            docstore = docstore or SimpleDocumentStore.from_persist_dir(\n                persist_dir, fs=fs\n            )\n            index_store = index_store or SimpleIndexStore.from_persist_dir(\n                persist_dir, fs=fs\n            )\n            graph_store = graph_store or SimpleGraphStore.from_persist_dir(\n                persist_dir, fs=fs\n            )\n\n            try:\n                property_graph_store = (\n                    property_graph_store\n                    or SimplePropertyGraphStore.from_persist_dir(persist_dir, fs=fs)\n                )\n            except FileNotFoundError:\n                property_graph_store = None\n\n            if vector_store:\n                vector_stores = {DEFAULT_VECTOR_STORE: vector_store}\n            elif vector_stores:\n                vector_stores = vector_stores\n            else:\n                vector_stores = SimpleVectorStore.from_namespaced_persist_dir(\n                    persist_dir, fs=fs\n                )\n            if image_store:\n                # append image store to vector stores\n                vector_stores[IMAGE_VECTOR_STORE_NAMESPACE] = image_store  # type: ignore\n\n        return cls(\n            docstore=docstore,\n            index_store=index_store,\n            vector_stores=vector_stores,  # type: ignore\n            graph_store=graph_store,\n            property_graph_store=property_graph_store,\n        )\n\n    def persist(\n        self,\n        persist_dir: Union[str, os.PathLike] = DEFAULT_PERSIST_DIR,\n        docstore_fname: str = DOCSTORE_FNAME,\n        index_store_fname: str = INDEX_STORE_FNAME,\n        vector_store_fname: str = VECTOR_STORE_FNAME,\n        image_store_fname: str = IMAGE_STORE_FNAME,\n        graph_store_fname: str = GRAPH_STORE_FNAME,\n        pg_graph_store_fname: str = PG_FNAME,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the storage context.\n\n        Args:\n            persist_dir (str): directory to persist the storage context\n        \"\"\"\n        if fs is not None:\n            persist_dir = str(persist_dir)  # NOTE: doesn't support Windows here\n            docstore_path = concat_dirs(persist_dir, docstore_fname)\n            index_store_path = concat_dirs(persist_dir, index_store_fname)\n            graph_store_path = concat_dirs(persist_dir, graph_store_fname)\n            pg_graph_store_path = concat_dirs(persist_dir, pg_graph_store_fname)\n        else:\n            persist_dir = Path(persist_dir)\n            docstore_path = str(persist_dir / docstore_fname)\n            index_store_path = str(persist_dir / index_store_fname)\n            graph_store_path = str(persist_dir / graph_store_fname)\n            pg_graph_store_path = str(persist_dir / pg_graph_store_fname)\n\n        self.docstore.persist(persist_path=docstore_path, fs=fs)\n        self.index_store.persist(persist_path=index_store_path, fs=fs)\n        self.graph_store.persist(persist_path=graph_store_path, fs=fs)\n\n        if self.property_graph_store:\n            self.property_graph_store.persist(persist_path=pg_graph_store_path, fs=fs)\n\n        # save each vector store under it's namespace\n        for vector_store_name, vector_store in self.vector_stores.items():\n            if fs is not None:\n                vector_store_path = concat_dirs(\n                    str(persist_dir),\n                    f\"{vector_store_name}{NAMESPACE_SEP}{vector_store_fname}\",\n                )\n            else:\n                vector_store_path = str(\n                    Path(persist_dir)\n                    / f\"{vector_store_name}{NAMESPACE_SEP}{vector_store_fname}\"\n                )\n\n            vector_store.persist(persist_path=vector_store_path, fs=fs)\n\n    def to_dict(self) -> dict:\n        all_simple = (\n            isinstance(self.docstore, SimpleDocumentStore)\n            and isinstance(self.index_store, SimpleIndexStore)\n            and isinstance(self.graph_store, SimpleGraphStore)\n            and isinstance(\n                self.property_graph_store, (SimplePropertyGraphStore, type(None))\n            )\n            and all(\n                isinstance(vs, SimpleVectorStore) for vs in self.vector_stores.values()\n            )\n        )\n        if not all_simple:\n            raise ValueError(\n                \"to_dict only available when using simple doc/index/vector stores\"\n            )\n\n        assert isinstance(self.docstore, SimpleDocumentStore)\n        assert isinstance(self.index_store, SimpleIndexStore)\n        assert isinstance(self.graph_store, SimpleGraphStore)\n        assert isinstance(\n            self.property_graph_store, (SimplePropertyGraphStore, type(None))\n        )\n\n        return {\n            VECTOR_STORE_KEY: {\n                key: vector_store.to_dict()\n                for key, vector_store in self.vector_stores.items()\n                if isinstance(vector_store, SimpleVectorStore)\n            },\n            DOC_STORE_KEY: self.docstore.to_dict(),\n            INDEX_STORE_KEY: self.index_store.to_dict(),\n            GRAPH_STORE_KEY: self.graph_store.to_dict(),\n            PG_STORE_KEY: (\n                self.property_graph_store.to_dict()\n                if self.property_graph_store\n                else None\n            ),\n        }\n\n    @classmethod\n    def from_dict(cls, save_dict: dict) -> \"StorageContext\":\n        \"\"\"Create a StorageContext from dict.\"\"\"\n        docstore = SimpleDocumentStore.from_dict(save_dict[DOC_STORE_KEY])\n        index_store = SimpleIndexStore.from_dict(save_dict[INDEX_STORE_KEY])\n        graph_store = SimpleGraphStore.from_dict(save_dict[GRAPH_STORE_KEY])\n        property_graph_store = (\n            SimplePropertyGraphStore.from_dict(save_dict[PG_STORE_KEY])\n            if save_dict[PG_STORE_KEY]\n            else None\n        )\n\n        vector_stores: Dict[str, BasePydanticVectorStore] = {}\n        for key, vector_store_dict in save_dict[VECTOR_STORE_KEY].items():\n            vector_stores[key] = SimpleVectorStore.from_dict(vector_store_dict)\n\n        return cls(\n            docstore=docstore,\n            index_store=index_store,\n            vector_stores=vector_stores,\n            graph_store=graph_store,\n            property_graph_store=property_graph_store,\n        )\n\n    @property\n    def vector_store(self) -> BasePydanticVectorStore:\n        \"\"\"Backwrds compatibility for vector_store property.\"\"\"\n        return self.vector_stores[DEFAULT_VECTOR_STORE]\n\n    def add_vector_store(\n        self, vector_store: BasePydanticVectorStore, namespace: str\n    ) -> None:\n        \"\"\"Add a vector store to the storage context.\"\"\"\n        self.vector_stores[namespace] = vector_store"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/chat_store/base.py",
    "filename": "base.py",
    "relpath": "storage/chat_store/base.py",
    "start_line": 1,
    "end_line": 76,
    "length": 76,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "set_messages",
      "get_messages",
      "add_message",
      "delete_messages",
      "delete_message",
      "delete_last_message",
      "get_keys",
      "aset_messages",
      "aget_messages",
      "async_add_message",
      "adelete_messages",
      "adelete_message",
      "adelete_last_message",
      "aget_keys"
    ],
    "chunk_class_names": [
      "for",
      "BaseChatStore",
      "name"
    ],
    "document_function_names": [
      "class_name",
      "set_messages",
      "get_messages",
      "add_message",
      "delete_messages",
      "delete_message",
      "delete_last_message",
      "get_keys",
      "aset_messages",
      "aget_messages",
      "async_add_message",
      "adelete_messages",
      "adelete_message",
      "adelete_last_message",
      "aget_keys"
    ],
    "document_class_names": [
      "for",
      "BaseChatStore",
      "name"
    ],
    "content": "\"\"\"Base interface class for storing chat history per user.\"\"\"\nfrom abc import abstractmethod\nfrom typing import List, Optional\n\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.schema import BaseComponent\n\n\nclass BaseChatStore(BaseComponent):\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"BaseChatStore\"\n\n    @abstractmethod\n    def set_messages(self, key: str, messages: List[ChatMessage]) -> None:\n        \"\"\"Set messages for a key.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_messages(self, key: str) -> List[ChatMessage]:\n        \"\"\"Get messages for a key.\"\"\"\n        ...\n\n    @abstractmethod\n    def add_message(self, key: str, message: ChatMessage) -> None:\n        \"\"\"Add a message for a key.\"\"\"\n        ...\n\n    @abstractmethod\n    def delete_messages(self, key: str) -> Optional[List[ChatMessage]]:\n        \"\"\"Delete messages for a key.\"\"\"\n        ...\n\n    @abstractmethod\n    def delete_message(self, key: str, idx: int) -> Optional[ChatMessage]:\n        \"\"\"Delete specific message for a key.\"\"\"\n        ...\n\n    @abstractmethod\n    def delete_last_message(self, key: str) -> Optional[ChatMessage]:\n        \"\"\"Delete last message for a key.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_keys(self) -> List[str]:\n        \"\"\"Get all keys.\"\"\"\n        ...\n\n    async def aset_messages(self, key: str, messages: List[ChatMessage]) -> None:\n        \"\"\"Async version of Get messages for a key.\"\"\"\n        self.set_messages(key, messages)\n\n    async def aget_messages(self, key: str) -> List[ChatMessage]:\n        \"\"\"Async version of Get messages for a key.\"\"\"\n        return self.get_messages(key)\n\n    async def async_add_message(self, key: str, message: ChatMessage) -> None:\n        \"\"\"Async version of Add a message for a key.\"\"\"\n        self.add_message(key, message)\n\n    async def adelete_messages(self, key: str) -> Optional[List[ChatMessage]]:\n        \"\"\"Async version of Delete messages for a key.\"\"\"\n        return self.delete_messages(key)\n\n    async def adelete_message(self, key: str, idx: int) -> Optional[ChatMessage]:\n        \"\"\"Async version of Delete specific message for a key.\"\"\"\n        return self.delete_message(key, idx)\n\n    async def adelete_last_message(self, key: str) -> Optional[ChatMessage]:\n        \"\"\"Async version of Delete last message for a key.\"\"\"\n        return self.delete_last_message(key)\n\n    async def aget_keys(self) -> List[str]:\n        \"\"\"Async version of Get all keys.\"\"\"\n        return self.get_keys()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/chat_store/loading.py",
    "filename": "loading.py",
    "relpath": "storage/chat_store/loading.py",
    "start_line": 1,
    "end_line": 18,
    "length": 18,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_chat_store"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "load_chat_store"
    ],
    "document_class_names": [],
    "content": "from llama_index.core.storage.chat_store.base import BaseChatStore\nfrom llama_index.core.storage.chat_store.simple_chat_store import SimpleChatStore\n\nRECOGNIZED_CHAT_STORES = {\n    SimpleChatStore.class_name(): SimpleChatStore,\n}\n\n\ndef load_chat_store(data: dict) -> BaseChatStore:\n    \"\"\"Load a chat store from a dict.\"\"\"\n    chat_store_name = data.get(\"class_name\", None)\n    if chat_store_name is None:\n        raise ValueError(\"ChatStore loading requires a class_name\")\n\n    if chat_store_name not in RECOGNIZED_CHAT_STORES:\n        raise ValueError(f\"Invalid ChatStore name: {chat_store_name}\")\n\n    return RECOGNIZED_CHAT_STORES[chat_store_name].from_dict(data)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/chat_store/__init__.py",
    "filename": "__init__.py",
    "relpath": "storage/chat_store/__init__.py",
    "start_line": 1,
    "end_line": 4,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.storage.chat_store.base import BaseChatStore\nfrom llama_index.core.storage.chat_store.simple_chat_store import SimpleChatStore\n\n__all__ = [\"BaseChatStore\", \"SimpleChatStore\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/chat_store/simple_chat_store.py",
    "filename": "simple_chat_store.py",
    "relpath": "storage/chat_store/simple_chat_store.py",
    "start_line": 1,
    "end_line": 112,
    "length": 112,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "chat_message_serialization",
      "class_name",
      "set_messages",
      "get_messages",
      "add_message",
      "delete_messages",
      "delete_message",
      "delete_last_message",
      "get_keys",
      "persist",
      "from_persist_path"
    ],
    "chunk_class_names": [
      "SimpleChatStore",
      "name"
    ],
    "document_function_names": [
      "chat_message_serialization",
      "class_name",
      "set_messages",
      "get_messages",
      "add_message",
      "delete_messages",
      "delete_message",
      "delete_last_message",
      "get_keys",
      "persist",
      "from_persist_path"
    ],
    "document_class_names": [
      "SimpleChatStore",
      "name"
    ],
    "content": "import json\nimport os\nfrom typing import Any, Dict, List, Optional\nfrom typing_extensions import Annotated\n\nimport fsspec\nfrom llama_index.core.bridge.pydantic import Field, WrapSerializer\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.storage.chat_store.base import BaseChatStore\n\n\ndef chat_message_serialization(\n    chat_message: Any, handler: Any, info: Any\n) -> Dict[str, Any]:\n    partial_result = handler(chat_message, info)\n\n    for key, value in partial_result.get(\"additional_kwargs\", {}).items():\n        value = chat_message._recursive_serialization(value)\n        if not isinstance(value, (str, int, float, bool, dict, list, type(None))):\n            raise ValueError(f\"Failed to serialize additional_kwargs value: {value}\")\n        partial_result[\"additional_kwargs\"][key] = value\n\n    return partial_result\n\n\nAnnotatedChatMessage = Annotated[\n    ChatMessage, WrapSerializer(chat_message_serialization)\n]\n\n\nclass SimpleChatStore(BaseChatStore):\n    \"\"\"Simple chat store. Async methods provide same functionality as sync methods in this class.\"\"\"\n\n    store: Dict[str, List[AnnotatedChatMessage]] = Field(default_factory=dict)\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"SimpleChatStore\"\n\n    def set_messages(self, key: str, messages: List[ChatMessage]) -> None:\n        \"\"\"Set messages for a key.\"\"\"\n        self.store[key] = messages\n\n    def get_messages(self, key: str) -> List[ChatMessage]:\n        \"\"\"Get messages for a key.\"\"\"\n        return self.store.get(key, [])\n\n    def add_message(\n        self, key: str, message: ChatMessage, idx: Optional[int] = None\n    ) -> None:\n        \"\"\"Add a message for a key.\"\"\"\n        if idx is None:\n            self.store.setdefault(key, []).append(message)\n        else:\n            self.store.setdefault(key, []).insert(idx, message)\n\n    def delete_messages(self, key: str) -> Optional[List[ChatMessage]]:\n        \"\"\"Delete messages for a key.\"\"\"\n        if key not in self.store:\n            return None\n        return self.store.pop(key)\n\n    def delete_message(self, key: str, idx: int) -> Optional[ChatMessage]:\n        \"\"\"Delete specific message for a key.\"\"\"\n        if key not in self.store:\n            return None\n        if idx >= len(self.store[key]):\n            return None\n        return self.store[key].pop(idx)\n\n    def delete_last_message(self, key: str) -> Optional[ChatMessage]:\n        \"\"\"Delete last message for a key.\"\"\"\n        if key not in self.store:\n            return None\n        return self.store[key].pop()\n\n    def get_keys(self) -> List[str]:\n        \"\"\"Get all keys.\"\"\"\n        return list(self.store.keys())\n\n    def persist(\n        self,\n        persist_path: str = \"chat_store.json\",\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the docstore to a file.\"\"\"\n        fs = fs or fsspec.filesystem(\"file\")\n        dirpath = os.path.dirname(persist_path)\n        if not fs.exists(dirpath):\n            fs.makedirs(dirpath)\n\n        with fs.open(persist_path, \"w\") as f:\n            f.write(self.json())\n\n    @classmethod\n    def from_persist_path(\n        cls,\n        persist_path: str = \"chat_store.json\",\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimpleChatStore\":\n        \"\"\"Create a SimpleChatStore from a persist path.\"\"\"\n        fs = fs or fsspec.filesystem(\"file\")\n        if not fs.exists(persist_path):\n            return cls()\n        with fs.open(persist_path, \"r\") as f:\n            data = json.load(f)\n\n        if isinstance(data, str):\n            return cls.model_validate_json(data)\n        else:\n            return cls.model_validate(data)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/kvstore/types.py",
    "filename": "types.py",
    "relpath": "storage/kvstore/types.py",
    "start_line": 1,
    "end_line": 88,
    "length": 88,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "put",
      "aput",
      "put_all",
      "aput_all",
      "get",
      "aget",
      "get_all",
      "aget_all",
      "delete",
      "adelete",
      "persist",
      "from_persist_path"
    ],
    "chunk_class_names": [
      "BaseKVStore",
      "BaseInMemoryKVStore"
    ],
    "document_function_names": [
      "put",
      "aput",
      "put_all",
      "aput_all",
      "get",
      "aget",
      "get_all",
      "aget_all",
      "delete",
      "adelete",
      "persist",
      "from_persist_path"
    ],
    "document_class_names": [
      "BaseKVStore",
      "BaseInMemoryKVStore"
    ],
    "content": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Tuple\n\nimport fsspec\n\nDEFAULT_COLLECTION = \"data\"\nDEFAULT_BATCH_SIZE = 1\n\n\nclass BaseKVStore(ABC):\n    \"\"\"Base key-value store.\"\"\"\n\n    @abstractmethod\n    def put(self, key: str, val: dict, collection: str = DEFAULT_COLLECTION) -> None:\n        pass\n\n    @abstractmethod\n    async def aput(\n        self, key: str, val: dict, collection: str = DEFAULT_COLLECTION\n    ) -> None:\n        pass\n\n    def put_all(\n        self,\n        kv_pairs: List[Tuple[str, dict]],\n        collection: str = DEFAULT_COLLECTION,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n    ) -> None:\n        # by default, support a batch size of 1\n        if batch_size != 1:\n            raise NotImplementedError(\"Batching not supported by this key-value store.\")\n        else:\n            for key, val in kv_pairs:\n                self.put(key, val, collection=collection)\n\n    async def aput_all(\n        self,\n        kv_pairs: List[Tuple[str, dict]],\n        collection: str = DEFAULT_COLLECTION,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n    ) -> None:\n        # by default, support a batch size of 1\n        if batch_size != 1:\n            raise NotImplementedError(\"Batching not supported by this key-value store.\")\n        else:\n            for key, val in kv_pairs:\n                await self.aput(key, val, collection=collection)\n\n    @abstractmethod\n    def get(self, key: str, collection: str = DEFAULT_COLLECTION) -> Optional[dict]:\n        pass\n\n    @abstractmethod\n    async def aget(\n        self, key: str, collection: str = DEFAULT_COLLECTION\n    ) -> Optional[dict]:\n        pass\n\n    @abstractmethod\n    def get_all(self, collection: str = DEFAULT_COLLECTION) -> Dict[str, dict]:\n        pass\n\n    @abstractmethod\n    async def aget_all(self, collection: str = DEFAULT_COLLECTION) -> Dict[str, dict]:\n        pass\n\n    @abstractmethod\n    def delete(self, key: str, collection: str = DEFAULT_COLLECTION) -> bool:\n        pass\n\n    @abstractmethod\n    async def adelete(self, key: str, collection: str = DEFAULT_COLLECTION) -> bool:\n        pass\n\n\nclass BaseInMemoryKVStore(BaseKVStore):\n    \"\"\"Base in-memory key-value store.\"\"\"\n\n    @abstractmethod\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_persist_path(cls, persist_path: str) -> \"BaseInMemoryKVStore\":\n        \"\"\"Create a BaseInMemoryKVStore from a persist directory.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/kvstore/__init__.py",
    "filename": "__init__.py",
    "relpath": "storage/kvstore/__init__.py",
    "start_line": 1,
    "end_line": 3,
    "length": 3,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.storage.kvstore.simple_kvstore import SimpleKVStore\n\n__all__ = [\"FirestoreKVStore\", \"SimpleKVStore\", \"MongoDBKVStore\", \"RedisKVStore\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/kvstore/simple_kvstore.py",
    "filename": "simple_kvstore.py",
    "relpath": "storage/kvstore/simple_kvstore.py",
    "start_line": 1,
    "end_line": 108,
    "length": 108,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "put",
      "aput",
      "get",
      "aget",
      "get_all",
      "aget_all",
      "delete",
      "adelete",
      "persist",
      "from_persist_path",
      "to_dict",
      "from_dict"
    ],
    "chunk_class_names": [
      "SimpleKVStore"
    ],
    "document_function_names": [
      "__init__",
      "put",
      "aput",
      "get",
      "aget",
      "get_all",
      "aget_all",
      "delete",
      "adelete",
      "persist",
      "from_persist_path",
      "to_dict",
      "from_dict"
    ],
    "document_class_names": [
      "SimpleKVStore"
    ],
    "content": "import json\nimport logging\nimport os\nfrom typing import Dict, Optional\n\nimport fsspec\nfrom llama_index.core.storage.kvstore.types import (\n    DEFAULT_COLLECTION,\n    BaseInMemoryKVStore,\n)\n\nlogger = logging.getLogger(__name__)\n\nDATA_TYPE = Dict[str, Dict[str, dict]]\n\n\nclass SimpleKVStore(BaseInMemoryKVStore):\n    \"\"\"Simple in-memory Key-Value store.\n\n    Args:\n        data (Optional[DATA_TYPE]): data to initialize the store with\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[DATA_TYPE] = None,\n    ) -> None:\n        \"\"\"Init a SimpleKVStore.\"\"\"\n        self._data: DATA_TYPE = data or {}\n\n    def put(self, key: str, val: dict, collection: str = DEFAULT_COLLECTION) -> None:\n        \"\"\"Put a key-value pair into the store.\"\"\"\n        if collection not in self._data:\n            self._data[collection] = {}\n        self._data[collection][key] = val.copy()\n\n    async def aput(\n        self, key: str, val: dict, collection: str = DEFAULT_COLLECTION\n    ) -> None:\n        \"\"\"Put a key-value pair into the store.\"\"\"\n        self.put(key, val, collection)\n\n    def get(self, key: str, collection: str = DEFAULT_COLLECTION) -> Optional[dict]:\n        \"\"\"Get a value from the store.\"\"\"\n        collection_data = self._data.get(collection, None)\n        if not collection_data:\n            return None\n        if key not in collection_data:\n            return None\n        return collection_data[key].copy()\n\n    async def aget(\n        self, key: str, collection: str = DEFAULT_COLLECTION\n    ) -> Optional[dict]:\n        \"\"\"Get a value from the store.\"\"\"\n        return self.get(key, collection)\n\n    def get_all(self, collection: str = DEFAULT_COLLECTION) -> Dict[str, dict]:\n        \"\"\"Get all values from the store.\"\"\"\n        return self._data.get(collection, {}).copy()\n\n    async def aget_all(self, collection: str = DEFAULT_COLLECTION) -> Dict[str, dict]:\n        \"\"\"Get all values from the store.\"\"\"\n        return self.get_all(collection)\n\n    def delete(self, key: str, collection: str = DEFAULT_COLLECTION) -> bool:\n        \"\"\"Delete a value from the store.\"\"\"\n        try:\n            self._data[collection].pop(key)\n            return True\n        except KeyError:\n            return False\n\n    async def adelete(self, key: str, collection: str = DEFAULT_COLLECTION) -> bool:\n        \"\"\"Delete a value from the store.\"\"\"\n        return self.delete(key, collection)\n\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        \"\"\"Persist the store.\"\"\"\n        fs = fs or fsspec.filesystem(\"file\")\n        dirpath = os.path.dirname(persist_path)\n        if not fs.exists(dirpath):\n            fs.makedirs(dirpath)\n\n        with fs.open(persist_path, \"w\") as f:\n            f.write(json.dumps(self._data))\n\n    @classmethod\n    def from_persist_path(\n        cls, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> \"SimpleKVStore\":\n        \"\"\"Load a SimpleKVStore from a persist path and filesystem.\"\"\"\n        fs = fs or fsspec.filesystem(\"file\")\n        logger.debug(f\"Loading {__name__} from {persist_path}.\")\n        with fs.open(persist_path, \"rb\") as f:\n            data = json.load(f)\n        return cls(data)\n\n    def to_dict(self) -> dict:\n        \"\"\"Save the store as dict.\"\"\"\n        return self._data\n\n    @classmethod\n    def from_dict(cls, save_dict: dict) -> \"SimpleKVStore\":\n        \"\"\"Load a SimpleKVStore from dict.\"\"\"\n        return cls(save_dict)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/simple_docstore.py",
    "filename": "simple_docstore.py",
    "relpath": "storage/docstore/simple_docstore.py",
    "start_line": 1,
    "end_line": 99,
    "length": 99,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_persist_dir",
      "from_persist_path",
      "persist",
      "from_dict",
      "to_dict"
    ],
    "chunk_class_names": [
      "SimpleDocumentStore"
    ],
    "document_function_names": [
      "__init__",
      "from_persist_dir",
      "from_persist_path",
      "persist",
      "from_dict",
      "to_dict"
    ],
    "document_class_names": [
      "SimpleDocumentStore"
    ],
    "content": "import os\nfrom typing import Optional\n\nimport fsspec\nfrom llama_index.core.storage.docstore.keyval_docstore import KVDocumentStore\nfrom llama_index.core.storage.docstore.types import (\n    DEFAULT_BATCH_SIZE,\n    DEFAULT_PERSIST_DIR,\n    DEFAULT_PERSIST_FNAME,\n    DEFAULT_PERSIST_PATH,\n)\nfrom llama_index.core.storage.kvstore.simple_kvstore import SimpleKVStore\nfrom llama_index.core.storage.kvstore.types import BaseInMemoryKVStore\nfrom llama_index.core.utils import concat_dirs\n\n\nclass SimpleDocumentStore(KVDocumentStore):\n    \"\"\"Simple Document (Node) store.\n\n    An in-memory store for Document and Node objects.\n\n    Args:\n        simple_kvstore (SimpleKVStore): simple key-value store\n        namespace (str): namespace for the docstore\n\n    \"\"\"\n\n    def __init__(\n        self,\n        simple_kvstore: Optional[SimpleKVStore] = None,\n        namespace: Optional[str] = None,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n    ) -> None:\n        \"\"\"Init a SimpleDocumentStore.\"\"\"\n        simple_kvstore = simple_kvstore or SimpleKVStore()\n        super().__init__(simple_kvstore, namespace=namespace, batch_size=batch_size)\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        namespace: Optional[str] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimpleDocumentStore\":\n        \"\"\"Create a SimpleDocumentStore from a persist directory.\n\n        Args:\n            persist_dir (str): directory to persist the store\n            namespace (Optional[str]): namespace for the docstore\n            fs (Optional[fsspec.AbstractFileSystem]): filesystem to use\n\n        \"\"\"\n        if fs is not None:\n            persist_path = concat_dirs(persist_dir, DEFAULT_PERSIST_FNAME)\n        else:\n            persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)\n        return cls.from_persist_path(persist_path, namespace=namespace, fs=fs)\n\n    @classmethod\n    def from_persist_path(\n        cls,\n        persist_path: str,\n        namespace: Optional[str] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimpleDocumentStore\":\n        \"\"\"Create a SimpleDocumentStore from a persist path.\n\n        Args:\n            persist_path (str): Path to persist the store\n            namespace (Optional[str]): namespace for the docstore\n            fs (Optional[fsspec.AbstractFileSystem]): filesystem to use\n\n        \"\"\"\n        simple_kvstore = SimpleKVStore.from_persist_path(persist_path, fs=fs)\n        return cls(simple_kvstore, namespace)\n\n    def persist(\n        self,\n        persist_path: str = DEFAULT_PERSIST_PATH,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the store.\"\"\"\n        if isinstance(self._kvstore, BaseInMemoryKVStore):\n            self._kvstore.persist(persist_path, fs=fs)\n\n    @classmethod\n    def from_dict(\n        cls, save_dict: dict, namespace: Optional[str] = None\n    ) -> \"SimpleDocumentStore\":\n        simple_kvstore = SimpleKVStore.from_dict(save_dict)\n        return cls(simple_kvstore, namespace)\n\n    def to_dict(self) -> dict:\n        assert isinstance(self._kvstore, SimpleKVStore)\n        return self._kvstore.to_dict()\n\n\n# alias for backwards compatibility\nDocumentStore = SimpleDocumentStore"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/types.py",
    "filename": "types.py",
    "relpath": "storage/docstore/types.py",
    "start_line": 1,
    "end_line": 220,
    "length": 220,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "persist",
      "docs",
      "add_documents",
      "async_add_documents",
      "get_document",
      "aget_document",
      "delete_document",
      "adelete_document",
      "document_exists",
      "adocument_exists",
      "set_document_hash",
      "aset_document_hash",
      "set_document_hashes",
      "aset_document_hashes",
      "get_document_hash",
      "aget_document_hash",
      "get_all_document_hashes",
      "aget_all_document_hashes",
      "get_all_ref_doc_info",
      "aget_all_ref_doc_info",
      "get_ref_doc_info",
      "aget_ref_doc_info",
      "delete_ref_doc",
      "adelete_ref_doc",
      "get_nodes",
      "aget_nodes",
      "get_node",
      "aget_node",
      "get_node_dict",
      "aget_node_dict"
    ],
    "chunk_class_names": [
      "class",
      "to",
      "BaseDocumentStore"
    ],
    "document_function_names": [
      "persist",
      "docs",
      "add_documents",
      "async_add_documents",
      "get_document",
      "aget_document",
      "delete_document",
      "adelete_document",
      "document_exists",
      "adocument_exists",
      "set_document_hash",
      "aset_document_hash",
      "set_document_hashes",
      "aset_document_hashes",
      "get_document_hash",
      "aget_document_hash",
      "get_all_document_hashes",
      "aget_all_document_hashes",
      "get_all_ref_doc_info",
      "aget_all_ref_doc_info",
      "get_ref_doc_info",
      "aget_ref_doc_info",
      "delete_ref_doc",
      "adelete_ref_doc",
      "get_nodes",
      "aget_nodes",
      "get_node",
      "aget_node",
      "get_node_dict",
      "aget_node_dict"
    ],
    "document_class_names": [
      "class",
      "to",
      "BaseDocumentStore"
    ],
    "content": "import os\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Sequence\n\nimport fsspec\nfrom dataclasses_json import DataClassJsonMixin\nfrom llama_index.core.schema import BaseNode\nfrom llama_index.core.storage.kvstore.types import DEFAULT_BATCH_SIZE\n\nDEFAULT_PERSIST_FNAME = \"docstore.json\"\nDEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\n\n\n@dataclass\nclass RefDocInfo(DataClassJsonMixin):\n    \"\"\"Dataclass to represent ingested documents.\"\"\"\n\n    node_ids: List = field(default_factory=list)\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n\nclass BaseDocumentStore(ABC):\n    # ===== Save/load =====\n    def persist(\n        self,\n        persist_path: str = DEFAULT_PERSIST_PATH,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the docstore to a file.\"\"\"\n\n    # ===== Main interface =====\n    @property\n    @abstractmethod\n    def docs(self) -> Dict[str, BaseNode]:\n        ...\n\n    @abstractmethod\n    def add_documents(\n        self,\n        docs: Sequence[BaseNode],\n        allow_update: bool = True,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n        store_text: bool = True,\n    ) -> None:\n        ...\n\n    @abstractmethod\n    async def async_add_documents(\n        self,\n        docs: Sequence[BaseNode],\n        allow_update: bool = True,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n        store_text: bool = True,\n    ) -> None:\n        ...\n\n    @abstractmethod\n    def get_document(self, doc_id: str, raise_error: bool = True) -> Optional[BaseNode]:\n        ...\n\n    @abstractmethod\n    async def aget_document(\n        self, doc_id: str, raise_error: bool = True\n    ) -> Optional[BaseNode]:\n        ...\n\n    @abstractmethod\n    def delete_document(self, doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a document from the store.\"\"\"\n        ...\n\n    @abstractmethod\n    async def adelete_document(self, doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a document from the store.\"\"\"\n        ...\n\n    @abstractmethod\n    def document_exists(self, doc_id: str) -> bool:\n        ...\n\n    @abstractmethod\n    async def adocument_exists(self, doc_id: str) -> bool:\n        ...\n\n    # ===== Hash =====\n    @abstractmethod\n    def set_document_hash(self, doc_id: str, doc_hash: str) -> None:\n        ...\n\n    @abstractmethod\n    async def aset_document_hash(self, doc_id: str, doc_hash: str) -> None:\n        ...\n\n    @abstractmethod\n    def set_document_hashes(self, doc_hashes: Dict[str, str]) -> None:\n        ...\n\n    @abstractmethod\n    async def aset_document_hashes(self, doc_hashes: Dict[str, str]) -> None:\n        ...\n\n    @abstractmethod\n    def get_document_hash(self, doc_id: str) -> Optional[str]:\n        ...\n\n    @abstractmethod\n    async def aget_document_hash(self, doc_id: str) -> Optional[str]:\n        ...\n\n    @abstractmethod\n    def get_all_document_hashes(self) -> Dict[str, str]:\n        ...\n\n    @abstractmethod\n    async def aget_all_document_hashes(self) -> Dict[str, str]:\n        ...\n\n    # ==== Ref Docs =====\n    @abstractmethod\n    def get_all_ref_doc_info(self) -> Optional[Dict[str, RefDocInfo]]:\n        \"\"\"Get a mapping of ref_doc_id -> RefDocInfo for all ingested documents.\"\"\"\n\n    @abstractmethod\n    async def aget_all_ref_doc_info(self) -> Optional[Dict[str, RefDocInfo]]:\n        \"\"\"Get a mapping of ref_doc_id -> RefDocInfo for all ingested documents.\"\"\"\n\n    @abstractmethod\n    def get_ref_doc_info(self, ref_doc_id: str) -> Optional[RefDocInfo]:\n        \"\"\"Get the RefDocInfo for a given ref_doc_id.\"\"\"\n\n    @abstractmethod\n    async def aget_ref_doc_info(self, ref_doc_id: str) -> Optional[RefDocInfo]:\n        \"\"\"Get the RefDocInfo for a given ref_doc_id.\"\"\"\n\n    @abstractmethod\n    def delete_ref_doc(self, ref_doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a ref_doc and all it's associated nodes.\"\"\"\n\n    @abstractmethod\n    async def adelete_ref_doc(self, ref_doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a ref_doc and all it's associated nodes.\"\"\"\n\n    # ===== Nodes =====\n    def get_nodes(\n        self, node_ids: List[str], raise_error: bool = True\n    ) -> List[BaseNode]:\n        \"\"\"Get nodes from docstore.\n\n        Args:\n            node_ids (List[str]): node ids\n            raise_error (bool): raise error if node_id not found\n\n        \"\"\"\n        return [self.get_node(node_id, raise_error=raise_error) for node_id in node_ids]\n\n    async def aget_nodes(\n        self, node_ids: List[str], raise_error: bool = True\n    ) -> List[BaseNode]:\n        \"\"\"Get nodes from docstore.\n\n        Args:\n            node_ids (List[str]): node ids\n            raise_error (bool): raise error if node_id not found\n\n        \"\"\"\n        return [\n            await self.aget_node(node_id, raise_error=raise_error)\n            for node_id in node_ids\n        ]\n\n    def get_node(self, node_id: str, raise_error: bool = True) -> BaseNode:\n        \"\"\"Get node from docstore.\n\n        Args:\n            node_id (str): node id\n            raise_error (bool): raise error if node_id not found\n\n        \"\"\"\n        doc = self.get_document(node_id, raise_error=raise_error)\n        if not isinstance(doc, BaseNode):\n            raise ValueError(f\"Document {node_id} is not a Node.\")\n        return doc\n\n    async def aget_node(self, node_id: str, raise_error: bool = True) -> BaseNode:\n        \"\"\"Get node from docstore.\n\n        Args:\n            node_id (str): node id\n            raise_error (bool): raise error if node_id not found\n\n        \"\"\"\n        doc = await self.aget_document(node_id, raise_error=raise_error)\n        if not isinstance(doc, BaseNode):\n            raise ValueError(f\"Document {node_id} is not a Node.\")\n        return doc\n\n    def get_node_dict(self, node_id_dict: Dict[int, str]) -> Dict[int, BaseNode]:\n        \"\"\"Get node dict from docstore given a mapping of index to node ids.\n\n        Args:\n            node_id_dict (Dict[int, str]): mapping of index to node ids\n\n        \"\"\"\n        return {\n            index: self.get_node(node_id) for index, node_id in node_id_dict.items()\n        }\n\n    async def aget_node_dict(self, node_id_dict: Dict[int, str]) -> Dict[int, BaseNode]:\n        \"\"\"Get node dict from docstore given a mapping of index to node ids.\n\n        Args:\n            node_id_dict (Dict[int, str]): mapping of index to node ids\n\n        \"\"\"\n        return {\n            index: await self.aget_node(node_id)\n            for index, node_id in node_id_dict.items()\n        }"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/__init__.py",
    "filename": "__init__.py",
    "relpath": "storage/docstore/__init__.py",
    "start_line": 1,
    "end_line": 12,
    "length": 12,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# alias for backwards compatibility\nfrom llama_index.core.storage.docstore.simple_docstore import (\n    DocumentStore,\n    SimpleDocumentStore,\n)\nfrom llama_index.core.storage.docstore.types import BaseDocumentStore\n\n__all__ = [\n    \"BaseDocumentStore\",\n    \"DocumentStore\",\n    \"SimpleDocumentStore\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/utils.py",
    "filename": "utils.py",
    "relpath": "storage/docstore/utils.py",
    "start_line": 1,
    "end_line": 91,
    "length": 91,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "doc_to_json",
      "json_to_doc",
      "legacy_json_to_doc"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "doc_to_json",
      "json_to_doc",
      "legacy_json_to_doc"
    ],
    "document_class_names": [],
    "content": "from llama_index.core.constants import DATA_KEY, TYPE_KEY\nfrom llama_index.core.schema import (\n    BaseNode,\n    Document,\n    ImageDocument,\n    ImageNode,\n    IndexNode,\n    NodeRelationship,\n    RelatedNodeInfo,\n    TextNode,\n)\n\n\ndef doc_to_json(doc: BaseNode) -> dict:\n    return {\n        DATA_KEY: doc.to_dict(),\n        TYPE_KEY: doc.get_type(),\n    }\n\n\ndef json_to_doc(doc_dict: dict) -> BaseNode:\n    doc_type = doc_dict[TYPE_KEY]\n    data_dict = doc_dict[DATA_KEY]\n    doc: BaseNode\n\n    if \"extra_info\" in data_dict:\n        return legacy_json_to_doc(doc_dict)\n    else:\n        if doc_type == Document.get_type():\n            if data_dict[\"class_name\"] == ImageDocument.class_name():\n                doc = ImageDocument.from_dict(data_dict)\n            else:\n                doc = Document.from_dict(data_dict)\n        elif doc_type == TextNode.get_type():\n            doc = TextNode.from_dict(data_dict)\n        elif doc_type == ImageNode.get_type():\n            doc = ImageNode.from_dict(data_dict)\n        elif doc_type == IndexNode.get_type():\n            doc = IndexNode.from_dict(data_dict)\n        else:\n            raise ValueError(f\"Unknown doc type: {doc_type}\")\n\n        return doc\n\n\ndef legacy_json_to_doc(doc_dict: dict) -> BaseNode:\n    \"\"\"Todo: Deprecated legacy support for old node versions.\"\"\"\n    doc_type = doc_dict[TYPE_KEY]\n    data_dict = doc_dict[DATA_KEY]\n    doc: BaseNode\n\n    text = data_dict.get(\"text\", \"\")\n    metadata = data_dict.get(\"extra_info\", {}) or {}\n    id_ = data_dict.get(\"doc_id\", None)\n\n    relationships = data_dict.get(\"relationships\", {})\n    relationships = {\n        NodeRelationship(k): RelatedNodeInfo(node_id=str(v))\n        for k, v in relationships.items()\n    }\n\n    if doc_type == Document.get_type():\n        doc = Document(\n            text=text, metadata=metadata, id=id_, relationships=relationships\n        )\n    elif doc_type == TextNode.get_type():\n        doc = TextNode(\n            text=text, metadata=metadata, id=id_, relationships=relationships\n        )\n    elif doc_type == ImageNode.get_type():\n        image = data_dict.get(\"image\", None)\n        doc = ImageNode(\n            text=text,\n            metadata=metadata,\n            id=id_,\n            relationships=relationships,\n            image=image,\n        )\n    elif doc_type == IndexNode.get_type():\n        index_id = data_dict.get(\"index_id\", None)\n        doc = IndexNode(\n            text=text,\n            metadata=metadata,\n            id=id_,\n            relationships=relationships,\n            index_id=index_id,\n        )\n    else:\n        raise ValueError(f\"Unknown doc type: {doc_type}\")\n\n    return doc"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/registry.py",
    "filename": "registry.py",
    "relpath": "storage/docstore/registry.py",
    "start_line": 1,
    "end_line": 24,
    "length": 24,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_default_docstore"
    ],
    "chunk_class_names": [
      "DocumentStoreType"
    ],
    "document_function_names": [
      "get_default_docstore"
    ],
    "document_class_names": [
      "DocumentStoreType"
    ],
    "content": "from enum import Enum\nfrom typing import Dict, Type\n\nfrom llama_index.core.storage.docstore.simple_docstore import SimpleDocumentStore\nfrom llama_index.core.storage.docstore.types import BaseDocumentStore\n\n\nclass DocumentStoreType(str, Enum):\n    MONGO = \"mongo\"\n    SIMPLE = \"simple\"\n\n\nDOCSTORE_TYPE_TO_CLASS: Dict[DocumentStoreType, Type[BaseDocumentStore]] = {\n    DocumentStoreType.SIMPLE: SimpleDocumentStore,\n}\n\n\nDOCSTORE_CLASS_TO_TYPE: Dict[Type[BaseDocumentStore], DocumentStoreType] = {\n    cls_: type_ for type_, cls_ in DOCSTORE_TYPE_TO_CLASS.items()\n}\n\n\ndef get_default_docstore() -> BaseDocumentStore:\n    return SimpleDocumentStore()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/keyval_docstore.py",
    "filename": "keyval_docstore.py",
    "relpath": "storage/docstore/keyval_docstore.py",
    "start_line": 1,
    "end_line": 21,
    "length": 21,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "docs",
      "_get_kv_pairs_for_insert",
      "_merge_ref_doc_kv_pairs",
      "_prepare_kv_pairs",
      "add_documents",
      "_async_prepare_kv_pairs",
      "async_add_documents",
      "get_document",
      "aget_document",
      "_remove_legacy_info",
      "get_ref_doc_info",
      "aget_ref_doc_info",
      "get_all_ref_doc_info",
      "aget_all_ref_doc_info",
      "ref_doc_exists",
      "aref_doc_exists",
      "document_exists",
      "adocument_exists",
      "_get_ref_doc_id",
      "_aget_ref_doc_id",
      "_remove_from_ref_doc_node",
      "_aremove_from_ref_doc_node",
      "delete_document",
      "adelete_document",
      "delete_ref_doc",
      "adelete_ref_doc",
      "set_document_hash",
      "set_document_hashes",
      "aset_document_hash",
      "aset_document_hashes",
      "get_document_hash",
      "aget_document_hash",
      "get_all_document_hashes",
      "aget_all_document_hashes"
    ],
    "document_class_names": [
      "KVDocumentStore"
    ],
    "content": "\"\"\"Document store.\"\"\"\n\nimport asyncio\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple\n\nfrom llama_index.core.schema import BaseNode, Document, TextNode\nfrom llama_index.core.storage.docstore.types import BaseDocumentStore, RefDocInfo\nfrom llama_index.core.storage.docstore.utils import doc_to_json, json_to_doc\nfrom llama_index.core.storage.kvstore.types import DEFAULT_BATCH_SIZE, BaseKVStore\n\n# The default namespace prefix for the document store.\nDEFAULT_NAMESPACE = \"docstore\"\n# The nodes collection contains the content of each node, along with metadata specific\n# to each node, including associated attributes like excluded metadata and relationships.\nDEFAULT_COLLECTION_DATA_SUFFIX = \"/data\"\n# Contains mappings from each document to the list of node IDs that belong to it\n# including the document's metadata.\nDEFAULT_REF_DOC_COLLECTION_SUFFIX = \"/ref_doc_info\"\n# Contains references from each node to its corresponding document,\n# including the node's document hash and reference document ID.\nDEFAULT_METADATA_COLLECTION_SUFFIX = \"/metadata\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/keyval_docstore.py",
    "filename": "keyval_docstore.py",
    "relpath": "storage/docstore/keyval_docstore.py",
    "start_line": 21,
    "end_line": 24,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "KVDocumentStore"
    ],
    "document_function_names": [
      "__init__",
      "docs",
      "_get_kv_pairs_for_insert",
      "_merge_ref_doc_kv_pairs",
      "_prepare_kv_pairs",
      "add_documents",
      "_async_prepare_kv_pairs",
      "async_add_documents",
      "get_document",
      "aget_document",
      "_remove_legacy_info",
      "get_ref_doc_info",
      "aget_ref_doc_info",
      "get_all_ref_doc_info",
      "aget_all_ref_doc_info",
      "ref_doc_exists",
      "aref_doc_exists",
      "document_exists",
      "adocument_exists",
      "_get_ref_doc_id",
      "_aget_ref_doc_id",
      "_remove_from_ref_doc_node",
      "_aremove_from_ref_doc_node",
      "delete_document",
      "adelete_document",
      "delete_ref_doc",
      "adelete_ref_doc",
      "set_document_hash",
      "set_document_hashes",
      "aset_document_hash",
      "aset_document_hashes",
      "get_document_hash",
      "aget_document_hash",
      "get_all_document_hashes",
      "aget_all_document_hashes"
    ],
    "document_class_names": [
      "KVDocumentStore"
    ],
    "content": "class KVDocumentStore(BaseDocumentStore):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/keyval_docstore.py",
    "filename": "keyval_docstore.py",
    "relpath": "storage/docstore/keyval_docstore.py",
    "start_line": 24,
    "end_line": 444,
    "length": 421,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "docs",
      "_get_kv_pairs_for_insert",
      "_merge_ref_doc_kv_pairs",
      "_prepare_kv_pairs",
      "add_documents",
      "_async_prepare_kv_pairs",
      "async_add_documents",
      "get_document",
      "aget_document",
      "_remove_legacy_info",
      "get_ref_doc_info",
      "aget_ref_doc_info",
      "get_all_ref_doc_info",
      "aget_all_ref_doc_info",
      "ref_doc_exists",
      "aref_doc_exists"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "docs",
      "_get_kv_pairs_for_insert",
      "_merge_ref_doc_kv_pairs",
      "_prepare_kv_pairs",
      "add_documents",
      "_async_prepare_kv_pairs",
      "async_add_documents",
      "get_document",
      "aget_document",
      "_remove_legacy_info",
      "get_ref_doc_info",
      "aget_ref_doc_info",
      "get_all_ref_doc_info",
      "aget_all_ref_doc_info",
      "ref_doc_exists",
      "aref_doc_exists",
      "document_exists",
      "adocument_exists",
      "_get_ref_doc_id",
      "_aget_ref_doc_id",
      "_remove_from_ref_doc_node",
      "_aremove_from_ref_doc_node",
      "delete_document",
      "adelete_document",
      "delete_ref_doc",
      "adelete_ref_doc",
      "set_document_hash",
      "set_document_hashes",
      "aset_document_hash",
      "aset_document_hashes",
      "get_document_hash",
      "aget_document_hash",
      "get_all_document_hashes",
      "aget_all_document_hashes"
    ],
    "document_class_names": [
      "KVDocumentStore"
    ],
    "content": "\"\"\"Document (Node) store.\n\n    NOTE: at the moment, this store is primarily used to store Node objects.\n    Each node will be assigned an ID.\n\n    The same docstore can be reused across index structures. This\n    allows you to reuse the same storage for multiple index structures;\n    otherwise, each index would create a docstore under the hood.\n\n    .. code-block:: python\n        nodes = SentenceSplitter().get_nodes_from_documents()\n        docstore = SimpleDocumentStore()\n        docstore.add_documents(nodes)\n        storage_context = StorageContext.from_defaults(docstore=docstore)\n\n        summary_index = SummaryIndex(nodes, storage_context=storage_context)\n        vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n        keyword_table_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)\n\n    This will use the same docstore for multiple index structures.\n\n    Args:\n        kvstore (BaseKVStore): key-value store\n        namespace (str): namespace for the docstore\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kvstore: BaseKVStore,\n        namespace: Optional[str] = None,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n        node_collection_suffix: Optional[str] = None,\n        ref_doc_collection_suffix: Optional[str] = None,\n        metadata_collection_suffix: Optional[str] = None,\n    ) -> None:\n        \"\"\"Init a KVDocumentStore.\"\"\"\n        self._kvstore = kvstore\n        self._namespace = namespace or DEFAULT_NAMESPACE\n        self._node_collection_suffix = (\n            node_collection_suffix or DEFAULT_COLLECTION_DATA_SUFFIX\n        )\n        self._ref_doc_collection_suffix = (\n            ref_doc_collection_suffix or DEFAULT_REF_DOC_COLLECTION_SUFFIX\n        )\n        self._metadata_collection_suffix = (\n            metadata_collection_suffix or DEFAULT_METADATA_COLLECTION_SUFFIX\n        )\n        self._node_collection = f\"{self._namespace}{self._node_collection_suffix}\"\n        self._ref_doc_collection = f\"{self._namespace}{self._ref_doc_collection_suffix}\"\n        self._metadata_collection = (\n            f\"{self._namespace}{self._metadata_collection_suffix}\"\n        )\n        self._batch_size = batch_size\n\n    @property\n    def docs(self) -> Dict[str, BaseNode]:\n        \"\"\"Get all documents.\n\n        Returns:\n            Dict[str, BaseDocument]: documents\n\n        \"\"\"\n        json_dict = self._kvstore.get_all(collection=self._node_collection)\n        return {key: json_to_doc(json) for key, json in json_dict.items()}\n\n    def _get_kv_pairs_for_insert(\n        self, node: BaseNode, ref_doc_info: Optional[RefDocInfo], store_text: bool\n    ) -> Tuple[\n        Optional[Tuple[str, dict]],\n        Optional[Tuple[str, dict]],\n        Optional[Tuple[str, dict]],\n    ]:\n        node_kv_pair = None\n        metadata_kv_pair = None\n        ref_doc_kv_pair = None\n\n        node_key = node.node_id\n        data = doc_to_json(node)\n        if store_text:\n            node_kv_pair = (node_key, data)\n\n        # update doc_collection if needed\n        metadata = {\"doc_hash\": node.hash}\n        if ref_doc_info is not None and node.ref_doc_id:\n            if node.node_id not in ref_doc_info.node_ids:\n                ref_doc_info.node_ids.append(node.node_id)\n            if not ref_doc_info.metadata:\n                ref_doc_info.metadata = node.metadata or {}\n\n            # update metadata with map\n            metadata[\"ref_doc_id\"] = node.ref_doc_id\n\n            metadata_kv_pair = (node_key, metadata)\n            ref_doc_kv_pair = (node.ref_doc_id, ref_doc_info.to_dict())\n        else:\n            metadata_kv_pair = (node_key, metadata)\n\n        return node_kv_pair, metadata_kv_pair, ref_doc_kv_pair\n\n    def _merge_ref_doc_kv_pairs(self, ref_doc_kv_pairs: dict) -> List[Tuple[str, dict]]:\n        merged_ref_doc_kv_pairs: List[Tuple[str, dict]] = []\n        for key, kv_pairs in ref_doc_kv_pairs.items():\n            merged_node_ids: List[str] = []\n            metadata: Dict[str, Any] = {}\n            for kv_pair in kv_pairs:\n                nodes = kv_pair[1].get(\"node_ids\", [])\n                new_nodes = set(nodes).difference(set(merged_node_ids))\n                merged_node_ids.extend([node for node in nodes if node in new_nodes])\n                metadata.update(kv_pair[1].get(\"metadata\", {}))\n            merged_ref_doc_kv_pairs.append(\n                (key, {\"node_ids\": merged_node_ids, \"metadata\": metadata})\n            )\n\n        return merged_ref_doc_kv_pairs\n\n    def _prepare_kv_pairs(\n        self, nodes: Sequence[BaseNode], allow_update: bool, store_text: bool\n    ) -> Tuple[List[Tuple[str, dict]], List[Tuple[str, dict]], List[Tuple[str, dict]]]:\n        \"\"\"\n        This method processes a sequence of document nodes and prepares key-value pairs for\n        nodes, their metadata, and reference documents. The key-value pairs are structured\n        for subsequent insertion into the key-value store. This method does not insert the\n        key-value pairs into the store; it only prepares them. The reference document key-value\n        pairs are merged to ensure each `ref_doc_id` has a consolidated entry.\n\n        Args:\n            nodes (Sequence[BaseNode]): A sequence of document nodes to be processed.\n            allow_update (bool): A flag indicating whether existing nodes should be updated.\n            store_text (bool): A flag indicating whether the text content of the nodes should be stored.\n\n        Returns:\n            Tuple[\n                list,          # List of key-value pairs for nodes\n                list,          # List of key-value pairs for metadata\n                List[Tuple[str, dict]]  # Dictionary of key-value pairs for reference documents, keyed by ref_doc_id\n            ]\n\n        Raises:\n            ValueError: If a node already exists in the store and `allow_update` is False.\n\n        \"\"\"\n        node_kv_pairs = []\n        metadata_kv_pairs = []\n        ref_doc_kv_pairs: Dict[str, List[Tuple[str, dict]]] = {}\n\n        for node in nodes:\n            # NOTE: doc could already exist in the store, but we overwrite it\n            if not allow_update and self.document_exists(node.node_id):\n                raise ValueError(\n                    f\"node_id {node.node_id} already exists. \"\n                    \"Set allow_update to True to overwrite.\"\n                )\n            ref_doc_info = None\n            if isinstance(node, (TextNode, Document)) and node.ref_doc_id is not None:\n                ref_doc_info = self.get_ref_doc_info(node.ref_doc_id) or RefDocInfo()\n\n            (\n                node_kv_pair,\n                metadata_kv_pair,\n                ref_doc_kv_pair,\n            ) = self._get_kv_pairs_for_insert(node, ref_doc_info, store_text)\n\n            if node_kv_pair is not None:\n                node_kv_pairs.append(node_kv_pair)\n            if metadata_kv_pair is not None:\n                metadata_kv_pairs.append(metadata_kv_pair)\n            if ref_doc_kv_pair is not None:\n                key = ref_doc_kv_pair[0]\n                if key not in ref_doc_kv_pairs:\n                    ref_doc_kv_pairs[key] = []\n                ref_doc_kv_pairs[key].append(ref_doc_kv_pair)\n\n        # multiple nodes can point to the same ref_doc_id\n        merged_ref_doc_kv_pairs = self._merge_ref_doc_kv_pairs(ref_doc_kv_pairs)\n\n        return node_kv_pairs, metadata_kv_pairs, merged_ref_doc_kv_pairs\n\n    def add_documents(\n        self,\n        docs: Sequence[BaseNode],\n        allow_update: bool = True,\n        batch_size: Optional[int] = None,\n        store_text: bool = True,\n    ) -> None:\n        \"\"\"Add a document to the store.\n\n        Args:\n            docs (List[BaseDocument]): documents\n            allow_update (bool): allow update of docstore from document\n\n        \"\"\"\n        batch_size = batch_size or self._batch_size\n\n        node_kv_pairs, metadata_kv_pairs, ref_doc_kv_pairs = self._prepare_kv_pairs(\n            docs, allow_update, store_text\n        )\n\n        self._kvstore.put_all(\n            node_kv_pairs,\n            collection=self._node_collection,\n            batch_size=batch_size,\n        )\n\n        self._kvstore.put_all(\n            metadata_kv_pairs,\n            collection=self._metadata_collection,\n            batch_size=batch_size,\n        )\n\n        self._kvstore.put_all(\n            ref_doc_kv_pairs,\n            collection=self._ref_doc_collection,\n            batch_size=batch_size,\n        )\n\n    async def _async_prepare_kv_pairs(\n        self, nodes: Sequence[BaseNode], allow_update: bool, store_text: bool\n    ) -> Tuple[List[Tuple[str, dict]], List[Tuple[str, dict]], List[Tuple[str, dict]]]:\n        \"\"\"\n        This method processes a sequence of document nodes asynchronously and prepares\n        key-value pairs for nodes, their metadata, and reference documents. The key-value\n        pairs are structured for subsequent insertion into the key-value store. This method\n        does not insert the key-value pairs into the store; it only prepares them. The reference\n        document key-value pairs are merged to ensure each `ref_doc_id` has a consolidated entry.\n\n        Args:\n            nodes (Sequence[BaseNode]): A sequence of document nodes to be processed.\n            allow_update (bool): A flag indicating whether existing nodes should be updated.\n            store_text (bool): A flag indicating whether the text content of the nodes should be stored.\n\n        Returns:\n            Tuple[\n                list,          # List of key-value pairs for nodes\n                list,          # List of key-value pairs for metadata\n                List[Tuple[str, dict]]  # List of key-value pairs for reference documents, keyed by ref_doc_id\n            ]\n\n        Raises:\n            ValueError: If a node already exists in the store and `allow_update` is False.\n\n        \"\"\"\n        node_kv_pairs = []\n        metadata_kv_pairs = []\n        ref_doc_kv_pairs: Dict[str, List[Tuple[str, dict]]] = {}\n\n        for node in nodes:\n            # NOTE: doc could already exist in the store, but we overwrite it\n            if not allow_update and await self.adocument_exists(node.node_id):\n                raise ValueError(\n                    f\"node_id {node.node_id} already exists. \"\n                    \"Set allow_update to True to overwrite.\"\n                )\n            ref_doc_info = None\n            if isinstance(node, TextNode) and node.ref_doc_id is not None:\n                ref_doc_info = (\n                    await self.aget_ref_doc_info(node.ref_doc_id) or RefDocInfo()\n                )\n\n            (\n                node_kv_pair,\n                metadata_kv_pair,\n                ref_doc_kv_pair,\n            ) = self._get_kv_pairs_for_insert(node, ref_doc_info, store_text)\n\n            if node_kv_pair is not None:\n                node_kv_pairs.append(node_kv_pair)\n            if metadata_kv_pair is not None:\n                metadata_kv_pairs.append(metadata_kv_pair)\n            if ref_doc_kv_pair is not None:\n                key = ref_doc_kv_pair[0]\n                if key not in ref_doc_kv_pairs:\n                    ref_doc_kv_pairs[key] = []\n                ref_doc_kv_pairs[key].append(ref_doc_kv_pair)\n\n        # multiple nodes can point to the same ref_doc_id\n        merged_ref_doc_kv_pairs = self._merge_ref_doc_kv_pairs(ref_doc_kv_pairs)\n\n        return node_kv_pairs, metadata_kv_pairs, merged_ref_doc_kv_pairs\n\n    async def async_add_documents(\n        self,\n        docs: Sequence[BaseNode],\n        allow_update: bool = True,\n        batch_size: Optional[int] = None,\n        store_text: bool = True,\n    ) -> None:\n        \"\"\"Add a document to the store.\n\n        Args:\n            docs (List[BaseDocument]): documents\n            allow_update (bool): allow update of docstore from document\n\n        \"\"\"\n        batch_size = batch_size or self._batch_size\n\n        (\n            node_kv_pairs,\n            metadata_kv_pairs,\n            ref_doc_kv_pairs,\n        ) = await self._async_prepare_kv_pairs(docs, allow_update, store_text)\n\n        await asyncio.gather(\n            self._kvstore.aput_all(\n                node_kv_pairs,\n                collection=self._node_collection,\n                batch_size=batch_size,\n            ),\n            self._kvstore.aput_all(\n                metadata_kv_pairs,\n                collection=self._metadata_collection,\n                batch_size=batch_size,\n            ),\n            self._kvstore.aput_all(\n                ref_doc_kv_pairs,\n                collection=self._ref_doc_collection,\n                batch_size=batch_size,\n            ),\n        )\n\n    def get_document(self, doc_id: str, raise_error: bool = True) -> Optional[BaseNode]:\n        \"\"\"Get a document from the store.\n\n        Args:\n            doc_id (str): document id\n            raise_error (bool): raise error if doc_id not found\n\n        \"\"\"\n        json = self._kvstore.get(doc_id, collection=self._node_collection)\n        if json is None:\n            if raise_error:\n                raise ValueError(f\"doc_id {doc_id} not found.\")\n            else:\n                return None\n        return json_to_doc(json)\n\n    async def aget_document(\n        self, doc_id: str, raise_error: bool = True\n    ) -> Optional[BaseNode]:\n        \"\"\"Get a document from the store.\n\n        Args:\n            doc_id (str): document id\n            raise_error (bool): raise error if doc_id not found\n\n        \"\"\"\n        json = await self._kvstore.aget(doc_id, collection=self._node_collection)\n        if json is None:\n            if raise_error:\n                raise ValueError(f\"doc_id {doc_id} not found.\")\n            else:\n                return None\n        return json_to_doc(json)\n\n    def _remove_legacy_info(self, ref_doc_info_dict: dict) -> RefDocInfo:\n        if \"doc_ids\" in ref_doc_info_dict:\n            ref_doc_info_dict[\"node_ids\"] = ref_doc_info_dict.get(\"doc_ids\", [])\n            ref_doc_info_dict.pop(\"doc_ids\")\n\n            ref_doc_info_dict[\"metadata\"] = ref_doc_info_dict.get(\"extra_info\", {})\n            ref_doc_info_dict.pop(\"extra_info\")\n\n        return RefDocInfo(**ref_doc_info_dict)\n\n    def get_ref_doc_info(self, ref_doc_id: str) -> Optional[RefDocInfo]:\n        \"\"\"Get the RefDocInfo for a given ref_doc_id.\"\"\"\n        ref_doc_info = self._kvstore.get(\n            ref_doc_id, collection=self._ref_doc_collection\n        )\n        if not ref_doc_info:\n            return None\n\n        # TODO: deprecated legacy support\n        return self._remove_legacy_info(ref_doc_info)\n\n    async def aget_ref_doc_info(self, ref_doc_id: str) -> Optional[RefDocInfo]:\n        \"\"\"Get the RefDocInfo for a given ref_doc_id.\"\"\"\n        ref_doc_info = await self._kvstore.aget(\n            ref_doc_id, collection=self._ref_doc_collection\n        )\n        if not ref_doc_info:\n            return None\n\n        # TODO: deprecated legacy support\n        return self._remove_legacy_info(ref_doc_info)\n\n    def get_all_ref_doc_info(self) -> Optional[Dict[str, RefDocInfo]]:\n        \"\"\"Get a mapping of ref_doc_id -> RefDocInfo for all ingested documents.\"\"\"\n        ref_doc_infos = self._kvstore.get_all(collection=self._ref_doc_collection)\n        if ref_doc_infos is None:\n            return None\n\n        # TODO: deprecated legacy support\n        all_ref_doc_infos = {}\n        for doc_id, ref_doc_info in ref_doc_infos.items():\n            all_ref_doc_infos[doc_id] = self._remove_legacy_info(ref_doc_info)\n\n        return all_ref_doc_infos\n\n    async def aget_all_ref_doc_info(self) -> Optional[Dict[str, RefDocInfo]]:\n        \"\"\"Get a mapping of ref_doc_id -> RefDocInfo for all ingested documents.\"\"\"\n        ref_doc_infos = await self._kvstore.aget_all(\n            collection=self._ref_doc_collection\n        )\n        if ref_doc_infos is None:\n            return None\n\n        # TODO: deprecated legacy support\n        all_ref_doc_infos = {}\n        for doc_id, ref_doc_info in ref_doc_infos.items():\n            all_ref_doc_infos[doc_id] = self._remove_legacy_info(ref_doc_info)\n        return all_ref_doc_infos\n\n    def ref_doc_exists(self, ref_doc_id: str) -> bool:\n        \"\"\"Check if a ref_doc_id has been ingested.\"\"\"\n        return self.get_ref_doc_info(ref_doc_id) is not None\n\n    async def aref_doc_exists(self, ref_doc_id: str) -> bool:\n        \"\"\"Check if a ref_doc_id has been ingested.\"\"\"\n        return await self.aget_ref_doc_info(ref_doc_id) is not None"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/docstore/keyval_docstore.py",
    "filename": "keyval_docstore.py",
    "relpath": "storage/docstore/keyval_docstore.py",
    "start_line": 444,
    "end_line": 657,
    "length": 214,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "document_exists",
      "adocument_exists",
      "_get_ref_doc_id",
      "_aget_ref_doc_id",
      "_remove_from_ref_doc_node",
      "_aremove_from_ref_doc_node",
      "delete_document",
      "adelete_document",
      "delete_ref_doc",
      "adelete_ref_doc",
      "set_document_hash",
      "set_document_hashes",
      "aset_document_hash",
      "aset_document_hashes",
      "get_document_hash",
      "aget_document_hash",
      "get_all_document_hashes",
      "aget_all_document_hashes"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "docs",
      "_get_kv_pairs_for_insert",
      "_merge_ref_doc_kv_pairs",
      "_prepare_kv_pairs",
      "add_documents",
      "_async_prepare_kv_pairs",
      "async_add_documents",
      "get_document",
      "aget_document",
      "_remove_legacy_info",
      "get_ref_doc_info",
      "aget_ref_doc_info",
      "get_all_ref_doc_info",
      "aget_all_ref_doc_info",
      "ref_doc_exists",
      "aref_doc_exists",
      "document_exists",
      "adocument_exists",
      "_get_ref_doc_id",
      "_aget_ref_doc_id",
      "_remove_from_ref_doc_node",
      "_aremove_from_ref_doc_node",
      "delete_document",
      "adelete_document",
      "delete_ref_doc",
      "adelete_ref_doc",
      "set_document_hash",
      "set_document_hashes",
      "aset_document_hash",
      "aset_document_hashes",
      "get_document_hash",
      "aget_document_hash",
      "get_all_document_hashes",
      "aget_all_document_hashes"
    ],
    "document_class_names": [
      "KVDocumentStore"
    ],
    "content": "def document_exists(self, doc_id: str) -> bool:\n        \"\"\"Check if document exists.\"\"\"\n        return self._kvstore.get(doc_id, self._node_collection) is not None\n\n    async def adocument_exists(self, doc_id: str) -> bool:\n        \"\"\"Check if document exists.\"\"\"\n        return await self._kvstore.aget(doc_id, self._node_collection) is not None\n\n    def _get_ref_doc_id(self, doc_id: str) -> Optional[str]:\n        \"\"\"Helper function to get ref_doc_info for a given doc_id.\"\"\"\n        metadata = self._kvstore.get(doc_id, collection=self._metadata_collection)\n        if metadata is None:\n            return None\n\n        return metadata.get(\"ref_doc_id\", None)\n\n    async def _aget_ref_doc_id(self, doc_id: str) -> Optional[str]:\n        \"\"\"Helper function to get ref_doc_info for a given doc_id.\"\"\"\n        metadata = await self._kvstore.aget(\n            doc_id, collection=self._metadata_collection\n        )\n        if metadata is None:\n            return None\n\n        return metadata.get(\"ref_doc_id\", None)\n\n    def _remove_from_ref_doc_node(self, doc_id: str) -> None:\n        \"\"\"\n        Helper function to remove node doc_id from ref_doc_collection.\n        If ref_doc has no more doc_ids, delete it from the collection.\n        \"\"\"\n        ref_doc_id = self._get_ref_doc_id(doc_id)\n        if ref_doc_id is None:\n            return\n        ref_doc_info = self.get_ref_doc_info(ref_doc_id)\n        if ref_doc_info is None:\n            return\n        if doc_id in ref_doc_info.node_ids:  # sanity check\n            ref_doc_info.node_ids.remove(doc_id)\n        # delete ref_doc from collection if it has no more doc_ids\n        if len(ref_doc_info.node_ids) > 0:\n            self._kvstore.put(\n                ref_doc_id,\n                ref_doc_info.to_dict(),\n                collection=self._ref_doc_collection,\n            )\n        else:\n            self._kvstore.delete(ref_doc_id, collection=self._metadata_collection)\n            self._kvstore.delete(ref_doc_id, collection=self._node_collection)\n            self._kvstore.delete(ref_doc_id, collection=self._ref_doc_collection)\n\n    async def _aremove_from_ref_doc_node(self, doc_id: str) -> None:\n        \"\"\"\n        Helper function to remove node doc_id from ref_doc_collection.\n        If ref_doc has no more doc_ids, delete it from the collection.\n        \"\"\"\n        ref_doc_id, ref_doc_info = await asyncio.gather(\n            self._aget_ref_doc_id(doc_id),\n            self.aget_ref_doc_info(doc_id),\n        )\n        if ref_doc_id is None or ref_doc_info is None:\n            return\n\n        if doc_id in ref_doc_info.node_ids:  # sanity check\n            ref_doc_info.node_ids.remove(doc_id)\n        # delete ref_doc from collection if it has no more doc_ids\n        if len(ref_doc_info.node_ids) > 0:\n            await self._kvstore.aput(\n                ref_doc_id,\n                ref_doc_info.to_dict(),\n                collection=self._ref_doc_collection,\n            )\n        else:\n            await asyncio.gather(\n                self._kvstore.adelete(ref_doc_id, collection=self._metadata_collection),\n                self._kvstore.adelete(ref_doc_id, collection=self._node_collection),\n                self._kvstore.adelete(ref_doc_id, collection=self._ref_doc_collection),\n            )\n\n    def delete_document(self, doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a document from the store.\"\"\"\n        self._remove_from_ref_doc_node(doc_id)\n        delete_success = self._kvstore.delete(doc_id, collection=self._node_collection)\n        self._kvstore.delete(doc_id, collection=self._metadata_collection)\n\n        if not delete_success and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n\n    async def adelete_document(self, doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a document from the store.\"\"\"\n        _, delete_success, _ = await asyncio.gather(\n            self._aremove_from_ref_doc_node(doc_id),\n            self._kvstore.adelete(doc_id, collection=self._node_collection),\n            self._kvstore.adelete(doc_id, collection=self._metadata_collection),\n        )\n\n        if not delete_success and raise_error:\n            raise ValueError(f\"doc_id {doc_id} not found.\")\n\n    def delete_ref_doc(self, ref_doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a ref_doc and all it's associated nodes.\"\"\"\n        ref_doc_info = self.get_ref_doc_info(ref_doc_id)\n        if ref_doc_info is None:\n            if raise_error:\n                raise ValueError(f\"ref_doc_id {ref_doc_id} not found.\")\n            else:\n                return\n\n        original_node_ids = (\n            ref_doc_info.node_ids.copy()\n        )  # copy to avoid mutation during iteration\n        for doc_id in original_node_ids:\n            self.delete_document(doc_id, raise_error=False)\n\n        # Deleting all the nodes should already delete the ref_doc, but just to be sure\n        self._kvstore.delete(ref_doc_id, collection=self._ref_doc_collection)\n        self._kvstore.delete(ref_doc_id, collection=self._metadata_collection)\n        self._kvstore.delete(ref_doc_id, collection=self._node_collection)\n\n    async def adelete_ref_doc(self, ref_doc_id: str, raise_error: bool = True) -> None:\n        \"\"\"Delete a ref_doc and all it's associated nodes.\"\"\"\n        ref_doc_info = await self.aget_ref_doc_info(ref_doc_id)\n        if ref_doc_info is None:\n            if raise_error:\n                raise ValueError(f\"ref_doc_id {ref_doc_id} not found.\")\n            else:\n                return\n\n        original_node_ids = (\n            ref_doc_info.node_ids.copy()\n        )  # copy to avoid mutation during iteration\n        for doc_id in original_node_ids:\n            await self.adelete_document(doc_id, raise_error=False)\n\n        # Deleting all the nodes should already delete the ref_doc, but just to be sure\n        await asyncio.gather(\n            self._kvstore.adelete(ref_doc_id, collection=self._ref_doc_collection),\n            self._kvstore.adelete(ref_doc_id, collection=self._metadata_collection),\n            self._kvstore.adelete(ref_doc_id, collection=self._node_collection),\n        )\n\n    def set_document_hash(self, doc_id: str, doc_hash: str) -> None:\n        \"\"\"Set the hash for a given doc_id.\"\"\"\n        metadata = {\"doc_hash\": doc_hash}\n        self._kvstore.put(doc_id, metadata, collection=self._metadata_collection)\n\n    def set_document_hashes(self, doc_hashes: Dict[str, str]) -> None:\n        \"\"\"Set the hash for a given doc_id.\"\"\"\n        metadata_kv_pairs = []\n        for doc_id, doc_hash in doc_hashes.items():\n            metadata_kv_pairs.append((doc_id, {\"doc_hash\": doc_hash}))\n\n        self._kvstore.put_all(\n            metadata_kv_pairs,\n            collection=self._metadata_collection,\n            batch_size=self._batch_size,\n        )\n\n    async def aset_document_hash(self, doc_id: str, doc_hash: str) -> None:\n        \"\"\"Set the hash for a given doc_id.\"\"\"\n        metadata = {\"doc_hash\": doc_hash}\n        await self._kvstore.aput(doc_id, metadata, collection=self._metadata_collection)\n\n    async def aset_document_hashes(self, doc_hashes: Dict[str, str]) -> None:\n        \"\"\"Set the hash for a given doc_id.\"\"\"\n        metadata_kv_pairs = []\n        for doc_id, doc_hash in doc_hashes.items():\n            metadata_kv_pairs.append((doc_id, {\"doc_hash\": doc_hash}))\n\n        await self._kvstore.aput_all(\n            metadata_kv_pairs,\n            collection=self._metadata_collection,\n            batch_size=self._batch_size,\n        )\n\n    def get_document_hash(self, doc_id: str) -> Optional[str]:\n        \"\"\"Get the stored hash for a document, if it exists.\"\"\"\n        metadata = self._kvstore.get(doc_id, collection=self._metadata_collection)\n        if metadata is not None:\n            return metadata.get(\"doc_hash\", None)\n        else:\n            return None\n\n    async def aget_document_hash(self, doc_id: str) -> Optional[str]:\n        \"\"\"Get the stored hash for a document, if it exists.\"\"\"\n        metadata = await self._kvstore.aget(\n            doc_id, collection=self._metadata_collection\n        )\n        if metadata is not None:\n            return metadata.get(\"doc_hash\", None)\n        else:\n            return None\n\n    def get_all_document_hashes(self) -> Dict[str, str]:\n        \"\"\"Get the stored hash for all documents.\"\"\"\n        hashes = {}\n        for doc_id in self._kvstore.get_all(collection=self._metadata_collection):\n            hash = self.get_document_hash(doc_id)\n            if hash is not None:\n                hashes[hash] = doc_id\n        return hashes\n\n    async def aget_all_document_hashes(self) -> Dict[str, str]:\n        \"\"\"Get the stored hash for all documents.\"\"\"\n        hashes = {}\n        for doc_id in await self._kvstore.aget_all(\n            collection=self._metadata_collection\n        ):\n            hash = await self.aget_document_hash(doc_id)\n            if hash is not None:\n                hashes[hash] = doc_id\n        return hashes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/index_store/keyval_index_store.py",
    "filename": "keyval_index_store.py",
    "relpath": "storage/index_store/keyval_index_store.py",
    "start_line": 1,
    "end_line": 84,
    "length": 84,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "add_index_struct",
      "delete_index_struct",
      "get_index_struct",
      "index_structs"
    ],
    "chunk_class_names": [
      "KVIndexStore"
    ],
    "document_function_names": [
      "__init__",
      "add_index_struct",
      "delete_index_struct",
      "get_index_struct",
      "index_structs"
    ],
    "document_class_names": [
      "KVIndexStore"
    ],
    "content": "from typing import List, Optional\n\nfrom llama_index.core.data_structs.data_structs import IndexStruct\nfrom llama_index.core.storage.index_store.types import BaseIndexStore\nfrom llama_index.core.storage.index_store.utils import (\n    index_struct_to_json,\n    json_to_index_struct,\n)\nfrom llama_index.core.storage.kvstore.types import BaseKVStore\n\nDEFAULT_NAMESPACE = \"index_store\"\nDEFAULT_COLLECTION_SUFFIX = \"/data\"\n\n\nclass KVIndexStore(BaseIndexStore):\n    \"\"\"Key-Value Index store.\n\n    Args:\n        kvstore (BaseKVStore): key-value store\n        namespace (str): namespace for the index store\n        collection_suffix (str): suffix for the collection name\n\n    \"\"\"\n\n    def __init__(\n        self,\n        kvstore: BaseKVStore,\n        namespace: Optional[str] = None,\n        collection_suffix: Optional[str] = None,\n    ) -> None:\n        \"\"\"Init a KVIndexStore.\"\"\"\n        self._kvstore = kvstore\n        self._namespace = namespace or DEFAULT_NAMESPACE\n        self._collection_suffix = collection_suffix or DEFAULT_COLLECTION_SUFFIX\n        self._collection = f\"{self._namespace}{self._collection_suffix}\"\n\n    def add_index_struct(self, index_struct: IndexStruct) -> None:\n        \"\"\"Add an index struct.\n\n        Args:\n            index_struct (IndexStruct): index struct\n\n        \"\"\"\n        key = index_struct.index_id\n        data = index_struct_to_json(index_struct)\n        self._kvstore.put(key, data, collection=self._collection)\n\n    def delete_index_struct(self, key: str) -> None:\n        \"\"\"Delete an index struct.\n\n        Args:\n            key (str): index struct key\n\n        \"\"\"\n        self._kvstore.delete(key, collection=self._collection)\n\n    def get_index_struct(\n        self, struct_id: Optional[str] = None\n    ) -> Optional[IndexStruct]:\n        \"\"\"Get an index struct.\n\n        Args:\n            struct_id (Optional[str]): index struct id\n\n        \"\"\"\n        if struct_id is None:\n            structs = self.index_structs()\n            assert len(structs) == 1\n            return structs[0]\n        else:\n            json = self._kvstore.get(struct_id, collection=self._collection)\n            if json is None:\n                return None\n            return json_to_index_struct(json)\n\n    def index_structs(self) -> List[IndexStruct]:\n        \"\"\"Get all index structs.\n\n        Returns:\n            List[IndexStruct]: index structs\n\n        \"\"\"\n        jsons = self._kvstore.get_all(collection=self._collection)\n        return [json_to_index_struct(json) for json in jsons.values()]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/index_store/types.py",
    "filename": "types.py",
    "relpath": "storage/index_store/types.py",
    "start_line": 1,
    "end_line": 37,
    "length": 37,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "index_structs",
      "add_index_struct",
      "delete_index_struct",
      "get_index_struct",
      "persist"
    ],
    "chunk_class_names": [
      "BaseIndexStore"
    ],
    "document_function_names": [
      "index_structs",
      "add_index_struct",
      "delete_index_struct",
      "get_index_struct",
      "persist"
    ],
    "document_class_names": [
      "BaseIndexStore"
    ],
    "content": "import os\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nimport fsspec\nfrom llama_index.core.data_structs.data_structs import IndexStruct\n\nDEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_FNAME = \"index_store.json\"\nDEFAULT_PERSIST_PATH = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME)\n\n\nclass BaseIndexStore(ABC):\n    @abstractmethod\n    def index_structs(self) -> List[IndexStruct]:\n        pass\n\n    @abstractmethod\n    def add_index_struct(self, index_struct: IndexStruct) -> None:\n        pass\n\n    @abstractmethod\n    def delete_index_struct(self, key: str) -> None:\n        pass\n\n    @abstractmethod\n    def get_index_struct(\n        self, struct_id: Optional[str] = None\n    ) -> Optional[IndexStruct]:\n        pass\n\n    def persist(\n        self,\n        persist_path: str = DEFAULT_PERSIST_PATH,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the index store to disk.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/index_store/__init__.py",
    "filename": "__init__.py",
    "relpath": "storage/index_store/__init__.py",
    "start_line": 1,
    "end_line": 7,
    "length": 7,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.storage.index_store.simple_index_store import (\n    SimpleIndexStore,\n)\n\n__all__ = [\n    \"SimpleIndexStore\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/index_store/utils.py",
    "filename": "utils.py",
    "relpath": "storage/index_store/utils.py",
    "start_line": 1,
    "end_line": 22,
    "length": 22,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "index_struct_to_json",
      "json_to_index_struct"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "index_struct_to_json",
      "json_to_index_struct"
    ],
    "document_class_names": [],
    "content": "from llama_index.core.constants import DATA_KEY, TYPE_KEY\nfrom llama_index.core.data_structs.data_structs import IndexStruct\nfrom llama_index.core.data_structs.registry import (\n    INDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS,\n)\n\n\ndef index_struct_to_json(index_struct: IndexStruct) -> dict:\n    return {\n        TYPE_KEY: index_struct.get_type(),\n        DATA_KEY: index_struct.to_json(),\n    }\n\n\ndef json_to_index_struct(struct_dict: dict) -> IndexStruct:\n    type = struct_dict[TYPE_KEY]\n    data_dict = struct_dict[DATA_KEY]\n    cls = INDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS[type]\n    try:\n        return cls.from_json(data_dict)\n    except TypeError:\n        return cls.from_dict(data_dict)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/storage/index_store/simple_index_store.py",
    "filename": "simple_index_store.py",
    "relpath": "storage/index_store/simple_index_store.py",
    "start_line": 1,
    "end_line": 72,
    "length": 72,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_persist_dir",
      "from_persist_path",
      "persist",
      "from_dict",
      "to_dict"
    ],
    "chunk_class_names": [
      "SimpleIndexStore"
    ],
    "document_function_names": [
      "__init__",
      "from_persist_dir",
      "from_persist_path",
      "persist",
      "from_dict",
      "to_dict"
    ],
    "document_class_names": [
      "SimpleIndexStore"
    ],
    "content": "import os\nfrom typing import Optional\n\nimport fsspec\nfrom llama_index.core.storage.index_store.keyval_index_store import KVIndexStore\nfrom llama_index.core.storage.index_store.types import (\n    DEFAULT_PERSIST_DIR,\n    DEFAULT_PERSIST_FNAME,\n    DEFAULT_PERSIST_PATH,\n)\nfrom llama_index.core.storage.kvstore.simple_kvstore import SimpleKVStore\nfrom llama_index.core.storage.kvstore.types import BaseInMemoryKVStore\nfrom llama_index.core.utils import concat_dirs\n\n\nclass SimpleIndexStore(KVIndexStore):\n    \"\"\"Simple in-memory Index store.\n\n    Args:\n        simple_kvstore (SimpleKVStore): simple key-value store\n\n    \"\"\"\n\n    def __init__(\n        self,\n        simple_kvstore: Optional[SimpleKVStore] = None,\n    ) -> None:\n        \"\"\"Init a SimpleIndexStore.\"\"\"\n        simple_kvstore = simple_kvstore or SimpleKVStore()\n        super().__init__(simple_kvstore)\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimpleIndexStore\":\n        \"\"\"Create a SimpleIndexStore from a persist directory.\"\"\"\n        if fs is not None:\n            persist_path = concat_dirs(persist_dir, DEFAULT_PERSIST_FNAME)\n        else:\n            persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)\n        return cls.from_persist_path(persist_path, fs=fs)\n\n    @classmethod\n    def from_persist_path(\n        cls,\n        persist_path: str,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimpleIndexStore\":\n        \"\"\"Create a SimpleIndexStore from a persist path.\"\"\"\n        fs = fs or fsspec.filesystem(\"file\")\n        simple_kvstore = SimpleKVStore.from_persist_path(persist_path, fs=fs)\n        return cls(simple_kvstore)\n\n    def persist(\n        self,\n        persist_path: str = DEFAULT_PERSIST_PATH,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the store.\"\"\"\n        if isinstance(self._kvstore, BaseInMemoryKVStore):\n            self._kvstore.persist(persist_path, fs=fs)\n\n    @classmethod\n    def from_dict(cls, save_dict: dict) -> \"SimpleIndexStore\":\n        simple_kvstore = SimpleKVStore.from_dict(save_dict)\n        return cls(simple_kvstore)\n\n    def to_dict(self) -> dict:\n        assert isinstance(self._kvstore, SimpleKVStore)\n        return self._kvstore.to_dict()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/types.py",
    "filename": "types.py",
    "relpath": "chat_engine/types.py",
    "start_line": 1,
    "end_line": 413,
    "length": 413,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "is_function",
      "set_source_nodes",
      "__post_init__",
      "__str__",
      "response_gen",
      "async_response_gen",
      "set_source_nodes",
      "__post_init__",
      "__str__",
      "_ensure_async_setup",
      "put_in_queue",
      "aput_in_queue",
      "write_response_to_history",
      "awrite_response_to_history",
      "response_gen",
      "async_response_gen",
      "print_response_stream",
      "aprint_response_stream",
      "reset",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "chat_repl",
      "streaming_chat_repl",
      "chat_history"
    ],
    "chunk_class_names": [
      "ChatResponseMode",
      "class",
      "class",
      "BaseChatEngine"
    ],
    "document_function_names": [
      "is_function",
      "set_source_nodes",
      "__post_init__",
      "__str__",
      "response_gen",
      "async_response_gen",
      "set_source_nodes",
      "__post_init__",
      "__str__",
      "_ensure_async_setup",
      "put_in_queue",
      "aput_in_queue",
      "write_response_to_history",
      "awrite_response_to_history",
      "response_gen",
      "async_response_gen",
      "print_response_stream",
      "aprint_response_stream",
      "reset",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "chat_repl",
      "streaming_chat_repl",
      "chat_history"
    ],
    "document_class_names": [
      "ChatResponseMode",
      "class",
      "class",
      "BaseChatEngine",
      "ChatMode"
    ],
    "content": "import asyncio\nimport logging\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom functools import partial\nfrom inspect import iscoroutinefunction\nfrom queue import Queue, Empty\nfrom threading import Event\nfrom typing import AsyncGenerator, Callable, Generator, List, Optional, Union, Dict, Any\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n)\nfrom llama_index.core.base.response.schema import Response, StreamingResponse\nfrom llama_index.core.memory import BaseMemory\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core.tools import ToolOutput\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.instrumentation.events.chat_engine import (\n    StreamChatErrorEvent,\n    StreamChatEndEvent,\n    StreamChatStartEvent,\n    StreamChatDeltaReceivedEvent,\n)\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\n\n\ndef is_function(message: ChatMessage) -> bool:\n    \"\"\"Utility for ChatMessage responses from OpenAI models.\"\"\"\n    return (\n        \"tool_calls\" in message.additional_kwargs\n        and len(message.additional_kwargs[\"tool_calls\"]) > 0\n    )\n\n\nclass ChatResponseMode(str, Enum):\n    \"\"\"Flag toggling waiting/streaming in `Agent._chat`.\"\"\"\n\n    WAIT = \"wait\"\n    STREAM = \"stream\"\n\n\n@dataclass\nclass AgentChatResponse:\n    \"\"\"Agent chat response.\"\"\"\n\n    response: str = \"\"\n    sources: List[ToolOutput] = field(default_factory=list)\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    is_dummy_stream: bool = False\n    metadata: Optional[Dict[str, Any]] = None\n\n    def set_source_nodes(self) -> None:\n        if self.sources and not self.source_nodes:\n            for tool_output in self.sources:\n                if isinstance(tool_output.raw_output, (Response, StreamingResponse)):\n                    self.source_nodes.extend(tool_output.raw_output.source_nodes)\n\n    def __post_init__(self) -> None:\n        self.set_source_nodes()\n\n    def __str__(self) -> str:\n        return self.response\n\n    @property\n    def response_gen(self) -> Generator[str, None, None]:\n        \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"\n        if not self.is_dummy_stream:\n            raise ValueError(\n                \"response_gen is only available for streaming responses. \"\n                \"Set is_dummy_stream=True if you still want a generator.\"\n            )\n\n        for token in self.response.split(\" \"):\n            yield token + \" \"\n            time.sleep(0.1)\n\n    async def async_response_gen(self) -> AsyncGenerator[str, None]:\n        \"\"\"Used for fake streaming, i.e. with tool outputs.\"\"\"\n        if not self.is_dummy_stream:\n            raise ValueError(\n                \"response_gen is only available for streaming responses. \"\n                \"Set is_dummy_stream=True if you still want a generator.\"\n            )\n\n        for token in self.response.split(\" \"):\n            yield token + \" \"\n            await asyncio.sleep(0.1)\n\n\n@dataclass\nclass StreamingAgentChatResponse:\n    \"\"\"Streaming chat response to user and writing to chat history.\"\"\"\n\n    response: str = \"\"\n    sources: List[ToolOutput] = field(default_factory=list)\n    chat_stream: Optional[ChatResponseGen] = None\n    achat_stream: Optional[ChatResponseAsyncGen] = None\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    unformatted_response: str = \"\"\n    queue: Queue = field(default_factory=Queue)\n    aqueue: Optional[asyncio.Queue] = None\n    # flag when chat message is a function call\n    is_function: Optional[bool] = None\n    # flag when processing done\n    is_done = False\n    # signal when a new item is added to the queue\n    new_item_event: Optional[asyncio.Event] = None\n    # NOTE: async code uses two events rather than one since it yields\n    # control when waiting for queue item\n    # signal when the OpenAI functions stop executing\n    is_function_false_event: Optional[asyncio.Event] = None\n    # signal when an OpenAI function is being executed\n    is_function_not_none_thread_event: Event = field(default_factory=Event)\n    is_writing_to_memory: bool = True\n    # Track if an exception occurred\n    exception: Optional[Exception] = None\n    awrite_response_to_history_task: Optional[asyncio.Task] = None\n\n    def set_source_nodes(self) -> None:\n        if self.sources and not self.source_nodes:\n            for tool_output in self.sources:\n                if isinstance(tool_output.raw_output, (Response, StreamingResponse)):\n                    self.source_nodes.extend(tool_output.raw_output.source_nodes)\n\n    def __post_init__(self) -> None:\n        self.set_source_nodes()\n\n    def __str__(self) -> str:\n        if self.is_done and not self.queue.empty() and not self.is_function:\n            while self.queue.queue:\n                delta = self.queue.queue.popleft()\n                self.unformatted_response += delta\n            self.response = self.unformatted_response.strip()\n        return self.response\n\n    def _ensure_async_setup(self) -> None:\n        if self.aqueue is None:\n            self.aqueue = asyncio.Queue()\n        if self.new_item_event is None:\n            self.new_item_event = asyncio.Event()\n        if self.is_function_false_event is None:\n            self.is_function_false_event = asyncio.Event()\n\n    def put_in_queue(self, delta: Optional[str]) -> None:\n        self.queue.put_nowait(delta)\n        self.is_function_not_none_thread_event.set()\n\n    def aput_in_queue(self, delta: Optional[str]) -> None:\n        assert self.aqueue is not None\n        assert self.new_item_event is not None\n\n        self.aqueue.put_nowait(delta)\n        self.new_item_event.set()\n\n    @dispatcher.span\n    def write_response_to_history(\n        self,\n        memory: BaseMemory,\n        on_stream_end_fn: Optional[Callable] = None,\n    ) -> None:\n        if self.chat_stream is None:\n            raise ValueError(\n                \"chat_stream is None. Cannot write to history without chat_stream.\"\n            )\n\n        # try/except to prevent hanging on error\n        dispatcher.event(StreamChatStartEvent())\n        try:\n            final_text = \"\"\n            for chat in self.chat_stream:\n                self.is_function = is_function(chat.message)\n                if chat.delta:\n                    dispatcher.event(\n                        StreamChatDeltaReceivedEvent(\n                            delta=chat.delta,\n                        )\n                    )\n                    self.put_in_queue(chat.delta)\n                final_text += chat.delta or \"\"\n            if self.is_function is not None:  # if loop has gone through iteration\n                # NOTE: this is to handle the special case where we consume some of the\n                # chat stream, but not all of it (e.g. in react agent)\n                chat.message.content = final_text.strip()  # final message\n                memory.put(chat.message)\n        except Exception as e:\n            dispatcher.event(StreamChatErrorEvent(exception=e))\n            self.exception = e\n\n            # This act as is_done events for any consumers waiting\n            self.is_function_not_none_thread_event.set()\n\n            # force the queue reader to see the exception\n            self.put_in_queue(\"\")\n            raise\n        dispatcher.event(StreamChatEndEvent())\n\n        self.is_done = True\n\n        # This act as is_done events for any consumers waiting\n        self.is_function_not_none_thread_event.set()\n        if on_stream_end_fn is not None and not self.is_function:\n            on_stream_end_fn()\n\n    @dispatcher.span\n    async def awrite_response_to_history(\n        self,\n        memory: BaseMemory,\n        on_stream_end_fn: Optional[Callable] = None,\n    ) -> None:\n        self._ensure_async_setup()\n        assert self.aqueue is not None\n        assert self.is_function_false_event is not None\n        assert self.new_item_event is not None\n\n        if self.achat_stream is None:\n            raise ValueError(\n                \"achat_stream is None. Cannot asynchronously write to \"\n                \"history without achat_stream.\"\n            )\n\n        # try/except to prevent hanging on error\n        dispatcher.event(StreamChatStartEvent())\n        try:\n            final_text = \"\"\n            async for chat in self.achat_stream:\n                self.is_function = is_function(chat.message)\n                if chat.delta:\n                    dispatcher.event(\n                        StreamChatDeltaReceivedEvent(\n                            delta=chat.delta,\n                        )\n                    )\n                    self.aput_in_queue(chat.delta)\n                final_text += chat.delta or \"\"\n                self.new_item_event.set()\n                if self.is_function is False:\n                    self.is_function_false_event.set()\n            if self.is_function is not None:  # if loop has gone through iteration\n                # NOTE: this is to handle the special case where we consume some of the\n                # chat stream, but not all of it (e.g. in react agent)\n                chat.message.content = final_text.strip()  # final message\n                await memory.aput(chat.message)\n        except Exception as e:\n            dispatcher.event(StreamChatErrorEvent(exception=e))\n            self.exception = e\n\n            # These act as is_done events for any consumers waiting\n            self.is_function_false_event.set()\n            self.new_item_event.set()\n\n            # force the queue reader to see the exception\n            self.aput_in_queue(\"\")\n            raise\n        dispatcher.event(StreamChatEndEvent())\n        self.is_done = True\n\n        # These act as is_done events for any consumers waiting\n        self.is_function_false_event.set()\n        self.new_item_event.set()\n        if on_stream_end_fn is not None and not self.is_function:\n            if iscoroutinefunction(\n                on_stream_end_fn.func\n                if isinstance(on_stream_end_fn, partial)\n                else on_stream_end_fn\n            ):\n                await on_stream_end_fn()\n            else:\n                on_stream_end_fn()\n\n    @property\n    def response_gen(self) -> Generator[str, None, None]:\n        if self.is_writing_to_memory:\n            while not self.is_done or not self.queue.empty():\n                if self.exception is not None:\n                    raise self.exception\n\n                try:\n                    delta = self.queue.get(block=False)\n                    self.unformatted_response += delta\n                    yield delta\n                except Empty:\n                    # Queue is empty, but we're not done yet. Sleep for 0 secs to release the GIL and allow other threads to run.\n                    time.sleep(0)\n        else:\n            if self.chat_stream is None:\n                raise ValueError(\"chat_stream is None!\")\n\n            for chat_response in self.chat_stream:\n                self.unformatted_response += chat_response.delta or \"\"\n                yield chat_response.delta or \"\"\n        self.response = self.unformatted_response.strip()\n\n    async def async_response_gen(self) -> AsyncGenerator[str, None]:\n        try:\n            self._ensure_async_setup()\n            assert self.aqueue is not None\n\n            if self.is_writing_to_memory:\n                while True:\n                    if not self.aqueue.empty() or not self.is_done:\n                        if self.exception is not None:\n                            raise self.exception\n\n                        try:\n                            delta = await asyncio.wait_for(\n                                self.aqueue.get(), timeout=0.1\n                            )\n                        except asyncio.TimeoutError:\n                            if self.is_done:\n                                break\n                            continue\n                        if delta is not None:\n                            self.unformatted_response += delta\n                            yield delta\n                    else:\n                        break\n            else:\n                if self.achat_stream is None:\n                    raise ValueError(\"achat_stream is None!\")\n\n                async for chat_response in self.achat_stream:\n                    self.unformatted_response += chat_response.delta or \"\"\n                    yield chat_response.delta or \"\"\n            self.response = self.unformatted_response.strip()\n        finally:\n            if self.awrite_response_to_history_task:\n                # Make sure that the background task ran to completion, retrieve any exceptions\n                await self.awrite_response_to_history_task\n                self.awrite_response_to_history_task = (\n                    None  # No need to keep the reference to the finished task\n                )\n\n    def print_response_stream(self) -> None:\n        for token in self.response_gen:\n            print(token, end=\"\", flush=True)\n\n    async def aprint_response_stream(self) -> None:\n        async for token in self.async_response_gen():\n            print(token, end=\"\", flush=True)\n\n\nAGENT_CHAT_RESPONSE_TYPE = Union[AgentChatResponse, StreamingAgentChatResponse]\n\n\nclass BaseChatEngine(DispatcherSpanMixin, ABC):\n    \"\"\"Base Chat Engine.\"\"\"\n\n    @abstractmethod\n    def reset(self) -> None:\n        \"\"\"Reset conversation state.\"\"\"\n\n    @abstractmethod\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Main chat interface.\"\"\"\n\n    @abstractmethod\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        \"\"\"Stream chat interface.\"\"\"\n\n    @abstractmethod\n    async def achat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Async version of main chat interface.\"\"\"\n\n    @abstractmethod\n    async def astream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        \"\"\"Async version of main chat interface.\"\"\"\n\n    def chat_repl(self) -> None:\n        \"\"\"Enter interactive chat REPL.\"\"\"\n        print(\"===== Entering Chat REPL =====\")\n        print('Type \"exit\" to exit.\\n')\n        self.reset()\n        message = input(\"Human: \")\n        while message != \"exit\":\n            response = self.chat(message)\n            print(f\"Assistant: {response}\\n\")\n            message = input(\"Human: \")\n\n    def streaming_chat_repl(self) -> None:\n        \"\"\"Enter interactive chat REPL with streaming responses.\"\"\"\n        print(\"===== Entering Chat REPL =====\")\n        print('Type \"exit\" to exit.\\n')\n        self.reset()\n        message = input(\"Human: \")\n        while message != \"exit\":\n            response = self.stream_chat(message)\n            print(\"Assistant: \", end=\"\", flush=True)\n            response.print_response_stream()\n            print(\"\\n\")\n            message = input(\"Human: \")\n\n    @property\n    @abstractmethod\n    def chat_history(self) -> List[ChatMessage]:\n        pass"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/types.py",
    "filename": "types.py",
    "relpath": "chat_engine/types.py",
    "start_line": 413,
    "end_line": 466,
    "length": 54,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "ChatMode"
    ],
    "document_function_names": [
      "is_function",
      "set_source_nodes",
      "__post_init__",
      "__str__",
      "response_gen",
      "async_response_gen",
      "set_source_nodes",
      "__post_init__",
      "__str__",
      "_ensure_async_setup",
      "put_in_queue",
      "aput_in_queue",
      "write_response_to_history",
      "awrite_response_to_history",
      "response_gen",
      "async_response_gen",
      "print_response_stream",
      "aprint_response_stream",
      "reset",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "chat_repl",
      "streaming_chat_repl",
      "chat_history"
    ],
    "document_class_names": [
      "ChatResponseMode",
      "class",
      "class",
      "BaseChatEngine",
      "ChatMode"
    ],
    "content": "class ChatMode(str, Enum):\n    \"\"\"Chat Engine Modes.\"\"\"\n\n    SIMPLE = \"simple\"\n    \"\"\"Corresponds to `SimpleChatEngine`.\n\n    Chat with LLM, without making use of a knowledge base.\n    \"\"\"\n\n    CONDENSE_QUESTION = \"condense_question\"\n    \"\"\"Corresponds to `CondenseQuestionChatEngine`.\n\n    First generate a standalone question from conversation context and last message,\n    then query the query engine for a response.\n    \"\"\"\n\n    CONTEXT = \"context\"\n    \"\"\"Corresponds to `ContextChatEngine`.\n\n    First retrieve text from the index using the user's message, then use the context\n    in the system prompt to generate a response.\n    \"\"\"\n\n    CONDENSE_PLUS_CONTEXT = \"condense_plus_context\"\n    \"\"\"Corresponds to `CondensePlusContextChatEngine`.\n\n    First condense a conversation and latest user message to a standalone question.\n    Then build a context for the standalone question from a retriever,\n    Then pass the context along with prompt and user message to LLM to generate a response.\n    \"\"\"\n\n    REACT = \"react\"\n    \"\"\"Corresponds to `ReActAgent`.\n\n    Use a ReAct agent loop with query engine tools.\n    \"\"\"\n\n    OPENAI = \"openai\"\n    \"\"\"Corresponds to `OpenAIAgent`.\n\n    Use an OpenAI function calling agent loop.\n\n    NOTE: only works with OpenAI models that support function calling API.\n    \"\"\"\n\n    BEST = \"best\"\n    \"\"\"Select the best chat engine based on the current LLM.\n\n    Corresponds to `OpenAIAgent` if using an OpenAI model that supports\n    function calling API, otherwise, corresponds to `ReActAgent`.\n    \"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/condense_question.py",
    "filename": "condense_question.py",
    "relpath": "chat_engine/condense_question.py",
    "start_line": 1,
    "end_line": 377,
    "length": 377,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "_condense_question",
      "_acondense_question",
      "_get_tool_output_from_response",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "reset",
      "chat_history"
    ],
    "chunk_class_names": [
      "CondenseQuestionChatEngine",
      "attribute",
      "attribute",
      "attribute",
      "attribute"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "_condense_question",
      "_acondense_question",
      "_get_tool_output_from_response",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "reset",
      "chat_history"
    ],
    "document_class_names": [
      "CondenseQuestionChatEngine",
      "attribute",
      "attribute",
      "attribute",
      "attribute"
    ],
    "content": "import asyncio\nimport logging\nfrom typing import Any, List, Optional, Type\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.base.response.schema import (\n    RESPONSE_TYPE,\n    StreamingResponse,\n    AsyncStreamingResponse,\n)\nfrom llama_index.core.callbacks import CallbackManager, trace_method\nfrom llama_index.core.chat_engine.types import (\n    AgentChatResponse,\n    BaseChatEngine,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.chat_engine.utils import (\n    response_gen_from_query_engine,\n    aresponse_gen_from_query_engine,\n)\nfrom llama_index.core.base.llms.generic_utils import messages_to_history_str\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.settings import Settings\n\nfrom llama_index.core.tools import ToolOutput\nfrom llama_index.core.types import Thread\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_TEMPLATE = \"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.\n\n<Chat History>\n{chat_history}\n\n<Follow Up Message>\n{question}\n\n<Standalone question>\n\"\"\"\n\nDEFAULT_PROMPT = PromptTemplate(DEFAULT_TEMPLATE)\n\n\nclass CondenseQuestionChatEngine(BaseChatEngine):\n    \"\"\"\n    Condense Question Chat Engine.\n\n    First generate a standalone question from conversation context and last message,\n    then query the query engine for a response.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_engine: BaseQueryEngine,\n        condense_question_prompt: BasePromptTemplate,\n        memory: BaseMemory,\n        llm: LLM,\n        verbose: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        self._query_engine = query_engine\n        self._condense_question_prompt = condense_question_prompt\n        self._memory = memory\n        self._llm = llm\n        self._verbose = verbose\n        self.callback_manager = callback_manager or CallbackManager([])\n\n    @classmethod\n    def from_defaults(\n        cls,\n        query_engine: BaseQueryEngine,\n        condense_question_prompt: Optional[BasePromptTemplate] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,\n        verbose: bool = False,\n        system_prompt: Optional[str] = None,\n        prefix_messages: Optional[List[ChatMessage]] = None,\n        llm: Optional[LLM] = None,\n        **kwargs: Any,\n    ) -> \"CondenseQuestionChatEngine\":\n        \"\"\"Initialize a CondenseQuestionChatEngine from default parameters.\"\"\"\n        condense_question_prompt = condense_question_prompt or DEFAULT_PROMPT\n\n        llm = llm or Settings.llm\n\n        chat_history = chat_history or []\n        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)\n\n        if system_prompt is not None:\n            raise NotImplementedError(\n                \"system_prompt is not supported for CondenseQuestionChatEngine.\"\n            )\n        if prefix_messages is not None:\n            raise NotImplementedError(\n                \"prefix_messages is not supported for CondenseQuestionChatEngine.\"\n            )\n\n        return cls(\n            query_engine,\n            condense_question_prompt,\n            memory,\n            llm,\n            verbose=verbose,\n            callback_manager=Settings.callback_manager,\n        )\n\n    def _condense_question(\n        self, chat_history: List[ChatMessage], last_message: str\n    ) -> str:\n        \"\"\"\n        Generate standalone question from conversation context and last message.\n        \"\"\"\n        if not chat_history:\n            # Keep the question as is if there's no conversation context.\n            return last_message\n\n        chat_history_str = messages_to_history_str(chat_history)\n        logger.debug(chat_history_str)\n\n        return self._llm.predict(\n            self._condense_question_prompt,\n            question=last_message,\n            chat_history=chat_history_str,\n        )\n\n    async def _acondense_question(\n        self, chat_history: List[ChatMessage], last_message: str\n    ) -> str:\n        \"\"\"\n        Generate standalone question from conversation context and last message.\n        \"\"\"\n        if not chat_history:\n            # Keep the question as is if there's no conversation context.\n            return last_message\n\n        chat_history_str = messages_to_history_str(chat_history)\n        logger.debug(chat_history_str)\n\n        return await self._llm.apredict(\n            self._condense_question_prompt,\n            question=last_message,\n            chat_history=chat_history_str,\n        )\n\n    def _get_tool_output_from_response(\n        self, query: str, response: RESPONSE_TYPE\n    ) -> ToolOutput:\n        if isinstance(response, (StreamingResponse, AsyncStreamingResponse)):\n            return ToolOutput(\n                content=\"\",\n                tool_name=\"query_engine\",\n                raw_input={\"query\": query},\n                raw_output=response,\n            )\n        else:\n            return ToolOutput(\n                content=str(response),\n                tool_name=\"query_engine\",\n                raw_input={\"query\": query},\n                raw_output=response,\n            )\n\n    @trace_method(\"chat\")\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        chat_history = chat_history or self._memory.get(input=message)\n\n        # Generate standalone question from conversation context and last message\n        condensed_question = self._condense_question(chat_history, message)\n\n        log_str = f\"Querying with: {condensed_question}\"\n        logger.info(log_str)\n        if self._verbose:\n            print(log_str)\n\n        # TODO: right now, query engine uses class attribute to configure streaming,\n        #       we are moving towards separate streaming and non-streaming methods.\n        #       In the meanwhile, use this hack to toggle streaming.\n        from llama_index.core.query_engine.retriever_query_engine import (\n            RetrieverQueryEngine,\n        )\n\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            is_streaming = self._query_engine._response_synthesizer._streaming\n            self._query_engine._response_synthesizer._streaming = False\n\n        # Query with standalone question\n        query_response = self._query_engine.query(condensed_question)\n\n        # NOTE: reset streaming flag\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            self._query_engine._response_synthesizer._streaming = is_streaming\n\n        tool_output = self._get_tool_output_from_response(\n            condensed_question, query_response\n        )\n\n        # Record response\n        self._memory.put(ChatMessage(role=MessageRole.USER, content=message))\n        self._memory.put(\n            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))\n        )\n\n        return AgentChatResponse(response=str(query_response), sources=[tool_output])\n\n    @trace_method(\"chat\")\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        chat_history = chat_history or self._memory.get(input=message)\n\n        # Generate standalone question from conversation context and last message\n        condensed_question = self._condense_question(chat_history, message)\n\n        log_str = f\"Querying with: {condensed_question}\"\n        logger.info(log_str)\n        if self._verbose:\n            print(log_str)\n\n        # TODO: right now, query engine uses class attribute to configure streaming,\n        #       we are moving towards separate streaming and non-streaming methods.\n        #       In the meanwhile, use this hack to toggle streaming.\n        from llama_index.core.query_engine.retriever_query_engine import (\n            RetrieverQueryEngine,\n        )\n\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            is_streaming = self._query_engine._response_synthesizer._streaming\n            self._query_engine._response_synthesizer._streaming = True\n\n        # Query with standalone question\n        query_response = self._query_engine.query(condensed_question)\n\n        # NOTE: reset streaming flag\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            self._query_engine._response_synthesizer._streaming = is_streaming\n\n        tool_output = self._get_tool_output_from_response(\n            condensed_question, query_response\n        )\n\n        # Record response\n        if (\n            isinstance(query_response, StreamingResponse)\n            and query_response.response_gen is not None\n        ):\n            # override the generator to include writing to chat history\n            self._memory.put(ChatMessage(role=MessageRole.USER, content=message))\n            response = StreamingAgentChatResponse(\n                chat_stream=response_gen_from_query_engine(query_response.response_gen),\n                sources=[tool_output],\n            )\n            thread = Thread(\n                target=response.write_response_to_history,\n                args=(self._memory,),\n            )\n            thread.start()\n        else:\n            raise ValueError(\"Streaming is not enabled. Please use chat() instead.\")\n        return response\n\n    @trace_method(\"chat\")\n    async def achat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        chat_history = chat_history or self._memory.get(input=message)\n\n        # Generate standalone question from conversation context and last message\n        condensed_question = await self._acondense_question(chat_history, message)\n\n        log_str = f\"Querying with: {condensed_question}\"\n        logger.info(log_str)\n        if self._verbose:\n            print(log_str)\n\n        # TODO: right now, query engine uses class attribute to configure streaming,\n        #       we are moving towards separate streaming and non-streaming methods.\n        #       In the meanwhile, use this hack to toggle streaming.\n        from llama_index.core.query_engine.retriever_query_engine import (\n            RetrieverQueryEngine,\n        )\n\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            is_streaming = self._query_engine._response_synthesizer._streaming\n            self._query_engine._response_synthesizer._streaming = False\n\n        # Query with standalone question\n        query_response = await self._query_engine.aquery(condensed_question)\n\n        # NOTE: reset streaming flag\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            self._query_engine._response_synthesizer._streaming = is_streaming\n\n        tool_output = self._get_tool_output_from_response(\n            condensed_question, query_response\n        )\n\n        # Record response\n        await self._memory.aput(ChatMessage(role=MessageRole.USER, content=message))\n        await self._memory.aput(\n            ChatMessage(role=MessageRole.ASSISTANT, content=str(query_response))\n        )\n\n        return AgentChatResponse(response=str(query_response), sources=[tool_output])\n\n    @trace_method(\"chat\")\n    async def astream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        chat_history = chat_history or self._memory.get(input=message)\n\n        # Generate standalone question from conversation context and last message\n        condensed_question = await self._acondense_question(chat_history, message)\n\n        log_str = f\"Querying with: {condensed_question}\"\n        logger.info(log_str)\n        if self._verbose:\n            print(log_str)\n\n        # TODO: right now, query engine uses class attribute to configure streaming,\n        #       we are moving towards separate streaming and non-streaming methods.\n        #       In the meanwhile, use this hack to toggle streaming.\n        from llama_index.core.query_engine.retriever_query_engine import (\n            RetrieverQueryEngine,\n        )\n\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            is_streaming = self._query_engine._response_synthesizer._streaming\n            self._query_engine._response_synthesizer._streaming = True\n\n        # Query with standalone question\n        query_response = await self._query_engine.aquery(condensed_question)\n\n        # NOTE: reset streaming flag\n        if isinstance(self._query_engine, RetrieverQueryEngine):\n            self._query_engine._response_synthesizer._streaming = is_streaming\n\n        tool_output = self._get_tool_output_from_response(\n            condensed_question, query_response\n        )\n\n        # Record response\n        if isinstance(query_response, AsyncStreamingResponse):\n            # override the generator to include writing to chat history\n            # TODO: query engine does not support async generator yet\n            await self._memory.aput(ChatMessage(role=MessageRole.USER, content=message))\n            response = StreamingAgentChatResponse(\n                achat_stream=aresponse_gen_from_query_engine(\n                    query_response.async_response_gen()\n                ),\n                sources=[tool_output],\n            )\n            response.awrite_response_to_history_task = asyncio.create_task(\n                response.awrite_response_to_history(self._memory)\n            )\n\n        else:\n            raise ValueError(\"Streaming is not enabled. Please use achat() instead.\")\n        return response\n\n    def reset(self) -> None:\n        # Clear chat history\n        self._memory.reset()\n\n    @property\n    def chat_history(self) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self._memory.get_all()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/__init__.py",
    "filename": "__init__.py",
    "relpath": "chat_engine/__init__.py",
    "start_line": 1,
    "end_line": 15,
    "length": 15,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.chat_engine.condense_plus_context import (\n    CondensePlusContextChatEngine,\n)\nfrom llama_index.core.chat_engine.condense_question import (\n    CondenseQuestionChatEngine,\n)\nfrom llama_index.core.chat_engine.context import ContextChatEngine\nfrom llama_index.core.chat_engine.simple import SimpleChatEngine\n\n__all__ = [\n    \"SimpleChatEngine\",\n    \"CondenseQuestionChatEngine\",\n    \"ContextChatEngine\",\n    \"CondensePlusContextChatEngine\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/utils.py",
    "filename": "utils.py",
    "relpath": "chat_engine/utils.py",
    "start_line": 1,
    "end_line": 75,
    "length": 75,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_prefix_messages_with_context",
      "get_response_synthesizer",
      "response_gen_from_query_engine",
      "aresponse_gen_from_query_engine"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_prefix_messages_with_context",
      "get_response_synthesizer",
      "response_gen_from_query_engine",
      "aresponse_gen_from_query_engine"
    ],
    "document_class_names": [],
    "content": "from typing import Callable, Dict, List, Optional\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseGen,\n    ChatResponseAsyncGen,\n    MessageRole,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import ChatPromptTemplate, PromptTemplate\nfrom llama_index.core.response_synthesizers import CompactAndRefine\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.types import TokenGen, TokenAsyncGen\n\n\ndef get_prefix_messages_with_context(\n    context_template: PromptTemplate,\n    system_prompt: str,\n    prefix_messages: List[ChatMessage],\n    chat_history: List[ChatMessage],\n    llm_metadata_system_role: MessageRole,\n) -> List[ChatMessage]:\n    context_str_w_sys_prompt = context_template.template + system_prompt.strip()\n    return [\n        ChatMessage(content=context_str_w_sys_prompt, role=llm_metadata_system_role),\n        *prefix_messages,\n        *chat_history,\n        ChatMessage(content=\"{query_str}\", role=MessageRole.USER),\n    ]\n\n\ndef get_response_synthesizer(\n    llm: LLM,\n    callback_manager: CallbackManager,\n    qa_messages: List[ChatMessage],\n    refine_messages: List[ChatMessage],\n    streaming: bool = False,\n    qa_function_mappings: Optional[Dict[str, Callable]] = None,\n    refine_function_mappings: Optional[Dict[str, Callable]] = None,\n) -> CompactAndRefine:\n    return CompactAndRefine(\n        llm=llm,\n        callback_manager=callback_manager,\n        text_qa_template=ChatPromptTemplate.from_messages(\n            qa_messages,\n            function_mappings=qa_function_mappings,\n        ),\n        refine_template=ChatPromptTemplate.from_messages(\n            refine_messages,\n            function_mappings=refine_function_mappings,\n        ),\n        streaming=streaming,\n    )\n\n\ndef response_gen_from_query_engine(response_gen: TokenGen) -> ChatResponseGen:\n    response_str = \"\"\n    for token in response_gen:\n        response_str += token\n        yield ChatResponse(\n            message=ChatMessage(role=MessageRole.ASSISTANT, content=response_str),\n            delta=token,\n        )\n\n\nasync def aresponse_gen_from_query_engine(\n    response_gen: TokenAsyncGen,\n) -> ChatResponseAsyncGen:\n    response_str = \"\"\n    async for token in response_gen:\n        response_str += token\n        yield ChatResponse(\n            message=ChatMessage(role=MessageRole.ASSISTANT, content=response_str),\n            delta=token,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py",
    "filename": "condense_plus_context.py",
    "relpath": "chat_engine/condense_plus_context.py",
    "start_line": 1,
    "end_line": 78,
    "length": 78,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "_condense_question",
      "_acondense_question",
      "_get_nodes",
      "_aget_nodes",
      "_get_response_synthesizer",
      "_run_c3",
      "_arun_c3",
      "chat",
      "stream_chat",
      "wrapped_gen",
      "achat",
      "astream_chat",
      "wrapped_gen",
      "reset",
      "chat_history"
    ],
    "document_class_names": [
      "CondensePlusContextChatEngine"
    ],
    "content": "import logging\nfrom typing import Any, List, Optional, Tuple, Union\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    MessageRole,\n)\nfrom llama_index.core.base.response.schema import (\n    AsyncStreamingResponse,\n    StreamingResponse,\n)\nfrom llama_index.core.callbacks import CallbackManager, trace_method\nfrom llama_index.core.chat_engine.types import (\n    AgentChatResponse,\n    BaseChatEngine,\n    StreamingAgentChatResponse,\n    ToolOutput,\n)\nfrom llama_index.core.indices.base_retriever import BaseRetriever\nfrom llama_index.core.indices.query.schema import QueryBundle\nfrom llama_index.core.base.llms.generic_utils import messages_to_history_str\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.response_synthesizers import CompactAndRefine\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utilities.token_counting import TokenCounter\nfrom llama_index.core.chat_engine.utils import (\n    get_prefix_messages_with_context,\n    get_response_synthesizer,\n)\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_CONTEXT_PROMPT_TEMPLATE = \"\"\"\n  The following is a friendly conversation between a user and an AI assistant.\n  The assistant is talkative and provides lots of specific details from its context.\n  If the assistant does not know the answer to a question, it truthfully says it\n  does not know.\n\n  Here are the relevant documents for the context:\n\n  {context_str}\n\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\n  Answer \"don't know\" if not present in the document.\n  \"\"\"\n\nDEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE = \"\"\"\n  The following is a friendly conversation between a user and an AI assistant.\n  The assistant is talkative and provides lots of specific details from its context.\n  If the assistant does not know the answer to a question, it truthfully says it\n  does not know.\n\n  Here are the relevant documents for the context:\n\n  {context_msg}\n\n  Existing Answer:\n  {existing_answer}\n\n  Instruction: Refine the existing answer using the provided context to assist the user.\n  If the context isn't helpful, just repeat the existing answer and nothing more.\n  \"\"\"\n\nDEFAULT_CONDENSE_PROMPT_TEMPLATE = \"\"\"\n  Given the following conversation between a user and an AI assistant and a follow up question from user,\n  rephrase the follow up question to be a standalone question.\n\n  Chat History:\n  {chat_history}\n  Follow Up Input: {question}\n  Standalone question:\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/condense_plus_context.py",
    "filename": "condense_plus_context.py",
    "relpath": "chat_engine/condense_plus_context.py",
    "start_line": 78,
    "end_line": 443,
    "length": 366,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "_condense_question",
      "_acondense_question",
      "_get_nodes",
      "_aget_nodes",
      "_get_response_synthesizer",
      "_run_c3",
      "_arun_c3",
      "chat",
      "stream_chat",
      "wrapped_gen",
      "achat",
      "astream_chat",
      "wrapped_gen",
      "reset",
      "chat_history"
    ],
    "chunk_class_names": [
      "CondensePlusContextChatEngine"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "_condense_question",
      "_acondense_question",
      "_get_nodes",
      "_aget_nodes",
      "_get_response_synthesizer",
      "_run_c3",
      "_arun_c3",
      "chat",
      "stream_chat",
      "wrapped_gen",
      "achat",
      "astream_chat",
      "wrapped_gen",
      "reset",
      "chat_history"
    ],
    "document_class_names": [
      "CondensePlusContextChatEngine"
    ],
    "content": "class CondensePlusContextChatEngine(BaseChatEngine):\n    \"\"\"\n    Condensed Conversation & Context Chat Engine.\n\n    First condense a conversation and latest user message to a standalone question\n    Then build a context for the standalone question from a retriever,\n    Then pass the context along with prompt and user message to LLM to generate a response.\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        llm: LLM,\n        memory: BaseMemory,\n        context_prompt: Optional[Union[str, PromptTemplate]] = None,\n        context_refine_prompt: Optional[Union[str, PromptTemplate]] = None,\n        condense_prompt: Optional[Union[str, PromptTemplate]] = None,\n        system_prompt: Optional[str] = None,\n        skip_condense: bool = False,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n    ):\n        self._retriever = retriever\n        self._llm = llm\n        self._memory = memory\n\n        context_prompt = context_prompt or DEFAULT_CONTEXT_PROMPT_TEMPLATE\n        if isinstance(context_prompt, str):\n            context_prompt = PromptTemplate(context_prompt)\n        self._context_prompt_template = context_prompt\n\n        context_refine_prompt = (\n            context_refine_prompt or DEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE\n        )\n        if isinstance(context_refine_prompt, str):\n            context_refine_prompt = PromptTemplate(context_refine_prompt)\n        self._context_refine_prompt_template = context_refine_prompt\n\n        condense_prompt = condense_prompt or DEFAULT_CONDENSE_PROMPT_TEMPLATE\n        if isinstance(condense_prompt, str):\n            condense_prompt = PromptTemplate(condense_prompt)\n        self._condense_prompt_template = condense_prompt\n\n        self._system_prompt = system_prompt\n        self._skip_condense = skip_condense\n        self._node_postprocessors = node_postprocessors or []\n        self.callback_manager = callback_manager or CallbackManager([])\n        for node_postprocessor in self._node_postprocessors:\n            node_postprocessor.callback_manager = self.callback_manager\n\n        self._token_counter = TokenCounter()\n        self._verbose = verbose\n\n    @classmethod\n    def from_defaults(\n        cls,\n        retriever: BaseRetriever,\n        llm: Optional[LLM] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        system_prompt: Optional[str] = None,\n        context_prompt: Optional[Union[str, PromptTemplate]] = None,\n        context_refine_prompt: Optional[Union[str, PromptTemplate]] = None,\n        condense_prompt: Optional[Union[str, PromptTemplate]] = None,\n        skip_condense: bool = False,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"CondensePlusContextChatEngine\":\n        \"\"\"Initialize a CondensePlusContextChatEngine from default parameters.\"\"\"\n        llm = llm or Settings.llm\n\n        chat_history = chat_history or []\n        memory = memory or ChatMemoryBuffer.from_defaults(\n            chat_history=chat_history, token_limit=llm.metadata.context_window - 256\n        )\n\n        return cls(\n            retriever=retriever,\n            llm=llm,\n            memory=memory,\n            context_prompt=context_prompt,\n            context_refine_prompt=context_refine_prompt,\n            condense_prompt=condense_prompt,\n            skip_condense=skip_condense,\n            callback_manager=Settings.callback_manager,\n            node_postprocessors=node_postprocessors,\n            system_prompt=system_prompt,\n            verbose=verbose,\n        )\n\n    def _condense_question(\n        self, chat_history: List[ChatMessage], latest_message: str\n    ) -> str:\n        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"\n        if self._skip_condense or len(chat_history) == 0:\n            return latest_message\n\n        chat_history_str = messages_to_history_str(chat_history)\n        logger.debug(chat_history_str)\n\n        llm_input = self._condense_prompt_template.format(\n            chat_history=chat_history_str, question=latest_message\n        )\n\n        return str(self._llm.complete(llm_input))\n\n    async def _acondense_question(\n        self, chat_history: List[ChatMessage], latest_message: str\n    ) -> str:\n        \"\"\"Condense a conversation history and latest user message to a standalone question.\"\"\"\n        if self._skip_condense or len(chat_history) == 0:\n            return latest_message\n\n        chat_history_str = messages_to_history_str(chat_history)\n        logger.debug(chat_history_str)\n\n        llm_input = self._condense_prompt_template.format(\n            chat_history=chat_history_str, question=latest_message\n        )\n\n        return str(await self._llm.acomplete(llm_input))\n\n    def _get_nodes(self, message: str) -> List[NodeWithScore]:\n        \"\"\"Generate context information from a message.\"\"\"\n        nodes = self._retriever.retrieve(message)\n        for postprocessor in self._node_postprocessors:\n            nodes = postprocessor.postprocess_nodes(\n                nodes, query_bundle=QueryBundle(message)\n            )\n\n        return nodes\n\n    async def _aget_nodes(self, message: str) -> List[NodeWithScore]:\n        \"\"\"Generate context information from a message.\"\"\"\n        nodes = await self._retriever.aretrieve(message)\n        for postprocessor in self._node_postprocessors:\n            nodes = postprocessor.postprocess_nodes(\n                nodes, query_bundle=QueryBundle(message)\n            )\n\n        return nodes\n\n    def _get_response_synthesizer(\n        self, chat_history: List[ChatMessage], streaming: bool = False\n    ) -> CompactAndRefine:\n        system_prompt = self._system_prompt or \"\"\n        qa_messages = get_prefix_messages_with_context(\n            self._context_prompt_template,\n            system_prompt,\n            [],\n            chat_history,\n            self._llm.metadata.system_role,\n        )\n        refine_messages = get_prefix_messages_with_context(\n            self._context_refine_prompt_template,\n            system_prompt,\n            [],\n            chat_history,\n            self._llm.metadata.system_role,\n        )\n\n        return get_response_synthesizer(\n            self._llm,\n            self.callback_manager,\n            qa_messages,\n            refine_messages,\n            streaming,\n            qa_function_mappings=self._context_prompt_template.function_mappings,\n            refine_function_mappings=self._context_refine_prompt_template.function_mappings,\n        )\n\n    def _run_c3(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        streaming: bool = False,\n    ) -> Tuple[CompactAndRefine, ToolOutput, List[NodeWithScore]]:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        chat_history = self._memory.get(input=message)\n\n        # Condense conversation history and latest message to a standalone question\n        condensed_question = self._condense_question(chat_history, message)  # type: ignore\n        logger.info(f\"Condensed question: {condensed_question}\")\n        if self._verbose:\n            print(f\"Condensed question: {condensed_question}\")\n\n        # get the context nodes using the condensed question\n        context_nodes = self._get_nodes(condensed_question)\n        context_source = ToolOutput(\n            tool_name=\"retriever\",\n            content=str(context_nodes),\n            raw_input={\"message\": condensed_question},\n            raw_output=context_nodes,\n        )\n\n        # build the response synthesizer\n        response_synthesizer = self._get_response_synthesizer(\n            chat_history, streaming=streaming\n        )\n\n        return response_synthesizer, context_source, context_nodes\n\n    async def _arun_c3(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        streaming: bool = False,\n    ) -> Tuple[CompactAndRefine, ToolOutput, List[NodeWithScore]]:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        chat_history = self._memory.get(input=message)\n\n        # Condense conversation history and latest message to a standalone question\n        condensed_question = await self._acondense_question(chat_history, message)  # type: ignore\n        logger.info(f\"Condensed question: {condensed_question}\")\n        if self._verbose:\n            print(f\"Condensed question: {condensed_question}\")\n\n        # get the context nodes using the condensed question\n        context_nodes = await self._aget_nodes(condensed_question)\n        context_source = ToolOutput(\n            tool_name=\"retriever\",\n            content=str(context_nodes),\n            raw_input={\"message\": condensed_question},\n            raw_output=context_nodes,\n        )\n\n        # build the response synthesizer\n        response_synthesizer = self._get_response_synthesizer(\n            chat_history, streaming=streaming\n        )\n\n        return response_synthesizer, context_source, context_nodes\n\n    @trace_method(\"chat\")\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        synthesizer, context_source, context_nodes = self._run_c3(message, chat_history)\n\n        response = synthesizer.synthesize(message, context_nodes)\n\n        user_message = ChatMessage(content=message, role=MessageRole.USER)\n        assistant_message = ChatMessage(\n            content=str(response), role=MessageRole.ASSISTANT\n        )\n        self._memory.put(user_message)\n        self._memory.put(assistant_message)\n\n        return AgentChatResponse(\n            response=str(response),\n            sources=[context_source],\n            source_nodes=context_nodes,\n        )\n\n    @trace_method(\"chat\")\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        synthesizer, context_source, context_nodes = self._run_c3(\n            message, chat_history, streaming=True\n        )\n\n        response = synthesizer.synthesize(message, context_nodes)\n        assert isinstance(response, StreamingResponse)\n\n        def wrapped_gen(response: StreamingResponse) -> ChatResponseGen:\n            full_response = \"\"\n            for token in response.response_gen:\n                full_response += token\n                yield ChatResponse(\n                    message=ChatMessage(\n                        content=full_response, role=MessageRole.ASSISTANT\n                    ),\n                    delta=token,\n                )\n\n            user_message = ChatMessage(content=message, role=MessageRole.USER)\n            assistant_message = ChatMessage(\n                content=full_response, role=MessageRole.ASSISTANT\n            )\n            self._memory.put(user_message)\n            self._memory.put(assistant_message)\n\n        return StreamingAgentChatResponse(\n            chat_stream=wrapped_gen(response),\n            sources=[context_source],\n            source_nodes=context_nodes,\n            is_writing_to_memory=False,\n        )\n\n    @trace_method(\"chat\")\n    async def achat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        synthesizer, context_source, context_nodes = await self._arun_c3(\n            message, chat_history\n        )\n\n        response = await synthesizer.asynthesize(message, context_nodes)\n\n        user_message = ChatMessage(content=message, role=MessageRole.USER)\n        assistant_message = ChatMessage(\n            content=str(response), role=MessageRole.ASSISTANT\n        )\n        await self._memory.aput(user_message)\n        await self._memory.aput(assistant_message)\n\n        return AgentChatResponse(\n            response=str(response),\n            sources=[context_source],\n            source_nodes=context_nodes,\n        )\n\n    @trace_method(\"chat\")\n    async def astream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        synthesizer, context_source, context_nodes = await self._arun_c3(\n            message, chat_history, streaming=True\n        )\n\n        response = await synthesizer.asynthesize(message, context_nodes)\n        assert isinstance(response, AsyncStreamingResponse)\n\n        async def wrapped_gen(response: AsyncStreamingResponse) -> ChatResponseAsyncGen:\n            full_response = \"\"\n            async for token in response.async_response_gen():\n                full_response += token\n                yield ChatResponse(\n                    message=ChatMessage(\n                        content=full_response, role=MessageRole.ASSISTANT\n                    ),\n                    delta=token,\n                )\n\n            user_message = ChatMessage(content=message, role=MessageRole.USER)\n            assistant_message = ChatMessage(\n                content=full_response, role=MessageRole.ASSISTANT\n            )\n            await self._memory.aput(user_message)\n            await self._memory.aput(assistant_message)\n\n        return StreamingAgentChatResponse(\n            achat_stream=wrapped_gen(response),\n            sources=[context_source],\n            source_nodes=context_nodes,\n            is_writing_to_memory=False,\n        )\n\n    def reset(self) -> None:\n        # Clear chat history\n        self._memory.reset()\n\n    @property\n    def chat_history(self) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self._memory.get_all()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/context.py",
    "filename": "context.py",
    "relpath": "chat_engine/context.py",
    "start_line": 1,
    "end_line": 393,
    "length": 393,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "_get_nodes",
      "_aget_nodes",
      "_get_response_synthesizer",
      "chat",
      "stream_chat",
      "wrapped_gen",
      "achat",
      "astream_chat",
      "wrapped_gen",
      "reset",
      "chat_history"
    ],
    "chunk_class_names": [
      "ContextChatEngine"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "_get_nodes",
      "_aget_nodes",
      "_get_response_synthesizer",
      "chat",
      "stream_chat",
      "wrapped_gen",
      "achat",
      "astream_chat",
      "wrapped_gen",
      "reset",
      "chat_history"
    ],
    "document_class_names": [
      "ContextChatEngine"
    ],
    "content": "from typing import Any, List, Optional, Union\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    MessageRole,\n)\nfrom llama_index.core.base.response.schema import (\n    StreamingResponse,\n    AsyncStreamingResponse,\n)\nfrom llama_index.core.callbacks import CallbackManager, trace_method\nfrom llama_index.core.chat_engine.types import (\n    AgentChatResponse,\n    BaseChatEngine,\n    StreamingAgentChatResponse,\n    ToolOutput,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.response_synthesizers import CompactAndRefine\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.chat_engine.utils import (\n    get_prefix_messages_with_context,\n    get_response_synthesizer,\n)\n\n\nDEFAULT_CONTEXT_TEMPLATE = (\n    \"Use the context information below to assist the user.\"\n    \"\\n--------------------\\n\"\n    \"{context_str}\"\n    \"\\n--------------------\\n\"\n)\n\nDEFAULT_REFINE_TEMPLATE = (\n    \"Using the context below, refine the following existing answer using the provided context to assist the user.\\n\"\n    \"If the context isn't helpful, just repeat the existing answer and nothing more.\\n\"\n    \"\\n--------------------\\n\"\n    \"{context_msg}\"\n    \"\\n--------------------\\n\"\n    \"Existing Answer:\\n\"\n    \"{existing_answer}\"\n    \"\\n--------------------\\n\"\n)\n\n\nclass ContextChatEngine(BaseChatEngine):\n    \"\"\"\n    Context Chat Engine.\n\n    Uses a retriever to retrieve a context, set the context in the system prompt,\n    and then uses an LLM to generate a response, for a fluid chat experience.\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        llm: LLM,\n        memory: BaseMemory,\n        prefix_messages: List[ChatMessage],\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        context_template: Optional[Union[str, PromptTemplate]] = None,\n        context_refine_template: Optional[Union[str, PromptTemplate]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        self._retriever = retriever\n        self._llm = llm\n        self._memory = memory\n        self._prefix_messages = prefix_messages\n        self._node_postprocessors = node_postprocessors or []\n\n        context_template = context_template or DEFAULT_CONTEXT_TEMPLATE\n        if isinstance(context_template, str):\n            context_template = PromptTemplate(context_template)\n        self._context_template = context_template\n\n        context_refine_template = context_refine_template or DEFAULT_REFINE_TEMPLATE\n        if isinstance(context_refine_template, str):\n            context_refine_template = PromptTemplate(context_refine_template)\n        self._context_refine_template = context_refine_template\n\n        self.callback_manager = callback_manager or CallbackManager([])\n        for node_postprocessor in self._node_postprocessors:\n            node_postprocessor.callback_manager = self.callback_manager\n\n    @classmethod\n    def from_defaults(\n        cls,\n        retriever: BaseRetriever,\n        chat_history: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        system_prompt: Optional[str] = None,\n        prefix_messages: Optional[List[ChatMessage]] = None,\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n        context_template: Optional[Union[str, PromptTemplate]] = None,\n        context_refine_template: Optional[Union[str, PromptTemplate]] = None,\n        llm: Optional[LLM] = None,\n        **kwargs: Any,\n    ) -> \"ContextChatEngine\":\n        \"\"\"Initialize a ContextChatEngine from default parameters.\"\"\"\n        llm = llm or Settings.llm\n\n        chat_history = chat_history or []\n        memory = memory or ChatMemoryBuffer.from_defaults(\n            chat_history=chat_history, token_limit=llm.metadata.context_window - 256\n        )\n\n        if system_prompt is not None:\n            if prefix_messages is not None:\n                raise ValueError(\n                    \"Cannot specify both system_prompt and prefix_messages\"\n                )\n            prefix_messages = [\n                ChatMessage(content=system_prompt, role=llm.metadata.system_role)\n            ]\n\n        prefix_messages = prefix_messages or []\n        node_postprocessors = node_postprocessors or []\n\n        return cls(\n            retriever,\n            llm=llm,\n            memory=memory,\n            prefix_messages=prefix_messages,\n            node_postprocessors=node_postprocessors,\n            callback_manager=Settings.callback_manager,\n            context_template=context_template,\n            context_refine_template=context_refine_template,\n        )\n\n    def _get_nodes(self, message: str) -> List[NodeWithScore]:\n        \"\"\"Generate context information from a message.\"\"\"\n        nodes = self._retriever.retrieve(message)\n        for postprocessor in self._node_postprocessors:\n            nodes = postprocessor.postprocess_nodes(\n                nodes, query_bundle=QueryBundle(message)\n            )\n\n        return nodes\n\n    async def _aget_nodes(self, message: str) -> List[NodeWithScore]:\n        \"\"\"Generate context information from a message.\"\"\"\n        nodes = await self._retriever.aretrieve(message)\n        for postprocessor in self._node_postprocessors:\n            nodes = postprocessor.postprocess_nodes(\n                nodes, query_bundle=QueryBundle(message)\n            )\n\n        return nodes\n\n    def _get_response_synthesizer(\n        self, chat_history: List[ChatMessage], streaming: bool = False\n    ) -> CompactAndRefine:\n        # Pull the system prompt from the prefix messages\n        system_prompt = \"\"\n        prefix_messages = self._prefix_messages\n        if (\n            len(self._prefix_messages) != 0\n            and self._prefix_messages[0].role == MessageRole.SYSTEM\n        ):\n            system_prompt = str(self._prefix_messages[0].content)\n            prefix_messages = self._prefix_messages[1:]\n\n        # Get the messages for the QA and refine prompts\n        qa_messages = get_prefix_messages_with_context(\n            self._context_template,\n            system_prompt,\n            prefix_messages,\n            chat_history,\n            self._llm.metadata.system_role,\n        )\n        refine_messages = get_prefix_messages_with_context(\n            self._context_refine_template,\n            system_prompt,\n            prefix_messages,\n            chat_history,\n            self._llm.metadata.system_role,\n        )\n\n        # Get the response synthesizer\n        return get_response_synthesizer(\n            self._llm,\n            self.callback_manager,\n            qa_messages,\n            refine_messages,\n            streaming,\n            qa_function_mappings=self._context_template.function_mappings,\n            refine_function_mappings=self._context_refine_template.function_mappings,\n        )\n\n    @trace_method(\"chat\")\n    def chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        prev_chunks: Optional[List[NodeWithScore]] = None,\n    ) -> AgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        # get nodes and postprocess them\n        nodes = self._get_nodes(message)\n        if len(nodes) == 0 and prev_chunks is not None:\n            nodes = prev_chunks\n\n        # Get the response synthesizer with dynamic prompts\n        chat_history = self._memory.get(\n            input=message,\n        )\n        synthesizer = self._get_response_synthesizer(chat_history)\n\n        response = synthesizer.synthesize(message, nodes)\n        user_message = ChatMessage(content=message, role=MessageRole.USER)\n        ai_message = ChatMessage(content=str(response), role=MessageRole.ASSISTANT)\n\n        self._memory.put(user_message)\n        self._memory.put(ai_message)\n\n        return AgentChatResponse(\n            response=str(response),\n            sources=[\n                ToolOutput(\n                    tool_name=\"retriever\",\n                    content=str(nodes),\n                    raw_input={\"message\": message},\n                    raw_output=nodes,\n                )\n            ],\n            source_nodes=nodes,\n        )\n\n    @trace_method(\"chat\")\n    def stream_chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        prev_chunks: Optional[List[NodeWithScore]] = None,\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        # get nodes and postprocess them\n        nodes = self._get_nodes(message)\n        if len(nodes) == 0 and prev_chunks is not None:\n            nodes = prev_chunks\n\n        # Get the response synthesizer with dynamic prompts\n        chat_history = self._memory.get(\n            input=message,\n        )\n        synthesizer = self._get_response_synthesizer(chat_history, streaming=True)\n\n        response = synthesizer.synthesize(message, nodes)\n        assert isinstance(response, StreamingResponse)\n\n        def wrapped_gen(response: StreamingResponse) -> ChatResponseGen:\n            full_response = \"\"\n            for token in response.response_gen:\n                full_response += token\n                yield ChatResponse(\n                    message=ChatMessage(\n                        content=full_response, role=MessageRole.ASSISTANT\n                    ),\n                    delta=token,\n                )\n\n            user_message = ChatMessage(content=message, role=MessageRole.USER)\n            ai_message = ChatMessage(content=full_response, role=MessageRole.ASSISTANT)\n            self._memory.put(user_message)\n            self._memory.put(ai_message)\n\n        return StreamingAgentChatResponse(\n            chat_stream=wrapped_gen(response),\n            sources=[\n                ToolOutput(\n                    tool_name=\"retriever\",\n                    content=str(nodes),\n                    raw_input={\"message\": message},\n                    raw_output=nodes,\n                )\n            ],\n            source_nodes=nodes,\n            is_writing_to_memory=False,\n        )\n\n    @trace_method(\"chat\")\n    async def achat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        prev_chunks: Optional[List[NodeWithScore]] = None,\n    ) -> AgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        # get nodes and postprocess them\n        nodes = await self._aget_nodes(message)\n        if len(nodes) == 0 and prev_chunks is not None:\n            nodes = prev_chunks\n\n        # Get the response synthesizer with dynamic prompts\n        chat_history = self._memory.get(\n            input=message,\n        )\n        synthesizer = self._get_response_synthesizer(chat_history)\n\n        response = await synthesizer.asynthesize(message, nodes)\n        user_message = ChatMessage(content=message, role=MessageRole.USER)\n        ai_message = ChatMessage(content=str(response), role=MessageRole.ASSISTANT)\n\n        await self._memory.aput(user_message)\n        await self._memory.aput(ai_message)\n\n        return AgentChatResponse(\n            response=str(response),\n            sources=[\n                ToolOutput(\n                    tool_name=\"retriever\",\n                    content=str(nodes),\n                    raw_input={\"message\": message},\n                    raw_output=nodes,\n                )\n            ],\n            source_nodes=nodes,\n        )\n\n    @trace_method(\"chat\")\n    async def astream_chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        prev_chunks: Optional[List[NodeWithScore]] = None,\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        # get nodes and postprocess them\n        nodes = await self._aget_nodes(message)\n        if len(nodes) == 0 and prev_chunks is not None:\n            nodes = prev_chunks\n\n        # Get the response synthesizer with dynamic prompts\n        chat_history = self._memory.get(\n            input=message,\n        )\n        synthesizer = self._get_response_synthesizer(chat_history, streaming=True)\n\n        response = await synthesizer.asynthesize(message, nodes)\n        assert isinstance(response, AsyncStreamingResponse)\n\n        async def wrapped_gen(response: AsyncStreamingResponse) -> ChatResponseAsyncGen:\n            full_response = \"\"\n            async for token in response.async_response_gen():\n                full_response += token\n                yield ChatResponse(\n                    message=ChatMessage(\n                        content=full_response, role=MessageRole.ASSISTANT\n                    ),\n                    delta=token,\n                )\n\n            user_message = ChatMessage(content=message, role=MessageRole.USER)\n            ai_message = ChatMessage(content=full_response, role=MessageRole.ASSISTANT)\n            await self._memory.aput(user_message)\n            await self._memory.aput(ai_message)\n\n        return StreamingAgentChatResponse(\n            achat_stream=wrapped_gen(response),\n            sources=[\n                ToolOutput(\n                    tool_name=\"retriever\",\n                    content=str(nodes),\n                    raw_input={\"message\": message},\n                    raw_output=nodes,\n                )\n            ],\n            source_nodes=nodes,\n            is_writing_to_memory=False,\n        )\n\n    def reset(self) -> None:\n        self._memory.reset()\n\n    @property\n    def chat_history(self) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self._memory.get_all()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/chat_engine/simple.py",
    "filename": "simple.py",
    "relpath": "chat_engine/simple.py",
    "start_line": 1,
    "end_line": 217,
    "length": 217,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "reset",
      "chat_history"
    ],
    "chunk_class_names": [
      "SimpleChatEngine"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "reset",
      "chat_history"
    ],
    "document_class_names": [
      "SimpleChatEngine"
    ],
    "content": "import asyncio\nfrom typing import Any, List, Optional, Type\n\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.callbacks import CallbackManager, trace_method\nfrom llama_index.core.chat_engine.types import (\n    AgentChatResponse,\n    BaseChatEngine,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.types import Thread\n\n\nclass SimpleChatEngine(BaseChatEngine):\n    \"\"\"\n    Simple Chat Engine.\n\n    Have a conversation with the LLM.\n    This does not make use of a knowledge base.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        memory: BaseMemory,\n        prefix_messages: List[ChatMessage],\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> None:\n        self._llm = llm\n        self._memory = memory\n        self._prefix_messages = prefix_messages\n        self.callback_manager = callback_manager or CallbackManager([])\n\n    @classmethod\n    def from_defaults(\n        cls,\n        chat_history: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,\n        system_prompt: Optional[str] = None,\n        prefix_messages: Optional[List[ChatMessage]] = None,\n        llm: Optional[LLM] = None,\n        **kwargs: Any,\n    ) -> \"SimpleChatEngine\":\n        \"\"\"Initialize a SimpleChatEngine from default parameters.\"\"\"\n        llm = llm or Settings.llm\n\n        chat_history = chat_history or []\n        memory = memory or memory_cls.from_defaults(chat_history=chat_history, llm=llm)\n\n        if system_prompt is not None:\n            if prefix_messages is not None:\n                raise ValueError(\n                    \"Cannot specify both system_prompt and prefix_messages\"\n                )\n            prefix_messages = [\n                ChatMessage(content=system_prompt, role=llm.metadata.system_role)\n            ]\n\n        prefix_messages = prefix_messages or []\n\n        return cls(\n            llm=llm,\n            memory=memory,\n            prefix_messages=prefix_messages,\n            callback_manager=Settings.callback_manager,\n        )\n\n    @trace_method(\"chat\")\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        self._memory.put(ChatMessage(content=message, role=\"user\"))\n\n        if hasattr(self._memory, \"tokenizer_fn\"):\n            initial_token_count = len(\n                self._memory.tokenizer_fn(\n                    \" \".join(\n                        [\n                            (m.content or \"\")\n                            for m in self._prefix_messages\n                            if isinstance(m.content, str)\n                        ]\n                    )\n                )\n            )\n        else:\n            initial_token_count = 0\n\n        all_messages = self._prefix_messages + self._memory.get(\n            initial_token_count=initial_token_count\n        )\n\n        chat_response = self._llm.chat(all_messages)\n        ai_message = chat_response.message\n        self._memory.put(ai_message)\n\n        return AgentChatResponse(response=str(chat_response.message.content))\n\n    @trace_method(\"chat\")\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        self._memory.put(ChatMessage(content=message, role=\"user\"))\n\n        if hasattr(self._memory, \"tokenizer_fn\"):\n            initial_token_count = len(\n                self._memory.tokenizer_fn(\n                    \" \".join(\n                        [\n                            (m.content or \"\")\n                            for m in self._prefix_messages\n                            if isinstance(m.content, str)\n                        ]\n                    )\n                )\n            )\n        else:\n            initial_token_count = 0\n\n        all_messages = self._prefix_messages + self._memory.get(\n            initial_token_count=initial_token_count\n        )\n\n        chat_response = StreamingAgentChatResponse(\n            chat_stream=self._llm.stream_chat(all_messages)\n        )\n        thread = Thread(\n            target=chat_response.write_response_to_history, args=(self._memory,)\n        )\n        thread.start()\n\n        return chat_response\n\n    @trace_method(\"chat\")\n    async def achat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        await self._memory.aput(ChatMessage(content=message, role=\"user\"))\n\n        if hasattr(self._memory, \"tokenizer_fn\"):\n            initial_token_count = len(\n                self._memory.tokenizer_fn(\n                    \" \".join(\n                        [\n                            (m.content or \"\")\n                            for m in self._prefix_messages\n                            if isinstance(m.content, str)\n                        ]\n                    )\n                )\n            )\n        else:\n            initial_token_count = 0\n\n        all_messages = self._prefix_messages + self._memory.get(\n            initial_token_count=initial_token_count\n        )\n\n        chat_response = await self._llm.achat(all_messages)\n        ai_message = chat_response.message\n        await self._memory.aput(ai_message)\n\n        return AgentChatResponse(response=str(chat_response.message.content))\n\n    @trace_method(\"chat\")\n    async def astream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        await self._memory.aput(ChatMessage(content=message, role=\"user\"))\n\n        if hasattr(self._memory, \"tokenizer_fn\"):\n            initial_token_count = len(\n                self._memory.tokenizer_fn(\n                    \" \".join(\n                        [\n                            (m.content or \"\")\n                            for m in self._prefix_messages\n                            if isinstance(m.content, str)\n                        ]\n                    )\n                )\n            )\n        else:\n            initial_token_count = 0\n\n        all_messages = self._prefix_messages + self._memory.get(\n            initial_token_count=initial_token_count\n        )\n\n        chat_response = StreamingAgentChatResponse(\n            achat_stream=await self._llm.astream_chat(all_messages)\n        )\n        chat_response.awrite_response_to_history_task = asyncio.create_task(\n            chat_response.awrite_response_to_history(self._memory)\n        )\n\n        return chat_response\n\n    def reset(self) -> None:\n        self._memory.reset()\n\n    @property\n    def chat_history(self) -> List[ChatMessage]:\n        \"\"\"Get chat history.\"\"\"\n        return self._memory.get_all()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/vector_stores/types.py",
    "filename": "types.py",
    "relpath": "vector_stores/types.py",
    "start_line": 1,
    "end_line": 432,
    "length": 432,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_dict",
      "from_dict",
      "from_dicts",
      "legacy_filters",
      "client",
      "add",
      "async_add",
      "delete",
      "adelete",
      "query",
      "aquery",
      "persist",
      "client",
      "get_nodes",
      "aget_nodes",
      "add",
      "async_add",
      "delete",
      "adelete",
      "delete_nodes",
      "adelete_nodes",
      "clear",
      "aclear",
      "query",
      "aquery",
      "persist"
    ],
    "chunk_class_names": [
      "from",
      "class",
      "VectorStoreQueryMode",
      "FilterOperator",
      "FilterCondition",
      "MetadataFilter",
      "for",
      "ExactMatchFilter",
      "MetadataFilters",
      "VectorStoreQuerySpec",
      "MetadataInfo",
      "VectorStoreInfo",
      "class",
      "VectorStore",
      "BasePydanticVectorStore"
    ],
    "document_function_names": [
      "from_dict",
      "from_dict",
      "from_dicts",
      "legacy_filters",
      "client",
      "add",
      "async_add",
      "delete",
      "adelete",
      "query",
      "aquery",
      "persist",
      "client",
      "get_nodes",
      "aget_nodes",
      "add",
      "async_add",
      "delete",
      "adelete",
      "delete_nodes",
      "adelete_nodes",
      "clear",
      "aclear",
      "query",
      "aquery",
      "persist"
    ],
    "document_class_names": [
      "from",
      "class",
      "VectorStoreQueryMode",
      "FilterOperator",
      "FilterCondition",
      "MetadataFilter",
      "for",
      "ExactMatchFilter",
      "MetadataFilters",
      "VectorStoreQuerySpec",
      "MetadataInfo",
      "VectorStoreInfo",
      "class",
      "VectorStore",
      "BasePydanticVectorStore"
    ],
    "content": "\"\"\"Vector store index types.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import (\n    Any,\n    Dict,\n    List,\n    Optional,\n    Protocol,\n    Sequence,\n    Union,\n    runtime_checkable,\n)\n\nimport fsspec\nfrom deprecated import deprecated\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    ConfigDict,\n    StrictFloat,\n    StrictInt,\n    StrictStr,\n)\nfrom llama_index.core.schema import BaseComponent, BaseNode, TextNode\n\nDEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_FNAME = \"vector_store.json\"\n\n\n# legacy: kept for backward compatibility\nNodeWithEmbedding = TextNode\n\n\n@dataclass\nclass VectorStoreQueryResult:\n    \"\"\"Vector store query result.\"\"\"\n\n    nodes: Optional[Sequence[BaseNode]] = None\n    similarities: Optional[List[float]] = None\n    ids: Optional[List[str]] = None\n\n\nclass VectorStoreQueryMode(str, Enum):\n    \"\"\"Vector store query mode.\"\"\"\n\n    DEFAULT = \"default\"\n    SPARSE = \"sparse\"\n    HYBRID = \"hybrid\"\n    TEXT_SEARCH = \"text_search\"\n    SEMANTIC_HYBRID = \"semantic_hybrid\"\n\n    # fit learners\n    SVM = \"svm\"\n    LOGISTIC_REGRESSION = \"logistic_regression\"\n    LINEAR_REGRESSION = \"linear_regression\"\n\n    # maximum marginal relevance\n    MMR = \"mmr\"\n\n\nclass FilterOperator(str, Enum):\n    \"\"\"Vector store filter operator.\"\"\"\n\n    # TODO add more operators\n    EQ = \"==\"  # default operator (string, int, float)\n    GT = \">\"  # greater than (int, float)\n    LT = \"<\"  # less than (int, float)\n    NE = \"!=\"  # not equal to (string, int, float)\n    GTE = \">=\"  # greater than or equal to (int, float)\n    LTE = \"<=\"  # less than or equal to (int, float)\n    IN = \"in\"  # In array (string or number)\n    NIN = \"nin\"  # Not in array (string or number)\n    ANY = \"any\"  # Contains any (array of strings)\n    ALL = \"all\"  # Contains all (array of strings)\n    TEXT_MATCH = \"text_match\"  # full text match (allows you to search for a specific substring, token or phrase within the text field)\n    TEXT_MATCH_INSENSITIVE = (\n        \"text_match_insensitive\"  # full text match (case insensitive)\n    )\n    CONTAINS = \"contains\"  # metadata array contains value (string or number)\n    IS_EMPTY = \"is_empty\"  # the field is not exist or empty (null or empty array)\n\n\nclass FilterCondition(str, Enum):\n    \"\"\"Vector store filter conditions to combine different filters.\"\"\"\n\n    # TODO add more conditions\n    AND = \"and\"\n    OR = \"or\"\n    NOT = \"not\"  # negates the filter condition\n\n\nclass MetadataFilter(BaseModel):\n    r\"\"\"Comprehensive metadata filter for vector stores to support more operators.\n\n    Value uses Strict* types, as int, float and str are compatible types and were all\n    converted to string before.\n\n    See: https://docs.pydantic.dev/latest/usage/types/#strict-types\n    \"\"\"\n\n    key: str\n    value: Optional[\n        Union[\n            StrictInt,\n            StrictFloat,\n            StrictStr,\n            List[StrictStr],\n            List[StrictFloat],\n            List[StrictInt],\n        ]\n    ]\n    operator: FilterOperator = FilterOperator.EQ\n\n    @classmethod\n    def from_dict(\n        cls,\n        filter_dict: Dict,\n    ) -> \"MetadataFilter\":\n        \"\"\"Create MetadataFilter from dictionary.\n\n        Args:\n            filter_dict: Dict with key, value and operator.\n\n        \"\"\"\n        return MetadataFilter.model_validate(filter_dict)\n\n\n# # TODO: Deprecate ExactMatchFilter and use MetadataFilter instead\n# # Keep class for now so that AutoRetriever can still work with old vector stores\n# class ExactMatchFilter(BaseModel):\n#     key: str\n#     value: Union[StrictInt, StrictFloat, StrictStr]\n\n# set ExactMatchFilter to MetadataFilter\nExactMatchFilter = MetadataFilter\n\n\nclass MetadataFilters(BaseModel):\n    \"\"\"Metadata filters for vector stores.\"\"\"\n\n    # Exact match filters and Advanced filters with operators like >, <, >=, <=, !=, etc.\n    filters: List[Union[MetadataFilter, ExactMatchFilter, \"MetadataFilters\"]]\n    # and/or such conditions for combining different filters\n    condition: Optional[FilterCondition] = FilterCondition.AND\n\n    @classmethod\n    @deprecated(\n        \"`from_dict()` is deprecated. \"\n        \"Please use `MetadataFilters(filters=.., condition='and')` directly instead.\"\n    )\n    def from_dict(cls, filter_dict: Dict) -> \"MetadataFilters\":\n        \"\"\"Create MetadataFilters from json.\"\"\"\n        filters = []\n        for k, v in filter_dict.items():\n            filter = MetadataFilter(key=k, value=v, operator=FilterOperator.EQ)\n            filters.append(filter)\n        return cls(filters=filters)\n\n    @classmethod\n    def from_dicts(\n        cls,\n        filter_dicts: List[Dict],\n        condition: Optional[FilterCondition] = FilterCondition.AND,\n    ) -> \"MetadataFilters\":\n        \"\"\"Create MetadataFilters from dicts.\n\n        This takes in a list of individual MetadataFilter objects, along\n        with the condition.\n\n        Args:\n            filter_dicts: List of dicts, each dict is a MetadataFilter.\n            condition: FilterCondition to combine different filters.\n\n        \"\"\"\n        return cls(\n            filters=[\n                MetadataFilter.from_dict(filter_dict) for filter_dict in filter_dicts\n            ],\n            condition=condition,\n        )\n\n    def legacy_filters(self) -> List[ExactMatchFilter]:\n        \"\"\"Convert MetadataFilters to legacy ExactMatchFilters.\"\"\"\n        filters = []\n        for filter in self.filters:\n            if (\n                isinstance(filter, MetadataFilters)\n                or filter.operator != FilterOperator.EQ\n            ):\n                raise ValueError(\n                    \"Vector Store only supports exact match filters. \"\n                    \"Please use ExactMatchFilter or FilterOperator.EQ instead.\"\n                )\n            filters.append(ExactMatchFilter(key=filter.key, value=filter.value))\n        return filters\n\n\nclass VectorStoreQuerySpec(BaseModel):\n    \"\"\"Schema for a structured request for vector store\n    (i.e. to be converted to a VectorStoreQuery).\n\n    Currently only used by VectorIndexAutoRetriever.\n    \"\"\"\n\n    query: str\n    filters: List[MetadataFilter]\n    top_k: Optional[int] = None\n\n\nclass MetadataInfo(BaseModel):\n    \"\"\"Information about a metadata filter supported by a vector store.\n\n    Currently only used by VectorIndexAutoRetriever.\n    \"\"\"\n\n    name: str\n    type: str\n    description: str\n\n\nclass VectorStoreInfo(BaseModel):\n    \"\"\"Information about a vector store (content and supported metadata filters).\n\n    Currently only used by VectorIndexAutoRetriever.\n    \"\"\"\n\n    metadata_info: List[MetadataInfo]\n    content_info: str\n\n\n@dataclass\nclass VectorStoreQuery:\n    \"\"\"Vector store query.\"\"\"\n\n    query_embedding: Optional[List[float]] = None\n    similarity_top_k: int = 1\n    doc_ids: Optional[List[str]] = None\n    node_ids: Optional[List[str]] = None\n    query_str: Optional[str] = None\n    output_fields: Optional[List[str]] = None\n    embedding_field: Optional[str] = None\n\n    mode: VectorStoreQueryMode = VectorStoreQueryMode.DEFAULT\n\n    # NOTE: only for hybrid search (0 for bm25, 1 for vector search)\n    alpha: Optional[float] = None\n\n    # metadata filters\n    filters: Optional[MetadataFilters] = None\n\n    # only for mmr\n    mmr_threshold: Optional[float] = None\n\n    # NOTE: currently only used by postgres hybrid search\n    sparse_top_k: Optional[int] = None\n    # NOTE: return top k results from hybrid search. similarity_top_k is used for dense search top k\n    hybrid_top_k: Optional[int] = None\n\n\n@runtime_checkable\nclass VectorStore(Protocol):\n    \"\"\"Abstract vector store protocol.\"\"\"\n\n    stores_text: bool\n    is_embedding_query: bool = True\n\n    @property\n    def client(self) -> Any:\n        \"\"\"Get client.\"\"\"\n        ...\n\n    def add(\n        self,\n        nodes: List[BaseNode],\n        **add_kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Add nodes with embedding to vector store.\"\"\"\n        ...\n\n    async def async_add(\n        self,\n        nodes: List[BaseNode],\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"\n        Asynchronously add nodes with embedding to vector store.\n        NOTE: this is not implemented for all vector stores. If not implemented,\n        it will just call add synchronously.\n        \"\"\"\n        return self.add(nodes)\n\n    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using with ref_doc_id.\"\"\"\n        ...\n\n    async def adelete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using with ref_doc_id.\n        NOTE: this is not implemented for all vector stores. If not implemented,\n        it will just call delete synchronously.\n        \"\"\"\n        self.delete(ref_doc_id, **delete_kwargs)\n\n    def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:\n        \"\"\"Query vector store.\"\"\"\n        ...\n\n    async def aquery(\n        self, query: VectorStoreQuery, **kwargs: Any\n    ) -> VectorStoreQueryResult:\n        \"\"\"\n        Asynchronously query vector store.\n        NOTE: this is not implemented for all vector stores. If not implemented,\n        it will just call query synchronously.\n        \"\"\"\n        return self.query(query, **kwargs)\n\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        return None\n\n\n# TODO: Temp copy of VectorStore for pydantic, can't mix with runtime_checkable\nclass BasePydanticVectorStore(BaseComponent, ABC):\n    \"\"\"Abstract vector store protocol.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    stores_text: bool\n    is_embedding_query: bool = True\n\n    @property\n    @abstractmethod\n    def client(self) -> Any:\n        \"\"\"Get client.\"\"\"\n\n    def get_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        filters: Optional[MetadataFilters] = None,\n    ) -> List[BaseNode]:\n        \"\"\"Get nodes from vector store.\"\"\"\n        raise NotImplementedError(\"get_nodes not implemented\")\n\n    async def aget_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        filters: Optional[MetadataFilters] = None,\n    ) -> List[BaseNode]:\n        \"\"\"Asynchronously get nodes from vector store.\"\"\"\n        return self.get_nodes(node_ids, filters)\n\n    @abstractmethod\n    def add(\n        self,\n        nodes: Sequence[BaseNode],\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Add nodes to vector store.\"\"\"\n\n    async def async_add(\n        self,\n        nodes: Sequence[BaseNode],\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"\n        Asynchronously add nodes to vector store.\n        NOTE: this is not implemented for all vector stores. If not implemented,\n        it will just call add synchronously.\n        \"\"\"\n        return self.add(nodes, **kwargs)\n\n    @abstractmethod\n    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using with ref_doc_id.\"\"\"\n\n    async def adelete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using with ref_doc_id.\n        NOTE: this is not implemented for all vector stores. If not implemented,\n        it will just call delete synchronously.\n        \"\"\"\n        self.delete(ref_doc_id, **delete_kwargs)\n\n    def delete_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        filters: Optional[MetadataFilters] = None,\n        **delete_kwargs: Any,\n    ) -> None:\n        \"\"\"Delete nodes from vector store.\"\"\"\n        raise NotImplementedError(\"delete_nodes not implemented\")\n\n    async def adelete_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        filters: Optional[MetadataFilters] = None,\n        **delete_kwargs: Any,\n    ) -> None:\n        \"\"\"Asynchronously delete nodes from vector store.\"\"\"\n        self.delete_nodes(node_ids, filters)\n\n    def clear(self) -> None:\n        \"\"\"Clear all nodes from configured vector store.\"\"\"\n        raise NotImplementedError(\"clear not implemented\")\n\n    async def aclear(self) -> None:\n        \"\"\"Asynchronously clear all nodes from configured vector store.\"\"\"\n        self.clear()\n\n    @abstractmethod\n    def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:\n        \"\"\"Query vector store.\"\"\"\n\n    async def aquery(\n        self, query: VectorStoreQuery, **kwargs: Any\n    ) -> VectorStoreQueryResult:\n        \"\"\"\n        Asynchronously query vector store.\n        NOTE: this is not implemented for all vector stores. If not implemented,\n        it will just call query synchronously.\n        \"\"\"\n        return self.query(query, **kwargs)\n\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        return None"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/vector_stores/__init__.py",
    "filename": "__init__.py",
    "relpath": "vector_stores/__init__.py",
    "start_line": 1,
    "end_line": 27,
    "length": 27,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Vector stores.\"\"\"\n\nfrom llama_index.core.vector_stores.simple import SimpleVectorStore\nfrom llama_index.core.vector_stores.types import (\n    ExactMatchFilter,\n    FilterCondition,\n    FilterOperator,\n    MetadataFilter,\n    MetadataFilters,\n    MetadataInfo,\n    VectorStoreQuery,\n    VectorStoreQueryResult,\n    VectorStoreInfo,\n)\n\n__all__ = [\n    \"VectorStoreQuery\",\n    \"VectorStoreQueryResult\",\n    \"MetadataFilters\",\n    \"MetadataFilter\",\n    \"MetadataInfo\",\n    \"ExactMatchFilter\",\n    \"FilterCondition\",\n    \"FilterOperator\",\n    \"SimpleVectorStore\",\n    \"VectorStoreInfo\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/vector_stores/utils.py",
    "filename": "utils.py",
    "relpath": "vector_stores/utils.py",
    "start_line": 1,
    "end_line": 142,
    "length": 142,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_validate_is_flat_dict",
      "node_to_metadata_dict",
      "metadata_dict_to_node",
      "legacy_metadata_dict_to_node"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "_validate_is_flat_dict",
      "node_to_metadata_dict",
      "metadata_dict_to_node",
      "legacy_metadata_dict_to_node"
    ],
    "document_class_names": [],
    "content": "import json\nfrom typing import Any, Dict, Optional, Tuple\n\nfrom llama_index.core.schema import (\n    BaseNode,\n    ImageNode,\n    IndexNode,\n    NodeRelationship,\n    RelatedNodeInfo,\n    TextNode,\n)\n\nDEFAULT_TEXT_KEY = \"text\"\nDEFAULT_EMBEDDING_KEY = \"embedding\"\nDEFAULT_DOC_ID_KEY = \"doc_id\"\n\n\ndef _validate_is_flat_dict(metadata_dict: dict) -> None:\n    \"\"\"\n    Validate that metadata dict is flat,\n    and key is str, and value is one of (str, int, float, None).\n    \"\"\"\n    for key, val in metadata_dict.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Metadata key must be str!\")\n        if not isinstance(val, (str, int, float, type(None))):\n            raise ValueError(\n                f\"Value for metadata {key} must be one of (str, int, float, None)\"\n            )\n\n\ndef node_to_metadata_dict(\n    node: BaseNode,\n    remove_text: bool = False,\n    text_field: str = DEFAULT_TEXT_KEY,\n    flat_metadata: bool = False,\n) -> Dict[str, Any]:\n    \"\"\"Common logic for saving Node data into metadata dict.\"\"\"\n    node_dict = node.dict()\n    metadata: Dict[str, Any] = node_dict.get(\"metadata\", {})\n\n    if flat_metadata:\n        _validate_is_flat_dict(metadata)\n\n    # store entire node as json string - some minor text duplication\n    if remove_text:\n        node_dict[text_field] = \"\"\n\n    # remove embedding from node_dict\n    node_dict[\"embedding\"] = None\n\n    # dump remainder of node_dict to json string\n    metadata[\"_node_content\"] = json.dumps(node_dict)\n    metadata[\"_node_type\"] = node.class_name()\n\n    # store ref doc id at top level to allow metadata filtering\n    # kept for backwards compatibility, will consolidate in future\n    metadata[\"document_id\"] = node.ref_doc_id or \"None\"  # for Chroma\n    metadata[\"doc_id\"] = node.ref_doc_id or \"None\"  # for Pinecone, Qdrant, Redis\n    metadata[\"ref_doc_id\"] = node.ref_doc_id or \"None\"  # for Weaviate\n\n    return metadata\n\n\ndef metadata_dict_to_node(metadata: dict, text: Optional[str] = None) -> BaseNode:\n    \"\"\"Common logic for loading Node data from metadata dict.\"\"\"\n    node_json = metadata.get(\"_node_content\", None)\n    node_type = metadata.get(\"_node_type\", None)\n    if node_json is None:\n        raise ValueError(\"Node content not found in metadata dict.\")\n\n    node: BaseNode\n    if node_type == IndexNode.class_name():\n        node = IndexNode.from_json(node_json)\n    elif node_type == ImageNode.class_name():\n        node = ImageNode.from_json(node_json)\n    else:\n        node = TextNode.from_json(node_json)\n\n    if text is not None:\n        node.set_content(text)\n\n    return node\n\n\n# TODO: Deprecated conversion functions\ndef legacy_metadata_dict_to_node(\n    metadata: dict, text_key: str = DEFAULT_TEXT_KEY\n) -> Tuple[dict, dict, dict]:\n    \"\"\"Common logic for loading Node data from metadata dict.\"\"\"\n    # make a copy first\n    if metadata is None:\n        metadata = {}\n    else:\n        metadata = metadata.copy()\n\n    # load node_info from json string\n    node_info_str = metadata.pop(\"node_info\", \"\")\n    if node_info_str == \"\":\n        node_info = {}\n    else:\n        node_info = json.loads(node_info_str)\n\n    # load relationships from json string\n    relationships_str = metadata.pop(\"relationships\", \"\")\n    relationships: Dict[NodeRelationship, RelatedNodeInfo]\n    if relationships_str == \"\":\n        relationships = {}\n    else:\n        relationships = {\n            NodeRelationship(k): RelatedNodeInfo(node_id=str(v))\n            for k, v in json.loads(relationships_str).items()\n        }\n\n    # remove other known fields\n    metadata.pop(text_key, None)\n\n    id_ = metadata.pop(\"id\", None)\n    document_id = metadata.pop(\"document_id\", None)\n    doc_id = metadata.pop(\"doc_id\", None)\n    ref_doc_id = metadata.pop(\"ref_doc_id\", None)\n\n    # don't remove id's from metadata that llama-index doesn't know about\n    ref_doc_id_info = relationships.get(NodeRelationship.PARENT, None)\n    if ref_doc_id_info is not None:\n        ref_doc_id = ref_doc_id_info.node_id\n\n    if id_ is not None and id_ != ref_doc_id:\n        metadata[\"id\"] = id_\n    if document_id is not None and document_id != ref_doc_id:\n        metadata[\"document_id\"] = document_id\n    if doc_id is not None and doc_id != ref_doc_id:\n        metadata[\"doc_id\"] = doc_id\n\n    # remaining metadata is metadata or node_info\n    new_metadata = {}\n    for key, val in metadata.items():\n        # don't enforce types on metadata anymore (we did in the past)\n        # since how we store this data now has been updated\n        new_metadata[key] = val\n\n    return new_metadata, node_info, relationships"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/vector_stores/simple.py",
    "filename": "simple.py",
    "relpath": "vector_stores/simple.py",
    "start_line": 1,
    "end_line": 424,
    "length": 424,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_build_metadata_filter_fn",
      "filter_fn",
      "_process_filter_match",
      "__init__",
      "from_persist_dir",
      "from_namespaced_persist_dir",
      "class_name",
      "client",
      "_data",
      "get",
      "get_nodes",
      "add",
      "delete",
      "delete_nodes",
      "node_filter_fn",
      "node_filter_fn",
      "clear",
      "query",
      "node_filter_fn",
      "node_filter_fn",
      "persist",
      "from_persist_path",
      "from_dict",
      "to_dict"
    ],
    "chunk_class_names": [
      "class",
      "SimpleVectorStore"
    ],
    "document_function_names": [
      "_build_metadata_filter_fn",
      "filter_fn",
      "_process_filter_match",
      "__init__",
      "from_persist_dir",
      "from_namespaced_persist_dir",
      "class_name",
      "client",
      "_data",
      "get",
      "get_nodes",
      "add",
      "delete",
      "delete_nodes",
      "node_filter_fn",
      "node_filter_fn",
      "clear",
      "query",
      "node_filter_fn",
      "node_filter_fn",
      "persist",
      "from_persist_path",
      "from_dict",
      "to_dict"
    ],
    "document_class_names": [
      "class",
      "SimpleVectorStore"
    ],
    "content": "\"\"\"Simple vector store index.\"\"\"\n\nimport json\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, cast\n\nimport fsspec\nfrom dataclasses_json import DataClassJsonMixin\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.indices.query.embedding_utils import (\n    get_top_k_embeddings,\n    get_top_k_embeddings_learner,\n    get_top_k_mmr_embeddings,\n)\nfrom llama_index.core.schema import BaseNode\nfrom llama_index.core.utils import concat_dirs\nfrom llama_index.core.vector_stores.types import (\n    DEFAULT_PERSIST_DIR,\n    DEFAULT_PERSIST_FNAME,\n    BasePydanticVectorStore,\n    MetadataFilters,\n    FilterCondition,\n    FilterOperator,\n    VectorStoreQuery,\n    VectorStoreQueryMode,\n    VectorStoreQueryResult,\n)\nfrom llama_index.core.vector_stores.utils import node_to_metadata_dict\n\nlogger = logging.getLogger(__name__)\n\nLEARNER_MODES = {\n    VectorStoreQueryMode.SVM,\n    VectorStoreQueryMode.LINEAR_REGRESSION,\n    VectorStoreQueryMode.LOGISTIC_REGRESSION,\n}\n\nMMR_MODE = VectorStoreQueryMode.MMR\n\nNAMESPACE_SEP = \"__\"\nDEFAULT_VECTOR_STORE = \"default\"\n\n\ndef _build_metadata_filter_fn(\n    metadata_lookup_fn: Callable[[str], Mapping[str, Any]],\n    metadata_filters: Optional[MetadataFilters] = None,\n) -> Callable[[str], bool]:\n    \"\"\"Build metadata filter function.\"\"\"\n    filter_list = metadata_filters.filters if metadata_filters else []\n    if not filter_list or not metadata_filters:\n        return lambda _: True\n\n    filter_condition = cast(MetadataFilters, metadata_filters.condition)\n\n    def filter_fn(node_id: str) -> bool:\n        def _process_filter_match(\n            operator: FilterOperator, value: Any, metadata_value: Any\n        ) -> bool:\n            if metadata_value is None:\n                return False\n            if operator == FilterOperator.EQ:\n                return metadata_value == value\n            if operator == FilterOperator.NE:\n                return metadata_value != value\n            if operator == FilterOperator.GT:\n                return metadata_value > value\n            if operator == FilterOperator.GTE:\n                return metadata_value >= value\n            if operator == FilterOperator.LT:\n                return metadata_value < value\n            if operator == FilterOperator.LTE:\n                return metadata_value <= value\n            if operator == FilterOperator.IN:\n                return metadata_value in value\n            if operator == FilterOperator.NIN:\n                return metadata_value not in value\n            if operator == FilterOperator.CONTAINS:\n                return value in metadata_value\n            if operator == FilterOperator.TEXT_MATCH:\n                return value.lower() in metadata_value.lower()\n            if operator == FilterOperator.ALL:\n                return all(val in metadata_value for val in value)\n            if operator == FilterOperator.ANY:\n                return any(val in metadata_value for val in value)\n            raise ValueError(f\"Invalid operator: {operator}\")\n\n        metadata = metadata_lookup_fn(node_id)\n\n        filter_matches_list = []\n        for filter_ in filter_list:\n            if isinstance(filter_, MetadataFilters):\n                raise ValueError(\"Nested MetadataFilters are not supported.\")\n\n            filter_matches = True\n            metadata_value = metadata.get(filter_.key, None)\n            if filter_.operator == FilterOperator.IS_EMPTY:\n                filter_matches = (\n                    metadata_value is None\n                    or metadata_value == \"\"\n                    or metadata_value == []\n                )\n            else:\n                filter_matches = _process_filter_match(\n                    operator=filter_.operator,\n                    value=filter_.value,\n                    metadata_value=metadata_value,\n                )\n\n            filter_matches_list.append(filter_matches)\n\n        if filter_condition == FilterCondition.AND:\n            return all(filter_matches_list)\n        elif filter_condition == FilterCondition.OR:\n            return any(filter_matches_list)\n        else:\n            raise ValueError(f\"Invalid filter condition: {filter_condition}\")\n\n    return filter_fn\n\n\n@dataclass\nclass SimpleVectorStoreData(DataClassJsonMixin):\n    \"\"\"Simple Vector Store Data container.\n\n    Args:\n        embedding_dict (Optional[dict]): dict mapping node_ids to embeddings.\n        text_id_to_ref_doc_id (Optional[dict]):\n            dict mapping text_ids/node_ids to ref_doc_ids.\n\n    \"\"\"\n\n    embedding_dict: Dict[str, List[float]] = field(default_factory=dict)\n    text_id_to_ref_doc_id: Dict[str, str] = field(default_factory=dict)\n    metadata_dict: Dict[str, Any] = field(default_factory=dict)\n\n\nclass SimpleVectorStore(BasePydanticVectorStore):\n    \"\"\"Simple Vector Store.\n\n    In this vector store, embeddings are stored within a simple, in-memory dictionary.\n\n    Args:\n        simple_vector_store_data_dict (Optional[dict]): data dict\n            containing the embeddings and doc_ids. See SimpleVectorStoreData\n            for more details.\n    \"\"\"\n\n    stores_text: bool = False\n\n    data: SimpleVectorStoreData = Field(default_factory=SimpleVectorStoreData)\n    _fs: fsspec.AbstractFileSystem = PrivateAttr()\n\n    def __init__(\n        self,\n        data: Optional[SimpleVectorStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        super().__init__(data=data or SimpleVectorStoreData())  # type: ignore[call-arg]\n        self._fs = fs or fsspec.filesystem(\"file\")\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        namespace: str = DEFAULT_VECTOR_STORE,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimpleVectorStore\":\n        \"\"\"Load from persist dir.\"\"\"\n        persist_fname = f\"{namespace}{NAMESPACE_SEP}{DEFAULT_PERSIST_FNAME}\"\n\n        if fs is not None:\n            persist_path = concat_dirs(persist_dir, persist_fname)\n        else:\n            persist_path = os.path.join(persist_dir, persist_fname)\n        return cls.from_persist_path(persist_path, fs=fs)\n\n    @classmethod\n    def from_namespaced_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> Dict[str, BasePydanticVectorStore]:\n        \"\"\"Load from namespaced persist dir.\"\"\"\n        listing_fn = os.listdir if fs is None else fs.listdir\n\n        vector_stores: Dict[str, BasePydanticVectorStore] = {}\n\n        try:\n            for fname in listing_fn(persist_dir):\n                if fname.endswith(DEFAULT_PERSIST_FNAME):\n                    namespace = fname.split(NAMESPACE_SEP)[0]\n\n                    # handle backwards compatibility with stores that were persisted\n                    if namespace == DEFAULT_PERSIST_FNAME:\n                        vector_stores[DEFAULT_VECTOR_STORE] = cls.from_persist_dir(\n                            persist_dir=persist_dir, fs=fs\n                        )\n                    else:\n                        vector_stores[namespace] = cls.from_persist_dir(\n                            persist_dir=persist_dir, namespace=namespace, fs=fs\n                        )\n        except Exception:\n            # failed to listdir, so assume there is only one store\n            try:\n                vector_stores[DEFAULT_VECTOR_STORE] = cls.from_persist_dir(\n                    persist_dir=persist_dir, fs=fs, namespace=DEFAULT_VECTOR_STORE\n                )\n            except Exception:\n                # no namespace backwards compat\n                vector_stores[DEFAULT_VECTOR_STORE] = cls.from_persist_dir(\n                    persist_dir=persist_dir, fs=fs\n                )\n\n        return vector_stores\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Class name.\"\"\"\n        return \"SimpleVectorStore\"\n\n    @property\n    def client(self) -> None:\n        \"\"\"Get client.\"\"\"\n        return\n\n    @property\n    def _data(self) -> SimpleVectorStoreData:\n        \"\"\"Backwards compatibility.\"\"\"\n        return self.data\n\n    def get(self, text_id: str) -> List[float]:\n        \"\"\"Get embedding.\"\"\"\n        return self.data.embedding_dict[text_id]\n\n    def get_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        filters: Optional[MetadataFilters] = None,\n    ) -> List[BaseNode]:\n        \"\"\"Get nodes.\"\"\"\n        raise NotImplementedError(\"SimpleVectorStore does not store nodes directly.\")\n\n    def add(\n        self,\n        nodes: Sequence[BaseNode],\n        **add_kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Add nodes to index.\"\"\"\n        for node in nodes:\n            self.data.embedding_dict[node.node_id] = node.get_embedding()\n            self.data.text_id_to_ref_doc_id[node.node_id] = node.ref_doc_id or \"None\"\n\n            metadata = node_to_metadata_dict(\n                node, remove_text=True, flat_metadata=False\n            )\n            metadata.pop(\"_node_content\", None)\n            self.data.metadata_dict[node.node_id] = metadata\n        return [node.node_id for node in nodes]\n\n    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:\n        \"\"\"\n        Delete nodes using with ref_doc_id.\n\n        Args:\n            ref_doc_id (str): The doc_id of the document to delete.\n\n        \"\"\"\n        text_ids_to_delete = set()\n        for text_id, ref_doc_id_ in self.data.text_id_to_ref_doc_id.items():\n            if ref_doc_id == ref_doc_id_:\n                text_ids_to_delete.add(text_id)\n\n        for text_id in text_ids_to_delete:\n            del self.data.embedding_dict[text_id]\n            del self.data.text_id_to_ref_doc_id[text_id]\n            # Handle metadata_dict not being present in stores that were persisted\n            # without metadata, or, not being present for nodes stored\n            # prior to metadata functionality.\n            if self.data.metadata_dict is not None:\n                self.data.metadata_dict.pop(text_id, None)\n\n    def delete_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        filters: Optional[MetadataFilters] = None,\n        **delete_kwargs: Any,\n    ) -> None:\n        filter_fn = _build_metadata_filter_fn(\n            lambda node_id: self.data.metadata_dict[node_id], filters\n        )\n\n        if node_ids is not None:\n            node_id_set = set(node_ids)\n\n            def node_filter_fn(node_id: str) -> bool:\n                return node_id in node_id_set and filter_fn(node_id)\n\n        else:\n\n            def node_filter_fn(node_id: str) -> bool:\n                return filter_fn(node_id)\n\n        for node_id in list(self.data.embedding_dict.keys()):\n            if node_filter_fn(node_id):\n                del self.data.embedding_dict[node_id]\n                del self.data.text_id_to_ref_doc_id[node_id]\n                self.data.metadata_dict.pop(node_id, None)\n\n    def clear(self) -> None:\n        \"\"\"Clear the store.\"\"\"\n        self.data = SimpleVectorStoreData()\n\n    def query(\n        self,\n        query: VectorStoreQuery,\n        **kwargs: Any,\n    ) -> VectorStoreQueryResult:\n        \"\"\"Get nodes for response.\"\"\"\n        # Prevent metadata filtering on stores that were persisted without metadata.\n        if (\n            query.filters is not None\n            and self.data.embedding_dict\n            and not self.data.metadata_dict\n        ):\n            raise ValueError(\n                \"Cannot filter stores that were persisted without metadata. \"\n                \"Please rebuild the store with metadata to enable filtering.\"\n            )\n        # Prefilter nodes based on the query filter and node ID restrictions.\n        query_filter_fn = _build_metadata_filter_fn(\n            lambda node_id: self.data.metadata_dict[node_id], query.filters\n        )\n\n        if query.node_ids is not None:\n            available_ids = set(query.node_ids)\n\n            def node_filter_fn(node_id: str) -> bool:\n                return node_id in available_ids\n\n        else:\n\n            def node_filter_fn(node_id: str) -> bool:\n                return True\n\n        node_ids = []\n        embeddings = []\n        # TODO: consolidate with get_query_text_embedding_similarities\n        for node_id, embedding in self.data.embedding_dict.items():\n            if node_filter_fn(node_id) and query_filter_fn(node_id):\n                node_ids.append(node_id)\n                embeddings.append(embedding)\n\n        query_embedding = cast(List[float], query.query_embedding)\n\n        if query.mode in LEARNER_MODES:\n            top_similarities, top_ids = get_top_k_embeddings_learner(\n                query_embedding,\n                embeddings,\n                similarity_top_k=query.similarity_top_k,\n                embedding_ids=node_ids,\n            )\n        elif query.mode == MMR_MODE:\n            mmr_threshold = kwargs.get(\"mmr_threshold\", None)\n            top_similarities, top_ids = get_top_k_mmr_embeddings(\n                query_embedding,\n                embeddings,\n                similarity_top_k=query.similarity_top_k,\n                embedding_ids=node_ids,\n                mmr_threshold=mmr_threshold,\n            )\n        elif query.mode == VectorStoreQueryMode.DEFAULT:\n            top_similarities, top_ids = get_top_k_embeddings(\n                query_embedding,\n                embeddings,\n                similarity_top_k=query.similarity_top_k,\n                embedding_ids=node_ids,\n            )\n        else:\n            raise ValueError(f\"Invalid query mode: {query.mode}\")\n\n        return VectorStoreQueryResult(similarities=top_similarities, ids=top_ids)\n\n    def persist(\n        self,\n        persist_path: str = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME),\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the SimpleVectorStore to a directory.\"\"\"\n        fs = fs or self._fs\n        dirpath = os.path.dirname(persist_path)\n        if not fs.exists(dirpath):\n            fs.makedirs(dirpath)\n\n        with fs.open(persist_path, \"w\") as f:\n            json.dump(self.data.to_dict(), f)\n\n    @classmethod\n    def from_persist_path(\n        cls, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> \"SimpleVectorStore\":\n        \"\"\"Create a SimpleKVStore from a persist directory.\"\"\"\n        fs = fs or fsspec.filesystem(\"file\")\n        if not fs.exists(persist_path):\n            raise ValueError(\n                f\"No existing {__name__} found at {persist_path}, skipping load.\"\n            )\n\n        logger.debug(f\"Loading {__name__} from {persist_path}.\")\n        with fs.open(persist_path, \"rb\") as f:\n            data_dict = json.load(f)\n            data = SimpleVectorStoreData.from_dict(data_dict)\n        return cls(data)\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> \"SimpleVectorStore\":\n        save_data = SimpleVectorStoreData.from_dict(data)\n        return cls(save_data)\n\n    def to_dict(self, **kwargs: Any) -> Dict[str, Any]:\n        return self.data.to_dict()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/data_structs/struct_type.py",
    "filename": "struct_type.py",
    "relpath": "data_structs/struct_type.py",
    "start_line": 1,
    "end_line": 116,
    "length": 116,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "IndexStructType",
      "NODE"
    ],
    "document_function_names": [],
    "document_class_names": [
      "IndexStructType",
      "NODE"
    ],
    "content": "\"\"\"IndexStructType class.\"\"\"\n\nfrom enum import Enum\n\n\nclass IndexStructType(str, Enum):\n    \"\"\"Index struct type. Identifier for a \"type\" of index.\n\n    Attributes:\n        TREE (\"tree\"): Tree index. See :ref:`Ref-Indices-Tree` for tree indices.\n        LIST (\"list\"): Summary index. See :ref:`Ref-Indices-List` for summary indices.\n        KEYWORD_TABLE (\"keyword_table\"): Keyword table index. See\n            :ref:`Ref-Indices-Table`\n            for keyword table indices.\n        DICT (\"dict\"): Faiss Vector Store Index. See\n            :ref:`Ref-Indices-VectorStore`\n            for more information on the faiss vector store index.\n        SIMPLE_DICT (\"simple_dict\"): Simple Vector Store Index. See\n            :ref:`Ref-Indices-VectorStore`\n            for more information on the simple vector store index.\n        WEAVIATE (\"weaviate\"): Weaviate Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Weaviate vector store index.\n        PINECONE (\"pinecone\"): Pinecone Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Pinecone vector store index.\n        DEEPLAKE (\"deeplake\"): DeepLake Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Pinecone vector store index.\n        QDRANT (\"qdrant\"): Qdrant Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Qdrant vector store index.\n        LANCEDB (\"lancedb\"): LanceDB Vector Store Index\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the LanceDB vector store index.\n        MILVUS (\"milvus\"): Milvus Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Milvus vector store index.\n        CHROMA (\"chroma\"): Chroma Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Chroma vector store index.\n        OPENSEARCH (\"opensearch\"): Opensearch Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Opensearch vector store index.\n        MYSCALE (\"myscale\"): MyScale Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the MyScale vector store index.\n        CLICKHOUSE (\"clickhouse\"): ClickHouse Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the ClickHouse vector store index.\n        EPSILLA (\"epsilla\"): Epsilla Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Epsilla vector store index.\n        CHATGPT_RETRIEVAL_PLUGIN (\"chatgpt_retrieval_plugin\"): ChatGPT\n            retrieval plugin index.\n        SQL (\"SQL\"): SQL Structured Store Index.\n            See :ref:`Ref-Indices-StructStore`\n            for more information on the SQL vector store index.\n        DASHVECTOR (\"dashvector\"): DashVector Vector Store Index.\n            See :ref:`Ref-Indices-VectorStore`\n            for more information on the Dashvecotor vector store index.\n        KG (\"kg\"): Knowledge Graph index.\n            See :ref:`Ref-Indices-Knowledge-Graph` for KG indices.\n        DOCUMENT_SUMMARY (\"document_summary\"): Document Summary Index.\n            See :ref:`Ref-Indices-Document-Summary` for Summary Indices.\n\n    \"\"\"\n\n    # TODO: refactor so these are properties on the base class\n\n    NODE = \"node\"\n    TREE = \"tree\"\n    LIST = \"list\"\n    KEYWORD_TABLE = \"keyword_table\"\n\n    # faiss\n    DICT = \"dict\"\n    # simple\n    SIMPLE_DICT = \"simple_dict\"\n    WEAVIATE = \"weaviate\"\n    PINECONE = \"pinecone\"\n    QDRANT = \"qdrant\"\n    LANCEDB = \"lancedb\"\n    MILVUS = \"milvus\"\n    CHROMA = \"chroma\"\n    MYSCALE = \"myscale\"\n    CLICKHOUSE = \"clickhouse\"\n    VECTOR_STORE = \"vector_store\"\n    OPENSEARCH = \"opensearch\"\n    DASHVECTOR = \"dashvector\"\n    CHATGPT_RETRIEVAL_PLUGIN = \"chatgpt_retrieval_plugin\"\n    DEEPLAKE = \"deeplake\"\n    EPSILLA = \"epsilla\"\n    # multimodal\n    MULTIMODAL_VECTOR_STORE = \"multimodal\"\n    # for SQL index\n    SQL = \"sql\"\n    # for KG index\n    KG = \"kg\"\n    SIMPLE_KG = \"simple_kg\"\n    SIMPLE_LPG = \"simple_lpg\"\n    NEBULAGRAPH = \"nebulagraph\"\n    FALKORDB = \"falkordb\"\n\n    # EMPTY\n    EMPTY = \"empty\"\n    COMPOSITE = \"composite\"\n\n    PANDAS = \"pandas\"\n\n    DOCUMENT_SUMMARY = \"document_summary\"\n\n    # Managed\n    VECTARA = \"vectara\"\n    ZILLIZ_CLOUD_PIPELINE = \"zilliz_cloud_pipeline\"\n    POSTGRESML = \"postgresml\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/data_structs/data_structs.py",
    "filename": "data_structs.py",
    "relpath": "data_structs/data_structs.py",
    "start_line": 1,
    "end_line": 278,
    "length": 278,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_summary",
      "get_type",
      "node_id_to_index",
      "size",
      "get_index",
      "insert",
      "get_children",
      "insert_under_parent",
      "get_type",
      "add_node",
      "node_ids",
      "keywords",
      "size",
      "get_type",
      "add_node",
      "get_type",
      "add_node",
      "get_type",
      "add_node",
      "delete",
      "get_type",
      "get_type",
      "node_ids",
      "add_to_embedding_dict",
      "add_node",
      "search_node_by_keyword",
      "get_type",
      "get_type"
    ],
    "chunk_class_names": [
      "Node",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class"
    ],
    "document_function_names": [
      "get_summary",
      "get_type",
      "node_id_to_index",
      "size",
      "get_index",
      "insert",
      "get_children",
      "insert_under_parent",
      "get_type",
      "add_node",
      "node_ids",
      "keywords",
      "size",
      "get_type",
      "add_node",
      "get_type",
      "add_node",
      "get_type",
      "add_node",
      "delete",
      "get_type",
      "get_type",
      "node_ids",
      "add_to_embedding_dict",
      "add_node",
      "search_node_by_keyword",
      "get_type",
      "get_type"
    ],
    "document_class_names": [
      "Node",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class",
      "class"
    ],
    "content": "\"\"\"Data structures.\n\nNodes are decoupled from the indices.\n\n\"\"\"\n\nimport uuid\nfrom abc import abstractmethod\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Sequence, Set\n\nfrom dataclasses_json import DataClassJsonMixin\nfrom llama_index.core.data_structs.struct_type import IndexStructType\nfrom llama_index.core.schema import BaseNode, TextNode\n\n# TODO: legacy backport of old Node class\nNode = TextNode\n\n\n@dataclass\nclass IndexStruct(DataClassJsonMixin):\n    \"\"\"A base data struct for a LlamaIndex.\"\"\"\n\n    index_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    summary: Optional[str] = None\n\n    def get_summary(self) -> str:\n        \"\"\"Get text summary.\"\"\"\n        if self.summary is None:\n            raise ValueError(\"summary field of the index_struct not set.\")\n        return self.summary\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get index struct type.\"\"\"\n\n\n@dataclass\nclass IndexGraph(IndexStruct):\n    \"\"\"A graph representing the tree-structured index.\"\"\"\n\n    # mapping from index in tree to Node doc id.\n    all_nodes: Dict[int, str] = field(default_factory=dict)\n    root_nodes: Dict[int, str] = field(default_factory=dict)\n    node_id_to_children_ids: Dict[str, List[str]] = field(default_factory=dict)\n\n    @property\n    def node_id_to_index(self) -> Dict[str, int]:\n        \"\"\"Map from node id to index.\"\"\"\n        return {node_id: index for index, node_id in self.all_nodes.items()}\n\n    @property\n    def size(self) -> int:\n        \"\"\"Get the size of the graph.\"\"\"\n        return len(self.all_nodes)\n\n    def get_index(self, node: BaseNode) -> int:\n        \"\"\"Get index of node.\"\"\"\n        return self.node_id_to_index[node.node_id]\n\n    def insert(\n        self,\n        node: BaseNode,\n        index: Optional[int] = None,\n        children_nodes: Optional[Sequence[BaseNode]] = None,\n    ) -> None:\n        \"\"\"Insert node.\"\"\"\n        index = index or self.size\n        node_id = node.node_id\n\n        self.all_nodes[index] = node_id\n\n        if children_nodes is None:\n            children_nodes = []\n        children_ids = [n.node_id for n in children_nodes]\n        self.node_id_to_children_ids[node_id] = children_ids\n\n    def get_children(self, parent_node: Optional[BaseNode]) -> Dict[int, str]:\n        \"\"\"Get children nodes.\"\"\"\n        if parent_node is None:\n            return self.root_nodes\n        else:\n            parent_id = parent_node.node_id\n            children_ids = self.node_id_to_children_ids[parent_id]\n            return {\n                self.node_id_to_index[child_id]: child_id for child_id in children_ids\n            }\n\n    def insert_under_parent(\n        self,\n        node: BaseNode,\n        parent_node: Optional[BaseNode],\n        new_index: Optional[int] = None,\n    ) -> None:\n        \"\"\"Insert under parent node.\"\"\"\n        new_index = new_index or self.size\n        if parent_node is None:\n            self.root_nodes[new_index] = node.node_id\n            self.node_id_to_children_ids[node.node_id] = []\n        else:\n            if parent_node.node_id not in self.node_id_to_children_ids:\n                self.node_id_to_children_ids[parent_node.node_id] = []\n            self.node_id_to_children_ids[parent_node.node_id].append(node.node_id)\n\n        self.all_nodes[new_index] = node.node_id\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.TREE\n\n\n@dataclass\nclass KeywordTable(IndexStruct):\n    \"\"\"A table of keywords mapping keywords to text chunks.\"\"\"\n\n    table: Dict[str, Set[str]] = field(default_factory=dict)\n\n    def add_node(self, keywords: List[str], node: BaseNode) -> None:\n        \"\"\"Add text to table.\"\"\"\n        for keyword in keywords:\n            if keyword not in self.table:\n                self.table[keyword] = set()\n            self.table[keyword].add(node.node_id)\n\n    @property\n    def node_ids(self) -> Set[str]:\n        \"\"\"Get all node ids.\"\"\"\n        return set.union(*self.table.values())\n\n    @property\n    def keywords(self) -> Set[str]:\n        \"\"\"Get all keywords in the table.\"\"\"\n        return set(self.table.keys())\n\n    @property\n    def size(self) -> int:\n        \"\"\"Get the size of the table.\"\"\"\n        return len(self.table)\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.KEYWORD_TABLE\n\n\n@dataclass\nclass IndexList(IndexStruct):\n    \"\"\"A list of documents.\"\"\"\n\n    nodes: List[str] = field(default_factory=list)\n\n    def add_node(self, node: BaseNode) -> None:\n        \"\"\"Add text to table, return current position in list.\"\"\"\n        # don't worry about child indices for now, nodes are all in order\n        self.nodes.append(node.node_id)\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.LIST\n\n\n@dataclass\nclass IndexLPG(IndexStruct):\n    \"\"\"An index struct for LPG index (doesn't actually store anything).\"\"\"\n\n    def add_node(self, node: BaseNode) -> None:\n        pass\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        return IndexStructType.SIMPLE_LPG\n\n\n@dataclass\nclass IndexDict(IndexStruct):\n    \"\"\"A simple dictionary of documents.\"\"\"\n\n    # TODO: slightly deprecated, should likely be a list or set now\n    # mapping from vector store id to node doc_id\n    nodes_dict: Dict[str, str] = field(default_factory=dict)\n\n    # TODO: deprecated, not used\n    # mapping from node doc_id to vector store id\n    doc_id_dict: Dict[str, List[str]] = field(default_factory=dict)\n\n    # TODO: deprecated, not used\n    # this should be empty for all other indices\n    embeddings_dict: Dict[str, List[float]] = field(default_factory=dict)\n\n    def add_node(\n        self,\n        node: BaseNode,\n        text_id: Optional[str] = None,\n    ) -> str:\n        \"\"\"Add text to table, return current position in list.\"\"\"\n        # # don't worry about child indices for now, nodes are all in order\n        # self.nodes_dict[int_id] = node\n        vector_id = text_id if text_id is not None else node.node_id\n        self.nodes_dict[vector_id] = node.node_id\n\n        return vector_id\n\n    def delete(self, doc_id: str) -> None:\n        \"\"\"Delete a Node.\"\"\"\n        del self.nodes_dict[doc_id]\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.VECTOR_STORE\n\n\n@dataclass\nclass MultiModelIndexDict(IndexDict):\n    \"\"\"A simple dictionary of documents, but loads a MultiModelVectorStore.\"\"\"\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.MULTIMODAL_VECTOR_STORE\n\n\n@dataclass\nclass KG(IndexStruct):\n    \"\"\"A table of keywords mapping keywords to text chunks.\"\"\"\n\n    # Unidirectional\n\n    # table of keywords to node ids\n    table: Dict[str, Set[str]] = field(default_factory=dict)\n\n    # TODO: legacy attribute, remove in future releases\n    rel_map: Dict[str, List[List[str]]] = field(default_factory=dict)\n\n    # TBD, should support vector store, now we just persist the embedding memory\n    # maybe chainable abstractions for *_stores could be designed\n    embedding_dict: Dict[str, List[float]] = field(default_factory=dict)\n\n    @property\n    def node_ids(self) -> Set[str]:\n        \"\"\"Get all node ids.\"\"\"\n        return set.union(*self.table.values())\n\n    def add_to_embedding_dict(self, triplet_str: str, embedding: List[float]) -> None:\n        \"\"\"Add embedding to dict.\"\"\"\n        self.embedding_dict[triplet_str] = embedding\n\n    def add_node(self, keywords: List[str], node: BaseNode) -> None:\n        \"\"\"Add text to table.\"\"\"\n        node_id = node.node_id\n        for keyword in keywords:\n            if keyword not in self.table:\n                self.table[keyword] = set()\n            self.table[keyword].add(node_id)\n\n    def search_node_by_keyword(self, keyword: str) -> List[str]:\n        \"\"\"Search for nodes by keyword.\"\"\"\n        if keyword not in self.table:\n            return []\n        return list(self.table[keyword])\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.KG\n\n\n@dataclass\nclass EmptyIndexStruct(IndexStruct):\n    \"\"\"Empty index.\"\"\"\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.EMPTY"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/data_structs/__init__.py",
    "filename": "__init__.py",
    "relpath": "data_structs/__init__.py",
    "start_line": 1,
    "end_line": 21,
    "length": 21,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file.\"\"\"\n\nfrom llama_index.core.data_structs.data_structs import (\n    IndexDict,\n    IndexGraph,\n    IndexList,\n    IndexLPG,\n    KeywordTable,\n    Node,\n)\nfrom llama_index.core.data_structs.table import StructDatapoint\n\n__all__ = [\n    \"IndexGraph\",\n    \"KeywordTable\",\n    \"IndexList\",\n    \"IndexLPG\",\n    \"IndexDict\",\n    \"StructDatapoint\",\n    \"Node\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/data_structs/table.py",
    "filename": "table.py",
    "relpath": "data_structs/table.py",
    "start_line": 1,
    "end_line": 44,
    "length": 44,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_type",
      "get_type"
    ],
    "chunk_class_names": [
      "class",
      "class",
      "class",
      "class"
    ],
    "document_function_names": [
      "get_type",
      "get_type"
    ],
    "document_class_names": [
      "class",
      "class",
      "class",
      "class"
    ],
    "content": "\"\"\"Struct store schema.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict\n\nfrom dataclasses_json import DataClassJsonMixin\nfrom llama_index.core.data_structs.data_structs import IndexStruct\nfrom llama_index.core.data_structs.struct_type import IndexStructType\n\n\n@dataclass\nclass StructDatapoint(DataClassJsonMixin):\n    \"\"\"Struct outputs.\"\"\"\n\n    # map from field name to StructValue\n    fields: Dict[str, Any]\n\n\n@dataclass\nclass BaseStructTable(IndexStruct):\n    \"\"\"Struct outputs.\"\"\"\n\n\n@dataclass\nclass SQLStructTable(BaseStructTable):\n    \"\"\"SQL struct outputs.\"\"\"\n\n    context_dict: Dict[str, str] = field(default_factory=dict)\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        # TODO: consolidate with IndexStructType\n        return IndexStructType.SQL\n\n\n@dataclass\nclass PandasStructTable(BaseStructTable):\n    \"\"\"Pandas struct outputs.\"\"\"\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.PANDAS"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/data_structs/registry.py",
    "filename": "registry.py",
    "relpath": "data_structs/registry.py",
    "start_line": 1,
    "end_line": 32,
    "length": 32,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Index registry.\"\"\"\n\nfrom typing import Dict, Type\n\nfrom llama_index.core.data_structs.data_structs import (\n    KG,\n    EmptyIndexStruct,\n    IndexDict,\n    IndexGraph,\n    IndexList,\n    IndexLPG,\n    IndexStruct,\n    KeywordTable,\n    MultiModelIndexDict,\n)\nfrom llama_index.core.data_structs.document_summary import IndexDocumentSummary\nfrom llama_index.core.data_structs.struct_type import IndexStructType\nfrom llama_index.core.data_structs.table import PandasStructTable, SQLStructTable\n\nINDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS: Dict[IndexStructType, Type[IndexStruct]] = {\n    IndexStructType.TREE: IndexGraph,\n    IndexStructType.LIST: IndexList,\n    IndexStructType.KEYWORD_TABLE: KeywordTable,\n    IndexStructType.VECTOR_STORE: IndexDict,\n    IndexStructType.SQL: SQLStructTable,\n    IndexStructType.PANDAS: PandasStructTable,\n    IndexStructType.KG: KG,\n    IndexStructType.SIMPLE_LPG: IndexLPG,\n    IndexStructType.EMPTY: EmptyIndexStruct,\n    IndexStructType.DOCUMENT_SUMMARY: IndexDocumentSummary,\n    IndexStructType.MULTIMODAL_VECTOR_STORE: MultiModelIndexDict,\n}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/data_structs/document_summary.py",
    "filename": "document_summary.py",
    "relpath": "data_structs/document_summary.py",
    "start_line": 1,
    "end_line": 73,
    "length": 73,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "add_summary_and_nodes",
      "summary_ids",
      "delete",
      "delete_nodes",
      "get_type"
    ],
    "chunk_class_names": [
      "class"
    ],
    "document_function_names": [
      "add_summary_and_nodes",
      "summary_ids",
      "delete",
      "delete_nodes",
      "get_type"
    ],
    "document_class_names": [
      "class"
    ],
    "content": "\"\"\"Data struct for document summary index.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List\n\nfrom llama_index.core.data_structs.data_structs import IndexStruct\nfrom llama_index.core.data_structs.struct_type import IndexStructType\nfrom llama_index.core.schema import BaseNode\n\n\n@dataclass\nclass IndexDocumentSummary(IndexStruct):\n    \"\"\"A simple struct containing a mapping from summary node_id to doc node_ids.\n\n    Also mapping vice versa.\n\n    \"\"\"\n\n    summary_id_to_node_ids: Dict[str, List[str]] = field(default_factory=dict)\n    node_id_to_summary_id: Dict[str, str] = field(default_factory=dict)\n\n    # track mapping from doc id to node summary id\n    doc_id_to_summary_id: Dict[str, str] = field(default_factory=dict)\n\n    def add_summary_and_nodes(\n        self,\n        summary_node: BaseNode,\n        nodes: List[BaseNode],\n    ) -> str:\n        \"\"\"Add node and summary.\"\"\"\n        summary_id = summary_node.node_id\n        ref_doc_id = summary_node.ref_doc_id\n        if ref_doc_id is None:\n            raise ValueError(\n                \"ref_doc_id of node cannot be None when building a document \"\n                \"summary index\"\n            )\n        self.doc_id_to_summary_id[ref_doc_id] = summary_id\n\n        for node in nodes:\n            node_id = node.node_id\n            if summary_id not in self.summary_id_to_node_ids:\n                self.summary_id_to_node_ids[summary_id] = []\n            self.summary_id_to_node_ids[summary_id].append(node_id)\n\n            self.node_id_to_summary_id[node_id] = summary_id\n\n        return summary_id\n\n    @property\n    def summary_ids(self) -> List[str]:\n        \"\"\"Get summary ids.\"\"\"\n        return list(self.summary_id_to_node_ids.keys())\n\n    def delete(self, doc_id: str) -> None:\n        \"\"\"Delete a document and its nodes.\"\"\"\n        summary_id = self.doc_id_to_summary_id[doc_id]\n        del self.doc_id_to_summary_id[doc_id]\n        node_ids = self.summary_id_to_node_ids[summary_id]\n        for node_id in node_ids:\n            del self.node_id_to_summary_id[node_id]\n        del self.summary_id_to_node_ids[summary_id]\n\n    def delete_nodes(self, node_ids: List[str]) -> None:\n        for node_id in node_ids:\n            summary_id = self.node_id_to_summary_id[node_id]\n            self.summary_id_to_node_ids[summary_id].remove(node_id)\n            del self.node_id_to_summary_id[node_id]\n\n    @classmethod\n    def get_type(cls) -> IndexStructType:\n        \"\"\"Get type.\"\"\"\n        return IndexStructType.DOCUMENT_SUMMARY"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/retrievers/transform_retriever.py",
    "filename": "transform_retriever.py",
    "relpath": "retrievers/transform_retriever.py",
    "start_line": 1,
    "end_line": 43,
    "length": 43,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_retrieve"
    ],
    "chunk_class_names": [
      "TransformRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "_retrieve"
    ],
    "document_class_names": [
      "TransformRetriever"
    ],
    "content": "from typing import List, Optional\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.query.query_transform.base import BaseQueryTransform\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\n\n\nclass TransformRetriever(BaseRetriever):\n    \"\"\"Transform Retriever.\n\n    Takes in an existing retriever and a query transform and runs the query transform\n    before running the retriever.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        retriever: BaseRetriever,\n        query_transform: BaseQueryTransform,\n        transform_metadata: Optional[dict] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n    ) -> None:\n        self._retriever = retriever\n        self._query_transform = query_transform\n        self._transform_metadata = transform_metadata\n        super().__init__(\n            callback_manager=callback_manager, object_map=object_map, verbose=verbose\n        )\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        # NOTE: don't include tools for now\n        return {\"query_transform\": self._query_transform}\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        query_bundle = self._query_transform.run(\n            query_bundle, metadata=self._transform_metadata\n        )\n        return self._retriever.retrieve(query_bundle)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/retrievers/recursive_retriever.py",
    "filename": "recursive_retriever.py",
    "relpath": "retrievers/recursive_retriever.py",
    "start_line": 1,
    "end_line": 217,
    "length": 217,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_deduplicate_nodes",
      "_query_retrieved_nodes",
      "_get_object",
      "_retrieve_rec",
      "_retrieve",
      "retrieve_all"
    ],
    "chunk_class_names": [
      "RecursiveRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_deduplicate_nodes",
      "_query_retrieved_nodes",
      "_get_object",
      "_retrieve_rec",
      "_retrieve",
      "retrieve_all"
    ],
    "document_class_names": [
      "RecursiveRetriever"
    ],
    "content": "from typing import Dict, List, Optional, Tuple, Union\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.schema import (\n    BaseNode,\n    IndexNode,\n    NodeWithScore,\n    QueryBundle,\n    TextNode,\n)\nfrom llama_index.core.utils import print_text\n\nDEFAULT_QUERY_RESPONSE_TMPL = \"Query: {query_str}\\nResponse: {response}\"\n\n\nRQN_TYPE = Union[BaseRetriever, BaseQueryEngine, BaseNode]\n\n\nclass RecursiveRetriever(BaseRetriever):\n    \"\"\"Recursive retriever.\n\n    This retriever will recursively explore links from nodes to other\n    retrievers/query engines.\n\n    For any retrieved nodes, if any of the nodes are IndexNodes,\n    then it will explore the linked retriever/query engine, and query that.\n\n    Args:\n        root_id (str): The root id of the query graph.\n        retriever_dict (Optional[Dict[str, BaseRetriever]]): A dictionary\n            of id to retrievers.\n        query_engine_dict (Optional[Dict[str, BaseQueryEngine]]): A dictionary of\n            id to query engines.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        root_id: str,\n        retriever_dict: Dict[str, BaseRetriever],\n        query_engine_dict: Optional[Dict[str, BaseQueryEngine]] = None,\n        node_dict: Optional[Dict[str, BaseNode]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        query_response_tmpl: Optional[str] = None,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._root_id = root_id\n        if root_id not in retriever_dict:\n            raise ValueError(\n                f\"Root id {root_id} not in retriever_dict, it must be a retriever.\"\n            )\n        self._retriever_dict = retriever_dict\n        self._query_engine_dict = query_engine_dict or {}\n        self._node_dict = node_dict or {}\n\n        # make sure keys don't overlap\n        if set(self._retriever_dict.keys()) & set(self._query_engine_dict.keys()):\n            raise ValueError(\"Retriever and query engine ids must not overlap.\")\n\n        self._query_response_tmpl = query_response_tmpl or DEFAULT_QUERY_RESPONSE_TMPL\n        super().__init__(callback_manager, verbose=verbose)\n\n    def _deduplicate_nodes(\n        self, nodes_with_score: List[NodeWithScore]\n    ) -> List[NodeWithScore]:\n        \"\"\"Deduplicate nodes according to node id.\n        Keep the node with the highest score/first returned.\n        \"\"\"\n        node_ids = set()\n        deduplicate_nodes = []\n        for node_with_score in nodes_with_score:\n            node = node_with_score.node\n            if node.id_ not in node_ids:\n                node_ids.add(node.id_)\n                deduplicate_nodes.append(node_with_score)\n        return deduplicate_nodes\n\n    def _query_retrieved_nodes(\n        self, query_bundle: QueryBundle, nodes_with_score: List[NodeWithScore]\n    ) -> Tuple[List[NodeWithScore], List[NodeWithScore]]:\n        \"\"\"Query for retrieved nodes.\n\n        If node is an IndexNode, then recursively query the retriever/query engine.\n        If node is a TextNode, then simply return the node.\n\n        \"\"\"\n        nodes_to_add = []\n        additional_nodes = []\n        visited_ids = set()\n\n        # dedup index nodes that reference same index id\n        new_nodes_with_score = []\n        for node_with_score in nodes_with_score:\n            node = node_with_score.node\n            if isinstance(node, IndexNode):\n                if node.index_id not in visited_ids:\n                    visited_ids.add(node.index_id)\n                    new_nodes_with_score.append(node_with_score)\n            else:\n                new_nodes_with_score.append(node_with_score)\n\n        nodes_with_score = new_nodes_with_score\n\n        # recursively retrieve\n        for node_with_score in nodes_with_score:\n            node = node_with_score.node\n            if isinstance(node, IndexNode):\n                if self._verbose:\n                    print_text(\n                        \"Retrieved node with id, entering: \" f\"{node.index_id}\\n\",\n                        color=\"pink\",\n                    )\n                cur_retrieved_nodes, cur_additional_nodes = self._retrieve_rec(\n                    query_bundle,\n                    query_id=node.index_id,\n                    cur_similarity=node_with_score.score,\n                )\n            else:\n                assert isinstance(node, TextNode)\n                if self._verbose:\n                    print_text(\n                        \"Retrieving text node: \" f\"{node.get_content()}\\n\",\n                        color=\"pink\",\n                    )\n                cur_retrieved_nodes = [node_with_score]\n                cur_additional_nodes = []\n            nodes_to_add.extend(cur_retrieved_nodes)\n            additional_nodes.extend(cur_additional_nodes)\n\n        # dedup nodes in case some nodes could be retrieved from multiple sources\n        nodes_to_add = self._deduplicate_nodes(nodes_to_add)\n        additional_nodes = self._deduplicate_nodes(additional_nodes)\n        return nodes_to_add, additional_nodes\n\n    def _get_object(self, query_id: str) -> RQN_TYPE:\n        \"\"\"Fetch retriever or query engine.\"\"\"\n        node = self._node_dict.get(query_id, None)\n        if node is not None:\n            return node\n        retriever = self._retriever_dict.get(query_id, None)\n        if retriever is not None:\n            return retriever\n        query_engine = self._query_engine_dict.get(query_id, None)\n        if query_engine is not None:\n            return query_engine\n        raise ValueError(\n            f\"Query id {query_id} not found in either `retriever_dict` \"\n            \"or `query_engine_dict`.\"\n        )\n\n    def _retrieve_rec(\n        self,\n        query_bundle: QueryBundle,\n        query_id: Optional[str] = None,\n        cur_similarity: Optional[float] = None,\n    ) -> Tuple[List[NodeWithScore], List[NodeWithScore]]:\n        \"\"\"Query recursively.\"\"\"\n        if self._verbose:\n            print_text(\n                f\"Retrieving with query id {query_id}: {query_bundle.query_str}\\n\",\n                color=\"blue\",\n            )\n        query_id = query_id or self._root_id\n        cur_similarity = cur_similarity or 1.0\n\n        obj = self._get_object(query_id)\n        if isinstance(obj, BaseNode):\n            nodes_to_add = [NodeWithScore(node=obj, score=cur_similarity)]\n            additional_nodes: List[NodeWithScore] = []\n        elif isinstance(obj, BaseRetriever):\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as event:\n                nodes = obj.retrieve(query_bundle)\n                event.on_end(payload={EventPayload.NODES: nodes})\n\n            nodes_to_add, additional_nodes = self._query_retrieved_nodes(\n                query_bundle, nodes\n            )\n\n        elif isinstance(obj, BaseQueryEngine):\n            sub_resp = obj.query(query_bundle)\n            if self._verbose:\n                print_text(\n                    f\"Got response: {sub_resp!s}\\n\",\n                    color=\"green\",\n                )\n            # format with both the query and the response\n            node_text = self._query_response_tmpl.format(\n                query_str=query_bundle.query_str, response=str(sub_resp)\n            )\n            node = TextNode(text=node_text)\n            nodes_to_add = [NodeWithScore(node=node, score=cur_similarity)]\n            additional_nodes = sub_resp.source_nodes\n        else:\n            raise ValueError(\"Must be a retriever or query engine.\")\n\n        return nodes_to_add, additional_nodes\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        retrieved_nodes, _ = self._retrieve_rec(query_bundle, query_id=None)\n        return retrieved_nodes\n\n    def retrieve_all(\n        self, query_bundle: QueryBundle\n    ) -> Tuple[List[NodeWithScore], List[NodeWithScore]]:\n        \"\"\"Retrieve all nodes.\n\n        Unlike default `retrieve` method, this also fetches additional sources.\n\n        \"\"\"\n        return self._retrieve_rec(query_bundle, query_id=None)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/retrievers/__init__.py",
    "filename": "__init__.py",
    "relpath": "retrievers/__init__.py",
    "start_line": 1,
    "end_line": 88,
    "length": 88,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.image_retriever import BaseImageRetriever\nfrom llama_index.core.indices.empty.retrievers import EmptyIndexRetriever\nfrom llama_index.core.indices.keyword_table.retrievers import (\n    KeywordTableSimpleRetriever,\n)\nfrom llama_index.core.indices.knowledge_graph.retrievers import (\n    KGTableRetriever,\n    KnowledgeGraphRAGRetriever,\n)\nfrom llama_index.core.indices.list.retrievers import (\n    ListIndexEmbeddingRetriever,\n    ListIndexRetriever,\n    SummaryIndexEmbeddingRetriever,\n    SummaryIndexLLMRetriever,\n    SummaryIndexRetriever,\n)\nfrom llama_index.core.indices.property_graph import (\n    BasePGRetriever,\n    CustomPGRetriever,\n    CypherTemplateRetriever,\n    LLMSynonymRetriever,\n    PGRetriever,\n    TextToCypherRetriever,\n    VectorContextRetriever,\n)\nfrom llama_index.core.indices.struct_store.sql_retriever import (\n    NLSQLRetriever,\n    SQLParserMode,\n    SQLRetriever,\n)\nfrom llama_index.core.indices.tree.all_leaf_retriever import TreeAllLeafRetriever\nfrom llama_index.core.indices.tree.select_leaf_embedding_retriever import (\n    TreeSelectLeafEmbeddingRetriever,\n)\nfrom llama_index.core.indices.tree.select_leaf_retriever import (\n    TreeSelectLeafRetriever,\n)\nfrom llama_index.core.indices.tree.tree_root_retriever import TreeRootRetriever\nfrom llama_index.core.indices.vector_store.retrievers import (\n    VectorIndexAutoRetriever,\n    VectorIndexRetriever,\n)\nfrom llama_index.core.retrievers.auto_merging_retriever import AutoMergingRetriever\nfrom llama_index.core.retrievers.fusion_retriever import QueryFusionRetriever\nfrom llama_index.core.retrievers.recursive_retriever import RecursiveRetriever\nfrom llama_index.core.retrievers.router_retriever import RouterRetriever\nfrom llama_index.core.retrievers.transform_retriever import TransformRetriever\n\n__all__ = [\n    \"VectorIndexRetriever\",\n    \"VectorIndexAutoRetriever\",\n    \"SummaryIndexRetriever\",\n    \"SummaryIndexEmbeddingRetriever\",\n    \"SummaryIndexLLMRetriever\",\n    \"KGTableRetriever\",\n    \"KnowledgeGraphRAGRetriever\",\n    \"EmptyIndexRetriever\",\n    \"TreeAllLeafRetriever\",\n    \"TreeSelectLeafEmbeddingRetriever\",\n    \"TreeSelectLeafRetriever\",\n    \"TreeRootRetriever\",\n    \"TransformRetriever\",\n    \"KeywordTableSimpleRetriever\",\n    \"BaseRetriever\",\n    \"RecursiveRetriever\",\n    \"AutoMergingRetriever\",\n    \"RouterRetriever\",\n    \"BM25Retriever\",\n    \"QueryFusionRetriever\",\n    # property graph\n    \"BasePGRetriever\",\n    \"PGRetriever\",\n    \"CustomPGRetriever\",\n    \"LLMSynonymRetriever\",\n    \"CypherTemplateRetriever\",\n    \"TextToCypherRetriever\",\n    \"VectorContextRetriever\",\n    # SQL\n    \"SQLRetriever\",\n    \"NLSQLRetriever\",\n    \"SQLParserMode\",\n    # legacy\n    \"ListIndexEmbeddingRetriever\",\n    \"ListIndexRetriever\",\n    # image\n    \"BaseImageRetriever\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/retrievers/router_retriever.py",
    "filename": "router_retriever.py",
    "relpath": "retrievers/router_retriever.py",
    "start_line": 1,
    "end_line": 141,
    "length": 141,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompt_modules",
      "from_defaults",
      "_retrieve",
      "_aretrieve"
    ],
    "chunk_class_names": [
      "RouterRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompt_modules",
      "from_defaults",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "RouterRetriever"
    ],
    "content": "\"\"\"Router retriever.\"\"\"\n\nimport asyncio\nimport logging\nfrom typing import List, Optional, Sequence\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.base.base_selector import BaseSelector\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.mixin import PromptMixinType\nfrom llama_index.core.schema import IndexNode, NodeWithScore, QueryBundle\nfrom llama_index.core.selectors.utils import get_selector_from_llm\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.retriever_tool import RetrieverTool\n\nlogger = logging.getLogger(__name__)\n\n\nclass RouterRetriever(BaseRetriever):\n    \"\"\"Router retriever.\n\n    Selects one (or multiple) out of several candidate retrievers to execute a query.\n\n    Args:\n        selector (BaseSelector): A selector that chooses one out of many options based\n            on each candidate's metadata and query.\n        retriever_tools (Sequence[RetrieverTool]): A sequence of candidate\n            retrievers. They must be wrapped as tools to expose metadata to\n            the selector.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        selector: BaseSelector,\n        retriever_tools: Sequence[RetrieverTool],\n        llm: Optional[LLM] = None,\n        objects: Optional[List[IndexNode]] = None,\n        object_map: Optional[dict] = None,\n        verbose: bool = False,\n    ) -> None:\n        self._llm = llm or Settings.llm\n        self._selector = selector\n        self._retrievers: List[BaseRetriever] = [x.retriever for x in retriever_tools]\n        self._metadatas = [x.metadata for x in retriever_tools]\n\n        super().__init__(\n            callback_manager=Settings.callback_manager,\n            object_map=object_map,\n            objects=objects,\n            verbose=verbose,\n        )\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        # NOTE: don't include tools for now\n        return {\"selector\": self._selector}\n\n    @classmethod\n    def from_defaults(\n        cls,\n        retriever_tools: Sequence[RetrieverTool],\n        llm: Optional[LLM] = None,\n        selector: Optional[BaseSelector] = None,\n        select_multi: bool = False,\n    ) -> \"RouterRetriever\":\n        llm = llm or Settings.llm\n        selector = selector or get_selector_from_llm(llm, is_multi=select_multi)\n\n        return cls(\n            selector,\n            retriever_tools,\n            llm=llm,\n        )\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        with self.callback_manager.event(\n            CBEventType.RETRIEVE,\n            payload={EventPayload.QUERY_STR: query_bundle.query_str},\n        ) as query_event:\n            result = self._selector.select(self._metadatas, query_bundle)\n\n            if len(result.inds) > 1:\n                retrieved_results = {}\n                for i, engine_ind in enumerate(result.inds):\n                    logger.info(\n                        f\"Selecting retriever {engine_ind}: \" f\"{result.reasons[i]}.\"\n                    )\n                    selected_retriever = self._retrievers[engine_ind]\n                    cur_results = selected_retriever.retrieve(query_bundle)\n                    retrieved_results.update({n.node.node_id: n for n in cur_results})\n            else:\n                try:\n                    selected_retriever = self._retrievers[result.ind]\n                    logger.info(f\"Selecting retriever {result.ind}: {result.reason}.\")\n                except ValueError as e:\n                    raise ValueError(\"Failed to select retriever\") from e\n\n                cur_results = selected_retriever.retrieve(query_bundle)\n                retrieved_results = {n.node.node_id: n for n in cur_results}\n\n            query_event.on_end(payload={EventPayload.NODES: retrieved_results.values()})\n\n        return list(retrieved_results.values())\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        with self.callback_manager.event(\n            CBEventType.RETRIEVE,\n            payload={EventPayload.QUERY_STR: query_bundle.query_str},\n        ) as query_event:\n            result = await self._selector.aselect(self._metadatas, query_bundle)\n\n            if len(result.inds) > 1:\n                retrieved_results = {}\n                tasks = []\n                for i, engine_ind in enumerate(result.inds):\n                    logger.info(\n                        f\"Selecting retriever {engine_ind}: \" f\"{result.reasons[i]}.\"\n                    )\n                    selected_retriever = self._retrievers[engine_ind]\n                    tasks.append(selected_retriever.aretrieve(query_bundle))\n\n                results_of_results = await asyncio.gather(*tasks)\n                cur_results = [\n                    item for sublist in results_of_results for item in sublist\n                ]\n                retrieved_results.update({n.node.node_id: n for n in cur_results})\n            else:\n                try:\n                    selected_retriever = self._retrievers[result.ind]\n                    logger.info(f\"Selecting retriever {result.ind}: {result.reason}.\")\n                except ValueError as e:\n                    raise ValueError(\"Failed to select retriever\") from e\n\n                cur_results = await selected_retriever.aretrieve(query_bundle)\n                retrieved_results = {n.node.node_id: n for n in cur_results}\n\n            query_event.on_end(payload={EventPayload.NODES: retrieved_results.values()})\n\n        return list(retrieved_results.values())"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/retrievers/auto_merging_retriever.py",
    "filename": "auto_merging_retriever.py",
    "relpath": "retrievers/auto_merging_retriever.py",
    "start_line": 1,
    "end_line": 192,
    "length": 192,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_parents_and_merge",
      "_fill_in_nodes",
      "_try_merging",
      "_retrieve"
    ],
    "chunk_class_names": [
      "AutoMergingRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_parents_and_merge",
      "_fill_in_nodes",
      "_try_merging",
      "_retrieve"
    ],
    "document_class_names": [
      "AutoMergingRetriever"
    ],
    "content": "# Auto Merging Retriever\n\nimport logging\nfrom collections import defaultdict\nfrom typing import Dict, List, Optional, Tuple, cast\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.query.schema import QueryBundle\nfrom llama_index.core.indices.utils import truncate_text\nfrom llama_index.core.indices.vector_store.retrievers.retriever import (\n    VectorIndexRetriever,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    IndexNode,\n    NodeWithScore,\n    MetadataMode,\n    QueryBundle,\n)\nfrom llama_index.core.storage.storage_context import StorageContext\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoMergingRetriever(BaseRetriever):\n    \"\"\"This retriever will try to merge context into parent context.\n\n    The retriever first retrieves chunks from a vector store.\n    Then, it will try to merge the chunks into a single context.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_retriever: VectorIndexRetriever,\n        storage_context: StorageContext,\n        simple_ratio_thresh: float = 0.5,\n        verbose: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[dict] = None,\n        objects: Optional[List[IndexNode]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._vector_retriever = vector_retriever\n        self._storage_context = storage_context\n        self._simple_ratio_thresh = simple_ratio_thresh\n        super().__init__(\n            callback_manager=callback_manager,\n            object_map=object_map,\n            objects=objects,\n            verbose=verbose,\n        )\n\n    def _get_parents_and_merge(\n        self, nodes: List[NodeWithScore]\n    ) -> Tuple[List[NodeWithScore], bool]:\n        \"\"\"Get parents and merge nodes.\"\"\"\n        # retrieve all parent nodes\n        parent_nodes: Dict[str, BaseNode] = {}\n        parent_cur_children_dict: Dict[str, List[NodeWithScore]] = defaultdict(list)\n        for node in nodes:\n            if node.node.parent_node is None:\n                continue\n            parent_node_info = node.node.parent_node\n\n            # Fetch actual parent node if doesn't exist in `parent_nodes` cache yet\n            parent_node_id = parent_node_info.node_id\n            if parent_node_id not in parent_nodes:\n                parent_node = self._storage_context.docstore.get_document(\n                    parent_node_id\n                )\n                parent_nodes[parent_node_id] = cast(BaseNode, parent_node)\n\n            # add reference to child from parent\n            parent_cur_children_dict[parent_node_id].append(node)\n\n        # compute ratios and \"merge\" nodes\n        # merging: delete some children nodes, add some parent nodes\n        node_ids_to_delete = set()\n        nodes_to_add: Dict[str, NodeWithScore] = {}\n        for parent_node_id, parent_node in parent_nodes.items():\n            parent_child_nodes = parent_node.child_nodes\n            parent_num_children = len(parent_child_nodes) if parent_child_nodes else 1\n            parent_cur_children = parent_cur_children_dict[parent_node_id]\n            ratio = len(parent_cur_children) / parent_num_children\n\n            # if ratio is high enough, merge\n            if ratio > self._simple_ratio_thresh:\n                node_ids_to_delete.update(\n                    set({n.node.node_id for n in parent_cur_children})\n                )\n\n                parent_node_text = truncate_text(\n                    parent_node.get_content(metadata_mode=MetadataMode.NONE), 100\n                )\n                info_str = (\n                    f\"> Merging {len(parent_cur_children)} nodes into parent node.\\n\"\n                    f\"> Parent node id: {parent_node_id}.\\n\"\n                    f\"> Parent node text: {parent_node_text}\\n\"\n                )\n                logger.info(info_str)\n                if self._verbose:\n                    print(info_str)\n\n                # add parent node\n                # can try averaging score across embeddings for now\n\n                avg_score = sum(\n                    [n.get_score() or 0.0 for n in parent_cur_children]\n                ) / len(parent_cur_children)\n                parent_node_with_score = NodeWithScore(\n                    node=parent_node, score=avg_score\n                )\n                nodes_to_add[parent_node_id] = parent_node_with_score\n\n        # delete old child nodes, add new parent nodes\n        new_nodes = [n for n in nodes if n.node.node_id not in node_ids_to_delete]\n        # add parent nodes\n        new_nodes.extend(list(nodes_to_add.values()))\n\n        is_changed = len(node_ids_to_delete) > 0\n\n        return new_nodes, is_changed\n\n    def _fill_in_nodes(\n        self, nodes: List[NodeWithScore]\n    ) -> Tuple[List[NodeWithScore], bool]:\n        \"\"\"Fill in nodes.\"\"\"\n        new_nodes = []\n        is_changed = False\n        for idx, node in enumerate(nodes):\n            new_nodes.append(node)\n            if idx >= len(nodes) - 1:\n                continue\n\n            cur_node = cast(BaseNode, node.node)\n            # if there's a node in the middle, add that to the queue\n            if (\n                cur_node.next_node is not None\n                and cur_node.next_node == nodes[idx + 1].node.prev_node\n            ):\n                is_changed = True\n                next_node = self._storage_context.docstore.get_document(\n                    cur_node.next_node.node_id\n                )\n                next_node = cast(BaseNode, next_node)\n\n                next_node_text = truncate_text(\n                    next_node.get_content(metadata_mode=MetadataMode.NONE), 100\n                )\n                info_str = (\n                    f\"> Filling in node. Node id: {cur_node.next_node.node_id}\"\n                    f\"> Node text: {next_node_text}\\n\"\n                )\n                logger.info(info_str)\n                if self._verbose:\n                    print(info_str)\n\n                # set score to be average of current node and next node\n                avg_score = (node.get_score() + nodes[idx + 1].get_score()) / 2\n                new_nodes.append(NodeWithScore(node=next_node, score=avg_score))\n        return new_nodes, is_changed\n\n    def _try_merging(\n        self, nodes: List[NodeWithScore]\n    ) -> Tuple[List[NodeWithScore], bool]:\n        \"\"\"Try different ways to merge nodes.\"\"\"\n        # first try filling in nodes\n        nodes, is_changed_0 = self._fill_in_nodes(nodes)\n        # then try merging nodes\n        nodes, is_changed_1 = self._get_parents_and_merge(nodes)\n        return nodes, is_changed_0 or is_changed_1\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"\n        initial_nodes = self._vector_retriever.retrieve(query_bundle)\n\n        cur_nodes, is_changed = self._try_merging(initial_nodes)\n        # cur_nodes, is_changed = self._get_parents_and_merge(initial_nodes)\n        while is_changed:\n            cur_nodes, is_changed = self._try_merging(cur_nodes)\n            # cur_nodes, is_changed = self._get_parents_and_merge(cur_nodes)\n\n        # sort by similarity\n        cur_nodes.sort(key=lambda x: x.get_score(), reverse=True)\n\n        return cur_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/retrievers/fusion_retriever.py",
    "filename": "fusion_retriever.py",
    "relpath": "retrievers/fusion_retriever.py",
    "start_line": 1,
    "end_line": 304,
    "length": 304,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_queries",
      "_reciprocal_rerank_fusion",
      "_relative_score_fusion",
      "_simple_fusion",
      "_run_nested_async_queries",
      "_run_async_queries",
      "_run_sync_queries",
      "_retrieve",
      "_aretrieve"
    ],
    "chunk_class_names": [
      "FUSION_MODES",
      "QueryFusionRetriever"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_queries",
      "_reciprocal_rerank_fusion",
      "_relative_score_fusion",
      "_simple_fusion",
      "_run_nested_async_queries",
      "_run_async_queries",
      "_run_sync_queries",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "FUSION_MODES",
      "QueryFusionRetriever"
    ],
    "content": "import asyncio\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Tuple, cast\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.constants import DEFAULT_SIMILARITY_TOP_K\nfrom llama_index.core.llms.utils import LLMType, resolve_llm\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.retrievers import BaseRetriever\nfrom llama_index.core.schema import IndexNode, NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\n\nQUERY_GEN_PROMPT = (\n    \"You are a helpful assistant that generates multiple search queries based on a \"\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\n    \"related to the following input query:\\n\"\n    \"Query: {query}\\n\"\n    \"Queries:\\n\"\n)\n\n\nclass FUSION_MODES(str, Enum):\n    \"\"\"Enum for different fusion modes.\"\"\"\n\n    RECIPROCAL_RANK = \"reciprocal_rerank\"  # apply reciprocal rank fusion\n    RELATIVE_SCORE = \"relative_score\"  # apply relative score fusion\n    DIST_BASED_SCORE = \"dist_based_score\"  # apply distance-based score fusion\n    SIMPLE = \"simple\"  # simple re-ordering of results based on original scores\n\n\nclass QueryFusionRetriever(BaseRetriever):\n    def __init__(\n        self,\n        retrievers: List[BaseRetriever],\n        llm: Optional[LLMType] = None,\n        query_gen_prompt: Optional[str] = None,\n        mode: FUSION_MODES = FUSION_MODES.SIMPLE,\n        similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,\n        num_queries: int = 4,\n        use_async: bool = True,\n        verbose: bool = False,\n        callback_manager: Optional[CallbackManager] = None,\n        objects: Optional[List[IndexNode]] = None,\n        object_map: Optional[dict] = None,\n        retriever_weights: Optional[List[float]] = None,\n    ) -> None:\n        self.num_queries = num_queries\n        self.query_gen_prompt = query_gen_prompt or QUERY_GEN_PROMPT\n        self.similarity_top_k = similarity_top_k\n        self.mode = mode\n        self.use_async = use_async\n\n        self._retrievers = retrievers\n        if retriever_weights is None:\n            self._retriever_weights = [1.0 / len(retrievers)] * len(retrievers)\n        else:\n            # Sum of retriever_weights must be 1\n            total_weight = sum(retriever_weights)\n            self._retriever_weights = [w / total_weight for w in retriever_weights]\n        self._llm = (\n            resolve_llm(llm, callback_manager=callback_manager) if llm else Settings.llm\n        )\n        super().__init__(\n            callback_manager=callback_manager,\n            object_map=object_map,\n            objects=objects,\n            verbose=verbose,\n        )\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"query_gen_prompt\": PromptTemplate(self.query_gen_prompt)}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"query_gen_prompt\" in prompts:\n            self.query_gen_prompt = cast(\n                PromptTemplate, prompts[\"query_gen_prompt\"]\n            ).template\n\n    def _get_queries(self, original_query: str) -> List[QueryBundle]:\n        prompt_str = self.query_gen_prompt.format(\n            num_queries=self.num_queries - 1,\n            query=original_query,\n        )\n        response = self._llm.complete(prompt_str)\n\n        # assume LLM proper put each query on a newline\n        queries = response.text.split(\"\\n\")\n        queries = [q.strip() for q in queries if q.strip()]\n        if self._verbose:\n            queries_str = \"\\n\".join(queries)\n            print(f\"Generated queries:\\n{queries_str}\")\n\n        # The LLM often returns more queries than we asked for, so trim the list.\n        return [QueryBundle(q) for q in queries[: self.num_queries - 1]]\n\n    def _reciprocal_rerank_fusion(\n        self, results: Dict[Tuple[str, int], List[NodeWithScore]]\n    ) -> List[NodeWithScore]:\n        \"\"\"\n        Apply reciprocal rank fusion.\n\n        The original paper uses k=60 for best results:\n        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n        \"\"\"\n        k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n        fused_scores = {}\n        hash_to_node = {}\n\n        # compute reciprocal rank scores\n        for nodes_with_scores in results.values():\n            for rank, node_with_score in enumerate(\n                sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)\n            ):\n                hash = node_with_score.node.hash\n                hash_to_node[hash] = node_with_score\n                if hash not in fused_scores:\n                    fused_scores[hash] = 0.0\n                fused_scores[hash] += 1.0 / (rank + k)\n\n        # sort results\n        reranked_results = dict(\n            sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n        )\n\n        # adjust node scores\n        reranked_nodes: List[NodeWithScore] = []\n        for hash, score in reranked_results.items():\n            reranked_nodes.append(hash_to_node[hash])\n            reranked_nodes[-1].score = score\n\n        return reranked_nodes\n\n    def _relative_score_fusion(\n        self,\n        results: Dict[Tuple[str, int], List[NodeWithScore]],\n        dist_based: Optional[bool] = False,\n    ) -> List[NodeWithScore]:\n        \"\"\"Apply relative score fusion.\"\"\"\n        # MinMax scale scores of each result set (highest value becomes 1, lowest becomes 0)\n        # then scale by the weight of the retriever\n        min_max_scores = {}\n        for query_tuple, nodes_with_scores in results.items():\n            if not nodes_with_scores:\n                min_max_scores[query_tuple] = (0.0, 0.0)\n                continue\n            scores = [\n                node_with_score.score or 0.0 for node_with_score in nodes_with_scores\n            ]\n            if dist_based:\n                # Set min and max based on mean and std dev\n                mean_score = sum(scores) / len(scores)\n                std_dev = (\n                    sum((x - mean_score) ** 2 for x in scores) / len(scores)\n                ) ** 0.5\n                min_score = mean_score - 3 * std_dev\n                max_score = mean_score + 3 * std_dev\n            else:\n                min_score = min(scores)\n                max_score = max(scores)\n            min_max_scores[query_tuple] = (min_score, max_score)\n\n        for query_tuple, nodes_with_scores in results.items():\n            for node_with_score in nodes_with_scores:\n                min_score, max_score = min_max_scores[query_tuple]\n                # Scale the score to be between 0 and 1\n                if max_score == min_score:\n                    node_with_score.score = 1.0 if max_score > 0 else 0.0\n                else:\n                    node_with_score.score = (node_with_score.score - min_score) / (\n                        max_score - min_score\n                    )\n                # Scale by the weight of the retriever\n                retriever_idx = query_tuple[1]\n                existing_score = node_with_score.score or 0.0\n                node_with_score.score = (\n                    existing_score * self._retriever_weights[retriever_idx]\n                )\n                # Divide by the number of queries\n                node_with_score.score /= self.num_queries\n\n        # Use a dict to de-duplicate nodes\n        all_nodes: Dict[str, NodeWithScore] = {}\n\n        # Sum scores for each node\n        for nodes_with_scores in results.values():\n            for node_with_score in nodes_with_scores:\n                hash = node_with_score.node.hash\n                if hash in all_nodes:\n                    cur_score = all_nodes[hash].score or 0.0\n                    all_nodes[hash].score = cur_score + (node_with_score.score or 0.0)\n                else:\n                    all_nodes[hash] = node_with_score\n\n        return sorted(all_nodes.values(), key=lambda x: x.score or 0.0, reverse=True)\n\n    def _simple_fusion(\n        self, results: Dict[Tuple[str, int], List[NodeWithScore]]\n    ) -> List[NodeWithScore]:\n        \"\"\"Apply simple fusion.\"\"\"\n        # Use a dict to de-duplicate nodes\n        all_nodes: Dict[str, NodeWithScore] = {}\n        for nodes_with_scores in results.values():\n            for node_with_score in nodes_with_scores:\n                hash = node_with_score.node.hash\n                if hash in all_nodes:\n                    max_score = max(\n                        node_with_score.score or 0.0, all_nodes[hash].score or 0.0\n                    )\n                    all_nodes[hash].score = max_score\n                else:\n                    all_nodes[hash] = node_with_score\n\n        return sorted(all_nodes.values(), key=lambda x: x.score or 0.0, reverse=True)\n\n    def _run_nested_async_queries(\n        self, queries: List[QueryBundle]\n    ) -> Dict[Tuple[str, int], List[NodeWithScore]]:\n        tasks, task_queries = [], []\n        for query in queries:\n            for i, retriever in enumerate(self._retrievers):\n                tasks.append(retriever.aretrieve(query))\n                task_queries.append((query.query_str, i))\n\n        task_results = run_async_tasks(tasks)\n\n        results = {}\n        for query_tuple, query_result in zip(task_queries, task_results):\n            results[query_tuple] = query_result\n\n        return results\n\n    async def _run_async_queries(\n        self, queries: List[QueryBundle]\n    ) -> Dict[Tuple[str, int], List[NodeWithScore]]:\n        tasks, task_queries = [], []\n        for query in queries:\n            for i, retriever in enumerate(self._retrievers):\n                tasks.append(retriever.aretrieve(query))\n                task_queries.append((query.query_str, i))\n\n        task_results = await asyncio.gather(*tasks)\n\n        results = {}\n        for query_tuple, query_result in zip(task_queries, task_results):\n            results[query_tuple] = query_result\n\n        return results\n\n    def _run_sync_queries(\n        self, queries: List[QueryBundle]\n    ) -> Dict[Tuple[str, int], List[NodeWithScore]]:\n        results = {}\n        for query in queries:\n            for i, retriever in enumerate(self._retrievers):\n                results[(query.query_str, i)] = retriever.retrieve(query)\n\n        return results\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        queries: List[QueryBundle] = [query_bundle]\n        if self.num_queries > 1:\n            queries.extend(self._get_queries(query_bundle.query_str))\n\n        if self.use_async:\n            results = self._run_nested_async_queries(queries)\n        else:\n            results = self._run_sync_queries(queries)\n\n        if self.mode == FUSION_MODES.RECIPROCAL_RANK:\n            return self._reciprocal_rerank_fusion(results)[: self.similarity_top_k]\n        elif self.mode == FUSION_MODES.RELATIVE_SCORE:\n            return self._relative_score_fusion(results)[: self.similarity_top_k]\n        elif self.mode == FUSION_MODES.DIST_BASED_SCORE:\n            return self._relative_score_fusion(results, dist_based=True)[\n                : self.similarity_top_k\n            ]\n        elif self.mode == FUSION_MODES.SIMPLE:\n            return self._simple_fusion(results)[: self.similarity_top_k]\n        else:\n            raise ValueError(f\"Invalid fusion mode: {self.mode}\")\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        queries: List[QueryBundle] = [query_bundle]\n        if self.num_queries > 1:\n            queries.extend(self._get_queries(query_bundle.query_str))\n\n        results = await self._run_async_queries(queries)\n\n        if self.mode == FUSION_MODES.RECIPROCAL_RANK:\n            return self._reciprocal_rerank_fusion(results)[: self.similarity_top_k]\n        elif self.mode == FUSION_MODES.RELATIVE_SCORE:\n            return self._relative_score_fusion(results)[: self.similarity_top_k]\n        elif self.mode == FUSION_MODES.DIST_BASED_SCORE:\n            return self._relative_score_fusion(results, dist_based=True)[\n                : self.similarity_top_k\n            ]\n        elif self.mode == FUSION_MODES.SIMPLE:\n            return self._simple_fusion(results)[: self.similarity_top_k]\n        else:\n            raise ValueError(f\"Invalid fusion mode: {self.mode}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/command_line/__init__.py",
    "filename": "__init__.py",
    "relpath": "command_line/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# Deprecated. CLI is now its own package llama-index-cli"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/command_line/upgrade.py",
    "filename": "upgrade.py",
    "relpath": "command_line/upgrade.py",
    "start_line": 1,
    "end_line": 287,
    "length": 287,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_parse_from_imports",
      "_parse_hub_downloads",
      "parse_lines",
      "_cell_installs_llama_hub",
      "_format_new_installs",
      "upgrade_nb_file",
      "upgrade_py_md_file",
      "upgrade_file",
      "_is_hidden",
      "upgrade_dir"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "_parse_from_imports",
      "_parse_hub_downloads",
      "parse_lines",
      "_cell_installs_llama_hub",
      "_format_new_installs",
      "upgrade_nb_file",
      "upgrade_py_md_file",
      "upgrade_file",
      "_is_hidden",
      "upgrade_dir"
    ],
    "document_class_names": [],
    "content": "import json\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nmappings_path = os.path.join(os.path.dirname(__file__), \"mappings.json\")\n\n\ndef _parse_from_imports(\n    mappings: Dict[str, str],\n    installed_modules: List[str],\n    line_idx: int,\n    lines: List[str],\n    verbose: bool = False,\n) -> Tuple[List[str], List[str], List[str], int]:\n    new_lines = []\n    new_installs = []\n    imported_modules = []\n    parsing_modules = False\n    skipped_lines = 0\n\n    for line in lines[line_idx:]:\n        skipped_lines += 1\n        if \"from \" in line:\n            imported_modules = [line, line.strip().split(\" import \")[-1].strip()]\n            if imported_modules[-1].startswith(\"(\"):\n                imported_modules[-1] = []  # type: ignore\n                parsing_modules = True\n            else:\n                imported_modules = [line, imported_modules[-1].split(\", \")]  # type: ignore\n\n        if parsing_modules:\n            if \")\" in line:\n                parsing_modules = False\n            elif \"(\" not in line:\n                imported_modules[-1].append(line.strip().replace(\",\", \"\"))  # type: ignore\n\n        if not parsing_modules and len(imported_modules) > 0:\n            imported_module_names = [x.strip() for x in imported_modules[-1]]\n            new_imports = {}\n            for module in imported_module_names:\n                if module in mappings:\n                    new_import_parent = mappings[module]\n                    if new_import_parent not in new_imports:\n                        new_imports[new_import_parent] = [module]\n                    else:\n                        new_imports[new_import_parent].append(module)\n                else:\n                    print(f\"Module not found: {module}\\nSwitching to core\")\n                    # get back the llama_index module that's being imported.\n                    new_import_parent = (\n                        imported_modules[0].split(\" import \")[0].split(\"from \")[-1]\n                    )\n                    # if the parent contains `llama_index.core` already, then skip\n                    if \"llama_index.core\" not in new_import_parent:\n                        new_import_parent = new_import_parent.replace(\n                            \"llama_index\", \"llama_index.core\"\n                        )\n\n                    if new_import_parent not in new_imports:\n                        new_imports[new_import_parent] = [module]\n                    else:\n                        new_imports[new_import_parent].append(module)\n\n            for new_import_parent, new_imports_list in new_imports.items():\n                new_install_parent = new_import_parent.replace(\".\", \"-\").replace(\n                    \"_\", \"-\"\n                )\n                if new_install_parent not in installed_modules:\n                    overlap = [x for x in installed_modules if x in new_install_parent]\n                    if len(overlap) == 0:\n                        installed_modules.append(new_install_parent)\n                        new_installs.append(f\"%pip install {new_install_parent}\\n\")\n                new_imports_str = \", \".join(new_imports_list)\n                new_lines.append(f\"from {new_import_parent} import {new_imports_str}\\n\")\n\n                parsing_modules = False\n                new_imports = {}\n                imported_modules = []\n\n            return new_lines, new_installs, installed_modules, skipped_lines\n\n        elif not parsing_modules:\n            new_lines.append(line)\n\n    return new_lines, new_installs, installed_modules, skipped_lines\n\n\ndef _parse_hub_downloads(\n    mappings: Dict[str, str],\n    installed_modules: List[str],\n    line: str,\n) -> Tuple[List[str], List[str], List[str]]:\n    regex = r\"download_loader\\([\\\"']([A-Z,a-z]+)[\\\"'][\\s,a-z,A-Z,_=]*\\)|download_tool\\([\\\"']([a-z,A-Z]+)[\\\"'][A-Z,a-z,\\s,_=]*\\)\"\n    result = re.search(regex, line)\n    new_lines = []\n    new_installs = []\n    if result:\n        tool, reader = result.groups()\n        module = tool if tool else reader\n        if module in mappings:\n            new_import_parent = mappings[module]\n            new_lines.append(f\"from {new_import_parent} import {module}\\n\")\n            new_install_parent = new_import_parent.replace(\".\", \"-\").replace(\"_\", \"-\")\n            if new_install_parent not in installed_modules:\n                new_installs.append(f\"%pip install {new_install_parent}\\n\")\n                installed_modules.append(new_install_parent)\n        else:\n            print(f\"Reader/Tool not found: {module}\\nKeeping line as is.\")\n            new_lines.append(line)\n\n    return new_lines, new_installs, installed_modules\n\n\ndef parse_lines(\n    lines: List[str], installed_modules: List[str], verbose: bool = False\n) -> Tuple[List[str], List[str]]:\n    with open(mappings_path) as f:\n        mappings = json.load(f)\n\n    new_installs = []\n    new_lines = []\n    just_found_imports = False\n    skipped_lines = 0\n\n    for idx, line in enumerate(lines):\n        this_new_lines: List[str] = []\n        this_new_installs: List[str] = []\n        this_installed_modules: List[str] = []\n\n        if skipped_lines != 0:\n            skipped_lines -= 1\n\n        if just_found_imports and skipped_lines > 0:\n            continue\n        else:\n            just_found_imports = False\n\n        if (\n            \"from llama_index.\" in line\n            or \"from llama_index import\" in line\n            or \"from llama_hub.\" in line\n        ):\n            (\n                this_new_lines,\n                this_new_installs,\n                this_installed_modules,\n                skipped_lines,\n            ) = _parse_from_imports(\n                mappings=mappings,\n                installed_modules=installed_modules,\n                line_idx=idx,\n                lines=lines,\n                verbose=verbose,\n            )\n            just_found_imports = True\n\n        elif \"download_loader(\" in line or \"download_tool(\" in line:\n            (\n                this_new_lines,\n                this_new_installs,\n                this_installed_modules,\n            ) = _parse_hub_downloads(\n                mappings=mappings,\n                installed_modules=installed_modules,\n                line=line,\n            )\n\n        elif not just_found_imports:\n            this_new_lines = [line]\n\n        new_lines += this_new_lines\n        new_installs += this_new_installs\n        installed_modules += this_installed_modules\n        installed_modules = list(set(installed_modules))\n\n    return new_lines, list(set(new_installs))\n\n\ndef _cell_installs_llama_hub(cell: Dict[str, Any]) -> bool:\n    lines = cell[\"source\"]\n    llama_hub_partial_statements = [\n        \"pip install llama-hub\",\n        \"import download_loader\",\n        \"import download_tool\",\n    ]\n\n    if len(lines) > 1:\n        return False\n    if cell[\"cell_type\"] == \"code\" and any(\n        el in lines[0] for el in llama_hub_partial_statements\n    ):\n        return True\n    return False\n\n\ndef _format_new_installs(new_installs: List[str]) -> List[str]:\n    if new_installs:\n        new_installs = list(set(new_installs))\n        return new_installs[:-1] + [new_installs[-1].replace(\"\\n\", \"\")]\n    return new_installs\n\n\ndef upgrade_nb_file(file_path: str) -> None:\n    print(f\"\\n=====================\\n{file_path}\\n\", flush=True)\n    with open(file_path) as f:\n        notebook = json.load(f)\n\n    verbose = False\n    if file_path == \"../docs/examples/managed/manage_retrieval_benchmark.ipynb\":\n        verbose = True\n\n    installed_modules = [\"llama-index-core\"]  # default installs\n    cur_cells = []\n    new_installs = []\n    first_code_idx = -1\n    for idx, cell in enumerate(notebook[\"cells\"]):\n        if cell[\"cell_type\"] == \"code\":\n            if verbose:\n                print(f\"cell: {cell}\", flush=True)\n            if first_code_idx == -1:\n                first_code_idx = idx\n\n            code = cell[\"source\"]\n\n            new_lines, cur_new_installs = parse_lines(code, installed_modules, verbose)\n            new_installs += cur_new_installs\n\n            cell[\"source\"] = new_lines\n\n        cur_cells.append(cell)\n\n    if len(new_installs) > 0:\n        notebook[\"cells\"] = cur_cells\n        new_cell = {\n            \"cell_type\": \"code\",\n            \"metadata\": {},\n            \"execution_count\": None,\n            \"outputs\": [],\n            \"source\": _format_new_installs(new_installs),\n        }\n        cur_cells.insert(first_code_idx, new_cell)\n\n    cur_cells = [cell for cell in cur_cells if not _cell_installs_llama_hub(cell)]\n    notebook[\"cells\"] = cur_cells\n    with open(file_path, \"w\") as f:\n        json.dump(notebook, f, indent=1, ensure_ascii=False)\n\n\ndef upgrade_py_md_file(file_path: str) -> None:\n    with open(file_path) as f:\n        lines = f.readlines()\n\n    installed_modules = [\"llama-index-core\"]  # default installs\n    new_lines, new_installs = parse_lines(lines, installed_modules)\n\n    with open(file_path, \"w\") as f:\n        f.write(\"\".join(new_lines))\n\n    if len(new_installs) > 0:\n        print(\"New installs:\")\n    for install in new_installs:\n        print(install.strip().replace(\"%\", \"\"))\n\n\ndef upgrade_file(file_path: str) -> None:\n    if file_path.endswith(\".ipynb\"):\n        upgrade_nb_file(file_path)\n    elif file_path.endswith((\".py\", \".md\")):\n        upgrade_py_md_file(file_path)\n    else:\n        raise Exception(f\"File type not supported: {file_path}\")\n\n\ndef _is_hidden(path: Path) -> bool:\n    return any(part.startswith(\".\") and part not in [\".\", \"..\"] for part in path.parts)\n\n\ndef upgrade_dir(input_dir: str) -> None:\n    file_refs = list(Path(input_dir).rglob(\"*.py\"))\n    file_refs += list(Path(input_dir).rglob(\"*.ipynb\"))\n    file_refs += list(Path(input_dir).rglob(\"*.md\"))\n    file_refs = [x for x in file_refs if not _is_hidden(x)]\n    for file_ref in file_refs:\n        if file_ref.is_file():\n            upgrade_file(str(file_ref))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/base.py",
    "filename": "base.py",
    "relpath": "response_synthesizers/base.py",
    "start_line": 1,
    "end_line": 381,
    "length": 381,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "empty_response_generator",
      "empty_response_agenerator",
      "__init__",
      "_get_prompt_modules",
      "callback_manager",
      "callback_manager",
      "get_response",
      "aget_response",
      "_log_prompt_and_response",
      "_get_metadata_for_response",
      "_prepare_response_output",
      "synthesize",
      "asynthesize",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "provides",
      "BaseSynthesizer",
      "SynthesizerComponent"
    ],
    "document_function_names": [
      "empty_response_generator",
      "empty_response_agenerator",
      "__init__",
      "_get_prompt_modules",
      "callback_manager",
      "callback_manager",
      "get_response",
      "aget_response",
      "_log_prompt_and_response",
      "_get_metadata_for_response",
      "_prepare_response_output",
      "synthesize",
      "asynthesize",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "provides",
      "BaseSynthesizer",
      "SynthesizerComponent"
    ],
    "content": "\"\"\"Response builder class.\n\nThis class provides general functions for taking in a set of text\nand generating a response.\n\nWill support different modes, from 1) stuffing chunks into prompt,\n2) create and refine separately over each chunk, 3) tree summarization.\n\n\"\"\"\n\nimport logging\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Generator, List, Optional, Sequence, AsyncGenerator, Type\n\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.base.response.schema import (\n    RESPONSE_TYPE,\n    PydanticResponse,\n    Response,\n    StreamingResponse,\n    AsyncStreamingResponse,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts.mixin import PromptMixin\nfrom llama_index.core.schema import (\n    BaseNode,\n    MetadataMode,\n    NodeWithScore,\n    QueryBundle,\n    QueryType,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.instrumentation.events.synthesis import (\n    SynthesizeStartEvent,\n    SynthesizeEndEvent,\n)\nfrom llama_index.core.llms.structured_llm import StructuredLLM\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\nlogger = logging.getLogger(__name__)\n\nQueryTextType = QueryType\n\n\ndef empty_response_generator() -> Generator[str, None, None]:\n    yield \"Empty Response\"\n\n\nasync def empty_response_agenerator() -> AsyncGenerator[str, None]:\n    yield \"Empty Response\"\n\n\nclass BaseSynthesizer(ChainableMixin, PromptMixin, DispatcherSpanMixin):\n    \"\"\"Response builder class.\"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        streaming: bool = False,\n        output_cls: Optional[Type[BaseModel]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n\n        if callback_manager:\n            self._llm.callback_manager = callback_manager\n\n        self._callback_manager = callback_manager or Settings.callback_manager\n\n        self._prompt_helper = (\n            prompt_helper\n            or Settings._prompt_helper\n            or PromptHelper.from_llm_metadata(\n                self._llm.metadata,\n            )\n        )\n\n        self._streaming = streaming\n        self._output_cls = output_cls\n\n    def _get_prompt_modules(self) -> Dict[str, Any]:\n        \"\"\"Get prompt modules.\"\"\"\n        # TODO: keep this for now since response synthesizers don't generally have sub-modules\n        return {}\n\n    @property\n    def callback_manager(self) -> CallbackManager:\n        return self._callback_manager\n\n    @callback_manager.setter\n    def callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self._callback_manager = callback_manager\n        # TODO: please fix this later\n        self._callback_manager = callback_manager\n        self._llm.callback_manager = callback_manager\n\n    @abstractmethod\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n\n    @abstractmethod\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"\n        ...\n\n    def _log_prompt_and_response(\n        self,\n        formatted_prompt: str,\n        response: RESPONSE_TEXT_TYPE,\n        log_prefix: str = \"\",\n    ) -> None:\n        \"\"\"Log prompt and response from LLM.\"\"\"\n        logger.debug(f\"> {log_prefix} prompt template: {formatted_prompt}\")\n        logger.debug(f\"> {log_prefix} response: {response}\")\n\n    def _get_metadata_for_response(\n        self,\n        nodes: List[BaseNode],\n    ) -> Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for response.\"\"\"\n        return {node.node_id: node.metadata for node in nodes}\n\n    def _prepare_response_output(\n        self,\n        response_str: Optional[RESPONSE_TEXT_TYPE],\n        source_nodes: List[NodeWithScore],\n    ) -> RESPONSE_TYPE:\n        \"\"\"Prepare response object from response string.\"\"\"\n        response_metadata = self._get_metadata_for_response(\n            [node_with_score.node for node_with_score in source_nodes]\n        )\n\n        if isinstance(self._llm, StructuredLLM):\n            # convert string to output_cls\n            output = self._llm.output_cls.model_validate_json(str(response_str))\n            return PydanticResponse(\n                output,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n\n        if isinstance(response_str, str):\n            return Response(\n                response_str,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n        if isinstance(response_str, Generator):\n            return StreamingResponse(\n                response_str,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n        if isinstance(response_str, AsyncGenerator):\n            return AsyncStreamingResponse(\n                response_str,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n\n        if self._output_cls is not None and isinstance(response_str, self._output_cls):\n            return PydanticResponse(\n                response_str, source_nodes=source_nodes, metadata=response_metadata\n            )\n\n        raise ValueError(\n            f\"Response must be a string or a generator. Found {type(response_str)}\"\n        )\n\n    @dispatcher.span\n    def synthesize(\n        self,\n        query: QueryTextType,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TYPE:\n        dispatcher.event(\n            SynthesizeStartEvent(\n                query=query,\n            )\n        )\n\n        if len(nodes) == 0:\n            if self._streaming:\n                empty_response_stream = StreamingResponse(\n                    response_gen=empty_response_generator()\n                )\n                dispatcher.event(\n                    SynthesizeEndEvent(\n                        query=query,\n                        response=empty_response_stream,\n                    )\n                )\n                return empty_response_stream\n            else:\n                empty_response = Response(\"Empty Response\")\n                dispatcher.event(\n                    SynthesizeEndEvent(\n                        query=query,\n                        response=empty_response,\n                    )\n                )\n                return empty_response\n\n        if isinstance(query, str):\n            query = QueryBundle(query_str=query)\n\n        with self._callback_manager.event(\n            CBEventType.SYNTHESIZE,\n            payload={EventPayload.QUERY_STR: query.query_str},\n        ) as event:\n            response_str = self.get_response(\n                query_str=query.query_str,\n                text_chunks=[\n                    n.node.get_content(metadata_mode=MetadataMode.LLM) for n in nodes\n                ],\n                **response_kwargs,\n            )\n\n            additional_source_nodes = additional_source_nodes or []\n            source_nodes = list(nodes) + list(additional_source_nodes)\n\n            response = self._prepare_response_output(response_str, source_nodes)\n\n            event.on_end(payload={EventPayload.RESPONSE: response})\n\n        dispatcher.event(\n            SynthesizeEndEvent(\n                query=query,\n                response=response,\n            )\n        )\n        return response\n\n    @dispatcher.span\n    async def asynthesize(\n        self,\n        query: QueryTextType,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TYPE:\n        dispatcher.event(\n            SynthesizeStartEvent(\n                query=query,\n            )\n        )\n        if len(nodes) == 0:\n            if self._streaming:\n                empty_response_stream = AsyncStreamingResponse(\n                    response_gen=empty_response_agenerator()\n                )\n                dispatcher.event(\n                    SynthesizeEndEvent(\n                        query=query,\n                        response=empty_response_stream,\n                    )\n                )\n                return empty_response_stream\n            else:\n                empty_response = Response(\"Empty Response\")\n                dispatcher.event(\n                    SynthesizeEndEvent(\n                        query=query,\n                        response=empty_response,\n                    )\n                )\n                return empty_response\n\n        if isinstance(query, str):\n            query = QueryBundle(query_str=query)\n\n        with self._callback_manager.event(\n            CBEventType.SYNTHESIZE,\n            payload={EventPayload.QUERY_STR: query.query_str},\n        ) as event:\n            response_str = await self.aget_response(\n                query_str=query.query_str,\n                text_chunks=[\n                    n.node.get_content(metadata_mode=MetadataMode.LLM) for n in nodes\n                ],\n                **response_kwargs,\n            )\n\n            additional_source_nodes = additional_source_nodes or []\n            source_nodes = list(nodes) + list(additional_source_nodes)\n\n            response = self._prepare_response_output(response_str, source_nodes)\n\n            event.on_end(payload={EventPayload.RESPONSE: response})\n\n        dispatcher.event(\n            SynthesizeEndEvent(\n                query=query,\n                response=response,\n            )\n        )\n        return response\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"As query component.\"\"\"\n        return SynthesizerComponent(synthesizer=self)\n\n\nclass SynthesizerComponent(QueryComponent):\n    \"\"\"Synthesizer component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    synthesizer: BaseSynthesizer = Field(..., description=\"Synthesizer\")\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.synthesizer.callback_manager = callback_manager\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # make sure both query_str and nodes are there\n        if \"query_str\" not in input:\n            raise ValueError(\"Input must have key 'query_str'\")\n        input[\"query_str\"] = validate_and_convert_stringable(input[\"query_str\"])\n\n        if \"nodes\" not in input:\n            raise ValueError(\"Input must have key 'nodes'\")\n        nodes = input[\"nodes\"]\n        if not isinstance(nodes, list):\n            raise ValueError(\"Input nodes must be a list\")\n        for node in nodes:\n            if not isinstance(node, NodeWithScore):\n                raise ValueError(\"Input nodes must be a list of NodeWithScore\")\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        output = self.synthesizer.synthesize(kwargs[\"query_str\"], kwargs[\"nodes\"])\n        return {\"output\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        output = await self.synthesizer.asynthesize(\n            kwargs[\"query_str\"], kwargs[\"nodes\"]\n        )\n        return {\"output\": output}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"query_str\", \"nodes\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
    "filename": "refine.py",
    "relpath": "response_synthesizers/refine.py",
    "start_line": 1,
    "end_line": 105,
    "length": 105,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "output_cls",
      "__call__",
      "acall"
    ],
    "chunk_class_names": [
      "StructuredRefineResponse",
      "DefaultRefineProgram"
    ],
    "document_function_names": [
      "__init__",
      "output_cls",
      "__call__",
      "acall",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "get_response",
      "_default_program_factory",
      "_give_response_single",
      "_refine_response_single",
      "aget_response",
      "_arefine_response_single",
      "_agive_response_single"
    ],
    "document_class_names": [
      "StructuredRefineResponse",
      "DefaultRefineProgram",
      "Refine"
    ],
    "content": "import logging\nfrom typing import (\n    Any,\n    Callable,\n    Generator,\n    Optional,\n    Sequence,\n    Type,\n    cast,\n    AsyncGenerator,\n)\n\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ValidationError\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.indices.utils import truncate_text\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_REFINE_PROMPT_SEL,\n    DEFAULT_TEXT_QA_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.response.utils import get_response_text, aget_response_text\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE, BasePydanticProgram\nfrom llama_index.core.instrumentation.events.synthesis import (\n    GetResponseEndEvent,\n    GetResponseStartEvent,\n)\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\nlogger = logging.getLogger(__name__)\n\n\nclass StructuredRefineResponse(BaseModel):\n    \"\"\"\n    Used to answer a given query based on the provided context.\n\n    Also indicates if the query was satisfied with the provided answer.\n    \"\"\"\n\n    answer: str = Field(\n        description=\"The answer for the given query, based on the context and not \"\n        \"prior knowledge.\"\n    )\n    query_satisfied: bool = Field(\n        description=\"True if there was enough context given to provide an answer \"\n        \"that satisfies the query.\"\n    )\n\n\nclass DefaultRefineProgram(BasePydanticProgram):\n    \"\"\"\n    Runs the query on the LLM as normal and always returns the answer with\n    query_satisfied=True. In effect, doesn't do any answer filtering.\n    \"\"\"\n\n    def __init__(\n        self,\n        prompt: BasePromptTemplate,\n        llm: LLM,\n        output_cls: Optional[Type[BaseModel]] = None,\n    ):\n        self._prompt = prompt\n        self._llm = llm\n        self._output_cls = output_cls\n\n    @property\n    def output_cls(self) -> Type[BaseModel]:\n        return StructuredRefineResponse\n\n    def __call__(self, *args: Any, **kwds: Any) -> StructuredRefineResponse:\n        if self._output_cls is not None:\n            answer = self._llm.structured_predict(\n                self._output_cls,\n                self._prompt,\n                **kwds,\n            )\n            if isinstance(answer, BaseModel):\n                answer = answer.model_dump_json()\n        else:\n            answer = self._llm.predict(\n                self._prompt,\n                **kwds,\n            )\n        return StructuredRefineResponse(answer=answer, query_satisfied=True)\n\n    async def acall(self, *args: Any, **kwds: Any) -> StructuredRefineResponse:\n        if self._output_cls is not None:\n            answer = await self._llm.astructured_predict(  # type: ignore\n                self._output_cls,\n                self._prompt,\n                **kwds,\n            )\n            if isinstance(answer, BaseModel):\n                answer = answer.model_dump_json()\n        else:\n            answer = await self._llm.apredict(\n                self._prompt,\n                **kwds,\n            )\n        return StructuredRefineResponse(answer=answer, query_satisfied=True)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
    "filename": "refine.py",
    "relpath": "response_synthesizers/refine.py",
    "start_line": 105,
    "end_line": 521,
    "length": 417,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "get_response",
      "_default_program_factory",
      "_give_response_single",
      "_refine_response_single",
      "aget_response",
      "_arefine_response_single",
      "_agive_response_single"
    ],
    "chunk_class_names": [
      "Refine"
    ],
    "document_function_names": [
      "__init__",
      "output_cls",
      "__call__",
      "acall",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "get_response",
      "_default_program_factory",
      "_give_response_single",
      "_refine_response_single",
      "aget_response",
      "_arefine_response_single",
      "_agive_response_single"
    ],
    "document_class_names": [
      "StructuredRefineResponse",
      "DefaultRefineProgram",
      "Refine"
    ],
    "content": "class Refine(BaseSynthesizer):\n    \"\"\"Refine a response to a query across text chunks.\"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        refine_template: Optional[BasePromptTemplate] = None,\n        output_cls: Optional[Type[BaseModel]] = None,\n        streaming: bool = False,\n        verbose: bool = False,\n        structured_answer_filtering: bool = False,\n        program_factory: Optional[\n            Callable[[BasePromptTemplate], BasePydanticProgram]\n        ] = None,\n    ) -> None:\n        super().__init__(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            streaming=streaming,\n        )\n        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT_SEL\n        self._refine_template = refine_template or DEFAULT_REFINE_PROMPT_SEL\n        self._verbose = verbose\n        self._structured_answer_filtering = structured_answer_filtering\n        self._output_cls = output_cls\n\n        if self._streaming and self._structured_answer_filtering:\n            raise ValueError(\n                \"Streaming not supported with structured answer filtering.\"\n            )\n        if not self._structured_answer_filtering and program_factory is not None:\n            raise ValueError(\n                \"Program factory not supported without structured answer filtering.\"\n            )\n        self._program_factory = program_factory or self._default_program_factory\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"text_qa_template\": self._text_qa_template,\n            \"refine_template\": self._refine_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"text_qa_template\" in prompts:\n            self._text_qa_template = prompts[\"text_qa_template\"]\n        if \"refine_template\" in prompts:\n            self._refine_template = prompts[\"refine_template\"]\n\n    @dispatcher.span\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Give response over chunks.\"\"\"\n        dispatcher.event(\n            GetResponseStartEvent(query_str=query_str, text_chunks=text_chunks)\n        )\n        response: Optional[RESPONSE_TEXT_TYPE] = None\n        for text_chunk in text_chunks:\n            if prev_response is None:\n                # if this is the first chunk, and text chunk already\n                # is an answer, then return it\n                response = self._give_response_single(\n                    query_str, text_chunk, **response_kwargs\n                )\n            else:\n                # refine response if possible\n                response = self._refine_response_single(\n                    prev_response, query_str, text_chunk, **response_kwargs\n                )\n            prev_response = response\n        if isinstance(response, str):\n            if self._output_cls is not None:\n                try:\n                    response = self._output_cls.model_validate_json(response)\n                except ValidationError:\n                    pass\n            else:\n                response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)\n        dispatcher.event(GetResponseEndEvent())\n        return response\n\n    def _default_program_factory(\n        self, prompt: BasePromptTemplate\n    ) -> BasePydanticProgram:\n        if self._structured_answer_filtering:\n            from llama_index.core.program.utils import get_program_for_llm\n\n            return get_program_for_llm(\n                StructuredRefineResponse,\n                prompt,\n                self._llm,\n                verbose=self._verbose,\n            )\n        else:\n            return DefaultRefineProgram(\n                prompt=prompt,\n                llm=self._llm,\n                output_cls=self._output_cls,\n            )\n\n    def _give_response_single(\n        self,\n        query_str: str,\n        text_chunk: str,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Give response given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n        text_chunks = self._prompt_helper.repack(\n            text_qa_template, [text_chunk], llm=self._llm\n        )\n\n        response: Optional[RESPONSE_TEXT_TYPE] = None\n        program = self._program_factory(text_qa_template)\n        # TODO: consolidate with loop in get_response_default\n        for cur_text_chunk in text_chunks:\n            query_satisfied = False\n            if response is None and not self._streaming:\n                try:\n                    structured_response = cast(\n                        StructuredRefineResponse,\n                        program(\n                            context_str=cur_text_chunk,\n                            **response_kwargs,\n                        ),\n                    )\n                    query_satisfied = structured_response.query_satisfied\n                    if query_satisfied:\n                        response = structured_response.answer\n                except ValidationError as e:\n                    logger.warning(\n                        f\"Validation error on structured response: {e}\", exc_info=True\n                    )\n            elif response is None and self._streaming:\n                response = self._llm.stream(\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                    **response_kwargs,\n                )\n                query_satisfied = True\n            else:\n                response = self._refine_response_single(\n                    cast(RESPONSE_TEXT_TYPE, response),\n                    query_str,\n                    cur_text_chunk,\n                    **response_kwargs,\n                )\n        if response is None:\n            response = \"Empty Response\"\n        if isinstance(response, str):\n            response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)\n        return response\n\n    def _refine_response_single(\n        self,\n        response: RESPONSE_TEXT_TYPE,\n        query_str: str,\n        text_chunk: str,\n        **response_kwargs: Any,\n    ) -> Optional[RESPONSE_TEXT_TYPE]:\n        \"\"\"Refine response.\"\"\"\n        # TODO: consolidate with logic in response/schema.py\n        if isinstance(response, Generator):\n            response = get_response_text(response)\n\n        fmt_text_chunk = truncate_text(text_chunk, 50)\n        logger.debug(f\"> Refine context: {fmt_text_chunk}\")\n        if self._verbose:\n            print(f\"> Refine context: {fmt_text_chunk}\")\n\n        # NOTE: partial format refine template with query_str and existing_answer here\n        refine_template = self._refine_template.partial_format(\n            query_str=query_str, existing_answer=response\n        )\n\n        # compute available chunk size to see if there is any available space\n        # determine if the refine template is too big (which can happen if\n        # prompt template + query + existing answer is too large)\n        avail_chunk_size = self._prompt_helper._get_available_chunk_size(\n            refine_template\n        )\n\n        if avail_chunk_size < 0:\n            # if the available chunk size is negative, then the refine template\n            # is too big and we just return the original response\n            return response\n\n        # obtain text chunks to add to the refine template\n        text_chunks = self._prompt_helper.repack(\n            refine_template, text_chunks=[text_chunk], llm=self._llm\n        )\n\n        program = self._program_factory(refine_template)\n        for cur_text_chunk in text_chunks:\n            query_satisfied = False\n            if not self._streaming:\n                try:\n                    structured_response = cast(\n                        StructuredRefineResponse,\n                        program(\n                            context_msg=cur_text_chunk,\n                            **response_kwargs,\n                        ),\n                    )\n                    query_satisfied = structured_response.query_satisfied\n                    if query_satisfied:\n                        response = structured_response.answer\n                except ValidationError as e:\n                    logger.warning(\n                        f\"Validation error on structured response: {e}\", exc_info=True\n                    )\n            else:\n                # TODO: structured response not supported for streaming\n                if isinstance(response, Generator):\n                    response = \"\".join(response)\n\n                refine_template = self._refine_template.partial_format(\n                    query_str=query_str, existing_answer=response\n                )\n\n                response = self._llm.stream(\n                    refine_template,\n                    context_msg=cur_text_chunk,\n                    **response_kwargs,\n                )\n\n        return response\n\n    @dispatcher.span\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        dispatcher.event(\n            GetResponseStartEvent(query_str=query_str, text_chunks=text_chunks)\n        )\n        response: Optional[RESPONSE_TEXT_TYPE] = None\n        for text_chunk in text_chunks:\n            if prev_response is None:\n                # if this is the first chunk, and text chunk already\n                # is an answer, then return it\n                response = await self._agive_response_single(\n                    query_str, text_chunk, **response_kwargs\n                )\n            else:\n                response = await self._arefine_response_single(\n                    prev_response, query_str, text_chunk, **response_kwargs\n                )\n            prev_response = response\n        if response is None:\n            response = \"Empty Response\"\n        if isinstance(response, str):\n            if self._output_cls is not None:\n                response = self._output_cls.model_validate_json(response)\n            else:\n                response = response or \"Empty Response\"\n        else:\n            response = cast(AsyncGenerator, response)\n        dispatcher.event(GetResponseEndEvent())\n        return response\n\n    async def _arefine_response_single(\n        self,\n        response: RESPONSE_TEXT_TYPE,\n        query_str: str,\n        text_chunk: str,\n        **response_kwargs: Any,\n    ) -> Optional[RESPONSE_TEXT_TYPE]:\n        \"\"\"Refine response.\"\"\"\n        # TODO: consolidate with logic in response/schema.py\n        if isinstance(response, AsyncGenerator):\n            response = await aget_response_text(response)\n\n        fmt_text_chunk = truncate_text(text_chunk, 50)\n        logger.debug(f\"> Refine context: {fmt_text_chunk}\")\n\n        # NOTE: partial format refine template with query_str and existing_answer here\n        refine_template = self._refine_template.partial_format(\n            query_str=query_str, existing_answer=response\n        )\n\n        # compute available chunk size to see if there is any available space\n        # determine if the refine template is too big (which can happen if\n        # prompt template + query + existing answer is too large)\n        avail_chunk_size = self._prompt_helper._get_available_chunk_size(\n            refine_template\n        )\n\n        if avail_chunk_size < 0:\n            # if the available chunk size is negative, then the refine template\n            # is too big and we just return the original response\n            return response\n\n        # obtain text chunks to add to the refine template\n        text_chunks = self._prompt_helper.repack(\n            refine_template, text_chunks=[text_chunk], llm=self._llm\n        )\n\n        program = self._program_factory(refine_template)\n        for cur_text_chunk in text_chunks:\n            query_satisfied = False\n            if not self._streaming:\n                try:\n                    structured_response = await program.acall(\n                        context_msg=cur_text_chunk,\n                        **response_kwargs,\n                    )\n                    structured_response = cast(\n                        StructuredRefineResponse, structured_response\n                    )\n                    query_satisfied = structured_response.query_satisfied\n                    if query_satisfied:\n                        response = structured_response.answer\n                except ValidationError as e:\n                    logger.warning(\n                        f\"Validation error on structured response: {e}\", exc_info=True\n                    )\n            else:\n                if isinstance(response, Generator):\n                    response = \"\".join(response)\n\n                if isinstance(response, AsyncGenerator):\n                    _r = \"\"\n                    async for text in response:\n                        _r += text\n                    response = _r\n\n                refine_template = self._refine_template.partial_format(\n                    query_str=query_str, existing_answer=response\n                )\n\n                response = await self._llm.astream(\n                    refine_template,\n                    context_msg=cur_text_chunk,\n                    **response_kwargs,\n                )\n\n            if query_satisfied:\n                refine_template = self._refine_template.partial_format(\n                    query_str=query_str, existing_answer=response\n                )\n\n        return response\n\n    async def _agive_response_single(\n        self,\n        query_str: str,\n        text_chunk: str,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Give response given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n        text_chunks = self._prompt_helper.repack(\n            text_qa_template, [text_chunk], llm=self._llm\n        )\n\n        response: Optional[RESPONSE_TEXT_TYPE] = None\n        program = self._program_factory(text_qa_template)\n        # TODO: consolidate with loop in get_response_default\n        for cur_text_chunk in text_chunks:\n            if response is None and not self._streaming:\n                try:\n                    structured_response = await program.acall(\n                        context_str=cur_text_chunk,\n                        **response_kwargs,\n                    )\n                    structured_response = cast(\n                        StructuredRefineResponse, structured_response\n                    )\n                    query_satisfied = structured_response.query_satisfied\n                    if query_satisfied:\n                        response = structured_response.answer\n                except ValidationError as e:\n                    logger.warning(\n                        f\"Validation error on structured response: {e}\", exc_info=True\n                    )\n            elif response is None and self._streaming:\n                response = await self._llm.astream(\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                    **response_kwargs,\n                )\n                query_satisfied = True\n            else:\n                response = await self._arefine_response_single(\n                    cast(RESPONSE_TEXT_TYPE, response),\n                    query_str,\n                    cur_text_chunk,\n                    **response_kwargs,\n                )\n        if response is None:\n            response = \"Empty Response\"\n        if isinstance(response, str):\n            response = response or \"Empty Response\"\n        else:\n            response = cast(AsyncGenerator, response)\n        return response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/accumulate.py",
    "filename": "accumulate.py",
    "relpath": "response_synthesizers/accumulate.py",
    "start_line": 1,
    "end_line": 151,
    "length": 151,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "flatten_list",
      "_format_response",
      "aget_response",
      "get_response",
      "_give_responses"
    ],
    "chunk_class_names": [
      "Accumulate"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "flatten_list",
      "_format_response",
      "aget_response",
      "get_response",
      "_give_responses"
    ],
    "document_class_names": [
      "Accumulate"
    ],
    "content": "import asyncio\nfrom typing import Any, Callable, List, Optional, Sequence, Type\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_TEXT_QA_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\n\n\nclass Accumulate(BaseSynthesizer):\n    \"\"\"Accumulate responses from multiple text chunks.\"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        output_cls: Optional[Type[BaseModel]] = None,\n        streaming: bool = False,\n        use_async: bool = False,\n    ) -> None:\n        super().__init__(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            streaming=streaming,\n        )\n        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT_SEL\n        self._use_async = use_async\n        self._output_cls = output_cls\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"text_qa_template\": self._text_qa_template}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"text_qa_template\" in prompts:\n            self._text_qa_template = prompts[\"text_qa_template\"]\n\n    def flatten_list(self, md_array: List[List[Any]]) -> List[Any]:\n        return [item for sublist in md_array for item in sublist]\n\n    def _format_response(self, outputs: List[Any], separator: str) -> str:\n        responses: List[str] = []\n        for response in outputs:\n            responses.append(response or \"Empty Response\")\n\n        return separator.join(\n            [f\"Response {index + 1}: {item}\" for index, item in enumerate(responses)]\n        )\n\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        separator: str = \"\\n---------------------\\n\",\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Apply the same prompt to text chunks and return async responses.\"\"\"\n        if self._streaming:\n            raise ValueError(\"Unable to stream in Accumulate response mode\")\n\n        tasks = [\n            self._give_responses(\n                query_str, text_chunk, use_async=True, **response_kwargs\n            )\n            for text_chunk in text_chunks\n        ]\n\n        flattened_tasks = self.flatten_list(tasks)\n        outputs = await asyncio.gather(*flattened_tasks)\n\n        return self._format_response(outputs, separator)\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        separator: str = \"\\n---------------------\\n\",\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Apply the same prompt to text chunks and return responses.\"\"\"\n        if self._streaming:\n            raise ValueError(\"Unable to stream in Accumulate response mode\")\n\n        tasks = [\n            self._give_responses(\n                query_str, text_chunk, use_async=self._use_async, **response_kwargs\n            )\n            for text_chunk in text_chunks\n        ]\n\n        outputs = self.flatten_list(tasks)\n\n        if self._use_async:\n            outputs = run_async_tasks(outputs)\n\n        return self._format_response(outputs, separator)\n\n    def _give_responses(\n        self,\n        query_str: str,\n        text_chunk: str,\n        use_async: bool = False,\n        **response_kwargs: Any,\n    ) -> List[Any]:\n        \"\"\"Give responses given a query and a corresponding text chunk.\"\"\"\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n\n        text_chunks = self._prompt_helper.repack(\n            text_qa_template, [text_chunk], llm=self._llm\n        )\n\n        predictor: Callable\n        if self._output_cls is None:\n            predictor = self._llm.apredict if use_async else self._llm.predict\n\n            return [\n                predictor(\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                    **response_kwargs,\n                )\n                for cur_text_chunk in text_chunks\n            ]\n        else:\n            predictor = (\n                self._llm.astructured_predict\n                if use_async\n                else self._llm.structured_predict\n            )\n\n            return [\n                predictor(\n                    self._output_cls,\n                    text_qa_template,\n                    context_str=cur_text_chunk,\n                    **response_kwargs,\n                )\n                for cur_text_chunk in text_chunks\n            ]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/type.py",
    "filename": "type.py",
    "relpath": "response_synthesizers/type.py",
    "start_line": 1,
    "end_line": 57,
    "length": 57,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "ResponseMode"
    ],
    "document_function_names": [],
    "document_class_names": [
      "ResponseMode"
    ],
    "content": "from enum import Enum\n\n\nclass ResponseMode(str, Enum):\n    \"\"\"Response modes of the response builder (and synthesizer).\"\"\"\n\n    REFINE = \"refine\"\n    \"\"\"\n    Refine is an iterative way of generating a response.\n    We first use the context in the first node, along with the query, to generate an \\\n    initial answer.\n    We then pass this answer, the query, and the context of the second node as input \\\n    into a \u201crefine prompt\u201d to generate a refined answer. We refine through N-1 nodes, \\\n    where N is the total number of nodes.\n    \"\"\"\n\n    COMPACT = \"compact\"\n    \"\"\"\n    Compact and refine mode first combine text chunks into larger consolidated chunks \\\n    that more fully utilize the available context window, then refine answers \\\n    across them.\n    This mode is faster than refine since we make fewer calls to the LLM.\n    \"\"\"\n\n    SIMPLE_SUMMARIZE = \"simple_summarize\"\n    \"\"\"\n    Merge all text chunks into one, and make a LLM call.\n    This will fail if the merged text chunk exceeds the context window size.\n    \"\"\"\n\n    TREE_SUMMARIZE = \"tree_summarize\"\n    \"\"\"\n    Build a tree index over the set of candidate nodes, with a summary prompt seeded \\\n    with the query.\n    The tree is built in a bottoms-up fashion, and in the end the root node is \\\n    returned as the response\n    \"\"\"\n\n    GENERATION = \"generation\"\n    \"\"\"Ignore context, just use LLM to generate a response.\"\"\"\n\n    NO_TEXT = \"no_text\"\n    \"\"\"Return the retrieved context nodes, without synthesizing a final response.\"\"\"\n\n    CONTEXT_ONLY = \"context_only\"\n    \"\"\"Returns a concatenated string of all text chunks.\"\"\"\n\n    ACCUMULATE = \"accumulate\"\n    \"\"\"Synthesize a response for each text chunk, and then return the concatenation.\"\"\"\n\n    COMPACT_ACCUMULATE = \"compact_accumulate\"\n    \"\"\"\n    Compact and accumulate mode first combine text chunks into larger consolidated \\\n    chunks that more fully utilize the available context window, then accumulate \\\n    answers for each of them and finally return the concatenation.\n    This mode is faster than accumulate since we make fewer calls to the LLM.\n    \"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/__init__.py",
    "filename": "__init__.py",
    "relpath": "response_synthesizers/__init__.py",
    "start_line": 1,
    "end_line": 29,
    "length": 29,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file.\"\"\"\n\nfrom llama_index.core.response_synthesizers.accumulate import Accumulate\nfrom llama_index.core.response_synthesizers.base import (\n    BaseSynthesizer,\n    SynthesizerComponent,\n)\nfrom llama_index.core.response_synthesizers.compact_and_refine import (\n    CompactAndRefine,\n)\nfrom llama_index.core.response_synthesizers.factory import get_response_synthesizer\nfrom llama_index.core.response_synthesizers.generation import Generation\nfrom llama_index.core.response_synthesizers.refine import Refine\nfrom llama_index.core.response_synthesizers.simple_summarize import SimpleSummarize\nfrom llama_index.core.response_synthesizers.tree_summarize import TreeSummarize\nfrom llama_index.core.response_synthesizers.type import ResponseMode\n\n__all__ = [\n    \"ResponseMode\",\n    \"BaseSynthesizer\",\n    \"SynthesizerComponent\",\n    \"Refine\",\n    \"SimpleSummarize\",\n    \"TreeSummarize\",\n    \"Generation\",\n    \"CompactAndRefine\",\n    \"Accumulate\",\n    \"get_response_synthesizer\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
    "filename": "tree_summarize.py",
    "relpath": "response_synthesizers/tree_summarize.py",
    "start_line": 1,
    "end_line": 230,
    "length": 230,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aget_response",
      "get_response"
    ],
    "chunk_class_names": [
      "TreeSummarize"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aget_response",
      "get_response"
    ],
    "document_class_names": [
      "TreeSummarize"
    ],
    "content": "import asyncio\nfrom typing import Any, Optional, Sequence, Type\n\nfrom llama_index.core.async_utils import run_async_tasks\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE, BaseModel\n\n\nclass TreeSummarize(BaseSynthesizer):\n    \"\"\"\n    Tree summarize response builder.\n\n    This response builder recursively merges text chunks and summarizes them\n    in a bottom-up fashion (i.e. building a tree from leaves to root).\n\n    More concretely, at each recursively step:\n    1. we repack the text chunks so that each chunk fills the context window of the LLM\n    2. if there is only one chunk, we give the final response\n    3. otherwise, we summarize each chunk and recursively summarize the summaries.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        summary_template: Optional[BasePromptTemplate] = None,\n        output_cls: Optional[Type[BaseModel]] = None,\n        streaming: bool = False,\n        use_async: bool = False,\n        verbose: bool = False,\n    ) -> None:\n        super().__init__(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            streaming=streaming,\n            output_cls=output_cls,\n        )\n        self._summary_template = summary_template or DEFAULT_TREE_SUMMARIZE_PROMPT_SEL\n        self._use_async = use_async\n        self._verbose = verbose\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"summary_template\": self._summary_template}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"summary_template\" in prompts:\n            self._summary_template = prompts[\"summary_template\"]\n\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get tree summarize response.\"\"\"\n        summary_template = self._summary_template.partial_format(query_str=query_str)\n        # repack text_chunks so that each chunk fills the context window\n        text_chunks = self._prompt_helper.repack(\n            summary_template, text_chunks=text_chunks, llm=self._llm\n        )\n\n        if self._verbose:\n            print(f\"{len(text_chunks)} text chunks after repacking\")\n\n        # give final response if there is only one chunk\n        if len(text_chunks) == 1:\n            response: RESPONSE_TEXT_TYPE\n            if self._streaming:\n                response = await self._llm.astream(\n                    summary_template, context_str=text_chunks[0], **response_kwargs\n                )\n            else:\n                if self._output_cls is None:\n                    response = await self._llm.apredict(\n                        summary_template,\n                        context_str=text_chunks[0],\n                        **response_kwargs,\n                    )\n                else:\n                    response = await self._llm.astructured_predict(\n                        self._output_cls,\n                        summary_template,\n                        context_str=text_chunks[0],\n                        **response_kwargs,\n                    )\n\n            # return pydantic object if output_cls is specified\n            return response\n\n        else:\n            # summarize each chunk\n            if self._output_cls is None:\n                str_tasks = [\n                    self._llm.apredict(\n                        summary_template,\n                        context_str=text_chunk,\n                        **response_kwargs,\n                    )\n                    for text_chunk in text_chunks\n                ]\n                summaries = await asyncio.gather(*str_tasks)\n            else:\n                model_tasks = [\n                    self._llm.astructured_predict(\n                        self._output_cls,\n                        summary_template,\n                        context_str=text_chunk,\n                        **response_kwargs,\n                    )\n                    for text_chunk in text_chunks\n                ]\n                summary_models = await asyncio.gather(*model_tasks)\n                summaries = [summary.model_dump_json() for summary in summary_models]\n\n            # recursively summarize the summaries\n            return await self.aget_response(\n                query_str=query_str,\n                text_chunks=summaries,\n                **response_kwargs,\n            )\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get tree summarize response.\"\"\"\n        summary_template = self._summary_template.partial_format(query_str=query_str)\n        # repack text_chunks so that each chunk fills the context window\n        text_chunks = self._prompt_helper.repack(\n            summary_template, text_chunks=text_chunks, llm=self._llm\n        )\n\n        if self._verbose:\n            print(f\"{len(text_chunks)} text chunks after repacking\")\n\n        # give final response if there is only one chunk\n        if len(text_chunks) == 1:\n            response: RESPONSE_TEXT_TYPE\n            if self._streaming:\n                response = self._llm.stream(\n                    summary_template, context_str=text_chunks[0], **response_kwargs\n                )\n            else:\n                if self._output_cls is None:\n                    response = self._llm.predict(\n                        summary_template,\n                        context_str=text_chunks[0],\n                        **response_kwargs,\n                    )\n                else:\n                    response = self._llm.structured_predict(\n                        self._output_cls,\n                        summary_template,\n                        context_str=text_chunks[0],\n                        **response_kwargs,\n                    )\n\n            return response\n\n        else:\n            # summarize each chunk\n            if self._use_async:\n                if self._output_cls is None:\n                    tasks = [\n                        self._llm.apredict(\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n                else:\n                    tasks = [\n                        self._llm.astructured_predict(\n                            self._output_cls,\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n\n                summary_responses = run_async_tasks(tasks)\n\n                if self._output_cls is not None:\n                    summaries = [\n                        summary.model_dump_json() for summary in summary_responses\n                    ]\n                else:\n                    summaries = summary_responses\n            else:\n                if self._output_cls is None:\n                    summaries = [\n                        self._llm.predict(\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n                else:\n                    summaries = [\n                        self._llm.structured_predict(\n                            self._output_cls,\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n                    summaries = [summary.model_dump_json() for summary in summaries]\n\n            # recursively summarize the summaries\n            return self.get_response(\n                query_str=query_str, text_chunks=summaries, **response_kwargs\n            )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/compact_and_refine.py",
    "filename": "compact_and_refine.py",
    "relpath": "response_synthesizers/compact_and_refine.py",
    "start_line": 1,
    "end_line": 57,
    "length": 57,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "aget_response",
      "get_response",
      "_make_compact_text_chunks"
    ],
    "chunk_class_names": [
      "CompactAndRefine"
    ],
    "document_function_names": [
      "aget_response",
      "get_response",
      "_make_compact_text_chunks"
    ],
    "document_class_names": [
      "CompactAndRefine"
    ],
    "content": "from typing import Any, List, Optional, Sequence\n\nfrom llama_index.core.prompts.prompt_utils import get_biggest_prompt\nfrom llama_index.core.response_synthesizers.refine import Refine\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass CompactAndRefine(Refine):\n    \"\"\"Refine responses across compact text chunks.\"\"\"\n\n    @dispatcher.span\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        compact_texts = self._make_compact_text_chunks(query_str, text_chunks)\n        return await super().aget_response(\n            query_str=query_str,\n            text_chunks=compact_texts,\n            prev_response=prev_response,\n            **response_kwargs,\n        )\n\n    @dispatcher.span\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        # TODO: This is a temporary fix - reason it's temporary is that\n        # the refine template does not account for size of previous answer.\n        new_texts = self._make_compact_text_chunks(query_str, text_chunks)\n        return super().get_response(\n            query_str=query_str,\n            text_chunks=new_texts,\n            prev_response=prev_response,\n            **response_kwargs,\n        )\n\n    def _make_compact_text_chunks(\n        self, query_str: str, text_chunks: Sequence[str]\n    ) -> List[str]:\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n        refine_template = self._refine_template.partial_format(query_str=query_str)\n\n        max_prompt = get_biggest_prompt([text_qa_template, refine_template])\n        return self._prompt_helper.repack(max_prompt, text_chunks, llm=self._llm)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/generation.py",
    "filename": "generation.py",
    "relpath": "response_synthesizers/generation.py",
    "start_line": 1,
    "end_line": 188,
    "length": 188,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aget_response",
      "get_response",
      "synthesize",
      "asynthesize"
    ],
    "chunk_class_names": [
      "Generation"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aget_response",
      "get_response",
      "synthesize",
      "asynthesize"
    ],
    "document_class_names": [
      "Generation"
    ],
    "content": "from typing import Any, List, Optional, Sequence\n\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.instrumentation.events.synthesis import (\n    SynthesizeStartEvent,\n    SynthesizeEndEvent,\n)\nimport llama_index.core.instrumentation as instrument\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_SIMPLE_INPUT_PROMPT\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.schema import (\n    MetadataMode,\n    NodeWithScore,\n    QueryBundle,\n    QueryType,\n)\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\n\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass Generation(BaseSynthesizer):\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        simple_template: Optional[BasePromptTemplate] = None,\n        streaming: bool = False,\n    ) -> None:\n        super().__init__(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            streaming=streaming,\n        )\n        self._input_prompt = simple_template or DEFAULT_SIMPLE_INPUT_PROMPT\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"simple_template\": self._input_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"simple_template\" in prompts:\n            self._input_prompt = prompts[\"simple_template\"]\n\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        # NOTE: ignore text chunks and previous response\n        del text_chunks\n\n        if not self._streaming:\n            return await self._llm.apredict(\n                self._input_prompt,\n                query_str=query_str,\n                **response_kwargs,\n            )\n        else:\n            return await self._llm.astream(\n                self._input_prompt,\n                query_str=query_str,\n                **response_kwargs,\n            )\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        # NOTE: ignore text chunks and previous response\n        del text_chunks\n\n        if not self._streaming:\n            return self._llm.predict(\n                self._input_prompt,\n                query_str=query_str,\n                **response_kwargs,\n            )\n        else:\n            return self._llm.stream(\n                self._input_prompt,\n                query_str=query_str,\n                **response_kwargs,\n            )\n\n    # NOTE: synthesize and asynthesize are copied from the base class,\n    #       but modified to return when zero nodes are provided\n\n    @dispatcher.span\n    def synthesize(\n        self,\n        query: QueryType,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TYPE:\n        dispatcher.event(\n            SynthesizeStartEvent(\n                query=query,\n            )\n        )\n\n        if isinstance(query, str):\n            query = QueryBundle(query_str=query)\n\n        with self._callback_manager.event(\n            CBEventType.SYNTHESIZE,\n            payload={EventPayload.QUERY_STR: query.query_str},\n        ) as event:\n            response_str = self.get_response(\n                query_str=query.query_str,\n                text_chunks=[\n                    n.node.get_content(metadata_mode=MetadataMode.LLM) for n in nodes\n                ],\n                **response_kwargs,\n            )\n\n            additional_source_nodes = additional_source_nodes or []\n            source_nodes = list(nodes) + list(additional_source_nodes)\n\n            response = self._prepare_response_output(response_str, source_nodes)\n\n            event.on_end(payload={EventPayload.RESPONSE: response})\n\n        dispatcher.event(\n            SynthesizeEndEvent(\n                query=query,\n                response=response,\n            )\n        )\n        return response\n\n    @dispatcher.span\n    async def asynthesize(\n        self,\n        query: QueryType,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TYPE:\n        dispatcher.event(\n            SynthesizeStartEvent(\n                query=query,\n            )\n        )\n\n        if isinstance(query, str):\n            query = QueryBundle(query_str=query)\n\n        with self._callback_manager.event(\n            CBEventType.SYNTHESIZE,\n            payload={EventPayload.QUERY_STR: query.query_str},\n        ) as event:\n            response_str = await self.aget_response(\n                query_str=query.query_str,\n                text_chunks=[\n                    n.node.get_content(metadata_mode=MetadataMode.LLM) for n in nodes\n                ],\n                **response_kwargs,\n            )\n\n            additional_source_nodes = additional_source_nodes or []\n            source_nodes = list(nodes) + list(additional_source_nodes)\n\n            response = self._prepare_response_output(response_str, source_nodes)\n\n            event.on_end(payload={EventPayload.RESPONSE: response})\n\n        dispatcher.event(\n            SynthesizeEndEvent(\n                query=query,\n                response=response,\n            )\n        )\n        return response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/factory.py",
    "filename": "factory.py",
    "relpath": "response_synthesizers/factory.py",
    "start_line": 1,
    "end_line": 151,
    "length": 151,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_response_synthesizer"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_response_synthesizer"
    ],
    "document_class_names": [],
    "content": "from typing import Callable, Optional, Type\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_REFINE_PROMPT_SEL,\n    DEFAULT_TEXT_QA_PROMPT_SEL,\n    DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.default_prompts import DEFAULT_SIMPLE_INPUT_PROMPT\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.response_synthesizers.accumulate import Accumulate\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.response_synthesizers.compact_and_accumulate import (\n    CompactAndAccumulate,\n)\nfrom llama_index.core.response_synthesizers.compact_and_refine import (\n    CompactAndRefine,\n)\nfrom llama_index.core.response_synthesizers.context_only import ContextOnly\nfrom llama_index.core.response_synthesizers.generation import Generation\nfrom llama_index.core.response_synthesizers.no_text import NoText\nfrom llama_index.core.response_synthesizers.refine import Refine\nfrom llama_index.core.response_synthesizers.simple_summarize import SimpleSummarize\nfrom llama_index.core.response_synthesizers.tree_summarize import TreeSummarize\nfrom llama_index.core.response_synthesizers.type import ResponseMode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.types import BasePydanticProgram\n\n\ndef get_response_synthesizer(\n    llm: Optional[LLM] = None,\n    prompt_helper: Optional[PromptHelper] = None,\n    text_qa_template: Optional[BasePromptTemplate] = None,\n    refine_template: Optional[BasePromptTemplate] = None,\n    summary_template: Optional[BasePromptTemplate] = None,\n    simple_template: Optional[BasePromptTemplate] = None,\n    response_mode: ResponseMode = ResponseMode.COMPACT,\n    callback_manager: Optional[CallbackManager] = None,\n    use_async: bool = False,\n    streaming: bool = False,\n    structured_answer_filtering: bool = False,\n    output_cls: Optional[Type[BaseModel]] = None,\n    program_factory: Optional[\n        Callable[[BasePromptTemplate], BasePydanticProgram]\n    ] = None,\n    verbose: bool = False,\n) -> BaseSynthesizer:\n    \"\"\"Get a response synthesizer.\"\"\"\n    text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT_SEL\n    refine_template = refine_template or DEFAULT_REFINE_PROMPT_SEL\n    simple_template = simple_template or DEFAULT_SIMPLE_INPUT_PROMPT\n    summary_template = summary_template or DEFAULT_TREE_SUMMARIZE_PROMPT_SEL\n\n    callback_manager = callback_manager or Settings.callback_manager\n    llm = llm or Settings.llm\n    prompt_helper = (\n        prompt_helper\n        or Settings._prompt_helper\n        or PromptHelper.from_llm_metadata(\n            llm.metadata,\n        )\n    )\n\n    if response_mode == ResponseMode.REFINE:\n        return Refine(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            structured_answer_filtering=structured_answer_filtering,\n            program_factory=program_factory,\n            verbose=verbose,\n        )\n    elif response_mode == ResponseMode.COMPACT:\n        return CompactAndRefine(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            structured_answer_filtering=structured_answer_filtering,\n            program_factory=program_factory,\n            verbose=verbose,\n        )\n    elif response_mode == ResponseMode.TREE_SUMMARIZE:\n        return TreeSummarize(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            summary_template=summary_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            use_async=use_async,\n            verbose=verbose,\n        )\n    elif response_mode == ResponseMode.SIMPLE_SUMMARIZE:\n        return SimpleSummarize(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            streaming=streaming,\n        )\n    elif response_mode == ResponseMode.GENERATION:\n        return Generation(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            simple_template=simple_template,\n            streaming=streaming,\n        )\n    elif response_mode == ResponseMode.ACCUMULATE:\n        return Accumulate(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            use_async=use_async,\n        )\n    elif response_mode == ResponseMode.COMPACT_ACCUMULATE:\n        return CompactAndAccumulate(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            use_async=use_async,\n        )\n    elif response_mode == ResponseMode.NO_TEXT:\n        return NoText(\n            callback_manager=callback_manager,\n            streaming=streaming,\n        )\n    elif response_mode == ResponseMode.CONTEXT_ONLY:\n        return ContextOnly(\n            callback_manager=callback_manager,\n            streaming=streaming,\n        )\n    else:\n        raise ValueError(f\"Unknown mode: {response_mode}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/no_text.py",
    "filename": "no_text.py",
    "relpath": "response_synthesizers/no_text.py",
    "start_line": 1,
    "end_line": 30,
    "length": 30,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompts",
      "_update_prompts",
      "get_response",
      "aget_response"
    ],
    "chunk_class_names": [
      "NoText"
    ],
    "document_function_names": [
      "_get_prompts",
      "_update_prompts",
      "get_response",
      "aget_response"
    ],
    "document_class_names": [
      "NoText"
    ],
    "content": "from typing import Any, Sequence\n\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\n\n\nclass NoText(BaseSynthesizer):\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        return \"\"\n\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        return \"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/simple_summarize.py",
    "filename": "simple_summarize.py",
    "relpath": "response_synthesizers/simple_summarize.py",
    "start_line": 1,
    "end_line": 109,
    "length": 109,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aget_response",
      "get_response"
    ],
    "chunk_class_names": [
      "SimpleSummarize"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aget_response",
      "get_response"
    ],
    "document_class_names": [
      "SimpleSummarize"
    ],
    "content": "from typing import Any, Generator, Optional, Sequence, cast\n\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.prompt_helper import PromptHelper\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompt_selectors import (\n    DEFAULT_TEXT_QA_PROMPT_SEL,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\n\n\nclass SimpleSummarize(BaseSynthesizer):\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        prompt_helper: Optional[PromptHelper] = None,\n        text_qa_template: Optional[BasePromptTemplate] = None,\n        streaming: bool = False,\n    ) -> None:\n        super().__init__(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            streaming=streaming,\n        )\n        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT_SEL\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"text_qa_template\": self._text_qa_template}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"text_qa_template\" in prompts:\n            self._text_qa_template = prompts[\"text_qa_template\"]\n\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n        single_text_chunk = \"\\n\".join(text_chunks)\n        truncated_chunks = self._prompt_helper.truncate(\n            prompt=text_qa_template,\n            text_chunks=[single_text_chunk],\n            llm=self._llm,\n        )\n\n        response: RESPONSE_TEXT_TYPE\n        if not self._streaming:\n            response = await self._llm.apredict(\n                text_qa_template,\n                context_str=truncated_chunks,\n                **response_kwargs,\n            )\n        else:\n            response = await self._llm.astream(\n                text_qa_template,\n                context_str=truncated_chunks,\n                **response_kwargs,\n            )\n\n        if isinstance(response, str):\n            response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)\n\n        return response\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n        single_text_chunk = \"\\n\".join(text_chunks)\n        truncated_chunks = self._prompt_helper.truncate(\n            prompt=text_qa_template,\n            text_chunks=[single_text_chunk],\n            llm=self._llm,\n        )\n\n        response: RESPONSE_TEXT_TYPE\n        if not self._streaming:\n            response = self._llm.predict(\n                text_qa_template,\n                context_str=truncated_chunks,\n                **kwargs,\n            )\n        else:\n            response = self._llm.stream(\n                text_qa_template,\n                context_str=truncated_chunks,\n                **kwargs,\n            )\n\n        if isinstance(response, str):\n            response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)\n\n        return response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/context_only.py",
    "filename": "context_only.py",
    "relpath": "response_synthesizers/context_only.py",
    "start_line": 1,
    "end_line": 30,
    "length": 30,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompts",
      "_update_prompts",
      "get_response",
      "aget_response"
    ],
    "chunk_class_names": [
      "ContextOnly"
    ],
    "document_function_names": [
      "_get_prompts",
      "_update_prompts",
      "get_response",
      "aget_response"
    ],
    "document_class_names": [
      "ContextOnly"
    ],
    "content": "from typing import Any, Sequence\n\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.response_synthesizers.base import BaseSynthesizer\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\n\n\nclass ContextOnly(BaseSynthesizer):\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        return \"\\n\\n\".join(text_chunks)\n\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        return \"\\n\\n\".join(text_chunks)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response_synthesizers/compact_and_accumulate.py",
    "filename": "compact_and_accumulate.py",
    "relpath": "response_synthesizers/compact_and_accumulate.py",
    "start_line": 1,
    "end_line": 55,
    "length": 55,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "aget_response",
      "get_response"
    ],
    "chunk_class_names": [
      "CompactAndAccumulate"
    ],
    "document_function_names": [
      "aget_response",
      "get_response"
    ],
    "document_class_names": [
      "CompactAndAccumulate"
    ],
    "content": "from typing import Any, Sequence\n\nfrom llama_index.core.response_synthesizers import Accumulate\nfrom llama_index.core.types import RESPONSE_TEXT_TYPE\nfrom llama_index.core.utils import temp_set_attrs\n\n\nclass CompactAndAccumulate(Accumulate):\n    \"\"\"Accumulate responses across compact text chunks.\"\"\"\n\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        separator: str = \"\\n---------------------\\n\",\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n\n        with temp_set_attrs(self._prompt_helper):\n            new_texts = self._prompt_helper.repack(\n                text_qa_template, text_chunks, llm=self._llm\n            )\n\n            return await super().aget_response(\n                query_str=query_str,\n                text_chunks=new_texts,\n                separator=separator,\n                **response_kwargs,\n            )\n\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        separator: str = \"\\n---------------------\\n\",\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)\n\n        with temp_set_attrs(self._prompt_helper):\n            new_texts = self._prompt_helper.repack(\n                text_qa_template, text_chunks, llm=self._llm\n            )\n\n            return super().get_response(\n                query_str=query_str,\n                text_chunks=new_texts,\n                separator=separator,\n                **response_kwargs,\n            )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/download/pack.py",
    "filename": "pack.py",
    "relpath": "download/pack.py",
    "start_line": 1,
    "end_line": 149,
    "length": 149,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "download_module_and_reqs",
      "download_llama_pack_template",
      "track_download"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "download_module_and_reqs",
      "download_llama_pack_template",
      "track_download"
    ],
    "document_class_names": [],
    "content": "\"\"\"Download llama-pack as template.\"\"\"\n\nimport logging\nimport os\nimport subprocess\nimport sys\nfrom importlib import util\nfrom pathlib import Path\nfrom typing import Any, Optional, Union\n\nimport requests\n\nfrom llama_index.core.download.utils import (\n    ChangeDirectory,\n    get_file_content,\n    initialize_directory,\n    get_source_files_recursive,\n)\n\nLLAMA_PACKS_CONTENTS_URL = (\n    \"https://raw.githubusercontent.com/run-llama/llama_index/main/llama-index-packs\"\n)\nLLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama_index/tree/main\"\n)\nPY_NAMESPACE = \"llama_index/packs\"\n\nPATH_TYPE = Union[str, Path]\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\n\nlogger = logging.getLogger(__name__)\n\n\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    package: str,\n    sub_module: str,\n    refresh_cache: bool = False,\n) -> None:\n    \"\"\"Load module.\"\"\"\n    if isinstance(local_dir_path, str):\n        local_dir_path = Path(local_dir_path)\n\n    module_path = f\"{local_dir_path}/{PY_NAMESPACE}/{sub_module}\"\n\n    if refresh_cache or not os.path.exists(module_path):\n        os.makedirs(module_path, exist_ok=True)\n\n        # download all source files\n        source_files = get_source_files_recursive(\n            str(remote_source_dir_path),\n            f\"/llama-index-packs/{package}/{PY_NAMESPACE}/{sub_module}\",\n        )\n\n        for source_file in source_files:\n            source_file_raw_content, _ = get_file_content(\n                str(remote_dir_path),\n                f\"{source_file}\",\n            )\n            local_source_file_path = (\n                f\"{local_dir_path}/{'/'.join(source_file.split('/')[2:])}\"\n            )\n            # ensure parent dir of file exists\n            Path(local_source_file_path).parent.absolute().mkdir(\n                parents=True, exist_ok=True\n            )\n            with open(local_source_file_path, \"w\") as f:\n                f.write(source_file_raw_content)\n\n    # pyproject.toml and README\n    pyproject_toml_path = f\"{local_dir_path}/pyproject.toml\"\n    readme_path = (\n        f\"{local_dir_path}/README.md\"  # needed to install deps from pyproject.toml\n    )\n\n    if not os.path.exists(pyproject_toml_path):\n        # NOTE: need to check the status code\n        response_txt, status_code = get_file_content(\n            str(remote_dir_path), f\"/{package}/pyproject.toml\"\n        )\n        if status_code == 200:\n            with open(pyproject_toml_path, \"w\") as f:\n                f.write(response_txt)\n\n    if not os.path.exists(readme_path):\n        with open(readme_path, \"w\") as f:\n            f.write(\n                \"DO NOT DELETE\\nThis readme file is needed to install from pyproject.toml.\"\n            )\n\n    # Install dependencies\n    if os.path.exists(pyproject_toml_path):\n        with ChangeDirectory(str(local_dir_path)):\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \".\"])\n\n\ndef download_llama_pack_template(\n    new_install_parent: str,\n    llama_pack_class: str,\n    llama_pack_url: str = LLAMA_PACKS_CONTENTS_URL,\n    llama_pack_source_files_dir_path: str = LLAMA_PACKS_SOURCE_FILES_GITHUB_TREE_URL,\n    refresh_cache: bool = False,\n    custom_dir: Optional[str] = None,\n    custom_path: Optional[str] = None,\n    base_file_name: str = \"__init__.py\",\n) -> Any:\n    # create directory / get path\n    dirpath = initialize_directory(custom_path=custom_path, custom_dir=custom_dir)\n\n    sub_module = new_install_parent.replace(\"llama-index-packs-\", \"\").replace(\"-\", \"_\")\n\n    # download the module, install requirements\n    download_module_and_reqs(\n        local_dir_path=dirpath,\n        remote_dir_path=llama_pack_url,\n        remote_source_dir_path=llama_pack_source_files_dir_path,\n        package=new_install_parent,\n        sub_module=sub_module,\n        refresh_cache=refresh_cache,\n    )\n\n    # loads the module into memory\n    path = f\"{dirpath}/{PY_NAMESPACE}/{sub_module}/{base_file_name}\"\n    spec = util.spec_from_file_location(\"llama_index.packs._custom\", location=path)\n    if spec is None:\n        raise ValueError(f\"Could not find file: {path}.\")\n\n    module = util.module_from_spec(spec)\n    spec.loader.exec_module(module)  # type: ignore\n\n    return getattr(module, llama_pack_class)\n\n\ndef track_download(module_class: str, module_type: str) -> None:\n    \"\"\"Tracks number of downloads via Llamahub proxy.\n\n    Args:\n        module_class: The name of the llama module being downloaded, e.g.,`GmailOpenAIAgentPack`.\n        module_type: Can be \"loader\", \"tool\", \"llamapack\", or \"datasets\"\n    \"\"\"\n    try:\n        requests.post(\n            LLAMAHUB_ANALYTICS_PROXY_SERVER,\n            json={\"type\": module_type, \"plugin\": module_class},\n        )\n    except Exception as e:\n        logger.info(f\"Error tracking downloads for {module_class} : {e}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/download/integration.py",
    "filename": "integration.py",
    "relpath": "download/integration.py",
    "start_line": 1,
    "end_line": 27,
    "length": 27,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "pip_install",
      "download_integration"
    ],
    "chunk_class_names": [
      "by"
    ],
    "document_function_names": [
      "pip_install",
      "download_integration"
    ],
    "document_class_names": [
      "by"
    ],
    "content": "\"\"\"Download pypi package.\"\"\"\n\nimport importlib\nimport subprocess\nimport sys\nfrom typing import Any\n\n\ndef pip_install(package: str) -> None:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n\ndef download_integration(module_str: str, module_import_str: str, cls_name: str) -> Any:\n    \"\"\"Returns an integration class by first pip installing its parent module.\"\"\"\n    try:\n        pip_install(module_str)  # this works for any integration not just packs\n    except Exception as e:\n        raise Exception(f\"Failed to pip install `{module_str}`\") from e\n\n    try:\n        module_spec = importlib.util.find_spec(module_import_str)\n        module = importlib.util.module_from_spec(module_spec)  # type: ignore\n        module_spec.loader.exec_module(module)  # type: ignore\n        pack_cls = getattr(module, cls_name)\n    except ImportError as e:\n        raise ImportError(f\"Unable to import {cls_name}\") from e\n    return pack_cls"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/download/utils.py",
    "filename": "utils.py",
    "relpath": "download/utils.py",
    "start_line": 1,
    "end_line": 152,
    "length": 152,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_file_content",
      "get_file_content_bytes",
      "get_exports",
      "rewrite_exports",
      "initialize_directory",
      "get_source_files_list",
      "recursive_tree_traverse",
      "get_source_files_recursive",
      "__init__",
      "__enter__",
      "__exit__"
    ],
    "chunk_class_names": [
      "names",
      "names",
      "names",
      "ChangeDirectory"
    ],
    "document_function_names": [
      "get_file_content",
      "get_file_content_bytes",
      "get_exports",
      "rewrite_exports",
      "initialize_directory",
      "get_source_files_list",
      "recursive_tree_traverse",
      "get_source_files_recursive",
      "__init__",
      "__enter__",
      "__exit__"
    ],
    "document_class_names": [
      "names",
      "names",
      "names",
      "ChangeDirectory"
    ],
    "content": "import os\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple\n\nimport requests\n\n\ndef get_file_content(url: str, path: str) -> Tuple[str, int]:\n    \"\"\"Get the content of a file from the GitHub REST API.\"\"\"\n    resp = requests.get(url + path)\n    return resp.text, resp.status_code\n\n\ndef get_file_content_bytes(url: str, path: str) -> Tuple[bytes, int]:\n    \"\"\"Get the content of a file from the GitHub REST API.\"\"\"\n    resp = requests.get(url + path)\n    return resp.content, resp.status_code\n\n\ndef get_exports(raw_content: str) -> List:\n    \"\"\"Read content of a Python file and returns a list of exported class names.\n\n    For example:\n    ```python\n    from .a import A\n    from .b import B\n\n    __all__ = [\"A\", \"B\"]\n    ```\n    will return `[\"A\", \"B\"]`.\n\n    Args:\n        - raw_content: The content of a Python file as a string.\n\n    Returns:\n        A list of exported class names.\n\n    \"\"\"\n    exports = []\n    for line in raw_content.splitlines():\n        line = line.strip()\n        if line.startswith(\"__all__\"):\n            exports = line.split(\"=\")[1].strip().strip(\"[\").strip(\"]\").split(\",\")\n            exports = [export.strip().strip(\"'\").strip('\"') for export in exports]\n    return exports\n\n\ndef rewrite_exports(exports: List[str], dirpath: str) -> None:\n    \"\"\"Write the `__all__` variable to the `__init__.py` file in the modules dir.\n\n    Removes the line that contains `__all__` and appends a new line with the updated\n    `__all__` variable.\n\n    Args:\n        - exports: A list of exported class names.\n\n    \"\"\"\n    init_path = f\"{dirpath}/__init__.py\"\n    with open(init_path) as f:\n        lines = f.readlines()\n    with open(init_path, \"w\") as f:\n        for line in lines:\n            line = line.strip()\n            if line.startswith(\"__all__\"):\n                continue\n            f.write(line + os.linesep)\n        f.write(f\"__all__ = {list(set(exports))}\" + os.linesep)\n\n\ndef initialize_directory(\n    custom_path: Optional[str] = None, custom_dir: Optional[str] = None\n) -> Path:\n    \"\"\"Initialize directory.\"\"\"\n    if custom_path is not None and custom_dir is not None:\n        raise ValueError(\n            \"You cannot specify both `custom_path` and `custom_dir` at the same time.\"\n        )\n\n    custom_dir = custom_dir or \"llamadatasets\"\n    if custom_path is not None:\n        dirpath = Path(custom_path)\n    else:\n        dirpath = Path(__file__).parent / custom_dir\n    if not os.path.exists(dirpath):\n        # Create a new directory because it does not exist\n        os.makedirs(dirpath)\n\n    return dirpath\n\n\ndef get_source_files_list(source_tree_url: str, path: str) -> List[str]:\n    \"\"\"Get the list of source files to download.\"\"\"\n    resp = requests.get(\n        source_tree_url + path + \"?recursive=1\", headers={\"Accept\": \"application/json\"}\n    )\n    payload = resp.json()[\"payload\"]\n    return [item[\"name\"] for item in payload[\"tree\"][\"items\"]]\n\n\ndef recursive_tree_traverse(\n    tree_urls: List[str], acc: List[str], source_tree_url: str\n) -> List[str]:\n    \"\"\"Recursively traversge Github trees to get all file paths in a folder.\"\"\"\n    if not tree_urls:\n        return acc\n    else:\n        url = tree_urls[0]\n\n        try:\n            res = requests.get(url, headers={\"Accept\": \"application/json\"})\n            tree_elements = res.json()[\"payload\"][\"tree\"][\"items\"]\n        except Exception:\n            raise ValueError(\"Failed to traverse github tree source.\")\n\n        new_trees = [\n            source_tree_url + \"/\" + el[\"path\"]\n            for el in tree_elements\n            if el[\"contentType\"] == \"directory\"\n        ]\n\n        acc += [\n            el[\"path\"].replace(\"llama-index-packs/\", \"/\")\n            for el in tree_elements\n            if el[\"contentType\"] == \"file\"\n        ]\n\n        return recursive_tree_traverse(\n            tree_urls=tree_urls[1:] + new_trees,\n            acc=acc,\n            source_tree_url=source_tree_url,\n        )\n\n\ndef get_source_files_recursive(source_tree_url: str, path: str) -> List[str]:\n    \"\"\"Get source files of a Github folder recursively.\"\"\"\n    initial_url = source_tree_url + path + \"?recursive=1\"\n    initial_tree_urls = [initial_url]\n    return recursive_tree_traverse(initial_tree_urls, [], source_tree_url)\n\n\nclass ChangeDirectory:\n    \"\"\"Context manager for changing the current working directory.\"\"\"\n\n    def __init__(self, new_path: str):\n        self.new_path = os.path.expanduser(new_path)\n\n    def __enter__(self) -> None:\n        self.saved_path = os.getcwd()\n        os.chdir(self.new_path)\n\n    def __exit__(self, etype: object, value: object, traceback: object) -> None:\n        os.chdir(self.saved_path)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/download/dataset.py",
    "filename": "dataset.py",
    "relpath": "download/dataset.py",
    "start_line": 1,
    "end_line": 260,
    "length": 260,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_resolve_dataset_file_name",
      "get_dataset_info",
      "download_dataset_and_source_files",
      "download_llama_dataset"
    ],
    "chunk_class_names": [
      "in",
      "not",
      "name",
      "name",
      "you"
    ],
    "document_function_names": [
      "_resolve_dataset_file_name",
      "get_dataset_info",
      "download_dataset_and_source_files",
      "download_llama_dataset"
    ],
    "document_class_names": [
      "in",
      "not",
      "name",
      "name",
      "you"
    ],
    "content": "\"\"\"Download.\"\"\"\n\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport tqdm\nfrom llama_index.core.download.utils import (\n    get_file_content,\n    get_file_content_bytes,\n    get_source_files_list,\n    initialize_directory,\n)\n\nLLAMA_INDEX_CONTENTS_URL = (\n    f\"https://raw.githubusercontent.com/run-llama/llama_index/main\"\n)\nLLAMA_DATASETS_PATH = \"/llama-datasets\"\nLLAMA_DATASETS_URL = LLAMA_INDEX_CONTENTS_URL + LLAMA_DATASETS_PATH\n\nLLAMA_DATASETS_LFS_URL = (\n    f\"https://media.githubusercontent.com/media/run-llama/llama-datasets/main\"\n)\n\nLLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL = (\n    \"https://github.com/run-llama/llama-datasets/tree/main\"\n)\nLLAMA_SOURCE_FILES_PATH = \"source_files\"\n\nDATASET_CLASS_FILENAME_REGISTRY = {\n    \"LabelledRagDataset\": \"rag_dataset.json\",\n    \"LabeledRagDataset\": \"rag_dataset.json\",\n    \"LabelledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabeledPairwiseEvaluatorDataset\": \"pairwise_evaluator_dataset.json\",\n    \"LabelledEvaluatorDataset\": \"evaluator_dataset.json\",\n    \"LabeledEvaluatorDataset\": \"evaluator_dataset.json\",\n}\n\n\nPATH_TYPE = Union[str, Path]\n\n\ndef _resolve_dataset_file_name(class_name: str) -> str:\n    \"\"\"Resolve filename based on dataset class.\"\"\"\n    try:\n        return DATASET_CLASS_FILENAME_REGISTRY[class_name]\n    except KeyError as err:\n        raise ValueError(\"Invalid dataset filename.\") from err\n\n\ndef get_dataset_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    remote_source_dir_path: PATH_TYPE,\n    dataset_class: str,\n    refresh_cache: bool = False,\n    library_path: str = \"library.json\",\n    source_files_path: str = \"source_files\",\n    disable_library_cache: bool = False,\n) -> Dict:\n    \"\"\"Get dataset info.\"\"\"\n    if isinstance(local_dir_path, str):\n        local_dir_path = Path(local_dir_path)\n\n    local_library_path = f\"{local_dir_path}/{library_path}\"\n    dataset_id = None\n    source_files = []\n\n    # Check cache first\n    if not refresh_cache and os.path.exists(local_library_path):\n        with open(local_library_path) as f:\n            library = json.load(f)\n        if dataset_class in library:\n            dataset_id = library[dataset_class][\"id\"]\n            source_files = library[dataset_class].get(\"source_files\", [])\n\n    # Fetch up-to-date library from remote repo if dataset_id not found\n    if dataset_id is None:\n        library_raw_content, _ = get_file_content(\n            str(remote_dir_path), f\"/{library_path}\"\n        )\n        library = json.loads(library_raw_content)\n        if dataset_class not in library:\n            raise ValueError(\"Loader class name not found in library\")\n\n        dataset_id = library[dataset_class][\"id\"]\n\n        # get data card\n        raw_card_content, _ = get_file_content(\n            str(remote_dir_path), f\"/{dataset_id}/card.json\"\n        )\n        card = json.loads(raw_card_content)\n        dataset_class_name = card[\"className\"]\n\n        source_files = []\n        if dataset_class_name == \"LabelledRagDataset\":\n            source_files = get_source_files_list(\n                str(remote_source_dir_path),\n                f\"/llama_datasets/{dataset_id}/{source_files_path}\",\n            )\n\n        # create cache dir if needed\n        local_library_dir = os.path.dirname(local_library_path)\n        if not disable_library_cache:\n            if not os.path.exists(local_library_dir):\n                os.makedirs(local_library_dir)\n\n            # Update cache\n            with open(local_library_path, \"w\") as f:\n                f.write(library_raw_content)\n\n    if dataset_id is None:\n        raise ValueError(\"Dataset class name not found in library\")\n\n    return {\n        \"dataset_id\": dataset_id,\n        \"dataset_class_name\": dataset_class_name,\n        \"source_files\": source_files,\n    }\n\n\ndef download_dataset_and_source_files(\n    local_dir_path: PATH_TYPE,\n    remote_lfs_dir_path: PATH_TYPE,\n    source_files_dir_path: PATH_TYPE,\n    dataset_id: str,\n    dataset_class_name: str,\n    source_files: List[str],\n    refresh_cache: bool = False,\n    base_file_name: str = \"rag_dataset.json\",\n    override_path: bool = False,\n    show_progress: bool = False,\n) -> None:\n    \"\"\"Download dataset and source files.\"\"\"\n    if isinstance(local_dir_path, str):\n        local_dir_path = Path(local_dir_path)\n\n    if override_path:\n        module_path = str(local_dir_path)\n    else:\n        module_path = f\"{local_dir_path}/{dataset_id}\"\n\n    if refresh_cache or not os.path.exists(module_path):\n        os.makedirs(module_path, exist_ok=True)\n\n        base_file_name = _resolve_dataset_file_name(dataset_class_name)\n\n        dataset_raw_content, _ = get_file_content(\n            str(remote_lfs_dir_path), f\"/llama_datasets/{dataset_id}/{base_file_name}\"\n        )\n\n        with open(f\"{module_path}/{base_file_name}\", \"w\") as f:\n            f.write(dataset_raw_content)\n\n        # Get content of source files\n        if dataset_class_name == \"LabelledRagDataset\":\n            os.makedirs(f\"{module_path}/{source_files_dir_path}\", exist_ok=True)\n            if show_progress:\n                source_files_iterator = tqdm.tqdm(source_files)\n            else:\n                source_files_iterator = source_files\n            for source_file in source_files_iterator:\n                if \".pdf\" in source_file:\n                    source_file_raw_content_bytes, _ = get_file_content_bytes(\n                        str(remote_lfs_dir_path),\n                        f\"/llama_datasets/{dataset_id}/{source_files_dir_path}/{source_file}\",\n                    )\n                    with open(\n                        f\"{module_path}/{source_files_dir_path}/{source_file}\", \"wb\"\n                    ) as f:\n                        f.write(source_file_raw_content_bytes)\n                else:\n                    source_file_raw_content, _ = get_file_content(\n                        str(remote_lfs_dir_path),\n                        f\"/llama_datasets/{dataset_id}/{source_files_dir_path}/{source_file}\",\n                    )\n                    with open(\n                        f\"{module_path}/{source_files_dir_path}/{source_file}\", \"w\"\n                    ) as f:\n                        f.write(source_file_raw_content)\n\n\ndef download_llama_dataset(\n    dataset_class: str,\n    llama_datasets_url: str = LLAMA_DATASETS_URL,\n    llama_datasets_lfs_url: str = LLAMA_DATASETS_LFS_URL,\n    llama_datasets_source_files_tree_url: str = LLAMA_DATASETS_SOURCE_FILES_GITHUB_TREE_URL,\n    refresh_cache: bool = False,\n    custom_dir: Optional[str] = None,\n    custom_path: Optional[str] = None,\n    source_files_dirpath: str = LLAMA_SOURCE_FILES_PATH,\n    library_path: str = \"llama_datasets/library.json\",\n    disable_library_cache: bool = False,\n    override_path: bool = False,\n    show_progress: bool = False,\n) -> Any:\n    \"\"\"\n    Download a module from LlamaHub.\n\n    Can be a loader, tool, pack, or more.\n\n    Args:\n        loader_class: The name of the llama module class you want to download,\n            such as `GmailOpenAIAgentPack`.\n        refresh_cache: If true, the local cache will be skipped and the\n            loader will be fetched directly from the remote repo.\n        custom_dir: Custom dir name to download loader into (under parent folder).\n        custom_path: Custom dirpath to download loader into.\n        library_path: File name of the library file.\n        use_gpt_index_import: If true, the loader files will use\n            llama_index as the base dependency. By default (False),\n            the loader files use llama_index as the base dependency.\n            NOTE: this is a temporary workaround while we fully migrate all usages\n            to llama_index.core.\n        is_dataset: whether or not downloading a LlamaDataset\n\n    Returns:\n        A Loader, A Pack, An Agent, or A Dataset\n    \"\"\"\n    # create directory / get path\n    dirpath = initialize_directory(custom_path=custom_path, custom_dir=custom_dir)\n\n    # fetch info from library.json file\n    dataset_info = get_dataset_info(\n        local_dir_path=dirpath,\n        remote_dir_path=llama_datasets_url,\n        remote_source_dir_path=llama_datasets_source_files_tree_url,\n        dataset_class=dataset_class,\n        refresh_cache=refresh_cache,\n        library_path=library_path,\n        disable_library_cache=disable_library_cache,\n    )\n    dataset_id = dataset_info[\"dataset_id\"]\n    source_files = dataset_info[\"source_files\"]\n    dataset_class_name = dataset_info[\"dataset_class_name\"]\n\n    dataset_filename = _resolve_dataset_file_name(dataset_class_name)\n\n    download_dataset_and_source_files(\n        local_dir_path=dirpath,\n        remote_lfs_dir_path=llama_datasets_lfs_url,\n        source_files_dir_path=source_files_dirpath,\n        dataset_id=dataset_id,\n        dataset_class_name=dataset_class_name,\n        source_files=source_files,\n        refresh_cache=refresh_cache,\n        override_path=override_path,\n        show_progress=show_progress,\n    )\n\n    if override_path:\n        module_path = str(dirpath)\n    else:\n        module_path = f\"{dirpath}/{dataset_id}\"\n\n    return (\n        f\"{module_path}/{dataset_filename}\",\n        f\"{module_path}/{LLAMA_SOURCE_FILES_PATH}\",\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/download/module.py",
    "filename": "module.py",
    "relpath": "download/module.py",
    "start_line": 1,
    "end_line": 274,
    "length": 274,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_module_info",
      "download_module_and_reqs",
      "download_llama_module",
      "track_download"
    ],
    "chunk_class_names": [
      "MODULE_TYPE",
      "in",
      "not",
      "name",
      "name",
      "you"
    ],
    "document_function_names": [
      "get_module_info",
      "download_module_and_reqs",
      "download_llama_module",
      "track_download"
    ],
    "document_class_names": [
      "MODULE_TYPE",
      "in",
      "not",
      "name",
      "name",
      "you"
    ],
    "content": "\"\"\"Download.\"\"\"\n\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nfrom enum import Enum\nfrom importlib import util\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport requests\nfrom llama_index.core.download.utils import (\n    get_exports,\n    get_file_content,\n    initialize_directory,\n    rewrite_exports,\n)\n\nLLAMA_HUB_CONTENTS_URL = f\"https://raw.githubusercontent.com/run-llama/llama-hub/main\"\nLLAMA_HUB_PATH = \"/llama_hub\"\nLLAMA_HUB_URL = LLAMA_HUB_CONTENTS_URL + LLAMA_HUB_PATH\n\nPATH_TYPE = Union[str, Path]\n\nlogger = logging.getLogger(__name__)\nLLAMAHUB_ANALYTICS_PROXY_SERVER = \"https://llamahub.ai/api/analytics/downloads\"\n\n\nclass MODULE_TYPE(str, Enum):\n    LOADER = \"loader\"\n    TOOL = \"tool\"\n    LLAMAPACK = \"llamapack\"\n    DATASETS = \"datasets\"\n\n\ndef get_module_info(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    module_class: str,\n    refresh_cache: bool = False,\n    library_path: str = \"library.json\",\n    disable_library_cache: bool = False,\n) -> Dict:\n    \"\"\"Get module info.\"\"\"\n    if isinstance(local_dir_path, str):\n        local_dir_path = Path(local_dir_path)\n\n    local_library_path = f\"{local_dir_path}/{library_path}\"\n    module_id = None  # e.g. `web/simple_web`\n    extra_files = []  # e.g. `web/simple_web/utils.py`\n\n    # Check cache first\n    if not refresh_cache and os.path.exists(local_library_path):\n        with open(local_library_path) as f:\n            library = json.load(f)\n        if module_class in library:\n            module_id = library[module_class][\"id\"]\n            extra_files = library[module_class].get(\"extra_files\", [])\n\n    # Fetch up-to-date library from remote repo if module_id not found\n    if module_id is None:\n        library_raw_content, _ = get_file_content(\n            str(remote_dir_path), f\"/{library_path}\"\n        )\n        library = json.loads(library_raw_content)\n        if module_class not in library:\n            raise ValueError(\"Loader class name not found in library\")\n\n        module_id = library[module_class][\"id\"]\n        extra_files = library[module_class].get(\"extra_files\", [])\n\n        # create cache dir if needed\n        local_library_dir = os.path.dirname(local_library_path)\n        if not disable_library_cache:\n            if not os.path.exists(local_library_dir):\n                os.makedirs(local_library_dir)\n\n            # Update cache\n            with open(local_library_path, \"w\") as f:\n                f.write(library_raw_content)\n\n    if module_id is None:\n        raise ValueError(\"Loader class name not found in library\")\n\n    return {\n        \"module_id\": module_id,\n        \"extra_files\": extra_files,\n    }\n\n\ndef download_module_and_reqs(\n    local_dir_path: PATH_TYPE,\n    remote_dir_path: PATH_TYPE,\n    module_id: str,\n    extra_files: List[str],\n    refresh_cache: bool = False,\n    use_gpt_index_import: bool = False,\n    base_file_name: str = \"base.py\",\n    override_path: bool = False,\n) -> None:\n    \"\"\"Load module.\"\"\"\n    if isinstance(local_dir_path, str):\n        local_dir_path = Path(local_dir_path)\n\n    if override_path:\n        module_path = str(local_dir_path)\n    else:\n        module_path = f\"{local_dir_path}/{module_id}\"\n\n    if refresh_cache or not os.path.exists(module_path):\n        os.makedirs(module_path, exist_ok=True)\n\n        basepy_raw_content, _ = get_file_content(\n            str(remote_dir_path), f\"/{module_id}/{base_file_name}\"\n        )\n        if use_gpt_index_import:\n            basepy_raw_content = basepy_raw_content.replace(\n                \"import llama_index.core\", \"import llama_index.core\"\n            )\n            basepy_raw_content = basepy_raw_content.replace(\n                \"from llama_index\", \"from llama_index\"\n            )\n\n        with open(f\"{module_path}/{base_file_name}\", \"w\") as f:\n            f.write(basepy_raw_content)\n\n    # Get content of extra files if there are any\n    # and write them under the loader directory\n    for extra_file in extra_files:\n        extra_file_raw_content, _ = get_file_content(\n            str(remote_dir_path), f\"/{module_id}/{extra_file}\"\n        )\n        # If the extra file is an __init__.py file, we need to\n        # add the exports to the __init__.py file in the modules directory\n        if extra_file == \"__init__.py\":\n            loader_exports = get_exports(extra_file_raw_content)\n            existing_exports = []\n            init_file_path = local_dir_path / \"__init__.py\"\n            # if the __init__.py file do not exists, we need to create it\n            mode = \"a+\" if not os.path.exists(init_file_path) else \"r+\"\n            with open(init_file_path, mode) as f:\n                f.write(f\"from .{module_id} import {', '.join(loader_exports)}\")\n                existing_exports = get_exports(f.read())\n            rewrite_exports(existing_exports + loader_exports, str(local_dir_path))\n\n        with open(f\"{module_path}/{extra_file}\", \"w\") as f:\n            f.write(extra_file_raw_content)\n\n    # install requirements\n    requirements_path = f\"{local_dir_path}/requirements.txt\"\n\n    if not os.path.exists(requirements_path):\n        # NOTE: need to check the status code\n        response_txt, status_code = get_file_content(\n            str(remote_dir_path), f\"/{module_id}/requirements.txt\"\n        )\n        if status_code == 200:\n            with open(requirements_path, \"w\") as f:\n                f.write(response_txt)\n\n    # Install dependencies if there are any and not already installed\n    if os.path.exists(requirements_path):\n        import pkg_resources\n        from pkg_resources import DistributionNotFound\n\n        try:\n            requirements = pkg_resources.parse_requirements(\n                Path(requirements_path).open()\n            )\n            pkg_resources.require([str(r) for r in requirements])\n        except DistributionNotFound:\n            subprocess.check_call(\n                [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path]\n            )\n\n\ndef download_llama_module(\n    module_class: str,\n    llama_hub_url: str = LLAMA_HUB_URL,\n    refresh_cache: bool = False,\n    custom_dir: Optional[str] = None,\n    custom_path: Optional[str] = None,\n    library_path: str = \"library.json\",\n    base_file_name: str = \"base.py\",\n    use_gpt_index_import: bool = False,\n    disable_library_cache: bool = False,\n    override_path: bool = False,\n    skip_load: bool = False,\n) -> Any:\n    \"\"\"Download a module from LlamaHub.\n\n    Can be a loader, tool, pack, or more.\n\n    Args:\n        loader_class: The name of the llama module class you want to download,\n            such as `GmailOpenAIAgentPack`.\n        refresh_cache: If true, the local cache will be skipped and the\n            loader will be fetched directly from the remote repo.\n        custom_dir: Custom dir name to download loader into (under parent folder).\n        custom_path: Custom dirpath to download loader into.\n        library_path: File name of the library file.\n        use_gpt_index_import: If true, the loader files will use\n            llama_index as the base dependency. By default (False),\n            the loader files use llama_index as the base dependency.\n            NOTE: this is a temporary workaround while we fully migrate all usages\n            to llama_index.core.\n        is_dataset: whether or not downloading a LlamaDataset\n\n    Returns:\n        A Loader, A Pack, An Agent, or A Dataset\n    \"\"\"\n    # create directory / get path\n    dirpath = initialize_directory(custom_path=custom_path, custom_dir=custom_dir)\n\n    # fetch info from library.json file\n    module_info = get_module_info(\n        local_dir_path=dirpath,\n        remote_dir_path=llama_hub_url,\n        module_class=module_class,\n        refresh_cache=refresh_cache,\n        library_path=library_path,\n        disable_library_cache=disable_library_cache,\n    )\n    module_id = module_info[\"module_id\"]\n    extra_files = module_info[\"extra_files\"]\n\n    # download the module, install requirements\n    download_module_and_reqs(\n        local_dir_path=dirpath,\n        remote_dir_path=llama_hub_url,\n        module_id=module_id,\n        extra_files=extra_files,\n        refresh_cache=refresh_cache,\n        use_gpt_index_import=use_gpt_index_import,\n        base_file_name=base_file_name,\n        override_path=override_path,\n    )\n    if skip_load:\n        return None\n\n    # loads the module into memory\n    if override_path:\n        path = f\"{dirpath}/{base_file_name}\"\n        spec = util.spec_from_file_location(\"custom_module\", location=path)\n        if spec is None:\n            raise ValueError(f\"Could not find file: {path}.\")\n    else:\n        path = f\"{dirpath}/{module_id}/{base_file_name}\"\n        spec = util.spec_from_file_location(\"custom_module\", location=path)\n        if spec is None:\n            raise ValueError(f\"Could not find file: {path}.\")\n\n    module = util.module_from_spec(spec)\n    spec.loader.exec_module(module)  # type: ignore\n\n    return getattr(module, module_class)\n\n\ndef track_download(module_class: str, module_type: str) -> None:\n    \"\"\"Tracks number of downloads via Llamahub proxy.\n\n    Args:\n        module_class: The name of the llama module being downloaded, e.g.,`GmailOpenAIAgentPack`.\n        module_type: Can be \"loader\", \"tool\", \"llamapack\", or \"datasets\"\n    \"\"\"\n    try:\n        requests.post(\n            LLAMAHUB_ANALYTICS_PROXY_SERVER,\n            json={\"type\": module_type, \"plugin\": module_class},\n        )\n    except Exception as e:\n        logger.info(f\"Error tracking downloads for {module_class} : {e}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/types.py",
    "filename": "types.py",
    "relpath": "postprocessor/types.py",
    "start_line": 1,
    "end_line": 119,
    "length": 119,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "class_name",
      "postprocess_nodes",
      "_postprocess_nodes",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "BaseNodePostprocessor",
      "PostprocessorComponent"
    ],
    "document_function_names": [
      "_get_prompts",
      "_update_prompts",
      "_get_prompt_modules",
      "class_name",
      "postprocess_nodes",
      "_postprocess_nodes",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "BaseNodePostprocessor",
      "PostprocessorComponent"
    ],
    "content": "from abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import Field, SerializeAsAny, ConfigDict\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixinType\nfrom llama_index.core.schema import BaseComponent, NodeWithScore, QueryBundle\n\n\nclass BaseNodePostprocessor(ChainableMixin, BaseComponent, DispatcherSpanMixin, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=CallbackManager, exclude=True\n    )\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        # set by default since most postprocessors don't require prompts\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    # implement class_name so users don't have to worry about it when extending\n    @classmethod\n    def class_name(cls) -> str:\n        return \"BaseNodePostprocessor\"\n\n    def postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n        query_str: Optional[str] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        if query_str is not None and query_bundle is not None:\n            raise ValueError(\"Cannot specify both query_str and query_bundle\")\n        elif query_str is not None:\n            query_bundle = QueryBundle(query_str)\n        else:\n            pass\n        return self._postprocess_nodes(nodes, query_bundle)\n\n    @abstractmethod\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"As query component.\"\"\"\n        return PostprocessorComponent(postprocessor=self)\n\n\nclass PostprocessorComponent(QueryComponent):\n    \"\"\"Postprocessor component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    postprocessor: SerializeAsAny[BaseNodePostprocessor] = Field(\n        ..., description=\"Postprocessor\"\n    )\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.postprocessor.callback_manager = callback_manager\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # make sure `nodes` is a list of nodes\n        if \"nodes\" not in input:\n            raise ValueError(\"Input must have key 'nodes'\")\n        nodes = input[\"nodes\"]\n        if not isinstance(nodes, list):\n            raise ValueError(\"Input nodes must be a list\")\n        for node in nodes:\n            if not isinstance(node, NodeWithScore):\n                raise ValueError(\"Input nodes must be a list of NodeWithScore\")\n\n        # if query_str exists, make sure `query_str` is stringable\n        if \"query_str\" in input:\n            input[\"query_str\"] = validate_and_convert_stringable(input[\"query_str\"])\n\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = self.postprocessor.postprocess_nodes(\n            kwargs[\"nodes\"], query_str=kwargs.get(\"query_str\", None)\n        )\n        return {\"nodes\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        # NOTE: no native async for postprocessor\n        return self._run_component(**kwargs)\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"nodes\"}, optional_keys={\"query_str\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"nodes\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/metadata_replacement.py",
    "filename": "metadata_replacement.py",
    "relpath": "postprocessor/metadata_replacement.py",
    "start_line": 1,
    "end_line": 33,
    "length": 33,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "_postprocess_nodes"
    ],
    "chunk_class_names": [
      "MetadataReplacementPostProcessor"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "_postprocess_nodes"
    ],
    "document_class_names": [
      "MetadataReplacementPostProcessor"
    ],
    "content": "from typing import List, Optional\n\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\n\n\nclass MetadataReplacementPostProcessor(BaseNodePostprocessor):\n    target_metadata_key: str = Field(\n        description=\"Target metadata key to replace node content with.\"\n    )\n\n    def __init__(self, target_metadata_key: str) -> None:\n        super().__init__(target_metadata_key=target_metadata_key)\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MetadataReplacementPostProcessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        for n in nodes:\n            n.node.set_content(\n                n.node.metadata.get(\n                    self.target_metadata_key,\n                    n.node.get_content(metadata_mode=MetadataMode.NONE),\n                )\n            )\n\n        return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/node.py",
    "filename": "node.py",
    "relpath": "postprocessor/node.py",
    "start_line": 1,
    "end_line": 394,
    "length": 394,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes",
      "get_forward_nodes",
      "get_backward_nodes",
      "_validate_mode",
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_parse_prediction",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes"
    ],
    "chunk_class_names": [
      "KeywordNodePostprocessor",
      "SimilarityPostprocessor",
      "PrevNextNodePostprocessor",
      "AutoPrevNextNodePostprocessor",
      "LongContextReorder"
    ],
    "document_function_names": [
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes",
      "get_forward_nodes",
      "get_backward_nodes",
      "_validate_mode",
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_parse_prediction",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes"
    ],
    "document_class_names": [
      "KeywordNodePostprocessor",
      "SimilarityPostprocessor",
      "PrevNextNodePostprocessor",
      "AutoPrevNextNodePostprocessor",
      "LongContextReorder"
    ],
    "content": "\"\"\"Node postprocessor.\"\"\"\n\nimport logging\nfrom typing import Dict, List, Optional, cast\n\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    field_validator,\n    SerializeAsAny,\n    ConfigDict,\n)\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.response_synthesizers import (\n    ResponseMode,\n    get_response_synthesizer,\n)\nfrom llama_index.core.schema import NodeRelationship, NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.docstore import BaseDocumentStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass KeywordNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"Keyword-based Node processor.\"\"\"\n\n    required_keywords: List[str] = Field(default_factory=list)\n    exclude_keywords: List[str] = Field(default_factory=list)\n    lang: str = Field(default=\"en\")\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"KeywordNodePostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        try:\n            import spacy\n        except ImportError:\n            raise ImportError(\n                \"Spacy is not installed, please install it with `pip install spacy`.\"\n            )\n        from spacy.matcher import PhraseMatcher\n\n        nlp = spacy.blank(self.lang)\n        required_matcher = PhraseMatcher(nlp.vocab)\n        exclude_matcher = PhraseMatcher(nlp.vocab)\n        required_matcher.add(\"RequiredKeywords\", list(nlp.pipe(self.required_keywords)))\n        exclude_matcher.add(\"ExcludeKeywords\", list(nlp.pipe(self.exclude_keywords)))\n\n        new_nodes = []\n        for node_with_score in nodes:\n            node = node_with_score.node\n            doc = nlp(node.get_content())\n            if self.required_keywords and not required_matcher(doc):\n                continue\n            if self.exclude_keywords and exclude_matcher(doc):\n                continue\n            new_nodes.append(node_with_score)\n\n        return new_nodes\n\n\nclass SimilarityPostprocessor(BaseNodePostprocessor):\n    \"\"\"Similarity-based Node processor.\"\"\"\n\n    similarity_cutoff: float = Field(default=0.0)\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SimilarityPostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        sim_cutoff_exists = self.similarity_cutoff is not None\n\n        new_nodes = []\n        for node in nodes:\n            should_use_node = True\n            if sim_cutoff_exists:\n                similarity = node.score\n                if similarity is None:\n                    should_use_node = False\n                elif cast(float, similarity) < cast(float, self.similarity_cutoff):\n                    should_use_node = False\n\n            if should_use_node:\n                new_nodes.append(node)\n\n        return new_nodes\n\n\ndef get_forward_nodes(\n    node_with_score: NodeWithScore, num_nodes: int, docstore: BaseDocumentStore\n) -> Dict[str, NodeWithScore]:\n    \"\"\"Get forward nodes.\"\"\"\n    node = node_with_score.node\n    nodes: Dict[str, NodeWithScore] = {node.node_id: node_with_score}\n    cur_count = 0\n    # get forward nodes in an iterative manner\n    while cur_count < num_nodes:\n        if NodeRelationship.NEXT not in node.relationships:\n            break\n\n        next_node_info = node.next_node\n        if next_node_info is None:\n            break\n\n        next_node_id = next_node_info.node_id\n        next_node = docstore.get_node(next_node_id)\n        nodes[next_node.node_id] = NodeWithScore(node=next_node)\n        node = next_node\n        cur_count += 1\n    return nodes\n\n\ndef get_backward_nodes(\n    node_with_score: NodeWithScore, num_nodes: int, docstore: BaseDocumentStore\n) -> Dict[str, NodeWithScore]:\n    \"\"\"Get backward nodes.\"\"\"\n    node = node_with_score.node\n    # get backward nodes in an iterative manner\n    nodes: Dict[str, NodeWithScore] = {node.node_id: node_with_score}\n    cur_count = 0\n    while cur_count < num_nodes:\n        prev_node_info = node.prev_node\n        if prev_node_info is None:\n            break\n        prev_node_id = prev_node_info.node_id\n        prev_node = docstore.get_node(prev_node_id)\n        if prev_node is None:\n            break\n        nodes[prev_node.node_id] = NodeWithScore(node=prev_node)\n        node = prev_node\n        cur_count += 1\n    return nodes\n\n\nclass PrevNextNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"Previous/Next Node post-processor.\n\n    Allows users to fetch additional nodes from the document store,\n    based on the relationships of the nodes.\n\n    NOTE: this is a beta feature.\n\n    Args:\n        docstore (BaseDocumentStore): The document store.\n        num_nodes (int): The number of nodes to return (default: 1)\n        mode (str): The mode of the post-processor.\n            Can be \"previous\", \"next\", or \"both.\n\n    \"\"\"\n\n    docstore: BaseDocumentStore\n    num_nodes: int = Field(default=1)\n    mode: str = Field(default=\"next\")\n\n    @field_validator(\"mode\")\n    @classmethod\n    def _validate_mode(cls, v: str) -> str:\n        \"\"\"Validate mode.\"\"\"\n        if v not in [\"next\", \"previous\", \"both\"]:\n            raise ValueError(f\"Invalid mode: {v}\")\n        return v\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"PrevNextNodePostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        all_nodes: Dict[str, NodeWithScore] = {}\n        for node in nodes:\n            all_nodes[node.node.node_id] = node\n            if self.mode == \"next\":\n                all_nodes.update(get_forward_nodes(node, self.num_nodes, self.docstore))\n            elif self.mode == \"previous\":\n                all_nodes.update(\n                    get_backward_nodes(node, self.num_nodes, self.docstore)\n                )\n            elif self.mode == \"both\":\n                all_nodes.update(get_forward_nodes(node, self.num_nodes, self.docstore))\n                all_nodes.update(\n                    get_backward_nodes(node, self.num_nodes, self.docstore)\n                )\n            else:\n                raise ValueError(f\"Invalid mode: {self.mode}\")\n\n        all_nodes_values: List[NodeWithScore] = list(all_nodes.values())\n        sorted_nodes: List[NodeWithScore] = []\n        for node in all_nodes_values:\n            # variable to check if cand node is inserted\n            node_inserted = False\n            for i, cand in enumerate(sorted_nodes):\n                node_id = node.node.node_id\n                # prepend to current candidate\n                prev_node_info = cand.node.prev_node\n                next_node_info = cand.node.next_node\n                if prev_node_info is not None and node_id == prev_node_info.node_id:\n                    node_inserted = True\n                    sorted_nodes.insert(i, node)\n                    break\n                # append to current candidate\n                elif next_node_info is not None and node_id == next_node_info.node_id:\n                    node_inserted = True\n                    sorted_nodes.insert(i + 1, node)\n                    break\n\n            if not node_inserted:\n                sorted_nodes.append(node)\n\n        return sorted_nodes\n\n\nDEFAULT_INFER_PREV_NEXT_TMPL = (\n    \"The current context information is provided. \\n\"\n    \"A question is also provided. \\n\"\n    \"You are a retrieval agent deciding whether to search the \"\n    \"document store for additional prior context or future context. \\n\"\n    \"Given the context and question, return PREVIOUS or NEXT or NONE. \\n\"\n    \"Examples: \\n\\n\"\n    \"Context: Describes the author's experience at Y Combinator.\"\n    \"Question: What did the author do after his time at Y Combinator? \\n\"\n    \"Answer: NEXT \\n\\n\"\n    \"Context: Describes the author's experience at Y Combinator.\"\n    \"Question: What did the author do before his time at Y Combinator? \\n\"\n    \"Answer: PREVIOUS \\n\\n\"\n    \"Context: Describe the author's experience at Y Combinator.\"\n    \"Question: What did the author do at Y Combinator? \\n\"\n    \"Answer: NONE \\n\\n\"\n    \"Context: {context_str}\\n\"\n    \"Question: {query_str}\\n\"\n    \"Answer: \"\n)\n\n\nDEFAULT_REFINE_INFER_PREV_NEXT_TMPL = (\n    \"The current context information is provided. \\n\"\n    \"A question is also provided. \\n\"\n    \"An existing answer is also provided.\\n\"\n    \"You are a retrieval agent deciding whether to search the \"\n    \"document store for additional prior context or future context. \\n\"\n    \"Given the context, question, and previous answer, \"\n    \"return PREVIOUS or NEXT or NONE.\\n\"\n    \"Examples: \\n\\n\"\n    \"Context: {context_msg}\\n\"\n    \"Question: {query_str}\\n\"\n    \"Existing Answer: {existing_answer}\\n\"\n    \"Answer: \"\n)\n\n\nclass AutoPrevNextNodePostprocessor(BaseNodePostprocessor):\n    \"\"\"Previous/Next Node post-processor.\n\n    Allows users to fetch additional nodes from the document store,\n    based on the prev/next relationships of the nodes.\n\n    NOTE: difference with PrevNextPostprocessor is that\n    this infers forward/backwards direction.\n\n    NOTE: this is a beta feature.\n\n    Args:\n        docstore (BaseDocumentStore): The document store.\n        num_nodes (int): The number of nodes to return (default: 1)\n        infer_prev_next_tmpl (str): The template to use for inference.\n            Required fields are {context_str} and {query_str}.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    docstore: BaseDocumentStore\n    llm: Optional[SerializeAsAny[LLM]] = None\n    num_nodes: int = Field(default=1)\n    infer_prev_next_tmpl: str = Field(default=DEFAULT_INFER_PREV_NEXT_TMPL)\n    refine_prev_next_tmpl: str = Field(default=DEFAULT_REFINE_INFER_PREV_NEXT_TMPL)\n    verbose: bool = Field(default=False)\n    response_mode: ResponseMode = Field(default=ResponseMode.COMPACT)\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"AutoPrevNextNodePostprocessor\"\n\n    def _parse_prediction(self, raw_pred: str) -> str:\n        \"\"\"Parse prediction.\"\"\"\n        pred = raw_pred.strip().lower()\n        if \"previous\" in pred:\n            return \"previous\"\n        elif \"next\" in pred:\n            return \"next\"\n        elif \"none\" in pred:\n            return \"none\"\n        raise ValueError(f\"Invalid prediction: {raw_pred}\")\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        llm = self.llm or Settings.llm\n\n        if query_bundle is None:\n            raise ValueError(\"Missing query bundle.\")\n\n        infer_prev_next_prompt = PromptTemplate(\n            self.infer_prev_next_tmpl,\n        )\n        refine_infer_prev_next_prompt = PromptTemplate(self.refine_prev_next_tmpl)\n\n        all_nodes: Dict[str, NodeWithScore] = {}\n        for node in nodes:\n            all_nodes[node.node.node_id] = node\n            # use response builder instead of llm directly\n            # to be more robust to handling long context\n            response_builder = get_response_synthesizer(\n                llm=llm,\n                text_qa_template=infer_prev_next_prompt,\n                refine_template=refine_infer_prev_next_prompt,\n                response_mode=self.response_mode,\n            )\n            raw_pred = response_builder.get_response(\n                text_chunks=[node.node.get_content()],\n                query_str=query_bundle.query_str,\n            )\n            raw_pred = cast(str, raw_pred)\n            mode = self._parse_prediction(raw_pred)\n\n            logger.debug(f\"> Postprocessor Predicted mode: {mode}\")\n            if self.verbose:\n                print(f\"> Postprocessor Predicted mode: {mode}\")\n\n            if mode == \"next\":\n                all_nodes.update(get_forward_nodes(node, self.num_nodes, self.docstore))\n            elif mode == \"previous\":\n                all_nodes.update(\n                    get_backward_nodes(node, self.num_nodes, self.docstore)\n                )\n            elif mode == \"none\":\n                pass\n            else:\n                raise ValueError(f\"Invalid mode: {mode}\")\n\n        sorted_nodes = sorted(all_nodes.values(), key=lambda x: x.node.node_id)\n        return list(sorted_nodes)\n\n\nclass LongContextReorder(BaseNodePostprocessor):\n    \"\"\"\n    Models struggle to access significant details found\n    in the center of extended contexts. A study\n    (https://arxiv.org/abs/2307.03172) observed that the best\n    performance typically arises when crucial data is positioned\n    at the start or conclusion of the input context. Additionally,\n    as the input context lengthens, performance drops notably, even\n    in models designed for long contexts.\".\n    \"\"\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"LongContextReorder\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        reordered_nodes: List[NodeWithScore] = []\n        ordered_nodes: List[NodeWithScore] = sorted(\n            nodes, key=lambda x: x.score if x.score is not None else 0\n        )\n        for i, node in enumerate(ordered_nodes):\n            if i % 2 == 0:\n                reordered_nodes.insert(0, node)\n            else:\n                reordered_nodes.append(node)\n        return reordered_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/optimizer.py",
    "filename": "optimizer.py",
    "relpath": "postprocessor/optimizer.py",
    "start_line": 1,
    "end_line": 167,
    "length": 167,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "_postprocess_nodes"
    ],
    "chunk_class_names": [
      "SentenceEmbeddingOptimizer",
      "that"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "_postprocess_nodes"
    ],
    "document_class_names": [
      "SentenceEmbeddingOptimizer",
      "that"
    ],
    "content": "\"\"\"Optimization related classes and functions.\"\"\"\n\nimport logging\nfrom typing import Callable, List, Optional\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.indices.query.embedding_utils import get_top_k_embeddings\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass SentenceEmbeddingOptimizer(BaseNodePostprocessor):\n    \"\"\"Optimization of a text chunk given the query by shortening the input text.\"\"\"\n\n    percentile_cutoff: Optional[float] = Field(\n        description=\"Percentile cutoff for the top k sentences to use.\"\n    )\n    threshold_cutoff: Optional[float] = Field(\n        description=\"Threshold cutoff for similarity for each sentence to use.\"\n    )\n\n    _embed_model: BaseEmbedding = PrivateAttr()\n    _tokenizer_fn: Callable[[str], List[str]] = PrivateAttr()\n\n    context_before: Optional[int] = Field(\n        description=\"Number of sentences before retrieved sentence for further context\"\n    )\n\n    context_after: Optional[int] = Field(\n        description=\"Number of sentences after retrieved sentence for further context\"\n    )\n\n    def __init__(\n        self,\n        embed_model: Optional[BaseEmbedding] = None,\n        percentile_cutoff: Optional[float] = None,\n        threshold_cutoff: Optional[float] = None,\n        tokenizer_fn: Optional[Callable[[str], List[str]]] = None,\n        context_before: Optional[int] = None,\n        context_after: Optional[int] = None,\n    ):\n        \"\"\"Optimizer class that is passed into BaseGPTIndexQuery.\n\n        Should be set like this:\n\n        .. code-block:: python\n        from llama_index.core.optimization.optimizer import Optimizer\n        optimizer = SentenceEmbeddingOptimizer(\n                        percentile_cutoff=0.5\n                        this means that the top 50% of sentences will be used.\n                        Alternatively, you can set the cutoff using a threshold\n                        on the similarity score. In this case only sentences with a\n                        similarity score higher than the threshold will be used.\n                        threshold_cutoff=0.7\n                        these cutoffs can also be used together.\n                    )\n\n        query_engine = index.as_query_engine(\n            optimizer=optimizer\n        )\n        response = query_engine.query(\"<query_str>\")\n        \"\"\"\n        super().__init__(\n            percentile_cutoff=percentile_cutoff,\n            threshold_cutoff=threshold_cutoff,\n            context_after=context_after,\n            context_before=context_before,\n        )\n        self._embed_model = embed_model or Settings.embed_model\n        if self._embed_model is None:\n            try:\n                from llama_index.embeddings.openai import (\n                    OpenAIEmbedding,\n                )  # pants: no-infer-dep\n\n                self._embed_model = OpenAIEmbedding()\n            except ImportError:\n                raise ImportError(\n                    \"`llama-index-embeddings-openai` package not found, \"\n                    \"please run `pip install llama-index-embeddings-openai`\"\n                )\n\n        if tokenizer_fn is None:\n            import nltk\n\n            tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n            tokenizer_fn = tokenizer.tokenize\n        self._tokenizer_fn = tokenizer_fn\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SentenceEmbeddingOptimizer\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Optimize a node text given the query by shortening the node text.\"\"\"\n        if query_bundle is None:\n            return nodes\n\n        for node_idx in range(len(nodes)):\n            text = nodes[node_idx].node.get_content(metadata_mode=MetadataMode.LLM)\n\n            split_text = self._tokenizer_fn(text)\n\n            if query_bundle.embedding is None:\n                query_bundle.embedding = (\n                    self._embed_model.get_agg_embedding_from_queries(\n                        query_bundle.embedding_strs\n                    )\n                )\n\n            text_embeddings = self._embed_model._get_text_embeddings(split_text)\n\n            num_top_k = None\n            threshold = None\n            if self.percentile_cutoff is not None:\n                num_top_k = int(len(split_text) * self.percentile_cutoff)\n            if self.threshold_cutoff is not None:\n                threshold = self.threshold_cutoff\n\n            top_similarities, top_idxs = get_top_k_embeddings(\n                query_embedding=query_bundle.embedding,\n                embeddings=text_embeddings,\n                similarity_fn=self._embed_model.similarity,\n                similarity_top_k=num_top_k,\n                embedding_ids=list(range(len(text_embeddings))),\n                similarity_cutoff=threshold,\n            )\n\n            if len(top_idxs) == 0:\n                raise ValueError(\"Optimizer returned zero sentences.\")\n\n            rangeMin, rangeMax = 0, len(split_text)\n\n            if self.context_before is None:\n                self.context_before = 1\n            if self.context_after is None:\n                self.context_after = 1\n\n            top_sentences = [\n                \" \".join(\n                    split_text[\n                        max(idx - self.context_before, rangeMin) : min(\n                            idx + self.context_after + 1, rangeMax\n                        )\n                    ]\n                )\n                for idx in top_idxs\n            ]\n\n            logger.debug(f\"> Top {len(top_idxs)} sentences with scores:\\n\")\n            if logger.isEnabledFor(logging.DEBUG):\n                for idx in range(len(top_idxs)):\n                    logger.debug(\n                        f\"{idx}. {top_sentences[idx]} ({top_similarities[idx]})\"\n                    )\n\n            nodes[node_idx].node.set_content(\" \".join(top_sentences))\n\n        return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/llm_rerank.py",
    "filename": "llm_rerank.py",
    "relpath": "postprocessor/llm_rerank.py",
    "start_line": 1,
    "end_line": 111,
    "length": 111,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "class_name",
      "_postprocess_nodes"
    ],
    "chunk_class_names": [
      "LLMRerank"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "class_name",
      "_postprocess_nodes"
    ],
    "document_class_names": [
      "LLMRerank"
    ],
    "content": "\"\"\"LLM reranker.\"\"\"\n\nfrom typing import Callable, List, Optional\n\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr, SerializeAsAny\nfrom llama_index.core.indices.utils import (\n    default_format_node_batch_fn,\n    default_parse_choice_select_answer_fn,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_CHOICE_SELECT_PROMPT\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\n\n\nclass LLMRerank(BaseNodePostprocessor):\n    \"\"\"LLM-based reranker.\"\"\"\n\n    top_n: int = Field(description=\"Top N nodes to return.\")\n    choice_select_prompt: SerializeAsAny[BasePromptTemplate] = Field(\n        description=\"Choice select prompt.\"\n    )\n    choice_batch_size: int = Field(description=\"Batch size for choice select.\")\n    llm: LLM = Field(description=\"The LLM to rerank with.\")\n\n    _format_node_batch_fn: Callable = PrivateAttr()\n    _parse_choice_select_answer_fn: Callable = PrivateAttr()\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        choice_select_prompt: Optional[BasePromptTemplate] = None,\n        choice_batch_size: int = 10,\n        format_node_batch_fn: Optional[Callable] = None,\n        parse_choice_select_answer_fn: Optional[Callable] = None,\n        top_n: int = 10,\n    ) -> None:\n        choice_select_prompt = choice_select_prompt or DEFAULT_CHOICE_SELECT_PROMPT\n\n        llm = llm or Settings.llm\n\n        super().__init__(\n            llm=llm,\n            choice_select_prompt=choice_select_prompt,\n            choice_batch_size=choice_batch_size,\n            top_n=top_n,\n        )\n        self._format_node_batch_fn = (\n            format_node_batch_fn or default_format_node_batch_fn\n        )\n        self._parse_choice_select_answer_fn = (\n            parse_choice_select_answer_fn or default_parse_choice_select_answer_fn\n        )\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"choice_select_prompt\": self.choice_select_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"choice_select_prompt\" in prompts:\n            self.choice_select_prompt = prompts[\"choice_select_prompt\"]\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"LLMRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n            raise ValueError(\"Query bundle must be provided.\")\n        if len(nodes) == 0:\n            return []\n\n        initial_results: List[NodeWithScore] = []\n        for idx in range(0, len(nodes), self.choice_batch_size):\n            nodes_batch = [\n                node.node for node in nodes[idx : idx + self.choice_batch_size]\n            ]\n\n            query_str = query_bundle.query_str\n            fmt_batch_str = self._format_node_batch_fn(nodes_batch)\n            # call each batch independently\n            raw_response = self.llm.predict(\n                self.choice_select_prompt,\n                context_str=fmt_batch_str,\n                query_str=query_str,\n            )\n\n            raw_choices, relevances = self._parse_choice_select_answer_fn(\n                raw_response, len(nodes_batch)\n            )\n            choice_idxs = [int(choice) - 1 for choice in raw_choices]\n            choice_nodes = [nodes_batch[idx] for idx in choice_idxs]\n            relevances = relevances or [1.0 for _ in choice_nodes]\n            initial_results.extend(\n                [\n                    NodeWithScore(node=node, score=relevance)\n                    for node, relevance in zip(choice_nodes, relevances)\n                ]\n            )\n\n        return sorted(initial_results, key=lambda x: x.score or 0.0, reverse=True)[\n            : self.top_n\n        ]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/node_recency.py",
    "filename": "node_recency.py",
    "relpath": "postprocessor/node_recency.py",
    "start_line": 1,
    "end_line": 222,
    "length": 222,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "parse_recency_pred",
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes"
    ],
    "chunk_class_names": [
      "FixedRecencyPostprocessor",
      "EmbeddingRecencyPostprocessor",
      "TimeWeightedPostprocessor"
    ],
    "document_function_names": [
      "parse_recency_pred",
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes",
      "class_name",
      "_postprocess_nodes"
    ],
    "document_class_names": [
      "FixedRecencyPostprocessor",
      "EmbeddingRecencyPostprocessor",
      "TimeWeightedPostprocessor"
    ],
    "content": "\"\"\"Node recency post-processor.\"\"\"\n\nfrom datetime import datetime\nfrom typing import List, Optional, Set\n\nimport numpy as np\n\n# NOTE: currently not being used\n# DEFAULT_INFER_RECENCY_TMPL = (\n#     \"A question is provided.\\n\"\n#     \"The goal is to determine whether the question requires finding the most recent \"\n#     \"context.\\n\"\n#     \"Please respond with YES or NO.\\n\"\n#     \"Question: What is the current status of the patient?\\n\"\n#     \"Answer: YES\\n\"\n#     \"Question: What happened in the Battle of Yorktown?\\n\"\n#     \"Answer: NO\\n\"\n#     \"Question: What are the most recent changes to the project?\\n\"\n#     \"Answer: YES\\n\"\n#     \"Question: How did Harry defeat Voldemort in the Battle of Hogwarts?\\n\"\n#     \"Answer: NO\\n\"\n#     \"Question: {query_str}\\n\"\n#     \"Answer: \"\n# )\n# def parse_recency_pred(pred: str) -> bool:\n#     \"\"\"Parse recency prediction.\"\"\"\n#     if \"YES\" in pred:\n#         return True\n#     elif \"NO\" in pred:\n#         return False\n#     else:\n#         raise ValueError(f\"Invalid recency prediction: {pred}.\")\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.bridge.pydantic import Field, SerializeAsAny\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\nfrom llama_index.core.settings import Settings\n\n\nclass FixedRecencyPostprocessor(BaseNodePostprocessor):\n    \"\"\"Fixed Recency post-processor.\n\n    This post-processor does the following steps orders nodes by date.\n\n    Assumes the date_key corresponds to a date field in the metadata.\n    \"\"\"\n\n    top_k: int = 1\n    date_key: str = \"date\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"FixedRecencyPostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        if query_bundle is None:\n            raise ValueError(\"Missing query bundle in extra info.\")\n\n        # sort nodes by date\n        node_dates = pd.to_datetime(\n            [node.node.metadata[self.date_key] for node in nodes]\n        )\n        sorted_node_idxs = np.flip(node_dates.argsort())\n        sorted_nodes = [nodes[idx] for idx in sorted_node_idxs]\n\n        return sorted_nodes[: self.top_k]\n\n\nDEFAULT_QUERY_EMBEDDING_TMPL = (\n    \"The current document is provided.\\n\"\n    \"----------------\\n\"\n    \"{context_str}\\n\"\n    \"----------------\\n\"\n    \"Given the document, we wish to find documents that contain \\n\"\n    \"similar context. Note that these documents are older \"\n    \"than the current document, meaning that certain details may be changed. \\n\"\n    \"However, the high-level context should be similar.\\n\"\n)\n\n\nclass EmbeddingRecencyPostprocessor(BaseNodePostprocessor):\n    \"\"\"Embedding Recency post-processor.\"\"\"\n\n    embed_model: SerializeAsAny[BaseEmbedding] = Field(\n        default_factory=lambda: Settings.embed_model\n    )\n    date_key: str = \"date\"\n    similarity_cutoff: float = Field(default=0.7)\n    query_embedding_tmpl: str = Field(default=DEFAULT_QUERY_EMBEDDING_TMPL)\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"EmbeddingRecencyPostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\n                \"pandas is required for this function. Please install it with `pip install pandas`.\"\n            )\n\n        if query_bundle is None:\n            raise ValueError(\"Missing query bundle in extra info.\")\n\n        # sort nodes by date\n        node_dates = pd.to_datetime(\n            [node.node.metadata[self.date_key] for node in nodes]\n        )\n        sorted_node_idxs = np.flip(node_dates.argsort())\n        sorted_nodes: List[NodeWithScore] = [nodes[idx] for idx in sorted_node_idxs]\n\n        # get embeddings for each node\n        texts = [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes]\n        text_embeddings = self.embed_model.get_text_embedding_batch(texts=texts)\n\n        node_ids_to_skip: Set[str] = set()\n        for idx, node in enumerate(sorted_nodes):\n            if node.node.node_id in node_ids_to_skip:\n                continue\n            # get query embedding for the \"query\" node\n            # NOTE: not the same as the text embedding because\n            # we want to optimize for retrieval results\n\n            query_text = self.query_embedding_tmpl.format(\n                context_str=node.node.get_content(metadata_mode=MetadataMode.EMBED),\n            )\n            query_embedding = self.embed_model.get_query_embedding(query_text)\n\n            for idx2 in range(idx + 1, len(sorted_nodes)):\n                if sorted_nodes[idx2].node.node_id in node_ids_to_skip:\n                    continue\n                node2 = sorted_nodes[idx2]\n                if (\n                    np.dot(query_embedding, text_embeddings[idx2])\n                    > self.similarity_cutoff\n                ):\n                    node_ids_to_skip.add(node2.node.node_id)\n\n        return [\n            node for node in sorted_nodes if node.node.node_id not in node_ids_to_skip\n        ]\n\n\nclass TimeWeightedPostprocessor(BaseNodePostprocessor):\n    \"\"\"Time-weighted post-processor.\n\n    Reranks a set of nodes based on their recency.\n\n    \"\"\"\n\n    time_decay: float = Field(default=0.99)\n    last_accessed_key: str = \"__last_accessed__\"\n    time_access_refresh: bool = True\n    # optionally set now (makes it easier to test)\n    now: Optional[float] = None\n    top_k: int = 1\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TimeWeightedPostprocessor\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        now = self.now or datetime.now().timestamp()\n        # TODO: refactor with get_top_k_embeddings\n\n        similarities = []\n        for node_with_score in nodes:\n            # embedding similarity score\n            score = node_with_score.score or 1.0\n            node = node_with_score.node\n            # time score\n            if node.metadata is None:\n                raise ValueError(\"metadata is None\")\n\n            last_accessed = node.metadata.get(self.last_accessed_key, None)\n            if last_accessed is None:\n                last_accessed = now\n\n            hours_passed = (now - last_accessed) / 3600\n            time_similarity = (1 - self.time_decay) ** hours_passed\n\n            similarity = score + time_similarity\n\n            similarities.append(similarity)\n\n        sorted_tups = sorted(zip(similarities, nodes), key=lambda x: x[0], reverse=True)\n\n        top_k = min(self.top_k, len(sorted_tups))\n        result_tups = sorted_tups[:top_k]\n        result_nodes = [\n            NodeWithScore(node=n.node, score=score) for score, n in result_tups\n        ]\n\n        # set __last_accessed__ to now\n        if self.time_access_refresh:\n            for node_with_score in result_nodes:\n                node_with_score.node.metadata[self.last_accessed_key] = now\n\n        return result_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/__init__.py",
    "filename": "__init__.py",
    "relpath": "postprocessor/__init__.py",
    "start_line": 1,
    "end_line": 42,
    "length": 42,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Node PostProcessor module.\"\"\"\n\n\nfrom llama_index.core.postprocessor.llm_rerank import LLMRerank\nfrom llama_index.core.postprocessor.metadata_replacement import (\n    MetadataReplacementPostProcessor,\n)\nfrom llama_index.core.postprocessor.node import (\n    AutoPrevNextNodePostprocessor,\n    KeywordNodePostprocessor,\n    LongContextReorder,\n    PrevNextNodePostprocessor,\n    SimilarityPostprocessor,\n)\nfrom llama_index.core.postprocessor.node_recency import (\n    EmbeddingRecencyPostprocessor,\n    FixedRecencyPostprocessor,\n    TimeWeightedPostprocessor,\n)\nfrom llama_index.core.postprocessor.optimizer import SentenceEmbeddingOptimizer\nfrom llama_index.core.postprocessor.pii import (\n    NERPIINodePostprocessor,\n    PIINodePostprocessor,\n)\nfrom llama_index.core.postprocessor.sbert_rerank import SentenceTransformerRerank\n\n__all__ = [\n    \"SimilarityPostprocessor\",\n    \"KeywordNodePostprocessor\",\n    \"PrevNextNodePostprocessor\",\n    \"AutoPrevNextNodePostprocessor\",\n    \"FixedRecencyPostprocessor\",\n    \"EmbeddingRecencyPostprocessor\",\n    \"TimeWeightedPostprocessor\",\n    \"PIINodePostprocessor\",\n    \"NERPIINodePostprocessor\",\n    \"LLMRerank\",\n    \"SentenceEmbeddingOptimizer\",\n    \"SentenceTransformerRerank\",\n    \"MetadataReplacementPostProcessor\",\n    \"LongContextReorder\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/pii.py",
    "filename": "pii.py",
    "relpath": "postprocessor/pii.py",
    "start_line": 1,
    "end_line": 144,
    "length": 144,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "mask_pii",
      "_postprocess_nodes",
      "class_name",
      "mask_pii",
      "_postprocess_nodes"
    ],
    "chunk_class_names": [
      "PIINodePostprocessor",
      "NERPIINodePostprocessor"
    ],
    "document_function_names": [
      "class_name",
      "mask_pii",
      "_postprocess_nodes",
      "class_name",
      "mask_pii",
      "_postprocess_nodes"
    ],
    "document_class_names": [
      "PIINodePostprocessor",
      "NERPIINodePostprocessor"
    ],
    "content": "\"\"\"PII postprocessor.\"\"\"\nimport json\nfrom copy import deepcopy\nfrom typing import Callable, Dict, List, Optional, Tuple\n\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\n\nDEFAULT_PII_TMPL = (\n    \"The current context information is provided. \\n\"\n    \"A task is also provided to mask the PII within the context. \\n\"\n    \"Return the text, with all PII masked out, and a mapping of the original PII \"\n    \"to the masked PII. \\n\"\n    \"Return the output of the task in JSON. \\n\"\n    \"Context:\\n\"\n    \"Hello Zhang Wei, I am John. \"\n    \"Your AnyCompany Financial Services, \"\n    \"LLC credit card account 1111-0000-1111-0008 \"\n    \"has a minimum payment of $24.53 that is due \"\n    \"by July 31st. Based on your autopay settings, we will withdraw your payment. \"\n    \"Task: Mask out the PII, replace each PII with a tag, and return the text. Return the mapping in JSON. \\n\"\n    \"Output: \\n\"\n    \"Hello [NAME1], I am [NAME2]. \"\n    \"Your AnyCompany Financial Services, \"\n    \"LLC credit card account [CREDIT_CARD_NUMBER] \"\n    \"has a minimum payment of $24.53 that is due \"\n    \"by [DATE_TIME]. Based on your autopay settings, we will withdraw your payment. \"\n    \"Output Mapping:\\n\"\n    '{{\"NAME1\": \"Zhang Wei\", \"NAME2\": \"John\", \"CREDIT_CARD_NUMBER\": \"1111-0000-1111-0008\", \"DATE_TIME\": \"July 31st\"}}\\n'\n    \"Context:\\n{context_str}\\n\"\n    \"Task: {query_str}\\n\"\n    \"Output: \\n\"\n    \"\"\n)\n\n\nclass PIINodePostprocessor(BaseNodePostprocessor):\n    \"\"\"PII Node processor.\n\n    NOTE: this is a beta feature, the API might change.\n\n    Args:\n        llm (LLM): The local LLM to use for prediction.\n\n    \"\"\"\n\n    llm: LLM\n    pii_str_tmpl: str = DEFAULT_PII_TMPL\n    pii_node_info_key: str = \"__pii_node_info__\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"PIINodePostprocessor\"\n\n    def mask_pii(self, text: str) -> Tuple[str, Dict]:\n        \"\"\"Mask PII in text.\"\"\"\n        pii_prompt = PromptTemplate(self.pii_str_tmpl)\n        # TODO: allow customization\n        task_str = (\n            \"Mask out the PII, replace each PII with a tag, and return the text. \"\n            \"Return the mapping in JSON.\"\n        )\n\n        response = self.llm.predict(pii_prompt, context_str=text, query_str=task_str)\n        splits = response.split(\"Output Mapping:\")\n        text_output = splits[0].strip()\n        json_str_output = splits[1].strip()\n        json_dict = json.loads(json_str_output)\n        return text_output, json_dict\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        # swap out text from nodes, with the original node mappings\n        new_nodes = []\n        for node_with_score in nodes:\n            node = node_with_score.node\n            new_text, mapping_info = self.mask_pii(\n                node.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            new_node = deepcopy(node)\n            new_node.excluded_embed_metadata_keys.append(self.pii_node_info_key)\n            new_node.excluded_llm_metadata_keys.append(self.pii_node_info_key)\n            new_node.metadata[self.pii_node_info_key] = mapping_info\n            new_node.set_content(new_text)\n            new_nodes.append(NodeWithScore(node=new_node, score=node_with_score.score))\n\n        return new_nodes\n\n\nclass NERPIINodePostprocessor(BaseNodePostprocessor):\n    \"\"\"NER PII Node processor.\n\n    Uses a HF transformers model.\n\n    \"\"\"\n\n    pii_node_info_key: str = \"__pii_node_info__\"\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"NERPIINodePostprocessor\"\n\n    def mask_pii(self, ner: Callable, text: str) -> Tuple[str, Dict]:\n        \"\"\"Mask PII in text.\"\"\"\n        new_text = text\n        response = ner(text)\n        mapping = {}\n        for entry in response:\n            entity_group_tag = f\"[{entry['entity_group']}_{entry['start']}]\"\n            new_text = new_text.replace(entry[\"word\"], entity_group_tag).strip()\n            mapping[entity_group_tag] = entry[\"word\"]\n        return new_text, mapping\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"\n        from transformers import pipeline  # pants: no-infer-dep\n\n        ner = pipeline(\"ner\", grouped_entities=True)\n\n        # swap out text from nodes, with the original node mappings\n        new_nodes = []\n        for node_with_score in nodes:\n            node = node_with_score.node\n            new_text, mapping_info = self.mask_pii(\n                ner, node.get_content(metadata_mode=MetadataMode.LLM)\n            )\n            new_node = deepcopy(node)\n            new_node.excluded_embed_metadata_keys.append(self.pii_node_info_key)\n            new_node.excluded_llm_metadata_keys.append(self.pii_node_info_key)\n            new_node.metadata[self.pii_node_info_key] = mapping_info\n            new_node.set_content(new_text)\n            new_nodes.append(NodeWithScore(node=new_node, score=node_with_score.score))\n\n        return new_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/rankGPT_rerank.py",
    "filename": "rankGPT_rerank.py",
    "relpath": "postprocessor/rankGPT_rerank.py",
    "start_line": 1,
    "end_line": 204,
    "length": 204,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_default_llm",
      "__init__",
      "class_name",
      "_postprocess_nodes",
      "_apostprocess_nodes",
      "apostprocess_nodes",
      "_llm_result_to_nodes",
      "_get_prompts",
      "_update_prompts",
      "_get_prefix_prompt",
      "_get_post_prompt",
      "create_permutation_instruction",
      "run_llm",
      "arun_llm",
      "_clean_response",
      "_remove_duplicate",
      "_receive_permutation"
    ],
    "chunk_class_names": [
      "RankGPTRerank"
    ],
    "document_function_names": [
      "get_default_llm",
      "__init__",
      "class_name",
      "_postprocess_nodes",
      "_apostprocess_nodes",
      "apostprocess_nodes",
      "_llm_result_to_nodes",
      "_get_prompts",
      "_update_prompts",
      "_get_prefix_prompt",
      "_get_post_prompt",
      "create_permutation_instruction",
      "run_llm",
      "arun_llm",
      "_clean_response",
      "_remove_duplicate",
      "_receive_permutation"
    ],
    "document_class_names": [
      "RankGPTRerank"
    ],
    "content": "import logging\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom llama_index.core.bridge.pydantic import Field, SerializeAsAny\nfrom llama_index.core.llms import LLM, ChatMessage, ChatResponse\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.prompts import BasePromptTemplate\nfrom llama_index.core.prompts.default_prompts import RANKGPT_RERANK_PROMPT\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\nfrom llama_index.core.utils import print_text\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\n\n\ndef get_default_llm() -> LLM:\n    from llama_index.llms.openai import OpenAI  # pants: no-infer-dep\n\n    return OpenAI(model=\"gpt-3.5-turbo-16k\")\n\n\nclass RankGPTRerank(BaseNodePostprocessor):\n    \"\"\"RankGPT-based reranker.\"\"\"\n\n    top_n: int = Field(default=5, description=\"Top N nodes to return from reranking.\")\n    llm: LLM = Field(\n        default_factory=get_default_llm,\n        description=\"LLM to use for rankGPT\",\n    )\n    verbose: bool = Field(\n        default=False, description=\"Whether to print intermediate steps.\"\n    )\n    rankgpt_rerank_prompt: SerializeAsAny[BasePromptTemplate] = Field(\n        description=\"rankGPT rerank prompt.\"\n    )\n\n    def __init__(\n        self,\n        top_n: int = 5,\n        llm: Optional[LLM] = None,\n        verbose: bool = False,\n        rankgpt_rerank_prompt: Optional[BasePromptTemplate] = None,\n    ):\n        rankgpt_rerank_prompt = rankgpt_rerank_prompt or RANKGPT_RERANK_PROMPT\n        super().__init__(\n            verbose=verbose,\n            llm=llm,\n            top_n=top_n,\n            rankgpt_rerank_prompt=rankgpt_rerank_prompt,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"RankGPTRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n            raise ValueError(\"Query bundle must be provided.\")\n\n        items = {\n            \"query\": query_bundle.query_str,\n            \"hits\": [{\"content\": node.get_content()} for node in nodes],\n        }\n\n        messages = self.create_permutation_instruction(item=items)\n        permutation = self.run_llm(messages=messages)\n        return self._llm_result_to_nodes(permutation, nodes, items)\n\n    async def _apostprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n            raise ValueError(\"Query bundle must be provided.\")\n\n        items = {\n            \"query\": query_bundle.query_str,\n            \"hits\": [{\"content\": node.get_content()} for node in nodes],\n        }\n\n        messages = self.create_permutation_instruction(item=items)\n        permutation = await self.arun_llm(messages=messages)\n        return self._llm_result_to_nodes(permutation, nodes, items)\n\n    async def apostprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n        query_str: Optional[str] = None,\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes asynchronously.\"\"\"\n        if query_str is not None and query_bundle is not None:\n            raise ValueError(\"Cannot specify both query_str and query_bundle\")\n        elif query_str is not None:\n            query_bundle = QueryBundle(query_str)\n        else:\n            pass\n        return await self._apostprocess_nodes(nodes, query_bundle)\n\n    def _llm_result_to_nodes(\n        self, permutation: ChatResponse, nodes: List[NodeWithScore], items: Dict\n    ) -> List[NodeWithScore]:\n        if permutation.message is not None and permutation.message.content is not None:\n            rerank_ranks = self._receive_permutation(\n                items, str(permutation.message.content)\n            )\n            if self.verbose:\n                print_text(f\"After Reranking, new rank list for nodes: {rerank_ranks}\")\n\n            initial_results: List[NodeWithScore] = []\n\n            for idx in rerank_ranks:\n                initial_results.append(\n                    NodeWithScore(node=nodes[idx].node, score=nodes[idx].score)\n                )\n            return initial_results[: self.top_n]\n        else:\n            return nodes[: self.top_n]\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"rankgpt_rerank_prompt\": self.rankgpt_rerank_prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"rankgpt_rerank_prompt\" in prompts:\n            self.rankgpt_rerank_prompt = prompts[\"rankgpt_rerank_prompt\"]\n\n    def _get_prefix_prompt(self, query: str, num: int) -> List[ChatMessage]:\n        return [\n            ChatMessage(\n                role=\"system\",\n                content=\"You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query.\",\n            ),\n            ChatMessage(\n                role=\"user\",\n                content=f\"I will provide you with {num} passages, each indicated by number identifier []. \\nRank the passages based on their relevance to query: {query}.\",\n            ),\n            ChatMessage(role=\"assistant\", content=\"Okay, please provide the passages.\"),\n        ]\n\n    def _get_post_prompt(self, query: str, num: int) -> str:\n        return self.rankgpt_rerank_prompt.format(query=query, num=num)\n\n    def create_permutation_instruction(self, item: Dict[str, Any]) -> List[ChatMessage]:\n        query = item[\"query\"]\n        num = len(item[\"hits\"])\n\n        messages = self._get_prefix_prompt(query, num)\n        rank = 0\n        for hit in item[\"hits\"]:\n            rank += 1\n            content = hit[\"content\"]\n            content = content.replace(\"Title: Content: \", \"\")\n            content = content.strip()\n            # For Japanese should cut by character: content = content[:int(max_length)]\n            content = \" \".join(content.split()[:300])\n            messages.append(ChatMessage(role=\"user\", content=f\"[{rank}] {content}\"))\n            messages.append(\n                ChatMessage(role=\"assistant\", content=f\"Received passage [{rank}].\")\n            )\n        messages.append(\n            ChatMessage(role=\"user\", content=self._get_post_prompt(query, num))\n        )\n        return messages\n\n    def run_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return self.llm.chat(messages)\n\n    async def arun_llm(self, messages: Sequence[ChatMessage]) -> ChatResponse:\n        return await self.llm.achat(messages)\n\n    def _clean_response(self, response: str) -> str:\n        new_response = \"\"\n        for c in response:\n            if not c.isdigit():\n                new_response += \" \"\n            else:\n                new_response += c\n        return new_response.strip()\n\n    def _remove_duplicate(self, response: List[int]) -> List[int]:\n        new_response = []\n        for c in response:\n            if c not in new_response:\n                new_response.append(c)\n        return new_response\n\n    def _receive_permutation(self, item: Dict[str, Any], permutation: str) -> List[int]:\n        rank_end = len(item[\"hits\"])\n\n        response = self._clean_response(permutation)\n        response_list = [int(x) - 1 for x in response.split()]\n        response_list = self._remove_duplicate(response_list)\n        response_list = [ss for ss in response_list if ss in range(rank_end)]\n        return response_list + [\n            tt for tt in range(rank_end) if tt not in response_list\n        ]  # add the rest of the rank"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/postprocessor/sbert_rerank.py",
    "filename": "sbert_rerank.py",
    "relpath": "postprocessor/sbert_rerank.py",
    "start_line": 1,
    "end_line": 104,
    "length": 104,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "_postprocess_nodes"
    ],
    "chunk_class_names": [
      "SentenceTransformerRerank"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "_postprocess_nodes"
    ],
    "document_class_names": [
      "SentenceTransformerRerank"
    ],
    "content": "from typing import Any, List, Optional\n\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.callbacks import CBEventType, EventPayload\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle\nfrom llama_index.core.utils import infer_torch_device\n\nDEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH = 512\n\n\nclass SentenceTransformerRerank(BaseNodePostprocessor):\n    model: str = Field(description=\"Sentence transformer model name.\")\n    top_n: int = Field(description=\"Number of nodes to return sorted by score.\")\n    device: str = Field(\n        default=\"cpu\",\n        description=\"Device to use for sentence transformer.\",\n    )\n    keep_retrieval_score: bool = Field(\n        default=False,\n        description=\"Whether to keep the retrieval score in metadata.\",\n    )\n    trust_remote_code: bool = Field(\n        default=False,\n        description=\"Whether to trust remote code.\",\n    )\n    _model: Any = PrivateAttr()\n\n    def __init__(\n        self,\n        top_n: int = 2,\n        model: str = \"cross-encoder/stsb-distilroberta-base\",\n        device: Optional[str] = None,\n        keep_retrieval_score: bool = False,\n        trust_remote_code: bool = True,\n    ):\n        try:\n            from sentence_transformers import CrossEncoder  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"Cannot import sentence-transformers or torch package,\",\n                \"please `pip install torch sentence-transformers`\",\n            )\n        device = infer_torch_device() if device is None else device\n        super().__init__(\n            top_n=top_n,\n            model=model,\n            device=device,\n            keep_retrieval_score=keep_retrieval_score,\n        )\n        self._model = CrossEncoder(\n            model,\n            max_length=DEFAULT_SENTENCE_TRANSFORMER_MAX_LENGTH,\n            device=device,\n            trust_remote_code=trust_remote_code,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SentenceTransformerRerank\"\n\n    def _postprocess_nodes(\n        self,\n        nodes: List[NodeWithScore],\n        query_bundle: Optional[QueryBundle] = None,\n    ) -> List[NodeWithScore]:\n        if query_bundle is None:\n            raise ValueError(\"Missing query bundle in extra info.\")\n        if len(nodes) == 0:\n            return []\n\n        query_and_nodes = [\n            (\n                query_bundle.query_str,\n                node.node.get_content(metadata_mode=MetadataMode.EMBED),\n            )\n            for node in nodes\n        ]\n\n        with self.callback_manager.event(\n            CBEventType.RERANKING,\n            payload={\n                EventPayload.NODES: nodes,\n                EventPayload.MODEL_NAME: self.model,\n                EventPayload.QUERY_STR: query_bundle.query_str,\n                EventPayload.TOP_K: self.top_n,\n            },\n        ) as event:\n            scores = self._model.predict(query_and_nodes)\n\n            assert len(scores) == len(nodes)\n\n            for node, score in zip(nodes, scores):\n                if self.keep_retrieval_score:\n                    # keep the retrieval score in metadata\n                    node.node.metadata[\"retrieval_score\"] = node.score\n                node.score = score\n\n            new_nodes = sorted(nodes, key=lambda x: -x.score if x.score else 0)[\n                : self.top_n\n            ]\n            event.on_end(payload={EventPayload.NODES: new_nodes})\n\n        return new_nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/text_splitter/__init__.py",
    "filename": "__init__.py",
    "relpath": "text_splitter/__init__.py",
    "start_line": 1,
    "end_line": 12,
    "length": 12,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# TODO: Deprecated import support for old text splitters\nfrom llama_index.core.node_parser.text.code import CodeSplitter\nfrom llama_index.core.node_parser.text.sentence import (\n    SentenceSplitter,\n)\nfrom llama_index.core.node_parser.text.token import TokenTextSplitter\n\n__all__ = [\n    \"SentenceSplitter\",\n    \"TokenTextSplitter\",\n    \"CodeSplitter\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/playground/base.py",
    "filename": "base.py",
    "relpath": "playground/base.py",
    "start_line": 1,
    "end_line": 184,
    "length": 184,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_docs",
      "_validate_indices",
      "indices",
      "indices",
      "_validate_modes",
      "retriever_modes",
      "retriever_modes",
      "compare"
    ],
    "chunk_class_names": [
      "Playground",
      "in"
    ],
    "document_function_names": [
      "__init__",
      "from_docs",
      "_validate_indices",
      "indices",
      "indices",
      "_validate_modes",
      "retriever_modes",
      "retriever_modes",
      "compare"
    ],
    "document_class_names": [
      "Playground",
      "in"
    ],
    "content": "\"\"\"Experiment with different indices, models, and more.\"\"\"\n\nfrom __future__ import annotations\n\nimport time\nfrom typing import Any, Dict, List, Type\n\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.indices.list.base import ListRetrieverMode, SummaryIndex\nfrom llama_index.core.indices.tree.base import TreeIndex, TreeRetrieverMode\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom llama_index.core.schema import Document\nfrom llama_index.core.utils import get_color_mapping, print_text\n\nDEFAULT_INDEX_CLASSES: List[Type[BaseIndex]] = [\n    VectorStoreIndex,\n    TreeIndex,\n    SummaryIndex,\n]\n\nINDEX_SPECIFIC_QUERY_MODES_TYPE = Dict[Type[BaseIndex], List[str]]\n\nDEFAULT_MODES: INDEX_SPECIFIC_QUERY_MODES_TYPE = {\n    TreeIndex: [e.value for e in TreeRetrieverMode],\n    SummaryIndex: [e.value for e in ListRetrieverMode],\n    VectorStoreIndex: [\"default\"],\n}\n\n\nclass Playground:\n    \"\"\"Experiment with indices, models, embeddings, retriever_modes, and more.\"\"\"\n\n    def __init__(\n        self,\n        indices: List[BaseIndex],\n        retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE = DEFAULT_MODES,\n    ):\n        \"\"\"Initialize with indices to experiment with.\n\n        Args:\n            indices: A list of BaseIndex's to experiment with\n            retriever_modes: A list of retriever_modes that specify which nodes are\n                chosen from the index when a query is made. A full list of\n                retriever_modes available to each index can be found here:\n                https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/retriever_modes.html\n        \"\"\"\n        self._validate_indices(indices)\n        self._indices = indices\n        self._validate_modes(retriever_modes)\n        self._retriever_modes = retriever_modes\n\n        index_range = [str(i) for i in range(len(indices))]\n        self.index_colors = get_color_mapping(index_range)\n\n    @classmethod\n    def from_docs(\n        cls,\n        documents: List[Document],\n        index_classes: List[Type[BaseIndex]] = DEFAULT_INDEX_CLASSES,\n        retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE = DEFAULT_MODES,\n        **kwargs: Any,\n    ) -> Playground:\n        \"\"\"Initialize with Documents using the default list of indices.\n\n        Args:\n            documents: A List of Documents to experiment with.\n        \"\"\"\n        if len(documents) == 0:\n            raise ValueError(\n                \"Playground must be initialized with a nonempty list of Documents.\"\n            )\n\n        indices = [\n            index_class.from_documents(documents, **kwargs)\n            for index_class in index_classes\n        ]\n        return cls(indices, retriever_modes)\n\n    def _validate_indices(self, indices: List[BaseIndex]) -> None:\n        \"\"\"Validate a list of indices.\"\"\"\n        if len(indices) == 0:\n            raise ValueError(\"Playground must have a non-empty list of indices.\")\n        for index in indices:\n            if not isinstance(index, BaseIndex):\n                raise ValueError(\n                    \"Every index in Playground should be an instance of BaseIndex.\"\n                )\n\n    @property\n    def indices(self) -> List[BaseIndex]:\n        \"\"\"Get Playground's indices.\"\"\"\n        return self._indices\n\n    @indices.setter\n    def indices(self, indices: List[BaseIndex]) -> None:\n        \"\"\"Set Playground's indices.\"\"\"\n        self._validate_indices(indices)\n        self._indices = indices\n\n    def _validate_modes(self, retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE) -> None:\n        \"\"\"Validate a list of retriever_modes.\"\"\"\n        if len(retriever_modes) == 0:\n            raise ValueError(\n                \"Playground must have a nonzero number of retriever_modes.\"\n                \"Initialize without the `retriever_modes` \"\n                \"argument to use the default list.\"\n            )\n\n    @property\n    def retriever_modes(self) -> dict:\n        \"\"\"Get Playground's indices.\"\"\"\n        return self._retriever_modes\n\n    @retriever_modes.setter\n    def retriever_modes(self, retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE) -> None:\n        \"\"\"Set Playground's indices.\"\"\"\n        self._validate_modes(retriever_modes)\n        self._retriever_modes = retriever_modes\n\n    def compare(\n        self, query_text: str, to_pandas: bool | None = True\n    ) -> Any | List[Dict[str, Any]]:\n        \"\"\"Compare index outputs on an input query.\n\n        Args:\n            query_text (str): Query to run all indices on.\n            to_pandas (Optional[bool]): Return results in a pandas dataframe.\n                True by default.\n\n        Returns:\n            The output of each index along with other data, such as the time it took to\n            compute. Results are stored in a Pandas Dataframe or a list of Dicts.\n        \"\"\"\n        print(f\"\\033[1mQuery:\\033[0m\\n{query_text}\\n\")\n        result = []\n        for i, index in enumerate(self._indices):\n            for retriever_mode in self._retriever_modes[type(index)]:\n                start_time = time.time()\n\n                index_name = type(index).__name__\n                print_text(\n                    f\"\\033[1m{index_name}\\033[0m, retriever mode = {retriever_mode}\",\n                    end=\"\\n\",\n                )\n\n                # insert token counter into service context\n                token_counter = TokenCountingHandler()\n                callback_manager = CallbackManager([token_counter])\n\n                try:\n                    query_engine = index.as_query_engine(retriever_mode=retriever_mode)\n                except ValueError:\n                    continue\n\n                output = query_engine.query(query_text)\n                print_text(str(output), color=self.index_colors[str(i)], end=\"\\n\\n\")\n\n                duration = time.time() - start_time\n\n                result.append(\n                    {\n                        \"Index\": index_name,\n                        \"Retriever Mode\": retriever_mode,\n                        \"Output\": str(output),\n                        \"Duration\": duration,\n                        \"Prompt Tokens\": token_counter.prompt_llm_token_count,\n                        \"Completion Tokens\": token_counter.completion_llm_token_count,\n                        \"Embed Tokens\": token_counter.total_embedding_token_count,\n                    }\n                )\n        print(f\"\\nRan {len(result)} combinations in total.\")\n\n        if to_pandas:\n            try:\n                import pandas as pd\n            except ImportError:\n                raise ImportError(\n                    \"pandas is required for this function. Please install it with `pip install pandas`.\"\n                )\n\n            return pd.DataFrame(result)\n        else:\n            return result"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/playground/__init__.py",
    "filename": "__init__.py",
    "relpath": "playground/__init__.py",
    "start_line": 1,
    "end_line": 10,
    "length": 10,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file of Playground.\"\"\"\n\n# playground\nfrom llama_index.core.playground.base import (\n    DEFAULT_INDEX_CLASSES,\n    DEFAULT_MODES,\n    Playground,\n)\n\n__all__ = [\"Playground\", \"DEFAULT_INDEX_CLASSES\", \"DEFAULT_MODES\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/utilities/sql_wrapper.py",
    "filename": "sql_wrapper.py",
    "relpath": "utilities/sql_wrapper.py",
    "start_line": 1,
    "end_line": 247,
    "length": 247,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "engine",
      "metadata_obj",
      "from_uri",
      "dialect",
      "get_usable_table_names",
      "get_table_columns",
      "get_single_table_info",
      "insert_into_table",
      "truncate_word",
      "run_sql"
    ],
    "chunk_class_names": [
      "SQLDatabase",
      "provides"
    ],
    "document_function_names": [
      "__init__",
      "engine",
      "metadata_obj",
      "from_uri",
      "dialect",
      "get_usable_table_names",
      "get_table_columns",
      "get_single_table_info",
      "insert_into_table",
      "truncate_word",
      "run_sql"
    ],
    "document_class_names": [
      "SQLDatabase",
      "provides"
    ],
    "content": "\"\"\"SQL wrapper around SQLDatabase in langchain.\"\"\"\n\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple\n\nfrom sqlalchemy import MetaData, create_engine, insert, inspect, text\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\n\nclass SQLDatabase:\n    \"\"\"SQL Database.\n\n    This class provides a wrapper around the SQLAlchemy engine to interact with a SQL\n    database.\n    It provides methods to execute SQL commands, insert data into tables, and retrieve\n    information about the database schema.\n    It also supports optional features such as including or excluding specific tables,\n    sampling rows for table info,\n    including indexes in table info, and supporting views.\n\n    Based on langchain SQLDatabase.\n    https://github.com/langchain-ai/langchain/blob/e355606b1100097665207ca259de6dc548d44c78/libs/langchain/langchain/utilities/sql_database.py#L39\n\n    Args:\n        engine (Engine): The SQLAlchemy engine instance to use for database operations.\n        schema (Optional[str]): The name of the schema to use, if any.\n        metadata (Optional[MetaData]): The metadata instance to use, if any.\n        ignore_tables (Optional[List[str]]): List of table names to ignore. If set,\n            include_tables must be None.\n        include_tables (Optional[List[str]]): List of table names to include. If set,\n            ignore_tables must be None.\n        sample_rows_in_table_info (int): The number of sample rows to include in table\n            info.\n        indexes_in_table_info (bool): Whether to include indexes in table info.\n        custom_table_info (Optional[dict]): Custom table info to use.\n        view_support (bool): Whether to support views.\n        max_string_length (int): The maximum string length to use.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        engine: Engine,\n        schema: Optional[str] = None,\n        metadata: Optional[MetaData] = None,\n        ignore_tables: Optional[List[str]] = None,\n        include_tables: Optional[List[str]] = None,\n        sample_rows_in_table_info: int = 3,\n        indexes_in_table_info: bool = False,\n        custom_table_info: Optional[dict] = None,\n        view_support: bool = False,\n        max_string_length: int = 300,\n    ):\n        \"\"\"Create engine from database URI.\"\"\"\n        self._engine = engine\n        self._schema = schema\n        if include_tables and ignore_tables:\n            raise ValueError(\"Cannot specify both include_tables and ignore_tables\")\n\n        self._inspector = inspect(self._engine)\n\n        # including view support by adding the views as well as tables to the all\n        # tables list if view_support is True\n        self._all_tables = set(\n            self._inspector.get_table_names(schema=schema)\n            + (self._inspector.get_view_names(schema=schema) if view_support else [])\n        )\n\n        self._include_tables = set(include_tables) if include_tables else set()\n        if self._include_tables:\n            missing_tables = self._include_tables - self._all_tables\n            if missing_tables:\n                raise ValueError(\n                    f\"include_tables {missing_tables} not found in database\"\n                )\n        self._ignore_tables = set(ignore_tables) if ignore_tables else set()\n        if self._ignore_tables:\n            missing_tables = self._ignore_tables - self._all_tables\n            if missing_tables:\n                raise ValueError(\n                    f\"ignore_tables {missing_tables} not found in database\"\n                )\n        usable_tables = self.get_usable_table_names()\n        self._usable_tables = set(usable_tables) if usable_tables else self._all_tables\n\n        if not isinstance(sample_rows_in_table_info, int):\n            raise TypeError(\"sample_rows_in_table_info must be an integer\")\n\n        self._sample_rows_in_table_info = sample_rows_in_table_info\n        self._indexes_in_table_info = indexes_in_table_info\n\n        self._custom_table_info = custom_table_info\n        if self._custom_table_info:\n            if not isinstance(self._custom_table_info, dict):\n                raise TypeError(\n                    \"table_info must be a dictionary with table names as keys and the \"\n                    \"desired table info as values\"\n                )\n            # only keep the tables that are also present in the database\n            intersection = set(self._custom_table_info).intersection(self._all_tables)\n            self._custom_table_info = {\n                table: info\n                for table, info in self._custom_table_info.items()\n                if table in intersection\n            }\n\n        self._max_string_length = max_string_length\n\n        self._metadata = metadata or MetaData()\n        # including view support if view_support = true\n        self._metadata.reflect(\n            views=view_support,\n            bind=self._engine,\n            only=list(self._usable_tables),\n            schema=self._schema,\n        )\n\n    @property\n    def engine(self) -> Engine:\n        \"\"\"Return SQL Alchemy engine.\"\"\"\n        return self._engine\n\n    @property\n    def metadata_obj(self) -> MetaData:\n        \"\"\"Return SQL Alchemy metadata.\"\"\"\n        return self._metadata\n\n    @classmethod\n    def from_uri(\n        cls, database_uri: str, engine_args: Optional[dict] = None, **kwargs: Any\n    ) -> \"SQLDatabase\":\n        \"\"\"Construct a SQLAlchemy engine from URI.\"\"\"\n        _engine_args = engine_args or {}\n        return cls(create_engine(database_uri, **_engine_args), **kwargs)\n\n    @property\n    def dialect(self) -> str:\n        \"\"\"Return string representation of dialect to use.\"\"\"\n        return self._engine.dialect.name\n\n    def get_usable_table_names(self) -> Iterable[str]:\n        \"\"\"Get names of tables available.\"\"\"\n        if self._include_tables:\n            return sorted(self._include_tables)\n        return sorted(self._all_tables - self._ignore_tables)\n\n    def get_table_columns(self, table_name: str) -> List[Any]:\n        \"\"\"Get table columns.\"\"\"\n        return self._inspector.get_columns(table_name)\n\n    def get_single_table_info(self, table_name: str) -> str:\n        \"\"\"Get table info for a single table.\"\"\"\n        # same logic as table_info, but with specific table names\n        template = \"Table '{table_name}' has columns: {columns}, \"\n        try:\n            # try to retrieve table comment\n            table_comment = self._inspector.get_table_comment(\n                table_name, schema=self._schema\n            )[\"text\"]\n            if table_comment:\n                template += f\"with comment: ({table_comment}) \"\n        except NotImplementedError:\n            # get_table_comment raises NotImplementedError for a dialect that does not support comments.\n            pass\n\n        template += \"{foreign_keys}.\"\n        columns = []\n        for column in self._inspector.get_columns(table_name, schema=self._schema):\n            if column.get(\"comment\"):\n                columns.append(\n                    f\"{column['name']} ({column['type']!s}): \"\n                    f\"'{column.get('comment')}'\"\n                )\n            else:\n                columns.append(f\"{column['name']} ({column['type']!s})\")\n\n        column_str = \", \".join(columns)\n        foreign_keys = []\n        for foreign_key in self._inspector.get_foreign_keys(\n            table_name, schema=self._schema\n        ):\n            foreign_keys.append(\n                f\"{foreign_key['constrained_columns']} -> \"\n                f\"{foreign_key['referred_table']}.{foreign_key['referred_columns']}\"\n            )\n        foreign_key_str = (\n            foreign_keys\n            and \" and foreign keys: {}\".format(\", \".join(foreign_keys))\n            or \"\"\n        )\n        return template.format(\n            table_name=table_name, columns=column_str, foreign_keys=foreign_key_str\n        )\n\n    def insert_into_table(self, table_name: str, data: dict) -> None:\n        \"\"\"Insert data into a table.\"\"\"\n        table = self._metadata.tables[table_name]\n        stmt = insert(table).values(**data)\n        with self._engine.begin() as connection:\n            connection.execute(stmt)\n\n    def truncate_word(self, content: Any, *, length: int, suffix: str = \"...\") -> str:\n        \"\"\"\n        Truncate a string to a certain number of words, based on the max string\n        length.\n        \"\"\"\n        if not isinstance(content, str) or length <= 0:\n            return content\n\n        if len(content) <= length:\n            return content\n\n        return content[: length - len(suffix)].rsplit(\" \", 1)[0] + suffix\n\n    def run_sql(self, command: str) -> Tuple[str, Dict]:\n        \"\"\"Execute a SQL statement and return a string representing the results.\n\n        If the statement returns rows, a string of the results is returned.\n        If the statement returns no rows, an empty string is returned.\n        \"\"\"\n        with self._engine.begin() as connection:\n            try:\n                if self._schema:\n                    command = command.replace(\"FROM \", f\"FROM {self._schema}.\")\n                    command = command.replace(\"JOIN \", f\"JOIN {self._schema}.\")\n                cursor = connection.execute(text(command))\n            except (ProgrammingError, OperationalError) as exc:\n                raise NotImplementedError(\n                    f\"Statement {command!r} is invalid SQL.\\nError: {exc.orig}\"\n                ) from exc\n            if cursor.returns_rows:\n                result = cursor.fetchall()\n                # truncate the results to the max string length\n                # we can't use str(result) directly because it automatically truncates long strings\n                truncated_results = []\n                for row in result:\n                    # truncate each column, then convert the row to a tuple\n                    truncated_row = tuple(\n                        self.truncate_word(column, length=self._max_string_length)\n                        for column in row\n                    )\n                    truncated_results.append(truncated_row)\n                return str(truncated_results), {\n                    \"result\": truncated_results,\n                    \"col_keys\": list(cursor.keys()),\n                }\n        return \"\", {}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/utilities/gemini_utils.py",
    "filename": "gemini_utils.py",
    "relpath": "utilities/gemini_utils.py",
    "start_line": 1,
    "end_line": 65,
    "length": 65,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "merge_neighboring_same_role_messages"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "merge_neighboring_same_role_messages"
    ],
    "document_class_names": [],
    "content": "\"\"\"Global Gemini Utilities (shared between Gemini LLM and Vertex).\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Sequence\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\n\nROLES_TO_GEMINI: dict[MessageRole, MessageRole] = {\n    MessageRole.USER: MessageRole.USER,\n    MessageRole.ASSISTANT: MessageRole.MODEL,\n    ## Gemini chat mode only has user and model roles. Put the rest in user role.\n    MessageRole.SYSTEM: MessageRole.USER,\n    MessageRole.MODEL: MessageRole.MODEL,\n    ## Gemini has function role, but chat mode only accepts user and model roles.\n    ## https://medium.com/@smallufo/openai-vs-gemini-function-calling-a664f7f2b29f\n    ## Agent response's 'tool/function' role is converted to 'user' role.\n    MessageRole.TOOL: MessageRole.USER,\n    MessageRole.FUNCTION: MessageRole.USER,\n}\nROLES_FROM_GEMINI: dict[str, MessageRole] = {\n    ## Gemini has user, model and function roles.\n    \"user\": MessageRole.USER,\n    \"model\": MessageRole.ASSISTANT,\n    \"function\": MessageRole.TOOL,\n}\n\n\ndef merge_neighboring_same_role_messages(\n    messages: Sequence[ChatMessage],\n) -> Sequence[ChatMessage]:\n    if len(messages) < 2:\n        # Nothing to merge\n        return messages\n\n    # Gemini does not support multiple messages of the same role in a row, so we merge them\n    merged_messages = []\n    i = 0\n\n    while i < len(messages):\n        current_message = messages[i]\n        # Initialize merged content with current message content\n        merged_content = [current_message.content]\n\n        # Check if the next message exists and has the same role\n        while (\n            i + 1 < len(messages)\n            and ROLES_TO_GEMINI[messages[i + 1].role]\n            == ROLES_TO_GEMINI[current_message.role]\n        ):\n            i += 1\n            next_message = messages[i]\n            merged_content.extend([next_message.content])\n\n        # Create a new ChatMessage or similar object with merged content\n        merged_message = ChatMessage(\n            role=ROLES_TO_GEMINI[current_message.role],\n            content=\"\\n\".join([str(msg_content) for msg_content in merged_content]),\n            additional_kwargs=current_message.additional_kwargs,\n        )\n\n        merged_messages.append(merged_message)\n        i += 1\n\n    return merged_messages"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/utilities/token_counting.py",
    "filename": "token_counting.py",
    "relpath": "utilities/token_counting.py",
    "start_line": 1,
    "end_line": 94,
    "length": 94,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "get_string_tokens",
      "estimate_tokens_in_messages",
      "estimate_tokens_in_tools"
    ],
    "chunk_class_names": [
      "TokenCounter"
    ],
    "document_function_names": [
      "__init__",
      "get_string_tokens",
      "estimate_tokens_in_messages",
      "estimate_tokens_in_tools"
    ],
    "document_class_names": [
      "TokenCounter"
    ],
    "content": "# Modified from:\n# https://github.com/nyno-ai/openai-token-counter\n\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.utils import get_tokenizer\n\n\nclass TokenCounter:\n    \"\"\"Token counter class.\n\n    Attributes:\n        model (Optional[str]): The model to use for token counting.\n    \"\"\"\n\n    def __init__(self, tokenizer: Optional[Callable[[str], list]] = None) -> None:\n        self.tokenizer = tokenizer or get_tokenizer()\n\n    def get_string_tokens(self, string: str) -> int:\n        \"\"\"Get the token count for a string.\n\n        Args:\n            string (str): The string to count.\n\n        Returns:\n            int: The token count.\n        \"\"\"\n        return len(self.tokenizer(string))\n\n    def estimate_tokens_in_messages(self, messages: List[ChatMessage]) -> int:\n        \"\"\"Estimate token count for a single message.\n\n        Args:\n            message (OpenAIMessage): The message to estimate the token count for.\n\n        Returns:\n            int: The estimated token count.\n        \"\"\"\n        tokens = 0\n\n        for message in messages:\n            if message.role:\n                tokens += self.get_string_tokens(message.role)\n\n            if isinstance(message.content, str):\n                tokens += self.get_string_tokens(message.content)\n\n            additional_kwargs = {**message.additional_kwargs}\n\n            # backward compatibility\n            if \"function_call\" in additional_kwargs:\n                function_call = additional_kwargs.pop(\"function_call\")\n                if function_call.get(\"name\", None) is not None:\n                    tokens += self.get_string_tokens(function_call[\"name\"])\n\n                if function_call.get(\"arguments\", None) is not None:\n                    tokens += self.get_string_tokens(function_call[\"arguments\"])\n\n                tokens += 3  # Additional tokens for function call\n\n            if \"tool_calls\" in additional_kwargs:\n                for tool_call in additional_kwargs[\"tool_calls\"]:\n                    if (\n                        hasattr(tool_call, \"function\")\n                        and tool_call.function is not None\n                    ):\n                        tokens += self.get_string_tokens(tool_call.function.name)\n                        tokens += self.get_string_tokens(tool_call.function.arguments)\n\n                        tokens += 3  # Additional tokens for tool call\n\n            tokens += 3  # Add three per message\n\n            if message.role == MessageRole.FUNCTION or message.role == MessageRole.TOOL:\n                tokens -= 2  # Subtract 2 if role is \"function\"\n\n        return tokens\n\n    def estimate_tokens_in_tools(self, tools: List[Dict[str, Any]]) -> int:\n        \"\"\"Estimate token count for the tools.\n\n        We take here a list of tools created using the `to_openai_tool()` function (or similar).\n\n        Args:\n            tools (list[Dict[str, Any]]): The tools to estimate the token count for.\n\n        Returns:\n            int: The estimated token count.\n        \"\"\"\n        if not tools:\n            return 0\n\n        return self.get_string_tokens(str(tools))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/utilities/aws_utils.py",
    "filename": "aws_utils.py",
    "relpath": "utilities/aws_utils.py",
    "start_line": 1,
    "end_line": 50,
    "length": 50,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_aws_service_client"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_aws_service_client"
    ],
    "document_class_names": [],
    "content": "from typing import TYPE_CHECKING, Optional\n\nif TYPE_CHECKING:\n    import botocore\n\n\ndef get_aws_service_client(\n    service_name: Optional[str] = None,\n    region_name: Optional[str] = None,\n    aws_access_key_id: Optional[str] = None,\n    aws_secret_access_key: Optional[str] = None,\n    aws_session_token: Optional[str] = None,\n    profile_name: Optional[str] = None,\n    max_retries: Optional[int] = 3,\n    timeout: Optional[float] = 60.0,\n) -> \"botocore.client.BaseClient\":\n    try:\n        import boto3\n        import botocore\n    except ImportError:\n        raise ImportError(\n            \"Please run `pip install boto3 botocore` to use AWS services.\"\n        )\n\n    config = botocore.config.Config(\n        retries={\"max_attempts\": max_retries or 0, \"mode\": \"standard\"},\n        connect_timeout=timeout,\n    )\n\n    try:\n        if not profile_name and aws_access_key_id:\n            session = boto3.Session(\n                aws_access_key_id=aws_access_key_id,\n                aws_secret_access_key=aws_secret_access_key,\n                aws_session_token=aws_session_token,\n                region_name=region_name,\n            )\n            client = session.client(service_name, config=config)  # type: ignore\n        else:\n            session = boto3.Session(profile_name=profile_name)\n            if region_name:\n                client = session.client(\n                    service_name, region_name=region_name, config=config  # type: ignore\n                )\n            else:\n                client = session.client(service_name, config=config)  # type: ignore\n    except Exception as e:\n        raise ValueError(\"Please verify the provided credentials.\") from (e)\n\n    return client"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response/__init__.py",
    "filename": "__init__.py",
    "relpath": "response/__init__.py",
    "start_line": 1,
    "end_line": 3,
    "length": 3,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.base.response.schema import Response\n\n__all__ = [\"Response\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response/utils.py",
    "filename": "utils.py",
    "relpath": "response/utils.py",
    "start_line": 1,
    "end_line": 19,
    "length": 19,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_response_text",
      "aget_response_text"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_response_text",
      "aget_response_text"
    ],
    "document_class_names": [],
    "content": "\"\"\"Utilities for response.\"\"\"\n\nfrom typing import AsyncGenerator, Generator\n\n\ndef get_response_text(response_gen: Generator) -> str:\n    \"\"\"Get response text.\"\"\"\n    response_text = \"\"\n    for response in response_gen:\n        response_text += response\n    return response_text\n\n\nasync def aget_response_text(response_gen: AsyncGenerator) -> str:\n    \"\"\"Get response text.\"\"\"\n    response_text = \"\"\n    async for response in response_gen:\n        response_text += response\n    return response_text"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response/pprint_utils.py",
    "filename": "pprint_utils.py",
    "relpath": "response/pprint_utils.py",
    "start_line": 1,
    "end_line": 49,
    "length": 49,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "pprint_metadata",
      "pprint_source_node",
      "pprint_response"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "pprint_metadata",
      "pprint_source_node",
      "pprint_response"
    ],
    "document_class_names": [],
    "content": "\"\"\"Utils for pretty print.\"\"\"\nimport textwrap\nfrom pprint import pprint\nfrom typing import Any, Dict\n\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core.utils import truncate_text\n\n\ndef pprint_metadata(metadata: Dict[str, Any]) -> None:\n    \"\"\"Display metadata for jupyter notebook.\"\"\"\n    pprint(metadata)\n\n\ndef pprint_source_node(\n    source_node: NodeWithScore, source_length: int = 350, wrap_width: int = 70\n) -> None:\n    \"\"\"Display source node for jupyter notebook.\"\"\"\n    source_text_fmt = truncate_text(\n        source_node.node.get_content().strip(), source_length\n    )\n    print(f\"Node ID: {source_node.node.node_id}\")\n    print(f\"Similarity: {source_node.score}\")\n    print(textwrap.fill(f\"Text: {source_text_fmt}\\n\", width=wrap_width))\n\n\ndef pprint_response(\n    response: Response,\n    source_length: int = 350,\n    wrap_width: int = 70,\n    show_source: bool = False,\n) -> None:\n    \"\"\"Pretty print response for jupyter notebook.\"\"\"\n    if response.response is None:\n        response_text = \"None\"\n    else:\n        response_text = response.response.strip()\n\n    response_text = f\"Final Response: {response_text}\"\n    print(textwrap.fill(response_text, width=wrap_width))\n\n    if show_source:\n        for ind, source_node in enumerate(response.source_nodes):\n            print(\"_\" * wrap_width)\n            print(f\"Source Node {ind + 1}/{len(response.source_nodes)}\")\n            pprint_source_node(\n                source_node, source_length=source_length, wrap_width=wrap_width\n            )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/response/notebook_utils.py",
    "filename": "notebook_utils.py",
    "relpath": "response/notebook_utils.py",
    "start_line": 1,
    "end_line": 147,
    "length": 147,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "display_image",
      "display_image_uris",
      "display_source_node",
      "display_metadata",
      "display_response",
      "display_query_and_multimodal_response"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "display_image",
      "display_image_uris",
      "display_source_node",
      "display_metadata",
      "display_response",
      "display_query_and_multimodal_response"
    ],
    "document_class_names": [],
    "content": "\"\"\"Utils for jupyter notebook.\"\"\"\nimport os\nfrom io import BytesIO\nfrom typing import Any, Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport requests\nfrom IPython.display import Markdown, display\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.img_utils import b64_2_img\nfrom llama_index.core.schema import ImageNode, MetadataMode, NodeWithScore\nfrom llama_index.core.utils import truncate_text\nfrom PIL import Image\n\nDEFAULT_THUMBNAIL_SIZE = (512, 512)\nDEFAULT_IMAGE_MATRIX = (3, 3)\nDEFAULT_SHOW_TOP_K = 3\n\n\ndef display_image(img_str: str, size: Tuple[int, int] = DEFAULT_THUMBNAIL_SIZE) -> None:\n    \"\"\"Display base64 encoded image str as image for jupyter notebook.\"\"\"\n    img = b64_2_img(img_str)\n    img.thumbnail(size)\n    display(img)\n\n\ndef display_image_uris(\n    image_paths: List[str],\n    image_matrix: Tuple[int, int] = DEFAULT_IMAGE_MATRIX,\n    top_k: int = DEFAULT_SHOW_TOP_K,\n) -> None:\n    \"\"\"Display base64 encoded image str as image for jupyter notebook.\"\"\"\n    images_shown = 0\n    plt.figure(figsize=(16, 9))\n    for img_path in image_paths[:top_k]:\n        if os.path.isfile(img_path):\n            image = Image.open(img_path)\n\n            plt.subplot(image_matrix[0], image_matrix[1], images_shown + 1)\n            plt.imshow(image)\n            plt.xticks([])\n            plt.yticks([])\n\n            images_shown += 1\n            if images_shown >= image_matrix[0] * image_matrix[1]:\n                break\n\n\ndef display_source_node(\n    source_node: NodeWithScore,\n    source_length: int = 100,\n    show_source_metadata: bool = False,\n    metadata_mode: MetadataMode = MetadataMode.NONE,\n) -> None:\n    \"\"\"Display source node for jupyter notebook.\"\"\"\n    source_text_fmt = truncate_text(\n        source_node.node.get_content(metadata_mode=metadata_mode).strip(), source_length\n    )\n    text_md = (\n        f\"**Node ID:** {source_node.node.node_id}<br>\"\n        f\"**Similarity:** {source_node.score}<br>\"\n        f\"**Text:** {source_text_fmt}<br>\"\n    )\n    if show_source_metadata:\n        text_md += f\"**Metadata:** {source_node.node.metadata}<br>\"\n    if isinstance(source_node.node, ImageNode):\n        text_md += \"**Image:**\"\n\n    display(Markdown(text_md))\n    if isinstance(source_node.node, ImageNode) and source_node.node.image is not None:\n        display_image(source_node.node.image)\n\n\ndef display_metadata(metadata: Dict[str, Any]) -> None:\n    \"\"\"Display metadata for jupyter notebook.\"\"\"\n    display(metadata)\n\n\ndef display_response(\n    response: Response,\n    source_length: int = 100,\n    show_source: bool = False,\n    show_metadata: bool = False,\n    show_source_metadata: bool = False,\n) -> None:\n    \"\"\"Display response for jupyter notebook.\"\"\"\n    if response.response is None:\n        response_text = \"None\"\n    else:\n        response_text = response.response.strip()\n\n    display(Markdown(f\"**`Final Response:`** {response_text}\"))\n    if show_source:\n        for ind, source_node in enumerate(response.source_nodes):\n            display(Markdown(\"---\"))\n            display(\n                Markdown(f\"**`Source Node {ind + 1}/{len(response.source_nodes)}`**\")\n            )\n            display_source_node(\n                source_node,\n                source_length=source_length,\n                show_source_metadata=show_source_metadata,\n            )\n    if show_metadata:\n        if response.metadata is not None:\n            display_metadata(response.metadata)\n\n\ndef display_query_and_multimodal_response(\n    query_str: str, response: Response, plot_height: int = 2, plot_width: int = 5\n) -> None:\n    \"\"\"For displaying a query and its multi-modal response.\"\"\"\n    if response.metadata:\n        image_nodes = response.metadata[\"image_nodes\"] or []\n    else:\n        image_nodes = []\n    num_subplots = len(image_nodes)\n\n    f, axarr = plt.subplots(1, num_subplots)\n    f.set_figheight(plot_height)\n    f.set_figwidth(plot_width)\n    ix = 0\n    for ix, scored_img_node in enumerate(image_nodes):\n        img_node = scored_img_node.node\n        image = None\n        if img_node.image_url:\n            img_response = requests.get(img_node.image_url)\n            image = Image.open(BytesIO(img_response.content)).convert(\"RGB\")\n        elif img_node.image_path:\n            image = Image.open(img_node.image_path).convert(\"RGB\")\n        else:\n            raise ValueError(\n                \"A retrieved image must have image_path or image_url specified.\"\n            )\n        if num_subplots > 1:\n            axarr[ix].imshow(image)\n            axarr[ix].set_title(f\"Retrieved Position: {ix}\", pad=10, fontsize=9)\n        else:\n            axarr.imshow(image)\n            axarr.set_title(f\"Retrieved Position: {ix}\", pad=10, fontsize=9)\n\n    f.tight_layout()\n    print(f\"Query: {query_str}\\n=======\")\n    print(f\"Retrieved Images:\\n\")\n    plt.show()\n    print(\"=======\")\n    print(f\"Response: {response.response}\\n=======\\n\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/selectors/__init__.py",
    "filename": "__init__.py",
    "relpath": "selectors/__init__.py",
    "start_line": 1,
    "end_line": 29,
    "length": 29,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.base.base_selector import (\n    BaseSelector,\n    MultiSelection,\n    SingleSelection,\n    SelectorResult,\n)\nfrom llama_index.core.selectors.embedding_selectors import EmbeddingSingleSelector\nfrom llama_index.core.selectors.llm_selectors import (\n    LLMMultiSelector,\n    LLMSingleSelector,\n)\nfrom llama_index.core.selectors.pydantic_selectors import (\n    PydanticMultiSelector,\n    PydanticSingleSelector,\n)\n\n__all__ = [\n    # Bases + Types\n    \"BaseSelector\",\n    \"MultiSelection\",\n    \"SelectorResult\",\n    \"SingleSelection\",\n    # Classes\n    \"LLMSingleSelector\",\n    \"LLMMultiSelector\",\n    \"EmbeddingSingleSelector\",\n    \"PydanticSingleSelector\",\n    \"PydanticMultiSelector\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/selectors/utils.py",
    "filename": "utils.py",
    "relpath": "selectors/utils.py",
    "start_line": 1,
    "end_line": 32,
    "length": 32,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_selector_from_llm"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_selector_from_llm"
    ],
    "document_class_names": [],
    "content": "from typing import Optional\n\nfrom llama_index.core.base.base_selector import BaseSelector\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.selectors.llm_selectors import (\n    LLMMultiSelector,\n    LLMSingleSelector,\n)\nfrom llama_index.core.selectors.pydantic_selectors import (\n    PydanticMultiSelector,\n    PydanticSingleSelector,\n)\n\n\ndef get_selector_from_llm(llm: LLM, is_multi: bool = False) -> BaseSelector:\n    \"\"\"Get a selector from a service context. Prefers Pydantic selectors if possible.\"\"\"\n    selector: Optional[BaseSelector] = None\n\n    if is_multi:\n        try:\n            selector = PydanticMultiSelector.from_defaults(llm=llm)  # type: ignore\n        except ValueError:\n            selector = LLMMultiSelector.from_defaults(llm=llm)\n    else:\n        try:\n            selector = PydanticSingleSelector.from_defaults(llm=llm)  # type: ignore\n        except ValueError:\n            selector = LLMSingleSelector.from_defaults(llm=llm)\n\n    assert selector is not None\n\n    return selector"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/selectors/embedding_selectors.py",
    "filename": "embedding_selectors.py",
    "relpath": "selectors/embedding_selectors.py",
    "start_line": 1,
    "end_line": 91,
    "length": 91,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect"
    ],
    "chunk_class_names": [
      "EmbeddingSingleSelector"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect"
    ],
    "document_class_names": [
      "EmbeddingSingleSelector"
    ],
    "content": "from typing import Any, Dict, Optional, Sequence\n\nfrom llama_index.core.base.base_selector import (\n    BaseSelector,\n    SelectorResult,\n    SingleSelection,\n)\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.indices.query.embedding_utils import get_top_k_embeddings\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.types import ToolMetadata\n\n\nclass EmbeddingSingleSelector(BaseSelector):\n    \"\"\"Embedding selector.\n\n    Embedding selector that chooses one out of many options.\n\n    Args:\n        embed_model (BaseEmbedding): An embedding model.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_model: BaseEmbedding,\n    ) -> None:\n        self._embed_model = embed_model\n\n    @classmethod\n    def from_defaults(\n        cls,\n        embed_model: Optional[BaseEmbedding] = None,\n    ) -> \"EmbeddingSingleSelector\":\n        # optionally initialize defaults\n        embed_model = embed_model or Settings.embed_model\n\n        # construct prompt\n        return cls(embed_model)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        query_embedding = self._embed_model.get_query_embedding(query.query_str)\n        text_embeddings = [\n            self._embed_model.get_text_embedding(choice.description)\n            for choice in choices\n        ]\n\n        top_similarities, top_ids = get_top_k_embeddings(\n            query_embedding,\n            text_embeddings,\n            similarity_top_k=1,\n            embedding_ids=list(range(len(choices))),\n        )\n        # get top choice\n        top_selection_reason = f\"Top similarity match: {top_similarities[0]:.2f}, {choices[top_ids[0]].name}\"\n        top_selection = SingleSelection(index=top_ids[0], reason=top_selection_reason)\n\n        # parse output\n        return SelectorResult(selections=[top_selection])\n\n    async def _aselect(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        query_embedding = await self._embed_model.aget_query_embedding(query.query_str)\n        text_embeddings = [\n            await self._embed_model.aget_text_embedding(choice.description)\n            for choice in choices\n        ]\n\n        top_similarities, top_ids = get_top_k_embeddings(\n            query_embedding,\n            text_embeddings,\n            similarity_top_k=1,\n            embedding_ids=list(range(len(choices))),\n        )\n        # get top choice\n        top_selection_reason = f\"Top similarity match: {top_similarities[0]:.2f}, {choices[top_ids[0]].name}\"\n        top_selection = SingleSelection(index=top_ids[0], reason=top_selection_reason)\n\n        # parse output\n        return SelectorResult(selections=[top_selection])"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/selectors/pydantic_selectors.py",
    "filename": "pydantic_selectors.py",
    "relpath": "selectors/pydantic_selectors.py",
    "start_line": 1,
    "end_line": 175,
    "length": 175,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_pydantic_output_to_selector_result",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect"
    ],
    "chunk_class_names": [
      "PydanticSingleSelector",
      "PydanticMultiSelector"
    ],
    "document_function_names": [
      "_pydantic_output_to_selector_result",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect"
    ],
    "document_class_names": [
      "PydanticSingleSelector",
      "PydanticMultiSelector"
    ],
    "content": "from typing import TYPE_CHECKING, Any, Dict, Optional, Sequence\n\nfrom llama_index.core.base.base_selector import (\n    BaseSelector,\n    MultiSelection,\n    SelectorResult,\n    SingleSelection,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.selectors.llm_selectors import _build_choices_text\nfrom llama_index.core.selectors.prompts import (\n    DEFAULT_MULTI_PYD_SELECT_PROMPT_TMPL,\n    DEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL,\n)\nfrom llama_index.core.tools.types import ToolMetadata\nfrom llama_index.core.types import BasePydanticProgram\n\nif TYPE_CHECKING:\n    from llama_index.llms.openai import OpenAI  # pants: no-infer-dep\n\n\ndef _pydantic_output_to_selector_result(output: Any) -> SelectorResult:\n    \"\"\"\n    Convert pydantic output to selector result.\n    Takes into account zero-indexing on answer indexes.\n    \"\"\"\n    if isinstance(output, SingleSelection):\n        output.index -= 1\n        return SelectorResult(selections=[output])\n    elif isinstance(output, MultiSelection):\n        for idx in range(len(output.selections)):\n            output.selections[idx].index -= 1\n        return SelectorResult(selections=output.selections)\n    else:\n        raise ValueError(f\"Unsupported output type: {type(output)}\")\n\n\nclass PydanticSingleSelector(BaseSelector):\n    def __init__(self, selector_program: BasePydanticProgram) -> None:\n        self._selector_program = selector_program\n\n    @classmethod\n    def from_defaults(\n        cls,\n        program: Optional[BasePydanticProgram] = None,\n        llm: Optional[\"OpenAI\"] = None,\n        prompt_template_str: str = DEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL,\n        verbose: bool = False,\n    ) -> \"PydanticSingleSelector\":\n        try:\n            from llama_index.program.openai import (\n                OpenAIPydanticProgram,\n            )  # pants: no-infer-dep\n        except ImportError as e:\n            raise ImportError(\n                \"`llama-index-program-openai` package is missing. \"\n                \"Please install using `pip install llama-index-program-openai`.\"\n            )\n        if program is None:\n            program = OpenAIPydanticProgram.from_defaults(\n                output_cls=SingleSelection,\n                prompt_template_str=prompt_template_str,\n                llm=llm,\n                verbose=verbose,\n            )\n\n        return cls(selector_program=program)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        # TODO: no accessible prompts for a base pydantic program\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        choices_text = _build_choices_text(choices)\n\n        # predict\n        prediction = self._selector_program(\n            num_choices=len(choices),\n            context_list=choices_text,\n            query_str=query.query_str,\n        )\n\n        # parse output\n        return _pydantic_output_to_selector_result(prediction)\n\n    async def _aselect(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        choices_text = _build_choices_text(choices)\n\n        # predict\n        prediction = await self._selector_program.acall(\n            num_choices=len(choices),\n            context_list=choices_text,\n            query_str=query.query_str,\n        )\n\n        # parse output\n        return _pydantic_output_to_selector_result(prediction)\n\n\nclass PydanticMultiSelector(BaseSelector):\n    def __init__(\n        self, selector_program: BasePydanticProgram, max_outputs: Optional[int] = None\n    ) -> None:\n        self._selector_program = selector_program\n        self._max_outputs = max_outputs\n\n    @classmethod\n    def from_defaults(\n        cls,\n        program: Optional[BasePydanticProgram] = None,\n        llm: Optional[\"OpenAI\"] = None,\n        prompt_template_str: str = DEFAULT_MULTI_PYD_SELECT_PROMPT_TMPL,\n        max_outputs: Optional[int] = None,\n        verbose: bool = False,\n    ) -> \"PydanticMultiSelector\":\n        try:\n            from llama_index.program.openai import (\n                OpenAIPydanticProgram,\n            )  # pants: no-infer-dep\n        except ImportError as e:\n            raise ImportError(\n                \"`llama-index-program-openai` package is missing. \"\n                \"Please install using `pip install llama-index-program-openai`.\"\n            )\n        if program is None:\n            program = OpenAIPydanticProgram.from_defaults(\n                output_cls=MultiSelection,\n                prompt_template_str=prompt_template_str,\n                llm=llm,\n                verbose=verbose,\n            )\n\n        return cls(selector_program=program, max_outputs=max_outputs)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        # TODO: no accessible prompts for a base pydantic program\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        context_list = _build_choices_text(choices)\n        max_outputs = self._max_outputs or len(choices)\n\n        # predict\n        prediction = self._selector_program(\n            num_choices=len(choices),\n            max_outputs=max_outputs,\n            context_list=context_list,\n            query_str=query.query_str,\n        )\n\n        # parse output\n        return _pydantic_output_to_selector_result(prediction)\n\n    async def _aselect(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        return self._select(choices, query)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/selectors/llm_selectors.py",
    "filename": "llm_selectors.py",
    "relpath": "selectors/llm_selectors.py",
    "start_line": 1,
    "end_line": 230,
    "length": 230,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_build_choices_text",
      "_structured_output_to_selector_result",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect"
    ],
    "chunk_class_names": [
      "LLMSingleSelector",
      "LLMMultiSelector"
    ],
    "document_function_names": [
      "_build_choices_text",
      "_structured_output_to_selector_result",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect",
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "_select",
      "_aselect"
    ],
    "document_class_names": [
      "LLMSingleSelector",
      "LLMMultiSelector"
    ],
    "content": "from typing import Any, Dict, List, Optional, Sequence, cast\n\nfrom llama_index.core.base.base_selector import (\n    BaseSelector,\n    SelectorResult,\n    SingleSelection,\n)\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.output_parsers.base import StructuredOutput\nfrom llama_index.core.output_parsers.selection import Answer, SelectionOutputParser\nfrom llama_index.core.prompts.base import BasePromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.prompts.prompt_type import PromptType\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.selectors.prompts import (\n    DEFAULT_MULTI_SELECT_PROMPT_TMPL,\n    DEFAULT_SINGLE_SELECT_PROMPT_TMPL,\n    MultiSelectPrompt,\n    SingleSelectPrompt,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.types import ToolMetadata\nfrom llama_index.core.types import BaseOutputParser\n\n\ndef _build_choices_text(choices: Sequence[ToolMetadata]) -> str:\n    \"\"\"Convert sequence of metadata to enumeration text.\"\"\"\n    texts: List[str] = []\n    for ind, choice in enumerate(choices):\n        text = \" \".join(choice.description.splitlines())\n        text = f\"({ind + 1}) {text}\"  # to one indexing\n        texts.append(text)\n    return \"\\n\\n\".join(texts)\n\n\ndef _structured_output_to_selector_result(output: Any) -> SelectorResult:\n    \"\"\"Convert structured output to selector result.\"\"\"\n    structured_output = cast(StructuredOutput, output)\n    answers = cast(List[Answer], structured_output.parsed_output)\n\n    # adjust for zero indexing\n    selections = [\n        SingleSelection(index=answer.choice - 1, reason=answer.reason)\n        for answer in answers\n    ]\n    return SelectorResult(selections=selections)\n\n\nclass LLMSingleSelector(BaseSelector):\n    \"\"\"LLM single selector.\n\n    LLM-based selector that chooses one out of many options.\n\n    Args:\n        LLM (LLM): An LLM.\n        prompt (SingleSelectPrompt): A LLM prompt for selecting one out of many options.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        prompt: SingleSelectPrompt,\n    ) -> None:\n        self._llm = llm\n        self._prompt: BasePromptTemplate = prompt\n\n        if self._prompt.output_parser is None:\n            raise ValueError(\"Prompt should have output parser.\")\n\n    @classmethod\n    def from_defaults(\n        cls,\n        llm: Optional[LLM] = None,\n        prompt_template_str: Optional[str] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n    ) -> \"LLMSingleSelector\":\n        # optionally initialize defaults\n        llm = llm or Settings.llm\n        prompt_template_str = prompt_template_str or DEFAULT_SINGLE_SELECT_PROMPT_TMPL\n        output_parser = output_parser or SelectionOutputParser()\n\n        # construct prompt\n        prompt = SingleSelectPrompt(\n            template=prompt_template_str,\n            output_parser=output_parser,\n            prompt_type=PromptType.SINGLE_SELECT,\n        )\n        return cls(llm, prompt)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\"prompt\": self._prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"prompt\" in prompts:\n            self._prompt = prompts[\"prompt\"]\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        choices_text = _build_choices_text(choices)\n\n        # predict\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            num_choices=len(choices),\n            context_list=choices_text,\n            query_str=query.query_str,\n        )\n\n        # parse output\n        assert self._prompt.output_parser is not None\n        parse = self._prompt.output_parser.parse(prediction)\n        return _structured_output_to_selector_result(parse)\n\n    async def _aselect(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        choices_text = _build_choices_text(choices)\n\n        # predict\n        prediction = await self._llm.apredict(\n            prompt=self._prompt,\n            num_choices=len(choices),\n            context_list=choices_text,\n            query_str=query.query_str,\n        )\n\n        # parse output\n        assert self._prompt.output_parser is not None\n        parse = self._prompt.output_parser.parse(prediction)\n        return _structured_output_to_selector_result(parse)\n\n\nclass LLMMultiSelector(BaseSelector):\n    \"\"\"LLM multi selector.\n\n    LLM-based selector that chooses multiple out of many options.\n\n    Args:\n        llm (LLM): An LLM.\n        prompt (SingleSelectPrompt): A LLM prompt for selecting multiple out of many\n            options.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        prompt: MultiSelectPrompt,\n        max_outputs: Optional[int] = None,\n    ) -> None:\n        self._llm = llm\n        self._prompt: BasePromptTemplate = prompt\n        self._max_outputs = max_outputs\n\n        if self._prompt.output_parser is None:\n            raise ValueError(\"Prompt should have output parser.\")\n\n    @classmethod\n    def from_defaults(\n        cls,\n        llm: Optional[LLM] = None,\n        prompt_template_str: Optional[str] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n        max_outputs: Optional[int] = None,\n    ) -> \"LLMMultiSelector\":\n        llm = llm or Settings.llm\n        prompt_template_str = prompt_template_str or DEFAULT_MULTI_SELECT_PROMPT_TMPL\n        output_parser = output_parser or SelectionOutputParser()\n\n        # add output formatting\n        prompt_template_str = output_parser.format(prompt_template_str)\n\n        # construct prompt\n        prompt = MultiSelectPrompt(\n            template=prompt_template_str,\n            output_parser=output_parser,\n            prompt_type=PromptType.MULTI_SELECT,\n        )\n        return cls(llm, prompt, max_outputs)\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {\"prompt\": self._prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"prompt\" in prompts:\n            self._prompt = prompts[\"prompt\"]\n\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        context_list = _build_choices_text(choices)\n        max_outputs = self._max_outputs or len(choices)\n\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            num_choices=len(choices),\n            max_outputs=max_outputs,\n            context_list=context_list,\n            query_str=query.query_str,\n        )\n\n        assert self._prompt.output_parser is not None\n        parsed = self._prompt.output_parser.parse(prediction)\n        return _structured_output_to_selector_result(parsed)\n\n    async def _aselect(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        # prepare input\n        context_list = _build_choices_text(choices)\n        max_outputs = self._max_outputs or len(choices)\n\n        prediction = await self._llm.apredict(\n            prompt=self._prompt,\n            num_choices=len(choices),\n            max_outputs=max_outputs,\n            context_list=context_list,\n            query_str=query.query_str,\n        )\n\n        assert self._prompt.output_parser is not None\n        parsed = self._prompt.output_parser.parse(prediction)\n        return _structured_output_to_selector_result(parsed)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/selectors/prompts.py",
    "filename": "prompts.py",
    "relpath": "selectors/prompts.py",
    "start_line": 1,
    "end_line": 87,
    "length": 87,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.prompts.prompt_type import PromptType\n\n\"\"\"Single select prompt.\n\nPromptTemplate to select one out of `num_choices` options provided in `context_list`,\ngiven a query `query_str`.\n\nRequired template variables: `num_chunks`, `context_list`, `query_str`\n\n\"\"\"\nSingleSelectPrompt = PromptTemplate\n\n\"\"\"Multiple select prompt.\n\nPromptTemplate to select multiple candidates (up to `max_outputs`) out of `num_choices`\noptions provided in `context_list`, given a query `query_str`.\n\nRequired template variables: `num_chunks`, `context_list`, `query_str`,\n    `max_outputs`\n\"\"\"\nMultiSelectPrompt = PromptTemplate\n\n\n# single select\nDEFAULT_SINGLE_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return \"\n    \"the choice that is most relevant to the question: '{query_str}'\\n\"\n)\n\n\nDEFAULT_SINGLE_SELECT_PROMPT = PromptTemplate(\n    template=DEFAULT_SINGLE_SELECT_PROMPT_TMPL, prompt_type=PromptType.SINGLE_SELECT\n)\n\n\n# multiple select\nDEFAULT_MULTI_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return the top choices \"\n    \"(no more than {max_outputs}, but only select what is needed) that \"\n    \"are most relevant to the question: '{query_str}'\\n\"\n)\n\n\nDEFAULT_MULTIPLE_SELECT_PROMPT = PromptTemplate(\n    template=DEFAULT_MULTI_SELECT_PROMPT_TMPL, prompt_type=PromptType.MULTI_SELECT\n)\n\n# single pydantic select\nDEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, generate \"\n    \"the selection object and reason that is most relevant to the \"\n    \"question: '{query_str}'\\n\"\n)\n\n\n# multiple pydantic select\nDEFAULT_MULTI_PYD_SELECT_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_choices}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return the top choice(s) \"\n    \"(no more than {max_outputs}, but only select what is needed) by generating \"\n    \"the selection object and reasons that are most relevant to the \"\n    \"question: '{query_str}'\\n\"\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/graph_stores/simple_labelled.py",
    "filename": "simple_labelled.py",
    "relpath": "graph_stores/simple_labelled.py",
    "start_line": 1,
    "end_line": 307,
    "length": 307,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "get",
      "get_triplets",
      "get_rel_map",
      "upsert_nodes",
      "upsert_relations",
      "delete",
      "persist",
      "from_persist_path",
      "from_persist_dir",
      "from_dict",
      "to_dict",
      "get_schema",
      "structured_query",
      "vector_query",
      "client",
      "save_networkx_graph",
      "show_jupyter_graph"
    ],
    "chunk_class_names": [
      "SimplePropertyGraphStore",
      "implements"
    ],
    "document_function_names": [
      "__init__",
      "get",
      "get_triplets",
      "get_rel_map",
      "upsert_nodes",
      "upsert_relations",
      "delete",
      "persist",
      "from_persist_path",
      "from_persist_dir",
      "from_dict",
      "to_dict",
      "get_schema",
      "structured_query",
      "vector_query",
      "client",
      "save_networkx_graph",
      "show_jupyter_graph"
    ],
    "document_class_names": [
      "SimplePropertyGraphStore",
      "implements"
    ],
    "content": "import fsspec\nimport json\nimport os\nfrom typing import Any, List, Dict, Sequence, Tuple, Optional\n\nfrom llama_index.core.graph_stores.types import (\n    PropertyGraphStore,\n    ChunkNode,\n    EntityNode,\n    Triplet,\n    LabelledNode,\n    LabelledPropertyGraph,\n    Relation,\n    DEFAULT_PERSIST_DIR,\n    DEFUALT_PG_PERSIST_FNAME,\n)\nfrom llama_index.core.vector_stores.types import VectorStoreQuery\n\n\nclass SimplePropertyGraphStore(PropertyGraphStore):\n    \"\"\"Simple Labelled Property Graph Store.\n\n    This class implements a simple in-memory labelled property graph store.\n\n    Args:\n        graph (Optional[LabelledPropertyGraph]): Labelled property graph to initialize the store.\n    \"\"\"\n\n    supports_structured_queries: bool = False\n    supports_vector_queries: bool = False\n\n    def __init__(\n        self,\n        graph: Optional[LabelledPropertyGraph] = None,\n    ) -> None:\n        self.graph = graph or LabelledPropertyGraph()\n\n    def get(\n        self,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[LabelledNode]:\n        \"\"\"Get nodes.\"\"\"\n        nodes = list(self.graph.nodes.values())\n        if properties:\n            nodes = [\n                n\n                for n in nodes\n                if any(n.properties.get(k) == v for k, v in properties.items())\n            ]\n\n        # Filter by node_ids\n        if ids:\n            nodes = [n for n in nodes if n.id in ids]\n\n        return nodes\n\n    def get_triplets(\n        self,\n        entity_names: Optional[List[str]] = None,\n        relation_names: Optional[List[str]] = None,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[Triplet]:\n        \"\"\"Get triplets.\"\"\"\n        # if nothing is passed, return empty list\n        if not ids and not properties and not entity_names and not relation_names:\n            return []\n\n        triplets = self.graph.get_triplets()\n        if entity_names:\n            triplets = [\n                t\n                for t in triplets\n                if t[0].id in entity_names or t[2].id in entity_names\n            ]\n\n        if relation_names:\n            triplets = [t for t in triplets if t[1].id in relation_names]\n\n        if properties:\n            triplets = [\n                t\n                for t in triplets\n                if any(\n                    t[0].properties.get(k) == v\n                    or t[1].properties.get(k) == v\n                    or t[2].properties.get(k) == v\n                    for k, v in properties.items()\n                )\n            ]\n\n        # Filter by node_ids\n        if ids:\n            triplets = [\n                t for t in triplets if any(t[0].id == i or t[2].id == i for i in ids)\n            ]\n\n        return triplets\n\n    def get_rel_map(\n        self,\n        graph_nodes: List[LabelledNode],\n        depth: int = 2,\n        limit: int = 30,\n        ignore_rels: Optional[List[str]] = None,\n    ) -> List[Triplet]:\n        \"\"\"Get depth-aware rel map.\"\"\"\n        triplets = []\n\n        cur_depth = 0\n        graph_triplets = self.get_triplets(ids=[gn.id for gn in graph_nodes])\n        seen_triplets = set()\n\n        while len(graph_triplets) > 0 and cur_depth < depth:\n            triplets.extend(graph_triplets)\n\n            # get next depth\n            graph_triplets = self.get_triplets(\n                entity_names=[t[2].id for t in graph_triplets]\n            )\n            graph_triplets = [t for t in graph_triplets if str(t) not in seen_triplets]\n            seen_triplets.update([str(t) for t in graph_triplets])\n            cur_depth += 1\n\n        ignore_rels = ignore_rels or []\n        triplets = [t for t in triplets if t[1].id not in ignore_rels]\n\n        return triplets[:limit]\n\n    def upsert_nodes(self, nodes: Sequence[LabelledNode]) -> None:\n        \"\"\"Add nodes.\"\"\"\n        for node in nodes:\n            self.graph.add_node(node)\n\n    def upsert_relations(self, relations: List[Relation]) -> None:\n        \"\"\"Add relations.\"\"\"\n        for relation in relations:\n            self.graph.add_relation(relation)\n\n    def delete(\n        self,\n        entity_names: Optional[List[str]] = None,\n        relation_names: Optional[List[str]] = None,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Delete matching data.\"\"\"\n        triplets = self.get_triplets(\n            entity_names=entity_names,\n            relation_names=relation_names,\n            properties=properties,\n            ids=ids,\n        )\n        for triplet in triplets:\n            self.graph.delete_triplet(triplet)\n\n        nodes = self.get(properties=properties, ids=ids)\n        for node in nodes:\n            self.graph.delete_node(node)\n\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        \"\"\"Persist the graph store to a file.\"\"\"\n        if fs is None:\n            fs = fsspec.filesystem(\"file\")\n        with fs.open(persist_path, \"w\") as f:\n            f.write(self.graph.model_dump_json())\n\n    @classmethod\n    def from_persist_path(\n        cls,\n        persist_path: str,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimplePropertyGraphStore\":\n        \"\"\"Load from persist path.\"\"\"\n        if fs is None:\n            fs = fsspec.filesystem(\"file\")\n\n        with fs.open(persist_path, \"r\") as f:\n            data = json.loads(f.read())\n\n        return cls.from_dict(data)\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimplePropertyGraphStore\":\n        \"\"\"Load from persist dir.\"\"\"\n        persist_path = os.path.join(persist_dir, DEFUALT_PG_PERSIST_FNAME)\n        return cls.from_persist_path(persist_path, fs=fs)\n\n    @classmethod\n    def from_dict(\n        cls,\n        data: dict,\n    ) -> \"SimplePropertyGraphStore\":\n        \"\"\"Load from dict.\"\"\"\n        # need to load nodes manually\n        node_dicts = data[\"nodes\"]\n\n        kg_nodes: Dict[str, LabelledNode] = {}\n        for id, node_dict in node_dicts.items():\n            if \"name\" in node_dict:\n                kg_nodes[id] = EntityNode.model_validate(node_dict)\n            elif \"text\" in node_dict:\n                kg_nodes[id] = ChunkNode.model_validate(node_dict)\n            else:\n                raise ValueError(f\"Could not infer node type for data: {node_dict!s}\")\n\n        # clear the nodes, to load later\n        data[\"nodes\"] = {}\n\n        # load the graph\n        graph = LabelledPropertyGraph.model_validate(data)\n\n        # add the node back\n        graph.nodes = kg_nodes\n\n        return cls(graph)\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dict.\"\"\"\n        return self.graph.model_dump()\n\n    # NOTE: Unimplemented methods for SimplePropertyGraphStore\n\n    def get_schema(self, refresh: bool = False) -> str:\n        \"\"\"Get the schema of the graph store.\"\"\"\n        raise NotImplementedError(\n            \"Schema not implemented for SimplePropertyGraphStore.\"\n        )\n\n    def structured_query(\n        self, query: str, param_map: Optional[Dict[str, Any]] = None\n    ) -> Any:\n        \"\"\"Query the graph store with statement and parameters.\"\"\"\n        raise NotImplementedError(\n            \"Structured query not implemented for SimplePropertyGraphStore.\"\n        )\n\n    def vector_query(\n        self, query: VectorStoreQuery, **kwargs: Any\n    ) -> Tuple[List[LabelledNode], List[float]]:\n        \"\"\"Query the graph store with a vector store query.\"\"\"\n        raise NotImplementedError(\n            \"Vector query not implemented for SimplePropertyGraphStore.\"\n        )\n\n    @property\n    def client(self) -> Any:\n        \"\"\"Get client.\"\"\"\n        raise NotImplementedError(\n            \"Client not implemented for SimplePropertyGraphStore.\"\n        )\n\n    def save_networkx_graph(self, name: str = \"kg.html\") -> None:\n        \"\"\"Display the graph store, useful for debugging.\"\"\"\n        import networkx as nx\n\n        G = nx.DiGraph()\n        for node in self.graph.nodes.values():\n            G.add_node(node.id, label=node.id)\n        for triplet in self.graph.triplets:\n            G.add_edge(triplet[0], triplet[2], label=triplet[1])\n\n        # save to html file\n        from pyvis.network import Network\n\n        net = Network(notebook=False, directed=True)\n        net.from_nx(G)\n        net.write_html(name)\n\n    def show_jupyter_graph(self) -> None:\n        \"\"\"Visualizes the graph structure of the graph store.\n\n        NOTE: This function requires yfiles_jupyter_graphs to be installed.\n        NOTE: This method exclusively works in jupyter environments.\n\n        \"\"\"\n        try:\n            from yfiles_jupyter_graphs import GraphWidget\n        except ImportError:\n            raise ImportError(\n                \"Please install yfiles_jupyter_graphs to visualize the graph: `pip install yfiles_jupyter_graphs`\"\n            )\n\n        w = GraphWidget()\n        nodes = []\n        edges = []\n        for node in self.graph.nodes.values():\n            node_dict = {\"id\": node.id, \"properties\": {\"label\": node.id}}\n            nodes.append(node_dict)\n        for triplet in self.graph.triplets:\n            edge = {\n                \"id\": triplet[1],\n                \"start\": triplet[0],\n                \"end\": triplet[2],\n                \"properties\": {\"label\": triplet[1]},\n            }\n            edges.append(edge)\n        w.nodes = nodes\n        w.edges = edges\n        display(w)  # type: ignore[name-defined]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/graph_stores/types.py",
    "filename": "types.py",
    "relpath": "graph_stores/types.py",
    "start_line": 1,
    "end_line": 271,
    "length": 271,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__str__",
      "id",
      "__str__",
      "id",
      "__str__",
      "id",
      "__str__",
      "id",
      "_get_relation_key",
      "get_all_nodes",
      "get_all_relations",
      "get_triplets",
      "add_triplet",
      "add_node",
      "add_relation",
      "delete_triplet",
      "delete_node",
      "delete_relation",
      "client",
      "get",
      "get_rel_map",
      "upsert_triplet",
      "delete",
      "persist",
      "get_schema",
      "query"
    ],
    "chunk_class_names": [
      "LabelledNode",
      "EntityNode",
      "ChunkNode",
      "Relation",
      "LabelledPropertyGraph",
      "GraphStore"
    ],
    "document_function_names": [
      "__str__",
      "id",
      "__str__",
      "id",
      "__str__",
      "id",
      "__str__",
      "id",
      "_get_relation_key",
      "get_all_nodes",
      "get_all_relations",
      "get_triplets",
      "add_triplet",
      "add_node",
      "add_relation",
      "delete_triplet",
      "delete_node",
      "delete_relation",
      "client",
      "get",
      "get_rel_map",
      "upsert_triplet",
      "delete",
      "persist",
      "get_schema",
      "query",
      "client",
      "get",
      "get_triplets",
      "get_rel_map",
      "get_llama_nodes",
      "upsert_nodes",
      "upsert_relations",
      "upsert_llama_nodes",
      "delete",
      "delete_llama_nodes",
      "structured_query",
      "vector_query",
      "persist",
      "get_schema",
      "get_schema_str",
      "aget",
      "aget_triplets",
      "aget_rel_map",
      "aget_llama_nodes",
      "aupsert_nodes",
      "aupsert_relations",
      "adelete",
      "adelete_llama_nodes",
      "astructured_query",
      "avector_query",
      "aget_schema",
      "aget_schema_str"
    ],
    "document_class_names": [
      "LabelledNode",
      "EntityNode",
      "ChunkNode",
      "Relation",
      "LabelledPropertyGraph",
      "GraphStore",
      "PropertyGraphStore"
    ],
    "content": "import fsspec\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    Any,\n    Dict,\n    List,\n    Optional,\n    Tuple,\n    Set,\n    Sequence,\n    Protocol,\n    runtime_checkable,\n)\n\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, SerializeAsAny\nfrom llama_index.core.graph_stores.prompts import DEFAULT_CYPHER_TEMPALTE\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.schema import BaseNode, MetadataMode\nfrom llama_index.core.vector_stores.utils import (\n    metadata_dict_to_node,\n    node_to_metadata_dict,\n)\nfrom llama_index.core.vector_stores.types import VectorStoreQuery\n\nDEFAULT_PERSIST_DIR = \"./storage\"\nDEFAULT_PERSIST_FNAME = \"graph_store.json\"\nDEFUALT_PG_PERSIST_FNAME = \"property_graph_store.json\"\n\nTRIPLET_SOURCE_KEY = \"triplet_source_id\"\nVECTOR_SOURCE_KEY = \"vector_source_id\"\nKG_NODES_KEY = \"nodes\"\nKG_RELATIONS_KEY = \"relations\"\nKG_SOURCE_REL = \"SOURCE\"\n\n\nclass LabelledNode(BaseModel):\n    \"\"\"An entity in a graph.\"\"\"\n\n    label: str = Field(default=\"node\", description=\"The label of the node.\")\n    embedding: Optional[List[float]] = Field(\n        default=None, description=\"The embeddings of the node.\"\n    )\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\n    @abstractmethod\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the node.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def id(self) -> str:\n        \"\"\"Get the node id.\"\"\"\n        ...\n\n\nclass EntityNode(LabelledNode):\n    \"\"\"An entity in a graph.\"\"\"\n\n    name: str = Field(description=\"The name of the entity.\")\n    label: str = Field(default=\"entity\", description=\"The label of the node.\")\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the node.\"\"\"\n        if self.properties:\n            return f\"{self.name} ({self.properties})\"\n        return self.name\n\n    @property\n    def id(self) -> str:\n        \"\"\"Get the node id.\"\"\"\n        return self.name.replace('\"', \" \")\n\n\nclass ChunkNode(LabelledNode):\n    \"\"\"A text chunk in a graph.\"\"\"\n\n    text: str = Field(description=\"The text content of the chunk.\")\n    id_: Optional[str] = Field(\n        default=None, description=\"The id of the node. Defaults to a hash of the text.\"\n    )\n    label: str = Field(default=\"text_chunk\", description=\"The label of the node.\")\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the node.\"\"\"\n        return self.text\n\n    @property\n    def id(self) -> str:\n        \"\"\"Get the node id.\"\"\"\n        return str(hash(self.text)) if self.id_ is None else self.id_\n\n\nclass Relation(BaseModel):\n    \"\"\"A relation connecting two entities in a graph.\"\"\"\n\n    label: str\n    source_id: str\n    target_id: str\n    properties: Dict[str, Any] = Field(default_factory=dict)\n\n    def __str__(self) -> str:\n        \"\"\"Return the string representation of the relation.\"\"\"\n        if self.properties:\n            return f\"{self.label} ({self.properties})\"\n        return self.label\n\n    @property\n    def id(self) -> str:\n        \"\"\"Get the relation id.\"\"\"\n        return self.label\n\n\nTriplet = Tuple[LabelledNode, Relation, LabelledNode]\n\n\nclass LabelledPropertyGraph(BaseModel):\n    \"\"\"In memory labelled property graph containing entities and relations.\"\"\"\n\n    nodes: SerializeAsAny[Dict[str, LabelledNode]] = Field(default_factory=dict)\n    relations: SerializeAsAny[Dict[str, Relation]] = Field(default_factory=dict)\n    triplets: Set[Tuple[str, str, str]] = Field(\n        default_factory=set, description=\"List of triplets (subject, relation, object).\"\n    )\n\n    def _get_relation_key(\n        self,\n        relation: Optional[Relation] = None,\n        subj_id: Optional[str] = None,\n        obj_id: Optional[str] = None,\n        rel_id: Optional[str] = None,\n    ) -> str:\n        \"\"\"Get relation id.\"\"\"\n        if relation:\n            return f\"{relation.source_id}_{relation.label}_{relation.target_id}\"\n        return f\"{subj_id}_{rel_id}_{obj_id}\"\n\n    def get_all_nodes(self) -> List[LabelledNode]:\n        \"\"\"Get all entities.\"\"\"\n        return list(self.nodes.values())\n\n    def get_all_relations(self) -> List[Relation]:\n        \"\"\"Get all relations.\"\"\"\n        return list(self.relations.values())\n\n    def get_triplets(self) -> List[Triplet]:\n        \"\"\"Get all triplets.\"\"\"\n        return [\n            (\n                self.nodes[subj],\n                self.relations[\n                    self._get_relation_key(obj_id=obj, subj_id=subj, rel_id=rel)\n                ],\n                self.nodes[obj],\n            )\n            for subj, rel, obj in self.triplets\n        ]\n\n    def add_triplet(self, triplet: Triplet) -> None:\n        \"\"\"Add a triplet.\"\"\"\n        subj, rel, obj = triplet\n        if (subj.id, rel.id, obj.id) in self.triplets:\n            return\n\n        self.triplets.add((subj.id, rel.id, obj.id))\n        self.nodes[subj.id] = subj\n        self.nodes[obj.id] = obj\n        self.relations[self._get_relation_key(relation=rel)] = rel\n\n    def add_node(self, node: LabelledNode) -> None:\n        \"\"\"Add a node.\"\"\"\n        self.nodes[node.id] = node\n\n    def add_relation(self, relation: Relation) -> None:\n        \"\"\"Add a relation.\"\"\"\n        if relation.source_id not in self.nodes:\n            self.nodes[relation.source_id] = EntityNode(name=relation.source_id)\n        if relation.target_id not in self.nodes:\n            self.nodes[relation.target_id] = EntityNode(name=relation.target_id)\n\n        self.add_triplet(\n            (self.nodes[relation.source_id], relation, self.nodes[relation.target_id])\n        )\n\n    def delete_triplet(self, triplet: Triplet) -> None:\n        \"\"\"Delete a triplet.\"\"\"\n        subj, rel, obj = triplet\n        if (subj.id, rel.id, obj.id) not in self.triplets:\n            return\n\n        self.triplets.remove((subj.id, rel.id, obj.id))\n        if subj.id in self.nodes:\n            del self.nodes[subj.id]\n        if obj.id in self.nodes:\n            del self.nodes[obj.id]\n\n        rel_key = self._get_relation_key(relation=rel)\n        if rel_key in self.relations:\n            del self.relations[rel_key]\n\n    def delete_node(self, node: LabelledNode) -> None:\n        \"\"\"Delete a node.\"\"\"\n        if node.id in self.nodes:\n            del self.nodes[node.id]\n\n    def delete_relation(self, relation: Relation) -> None:\n        \"\"\"Delete a relation.\"\"\"\n        rel_key = self._get_relation_key(relation=relation)\n        if rel_key in self.relations:\n            del self.relations[rel_key]\n\n\n@runtime_checkable\nclass GraphStore(Protocol):\n    \"\"\"Abstract graph store protocol.\n\n    This protocol defines the interface for a graph store, which is responsible\n    for storing and retrieving knowledge graph data.\n\n    Attributes:\n        client: Any: The client used to connect to the graph store.\n        get: Callable[[str], List[List[str]]]: Get triplets for a given subject.\n        get_rel_map: Callable[[Optional[List[str]], int], Dict[str, List[List[str]]]]:\n            Get subjects' rel map in max depth.\n        upsert_triplet: Callable[[str, str, str], None]: Upsert a triplet.\n        delete: Callable[[str, str, str], None]: Delete a triplet.\n        persist: Callable[[str, Optional[fsspec.AbstractFileSystem]], None]:\n            Persist the graph store to a file.\n        get_schema: Callable[[bool], str]: Get the schema of the graph store.\n    \"\"\"\n\n    schema: str = \"\"\n\n    @property\n    def client(self) -> Any:\n        \"\"\"Get client.\"\"\"\n        ...\n\n    def get(self, subj: str) -> List[List[str]]:\n        \"\"\"Get triplets.\"\"\"\n        ...\n\n    def get_rel_map(\n        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n    ) -> Dict[str, List[List[str]]]:\n        \"\"\"Get depth-aware rel map.\"\"\"\n        ...\n\n    def upsert_triplet(self, subj: str, rel: str, obj: str) -> None:\n        \"\"\"Add triplet.\"\"\"\n        ...\n\n    def delete(self, subj: str, rel: str, obj: str) -> None:\n        \"\"\"Delete triplet.\"\"\"\n        ...\n\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        \"\"\"Persist the graph store to a file.\"\"\"\n        return\n\n    def get_schema(self, refresh: bool = False) -> str:\n        \"\"\"Get the schema of the graph store.\"\"\"\n        ...\n\n    def query(self, query: str, param_map: Optional[Dict[str, Any]] = {}) -> Any:\n        \"\"\"Query the graph store with statement and parameters.\"\"\"\n        ..."
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/graph_stores/types.py",
    "filename": "types.py",
    "relpath": "graph_stores/types.py",
    "start_line": 271,
    "end_line": 523,
    "length": 253,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "client",
      "get",
      "get_triplets",
      "get_rel_map",
      "get_llama_nodes",
      "upsert_nodes",
      "upsert_relations",
      "upsert_llama_nodes",
      "delete",
      "delete_llama_nodes",
      "structured_query",
      "vector_query",
      "persist",
      "get_schema",
      "get_schema_str",
      "aget",
      "aget_triplets",
      "aget_rel_map",
      "aget_llama_nodes",
      "aupsert_nodes",
      "aupsert_relations",
      "adelete",
      "adelete_llama_nodes",
      "astructured_query",
      "avector_query",
      "aget_schema",
      "aget_schema_str"
    ],
    "chunk_class_names": [
      "PropertyGraphStore"
    ],
    "document_function_names": [
      "__str__",
      "id",
      "__str__",
      "id",
      "__str__",
      "id",
      "__str__",
      "id",
      "_get_relation_key",
      "get_all_nodes",
      "get_all_relations",
      "get_triplets",
      "add_triplet",
      "add_node",
      "add_relation",
      "delete_triplet",
      "delete_node",
      "delete_relation",
      "client",
      "get",
      "get_rel_map",
      "upsert_triplet",
      "delete",
      "persist",
      "get_schema",
      "query",
      "client",
      "get",
      "get_triplets",
      "get_rel_map",
      "get_llama_nodes",
      "upsert_nodes",
      "upsert_relations",
      "upsert_llama_nodes",
      "delete",
      "delete_llama_nodes",
      "structured_query",
      "vector_query",
      "persist",
      "get_schema",
      "get_schema_str",
      "aget",
      "aget_triplets",
      "aget_rel_map",
      "aget_llama_nodes",
      "aupsert_nodes",
      "aupsert_relations",
      "adelete",
      "adelete_llama_nodes",
      "astructured_query",
      "avector_query",
      "aget_schema",
      "aget_schema_str"
    ],
    "document_class_names": [
      "LabelledNode",
      "EntityNode",
      "ChunkNode",
      "Relation",
      "LabelledPropertyGraph",
      "GraphStore",
      "PropertyGraphStore"
    ],
    "content": "class PropertyGraphStore(ABC):\n    \"\"\"Abstract labelled graph store protocol.\n\n    This protocol defines the interface for a graph store, which is responsible\n    for storing and retrieving knowledge graph data.\n\n    Attributes:\n        client: Any: The client used to connect to the graph store.\n        get: Callable[[str], List[List[str]]]: Get triplets for a given subject.\n        get_rel_map: Callable[[Optional[List[str]], int], Dict[str, List[List[str]]]]:\n            Get subjects' rel map in max depth.\n        upsert_triplet: Callable[[str, str, str], None]: Upsert a triplet.\n        delete: Callable[[str, str, str], None]: Delete a triplet.\n        persist: Callable[[str, Optional[fsspec.AbstractFileSystem]], None]:\n            Persist the graph store to a file.\n    \"\"\"\n\n    supports_structured_queries: bool = False\n    supports_vector_queries: bool = False\n    text_to_cypher_template: PromptTemplate = DEFAULT_CYPHER_TEMPALTE\n\n    @property\n    def client(self) -> Any:\n        \"\"\"Get client.\"\"\"\n        ...\n\n    @abstractmethod\n    def get(\n        self,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[LabelledNode]:\n        \"\"\"Get nodes with matching values.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_triplets(\n        self,\n        entity_names: Optional[List[str]] = None,\n        relation_names: Optional[List[str]] = None,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[Triplet]:\n        \"\"\"Get triplets with matching values.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_rel_map(\n        self,\n        graph_nodes: List[LabelledNode],\n        depth: int = 2,\n        limit: int = 30,\n        ignore_rels: Optional[List[str]] = None,\n    ) -> List[Triplet]:\n        \"\"\"Get depth-aware rel map.\"\"\"\n        ...\n\n    def get_llama_nodes(self, node_ids: List[str]) -> List[BaseNode]:\n        \"\"\"Get llama-index nodes.\"\"\"\n        nodes = self.get(ids=node_ids)\n        converted_nodes = []\n        for node in nodes:\n            try:\n                converted_nodes.append(metadata_dict_to_node(node.properties))\n                converted_nodes[-1].set_content(node.text)  # type: ignore\n            except Exception:\n                continue\n\n        return converted_nodes\n\n    @abstractmethod\n    def upsert_nodes(self, nodes: Sequence[LabelledNode]) -> None:\n        \"\"\"Upsert nodes.\"\"\"\n        ...\n\n    @abstractmethod\n    def upsert_relations(self, relations: List[Relation]) -> None:\n        \"\"\"Upsert relations.\"\"\"\n        ...\n\n    def upsert_llama_nodes(self, llama_nodes: List[BaseNode]) -> None:\n        \"\"\"Add llama-index nodes.\"\"\"\n        converted_nodes = []\n        for llama_node in llama_nodes:\n            metadata_dict = node_to_metadata_dict(llama_node, remove_text=True)\n            converted_nodes.append(\n                ChunkNode(\n                    text=llama_node.get_content(metadata_mode=MetadataMode.NONE),\n                    id_=llama_node.id_,\n                    properties=metadata_dict,\n                    embedding=llama_node.embedding,\n                )\n            )\n        self.upsert_nodes(converted_nodes)\n\n    @abstractmethod\n    def delete(\n        self,\n        entity_names: Optional[List[str]] = None,\n        relation_names: Optional[List[str]] = None,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Delete matching data.\"\"\"\n        ...\n\n    def delete_llama_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        ref_doc_ids: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Delete llama-index nodes.\n\n        Intended to delete any nodes in the graph store associated\n        with the given llama-index node_ids or ref_doc_ids.\n        \"\"\"\n        nodes = []\n\n        node_ids = node_ids or []\n        for id_ in node_ids:\n            nodes.extend(self.get(properties={TRIPLET_SOURCE_KEY: id_}))\n\n        if len(node_ids) > 0:\n            nodes.extend(self.get(ids=node_ids))\n\n        ref_doc_ids = ref_doc_ids or []\n        for id_ in ref_doc_ids:\n            nodes.extend(self.get(properties={\"ref_doc_id\": id_}))\n\n        if len(ref_doc_ids) > 0:\n            nodes.extend(self.get(ids=ref_doc_ids))\n\n        self.delete(ids=[node.id for node in nodes])\n\n    @abstractmethod\n    def structured_query(\n        self, query: str, param_map: Optional[Dict[str, Any]] = None\n    ) -> Any:\n        \"\"\"Query the graph store with statement and parameters.\"\"\"\n        ...\n\n    @abstractmethod\n    def vector_query(\n        self, query: VectorStoreQuery, **kwargs: Any\n    ) -> Tuple[List[LabelledNode], List[float]]:\n        \"\"\"Query the graph store with a vector store query.\"\"\"\n        ...\n\n    def persist(\n        self, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> None:\n        \"\"\"Persist the graph store to a file.\"\"\"\n        return\n\n    def get_schema(self, refresh: bool = False) -> Any:\n        \"\"\"Get the schema of the graph store.\"\"\"\n        return None\n\n    def get_schema_str(self, refresh: bool = False) -> str:\n        \"\"\"Get the schema of the graph store as a string.\"\"\"\n        return str(self.get_schema(refresh=refresh))\n\n    ### ----- Async Methods ----- ###\n\n    async def aget(\n        self,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[LabelledNode]:\n        \"\"\"Asynchronously get nodes with matching values.\"\"\"\n        return self.get(properties, ids)\n\n    async def aget_triplets(\n        self,\n        entity_names: Optional[List[str]] = None,\n        relation_names: Optional[List[str]] = None,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List[Triplet]:\n        \"\"\"Asynchronously get triplets with matching values.\"\"\"\n        return self.get_triplets(entity_names, relation_names, properties, ids)\n\n    async def aget_rel_map(\n        self,\n        graph_nodes: List[LabelledNode],\n        depth: int = 2,\n        limit: int = 30,\n        ignore_rels: Optional[List[str]] = None,\n    ) -> List[Triplet]:\n        \"\"\"Asynchronously get depth-aware rel map.\"\"\"\n        return self.get_rel_map(graph_nodes, depth, limit, ignore_rels)\n\n    async def aget_llama_nodes(self, node_ids: List[str]) -> List[BaseNode]:\n        \"\"\"Asynchronously get nodes.\"\"\"\n        nodes = await self.aget(ids=node_ids)\n        converted_nodes = []\n        for node in nodes:\n            try:\n                converted_nodes.append(metadata_dict_to_node(node.properties))\n                converted_nodes[-1].set_content(node.text)  # type: ignore\n            except Exception:\n                continue\n\n        return converted_nodes\n\n    async def aupsert_nodes(self, nodes: List[LabelledNode]) -> None:\n        \"\"\"Asynchronously add nodes.\"\"\"\n        return self.upsert_nodes(nodes)\n\n    async def aupsert_relations(self, relations: List[Relation]) -> None:\n        \"\"\"Asynchronously add relations.\"\"\"\n        return self.upsert_relations(relations)\n\n    async def adelete(\n        self,\n        entity_names: Optional[List[str]] = None,\n        relation_names: Optional[List[str]] = None,\n        properties: Optional[dict] = None,\n        ids: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Asynchronously delete matching data.\"\"\"\n        return self.delete(entity_names, relation_names, properties, ids)\n\n    async def adelete_llama_nodes(\n        self,\n        node_ids: Optional[List[str]] = None,\n        ref_doc_ids: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Asynchronously delete llama-index nodes.\"\"\"\n        return self.delete_llama_nodes(node_ids, ref_doc_ids)\n\n    async def astructured_query(\n        self, query: str, param_map: Optional[Dict[str, Any]] = {}\n    ) -> Any:\n        \"\"\"Asynchronously query the graph store with statement and parameters.\"\"\"\n        return self.structured_query(query, param_map)\n\n    async def avector_query(\n        self, query: VectorStoreQuery, **kwargs: Any\n    ) -> Tuple[List[LabelledNode], List[float]]:\n        \"\"\"Asynchronously query the graph store with a vector store query.\"\"\"\n        return self.vector_query(query, **kwargs)\n\n    async def aget_schema(self, refresh: bool = False) -> str:\n        \"\"\"Asynchronously get the schema of the graph store.\"\"\"\n        return self.get_schema(refresh=refresh)\n\n    async def aget_schema_str(self, refresh: bool = False) -> str:\n        \"\"\"Asynchronously get the schema of the graph store as a string.\"\"\"\n        return str(await self.aget_schema(refresh=refresh))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/graph_stores/__init__.py",
    "filename": "__init__.py",
    "relpath": "graph_stores/__init__.py",
    "start_line": 1,
    "end_line": 21,
    "length": 21,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Graph stores.\"\"\"\n\nfrom llama_index.core.graph_stores.simple import SimpleGraphStore\nfrom llama_index.core.graph_stores.simple_labelled import SimplePropertyGraphStore\nfrom llama_index.core.graph_stores.types import (\n    LabelledNode,\n    Relation,\n    EntityNode,\n    ChunkNode,\n    PropertyGraphStore,\n)\n\n__all__ = [\n    \"SimpleGraphStore\",\n    \"LabelledNode\",\n    \"Relation\",\n    \"EntityNode\",\n    \"ChunkNode\",\n    \"PropertyGraphStore\",\n    \"SimplePropertyGraphStore\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/graph_stores/utils.py",
    "filename": "utils.py",
    "relpath": "graph_stores/utils.py",
    "start_line": 1,
    "end_line": 52,
    "length": 52,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "clean_string_values",
      "value_sanitize"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "clean_string_values",
      "value_sanitize"
    ],
    "document_class_names": [],
    "content": "\"\"\"Borrowed from Langchain's Neo4j graph utility functions.\n\nhttps://github.com/langchain-ai/langchain/blob/95c3e5f85f8ed8026a11e351b57bfae488d654c4/libs/community/langchain_community/graphs/neo4j_graph.py\n\"\"\"\n\nfrom typing import Any\n\nLIST_LIMIT = 128\n\n\ndef clean_string_values(text: str) -> str:\n    return text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n\n\ndef value_sanitize(d: Any) -> Any:\n    \"\"\"Sanitize the input dictionary or list.\n\n    Sanitizes the input by removing embedding-like values,\n    lists with more than 128 elements, that are mostly irrelevant for\n    generating answers in a LLM context. These properties, if left in\n    results, can occupy significant context space and detract from\n    the LLM's performance by introducing unnecessary noise and cost.\n    \"\"\"\n    if isinstance(d, dict):\n        new_dict = {}\n        for key, value in d.items():\n            if isinstance(value, dict):\n                sanitized_value = value_sanitize(value)\n                if (\n                    sanitized_value is not None\n                ):  # Check if the sanitized value is not None\n                    new_dict[key] = sanitized_value\n            elif isinstance(value, list):\n                if len(value) < LIST_LIMIT:\n                    sanitized_value = value_sanitize(value)\n                    if (\n                        sanitized_value is not None\n                    ):  # Check if the sanitized value is not None\n                        new_dict[key] = sanitized_value\n                # Do not include the key if the list is oversized\n            else:\n                new_dict[key] = value\n        return new_dict\n    elif isinstance(d, list):\n        if len(d) < LIST_LIMIT:\n            return [\n                value_sanitize(item) for item in d if value_sanitize(item) is not None\n            ]\n        else:\n            return None\n    else:\n        return d"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/graph_stores/prompts.py",
    "filename": "prompts.py",
    "relpath": "graph_stores/prompts.py",
    "start_line": 1,
    "end_line": 16,
    "length": 16,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.prompts import PromptTemplate\n\nDEFAULT_CYPHER_TEMPALTE_STR = \"\"\"Task:Generate Cypher statement to query a graph database.\nInstructions:\nUse only the provided relationship types and properties in the schema.\nDo not use any other relationship types or properties that are not provided.\nSchema:\n{schema}\nNote: Do not include any explanations or apologies in your responses.\nDo not respond to any questions that might ask anything else than for you to construct a Cypher statement.\nDo not include any text except the generated Cypher statement.\n\nThe question is:\n{question}\"\"\"\n\nDEFAULT_CYPHER_TEMPALTE = PromptTemplate(DEFAULT_CYPHER_TEMPALTE_STR)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/graph_stores/simple.py",
    "filename": "simple.py",
    "relpath": "graph_stores/simple.py",
    "start_line": 1,
    "end_line": 180,
    "length": 180,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_rel_map",
      "_get_rel_map",
      "__init__",
      "from_persist_dir",
      "client",
      "get",
      "get_rel_map",
      "upsert_triplet",
      "delete",
      "persist",
      "get_schema",
      "query",
      "from_persist_path",
      "from_dict",
      "to_dict"
    ],
    "chunk_class_names": [
      "class",
      "SimpleGraphStore"
    ],
    "document_function_names": [
      "get_rel_map",
      "_get_rel_map",
      "__init__",
      "from_persist_dir",
      "client",
      "get",
      "get_rel_map",
      "upsert_triplet",
      "delete",
      "persist",
      "get_schema",
      "query",
      "from_persist_path",
      "from_dict",
      "to_dict"
    ],
    "document_class_names": [
      "class",
      "SimpleGraphStore"
    ],
    "content": "\"\"\"Simple graph store index.\"\"\"\n\nimport json\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional\n\nimport fsspec\nfrom dataclasses_json import DataClassJsonMixin\nfrom llama_index.core.graph_stores.types import (\n    DEFAULT_PERSIST_DIR,\n    DEFAULT_PERSIST_FNAME,\n    GraphStore,\n)\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass SimpleGraphStoreData(DataClassJsonMixin):\n    \"\"\"Simple Graph Store Data container.\n\n    Args:\n        graph_dict (Optional[dict]): dict mapping subject to\n    \"\"\"\n\n    graph_dict: Dict[str, List[List[str]]] = field(default_factory=dict)\n\n    def get_rel_map(\n        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n    ) -> Dict[str, List[List[str]]]:\n        \"\"\"Get subjects' rel map in max depth.\"\"\"\n        if subjs is None:\n            subjs = list(self.graph_dict.keys())\n        rel_map = {}\n        for subj in subjs:\n            rel_map[subj] = self._get_rel_map(subj, depth=depth, limit=limit)\n        # TBD, truncate the rel_map in a spread way, now just truncate based\n        # on iteration order\n        rel_count = 0\n        return_map = {}\n        for subj in rel_map:\n            if rel_count + len(rel_map[subj]) > limit:\n                return_map[subj] = rel_map[subj][: limit - rel_count]\n                break\n            else:\n                return_map[subj] = rel_map[subj]\n                rel_count += len(rel_map[subj])\n        return return_map\n\n    def _get_rel_map(\n        self, subj: str, depth: int = 2, limit: int = 30\n    ) -> List[List[str]]:\n        \"\"\"Get one subect's rel map in max depth.\"\"\"\n        if depth == 0:\n            return []\n        rel_map = []\n        rel_count = 0\n        if subj in self.graph_dict:\n            for rel, obj in self.graph_dict[subj]:\n                if rel_count >= limit:\n                    break\n                rel_map.append([subj, rel, obj])\n                rel_map += self._get_rel_map(obj, depth=depth - 1)\n                rel_count += 1\n        return rel_map\n\n\nclass SimpleGraphStore(GraphStore):\n    \"\"\"Simple Graph Store.\n\n    In this graph store, triplets are stored within a simple, in-memory dictionary.\n\n    Args:\n        simple_graph_store_data_dict (Optional[dict]): data dict\n            containing the triplets. See SimpleGraphStoreData\n            for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[SimpleGraphStoreData] = None,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize params.\"\"\"\n        self._data = data or SimpleGraphStoreData()\n        self._fs = fs or fsspec.filesystem(\"file\")\n\n    @classmethod\n    def from_persist_dir(\n        cls,\n        persist_dir: str = DEFAULT_PERSIST_DIR,\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> \"SimpleGraphStore\":\n        \"\"\"Load from persist dir.\"\"\"\n        persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)\n        return cls.from_persist_path(persist_path, fs=fs)\n\n    @property\n    def client(self) -> None:\n        \"\"\"Get client.\n        Not applicable for this store.\n        \"\"\"\n        return\n\n    def get(self, subj: str) -> List[List[str]]:\n        \"\"\"Get triplets.\"\"\"\n        return self._data.graph_dict.get(subj, [])\n\n    def get_rel_map(\n        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n    ) -> Dict[str, List[List[str]]]:\n        \"\"\"Get depth-aware rel map.\"\"\"\n        return self._data.get_rel_map(subjs=subjs, depth=depth, limit=limit)\n\n    def upsert_triplet(self, subj: str, rel: str, obj: str) -> None:\n        \"\"\"Add triplet.\"\"\"\n        if subj not in self._data.graph_dict:\n            self._data.graph_dict[subj] = []\n        if (rel, obj) not in self._data.graph_dict[subj]:\n            self._data.graph_dict[subj].append([rel, obj])\n\n    def delete(self, subj: str, rel: str, obj: str) -> None:\n        \"\"\"Delete triplet.\"\"\"\n        if subj in self._data.graph_dict:\n            if (rel, obj) in self._data.graph_dict[subj]:\n                self._data.graph_dict[subj].remove([rel, obj])\n                if len(self._data.graph_dict[subj]) == 0:\n                    del self._data.graph_dict[subj]\n\n    def persist(\n        self,\n        persist_path: str = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME),\n        fs: Optional[fsspec.AbstractFileSystem] = None,\n    ) -> None:\n        \"\"\"Persist the SimpleGraphStore to a directory.\"\"\"\n        fs = fs or self._fs\n        dirpath = os.path.dirname(persist_path)\n        if not fs.exists(dirpath):\n            fs.makedirs(dirpath)\n\n        with fs.open(persist_path, \"w\") as f:\n            json.dump(self._data.to_dict(), f)\n\n    def get_schema(self, refresh: bool = False) -> str:\n        \"\"\"Get the schema of the Simple Graph store.\"\"\"\n        raise NotImplementedError(\"SimpleGraphStore does not support get_schema\")\n\n    def query(self, query: str, param_map: Optional[Dict[str, Any]] = {}) -> Any:\n        \"\"\"Query the Simple Graph store.\"\"\"\n        raise NotImplementedError(\"SimpleGraphStore does not support query\")\n\n    @classmethod\n    def from_persist_path(\n        cls, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n    ) -> \"SimpleGraphStore\":\n        \"\"\"Create a SimpleGraphStore from a persist directory.\"\"\"\n        fs = fs or fsspec.filesystem(\"file\")\n        if not fs.exists(persist_path):\n            logger.warning(\n                f\"No existing {__name__} found at {persist_path}. \"\n                \"Initializing a new graph_store from scratch. \"\n            )\n            return cls()\n\n        logger.debug(f\"Loading {__name__} from {persist_path}.\")\n        with fs.open(persist_path, \"rb\") as f:\n            data_dict = json.load(f)\n            data = SimpleGraphStoreData.from_dict(data_dict)\n        return cls(data)\n\n    @classmethod\n    def from_dict(cls, save_dict: dict) -> \"SimpleGraphStore\":\n        data = SimpleGraphStoreData.from_dict(save_dict)\n        return cls(data)\n\n    def to_dict(self) -> dict:\n        return self._data.to_dict()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/sparse_embeddings/mock_sparse_embedding.py",
    "filename": "mock_sparse_embedding.py",
    "relpath": "sparse_embeddings/mock_sparse_embedding.py",
    "start_line": 1,
    "end_line": 42,
    "length": 42,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "_get_embedding",
      "_get_text_embedding",
      "_aget_text_embedding",
      "_get_query_embedding",
      "_aget_query_embedding"
    ],
    "chunk_class_names": [
      "MockSparseEmbedding"
    ],
    "document_function_names": [
      "class_name",
      "_get_embedding",
      "_get_text_embedding",
      "_aget_text_embedding",
      "_get_query_embedding",
      "_aget_query_embedding"
    ],
    "document_class_names": [
      "MockSparseEmbedding"
    ],
    "content": "from typing import Dict, Optional\n\nfrom llama_index.core.base.embeddings.base_sparse import (\n    BaseSparseEmbedding,\n    SparseEmbedding,\n)\nfrom llama_index.core.bridge.pydantic import Field\n\n\nclass MockSparseEmbedding(BaseSparseEmbedding):\n    \"\"\"A mock sparse embedding model for testing.\"\"\"\n\n    default_embedding: SparseEmbedding = Field(\n        default_factory=lambda: {0: 1.0},\n        description=\"The default embedding to return.\",\n    )\n\n    text_to_embedding: Optional[Dict[str, SparseEmbedding]] = Field(\n        default=None,\n        description=\"The mapping of text to embeddings for lookup.\",\n    )\n\n    @classmethod\n    def class_name(self) -> str:\n        return \"MockSparseEmbedding\"\n\n    def _get_embedding(self, text: str) -> SparseEmbedding:\n        if self.text_to_embedding is not None:\n            return self.text_to_embedding.get(text, self.default_embedding)\n        return self.default_embedding\n\n    def _get_text_embedding(self, text: str) -> SparseEmbedding:\n        return self._get_embedding(text)\n\n    async def _aget_text_embedding(self, text: str) -> SparseEmbedding:\n        return self._get_embedding(text)\n\n    def _get_query_embedding(self, query: str) -> SparseEmbedding:\n        return self._get_embedding(query)\n\n    async def _aget_query_embedding(self, query: str) -> SparseEmbedding:\n        return self._get_embedding(query)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_pack/base.py",
    "filename": "base.py",
    "relpath": "llama_pack/base.py",
    "start_line": 1,
    "end_line": 14,
    "length": 14,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_modules",
      "run"
    ],
    "chunk_class_names": [
      "BaseLlamaPack"
    ],
    "document_function_names": [
      "get_modules",
      "run"
    ],
    "document_class_names": [
      "BaseLlamaPack"
    ],
    "content": "\"\"\"Llama pack class.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict\n\n\nclass BaseLlamaPack:\n    @abstractmethod\n    def get_modules(self) -> Dict[str, Any]:\n        \"\"\"Get modules.\"\"\"\n\n    @abstractmethod\n    def run(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Run.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_pack/__init__.py",
    "filename": "__init__.py",
    "relpath": "llama_pack/__init__.py",
    "start_line": 1,
    "end_line": 8,
    "length": 8,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file.\"\"\"\nfrom llama_index.core.llama_pack.base import BaseLlamaPack\nfrom llama_index.core.llama_pack.download import download_llama_pack\n\n__all__ = [\n    \"BaseLlamaPack\",\n    \"download_llama_pack\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llama_pack/download.py",
    "filename": "download.py",
    "relpath": "llama_pack/download.py",
    "start_line": 1,
    "end_line": 72,
    "length": 72,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "download_llama_pack"
    ],
    "chunk_class_names": [
      "you",
      "in",
      "of"
    ],
    "document_function_names": [
      "download_llama_pack"
    ],
    "document_class_names": [
      "you",
      "in",
      "of"
    ],
    "content": "import json\nimport os\nfrom typing import Optional, Type\n\nfrom llama_index.core.download.integration import download_integration\nfrom llama_index.core.download.pack import (\n    LLAMA_PACKS_CONTENTS_URL,\n    download_llama_pack_template,\n    track_download,\n)\nfrom llama_index.core.llama_pack.base import BaseLlamaPack\n\n\ndef download_llama_pack(\n    llama_pack_class: str,\n    download_dir: Optional[str] = None,\n    llama_pack_url: str = LLAMA_PACKS_CONTENTS_URL,\n    refresh_cache: bool = True,\n) -> Optional[Type[BaseLlamaPack]]:\n    \"\"\"Download a single LlamaPack PyPi Package.\n\n    Args:\n        llama_pack_class: The name of the LlamaPack class you want to download,\n            such as `GmailOpenAIAgentPack`.\n        refresh_cache: If true, the local cache will be skipped and the\n            loader will be fetched directly from the remote repo.\n        download_dir: Custom dirpath to download the pack into.\n\n    Returns:\n        A Loader.\n    \"\"\"\n    pack_cls = None\n\n    mappings_path = os.path.join(\n        os.path.abspath(\n            os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir)\n        ),\n        \"command_line/mappings.json\",\n    )\n    with open(mappings_path) as f:\n        mappings = json.load(f)\n\n    if llama_pack_class in mappings:\n        new_import_parent = mappings[llama_pack_class]\n        new_install_parent = new_import_parent.replace(\".\", \"-\").replace(\"_\", \"-\")\n    else:\n        raise ValueError(f\"Failed to find python package for class {llama_pack_class}\")\n\n    if not download_dir:\n        pack_cls = download_integration(\n            module_str=new_install_parent,\n            module_import_str=new_import_parent,\n            cls_name=llama_pack_class,\n        )\n    else:\n        pack_cls = download_llama_pack_template(\n            new_install_parent=new_install_parent,\n            llama_pack_class=llama_pack_class,\n            llama_pack_url=llama_pack_url,\n            refresh_cache=refresh_cache,\n            custom_path=download_dir,\n        )\n        track_download(llama_pack_class, \"llamapack\")\n        if pack_cls is None:\n            return None\n\n        if not issubclass(pack_cls, BaseLlamaPack):\n            raise ValueError(\n                f\"Pack class {pack_cls} must be a subclass of BaseLlamaPack.\"\n            )\n\n    return pack_cls"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/types.py",
    "filename": "types.py",
    "relpath": "workflow/types.py",
    "start_line": 1,
    "end_line": 8,
    "length": 8,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from typing import Any, TypeVar, Union\n\nfrom .events import StopEvent\n\nStopEventT = TypeVar(\"StopEventT\", bound=StopEvent)\n# TODO: When releasing 1.0, remove support for Any\n# and enforce usage of StopEventT\nRunResultT = Union[StopEventT, Any]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/retry_policy.py",
    "filename": "retry_policy.py",
    "relpath": "workflow/retry_policy.py",
    "start_line": 1,
    "end_line": 40,
    "length": 40,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "next",
      "__init__",
      "next"
    ],
    "chunk_class_names": [
      "RetryPolicy",
      "ConstantDelayRetryPolicy"
    ],
    "document_function_names": [
      "next",
      "__init__",
      "next"
    ],
    "document_class_names": [
      "RetryPolicy",
      "ConstantDelayRetryPolicy"
    ],
    "content": "from typing import Protocol, Optional, runtime_checkable\n\n\n@runtime_checkable\nclass RetryPolicy(Protocol):\n    def next(\n        self, elapsed_time: float, attempts: int, error: Exception\n    ) -> Optional[float]:\n        \"\"\"Decides if we should make another retry, returning the number of seconds to wait before the next run.\n\n        Args:\n            elapsed_time: Time in seconds that passed since the last attempt.\n            attempts: The number of attempts done so far.\n            error: The last error occurred.\n\n        Returns:\n            The amount of seconds to wait before the next attempt, or None if we stop retrying.\n        \"\"\"\n\n\nclass ConstantDelayRetryPolicy:\n    \"\"\"A simple policy that retries a step at regular intervals for a number of times.\"\"\"\n\n    def __init__(self, maximum_attempts: int = 3, delay: float = 5) -> None:\n        \"\"\"Creates a ConstantDelayRetryPolicy instance.\n\n        Args:\n            maximum_attempts: How many consecutive times the workflow should try to run the step in case of an error.\n            delay: how much time in seconds must pass before another attempt.\n        \"\"\"\n        self.maximum_attempts = maximum_attempts\n        self.delay = delay\n\n    def next(\n        self, elapsed_time: float, attempts: int, error: Exception\n    ) -> Optional[float]:\n        if attempts >= self.maximum_attempts:\n            return None\n\n        return self.delay"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/decorators.py",
    "filename": "decorators.py",
    "relpath": "workflow/decorators.py",
    "start_line": 1,
    "end_line": 88,
    "length": 88,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "step",
      "decorator"
    ],
    "chunk_class_names": [
      "StepConfig",
      "to",
      "methods",
      "to"
    ],
    "document_function_names": [
      "step",
      "decorator"
    ],
    "document_class_names": [
      "StepConfig",
      "to",
      "methods",
      "to"
    ],
    "content": "from typing import TYPE_CHECKING, Any, Callable, List, Optional, Type\n\nfrom llama_index.core.bridge.pydantic import BaseModel, ConfigDict\n\nfrom .errors import WorkflowValidationError\nfrom .utils import (\n    is_free_function,\n    validate_step_signature,\n    inspect_signature,\n    ServiceDefinition,\n)\n\nif TYPE_CHECKING:  # pragma: no cover\n    from .workflow import Workflow\nfrom .retry_policy import RetryPolicy\n\n\nclass StepConfig(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    accepted_events: List[Any]\n    event_name: str\n    return_types: List[Any]\n    context_parameter: Optional[str]\n    num_workers: int\n    requested_services: List[ServiceDefinition]\n    retry_policy: Optional[RetryPolicy]\n\n\ndef step(\n    *args: Any,\n    workflow: Optional[Type[\"Workflow\"]] = None,\n    pass_context: bool = False,\n    num_workers: int = 4,\n    retry_policy: Optional[RetryPolicy] = None,\n) -> Callable:\n    \"\"\"Decorator used to mark methods and functions as workflow steps.\n\n    Decorators are evaluated at import time, but we need to wait for\n    starting the communication channels until runtime. For this reason,\n    we temporarily store the list of events that will be consumed by this\n    step in the function object itself.\n\n    Args:\n        workflow: Workflow class to which the decorated step will be added. Only needed when using the\n            decorator on free functions instead of class methods.\n        num_workers: The number of workers that will process events for the decorated step. The default\n            value works most of the times.\n        retry_policy: The policy used to retry a step that encountered an error while running.\n    \"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        if not isinstance(num_workers, int) or num_workers <= 0:\n            raise WorkflowValidationError(\n                \"num_workers must be an integer greater than 0\"\n            )\n\n        # This will raise providing a message with the specific validation failure\n        spec = inspect_signature(func)\n        validate_step_signature(spec)\n        event_name, accepted_events = next(iter(spec.accepted_events.items()))\n\n        # store the configuration in the function object\n        func.__step_config = StepConfig(  # type: ignore[attr-defined]\n            accepted_events=accepted_events,\n            event_name=event_name,\n            return_types=spec.return_types,\n            context_parameter=spec.context_parameter,\n            num_workers=num_workers,\n            requested_services=spec.requested_services or [],\n            retry_policy=retry_policy,\n        )\n\n        # If this is a free function, call add_step() explicitly.\n        if is_free_function(func.__qualname__):\n            if workflow is None:\n                msg = f\"To decorate {func.__name__} please pass a workflow class to the @step decorator.\"\n                raise WorkflowValidationError(msg)\n            workflow.add_step(func)\n\n        return func\n\n    if len(args):\n        # The decorator was used without parentheses, like `@step`\n        func = args[0]\n        decorator(func)\n        return func\n    return decorator"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/drawing.py",
    "filename": "drawing.py",
    "relpath": "workflow/drawing.py",
    "start_line": 1,
    "end_line": 107,
    "length": 107,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "draw_all_possible_flows",
      "draw_most_recent_execution"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "draw_all_possible_flows",
      "draw_most_recent_execution"
    ],
    "document_class_names": [],
    "content": "from deprecated import deprecated\nfrom typing import Optional\n\nfrom .workflow import Workflow\nfrom .events import StartEvent, StopEvent\nfrom .decorators import StepConfig\nfrom .utils import get_steps_from_class, get_steps_from_instance\n\n\n@deprecated(\n    reason=\"Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.\"\n)\ndef draw_all_possible_flows(\n    workflow: Workflow,\n    filename: str = \"workflow_all_flows.html\",\n    notebook: bool = False,\n) -> None:\n    \"\"\"Draws all possible flows of the workflow.\"\"\"\n    from pyvis.network import Network\n\n    net = Network(directed=True, height=\"750px\", width=\"100%\")\n\n    # Add the nodes + edge for stop events\n    net.add_node(\n        StopEvent.__name__,\n        label=StopEvent.__name__,\n        color=\"#FFA07A\",\n        shape=\"ellipse\",\n    )\n    net.add_node(\"_done\", label=\"_done\", color=\"#ADD8E6\", shape=\"box\")\n    net.add_edge(StopEvent.__name__, \"_done\")\n\n    # Add nodes from all steps\n    steps = get_steps_from_class(workflow)\n    if not steps:\n        # If no steps are defined in the class, try to get them from the instance\n        steps = get_steps_from_instance(workflow)\n\n    step_config: Optional[StepConfig] = None\n    for step_name, step_func in steps.items():\n        step_config = getattr(step_func, \"__step_config\", None)\n        if step_config is None:\n            continue\n\n        net.add_node(\n            step_name, label=step_name, color=\"#ADD8E6\", shape=\"box\"\n        )  # Light blue for steps\n\n        for event_type in step_config.accepted_events:\n            net.add_node(\n                event_type.__name__,\n                label=event_type.__name__,\n                color=\"#90EE90\" if event_type != StartEvent else \"#E27AFF\",\n                shape=\"ellipse\",\n            )  # Light green for events\n\n    # Add edges from all steps\n    for step_name, step_func in steps.items():\n        step_config = getattr(step_func, \"__step_config\", None)\n\n        if step_config is None:\n            continue\n\n        for return_type in step_config.return_types:\n            if return_type != type(None):\n                net.add_edge(step_name, return_type.__name__)\n\n        for event_type in step_config.accepted_events:\n            net.add_edge(event_type.__name__, step_name)\n\n    net.show(filename, notebook=notebook)\n\n\n@deprecated(\n    reason=\"Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.\"\n)\ndef draw_most_recent_execution(\n    workflow: Workflow,\n    filename: str = \"workflow_recent_execution.html\",\n    notebook: bool = False,\n) -> None:\n    \"\"\"Draws the most recent execution of the workflow.\"\"\"\n    from pyvis.network import Network\n\n    net = Network(directed=True, height=\"750px\", width=\"100%\")\n\n    # Add nodes and edges based on execution history\n    existing_context = next(iter(workflow._contexts), None)\n    if existing_context is None:\n        raise ValueError(\"No runs found in workflow\")\n\n    for i, (step, event) in enumerate(existing_context._accepted_events):\n        event_node = f\"{event}_{i}\"\n        step_node = f\"{step}_{i}\"\n        net.add_node(\n            event_node, label=event, color=\"#90EE90\", shape=\"ellipse\"\n        )  # Light green for events\n        net.add_node(\n            step_node, label=step, color=\"#ADD8E6\", shape=\"box\"\n        )  # Light blue for steps\n        net.add_edge(event_node, step_node)\n\n        if i > 0:\n            prev_step_node = f\"{existing_context._accepted_events[i - 1][0]}_{i - 1}\"\n            net.add_edge(prev_step_node, event_node)\n\n    net.show(filename, notebook=notebook)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/events.py",
    "filename": "events.py",
    "relpath": "workflow/events.py",
    "start_line": 1,
    "end_line": 176,
    "length": 176,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "__getattr__",
      "__setattr__",
      "__getitem__",
      "__setitem__",
      "get",
      "__contains__",
      "keys",
      "values",
      "items",
      "__len__",
      "__iter__",
      "dict",
      "__bool__",
      "custom_model_dump",
      "__init__",
      "_get_result",
      "result"
    ],
    "chunk_class_names": [
      "Event",
      "for",
      "CustomEvent",
      "StartEvent",
      "StopEvent",
      "InputRequiredEvent",
      "HumanResponseEvent"
    ],
    "document_function_names": [
      "__init__",
      "__getattr__",
      "__setattr__",
      "__getitem__",
      "__setitem__",
      "get",
      "__contains__",
      "keys",
      "values",
      "items",
      "__len__",
      "__iter__",
      "dict",
      "__bool__",
      "custom_model_dump",
      "__init__",
      "_get_result",
      "result"
    ],
    "document_class_names": [
      "Event",
      "for",
      "CustomEvent",
      "StartEvent",
      "StopEvent",
      "InputRequiredEvent",
      "HumanResponseEvent"
    ],
    "content": "from _collections_abc import dict_items, dict_keys, dict_values\nfrom typing import Any, Dict, Type\n\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    PrivateAttr,\n    model_serializer,\n)\n\n\nclass Event(BaseModel):\n    \"\"\"Base class for event types that mimics dict interface.\n\n    PrivateAttr:\n        _data (Dict[str, Any]): Underlying Python dict.\n\n    Examples:\n        Basic example usage\n\n        ```python\n        from llama_index.core.workflows.events import Event\n\n        evt = Event(a=1, b=2)\n\n        # can use dot access to get values of `a` and `b`\n        print((evt.a, evt.b))\n\n        # can also set the attrs\n        evt.a = 2\n        ```\n\n        Custom event with additional Fields/PrivateAttr\n\n        ```python\n        from llama_index.core.workflows.events import Event\n        from llama_index.core.bridge.pydantic import Field, PrivateAttr\n\n        class CustomEvent(Event):\n            field_1: int = Field(description=\"my custom field\")\n            _private_attr_1: int = PrivateAttr()\n\n        evt = CustomEvent(a=1, b=2, field_1=3, _private_attr_1=4)\n\n        # `field_1` and `_private_attr_1` get set as they do with Pydantic BaseModel\n        print(evt.field_1)\n        print(evt._private_attr_1)\n\n        # `a` and `b` get set in the underlying dict, namely `evt._data`\n        print((evt.a, evt.b))\n        ```\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    _data: Dict[str, Any] = PrivateAttr(default_factory=dict)\n\n    def __init__(self, **params: Any):\n        \"\"\"__init__.\n\n        NOTE: fields and private_attrs are pulled from params by name.\n        \"\"\"\n        # extract and set fields, private attrs and remaining shove in _data\n        fields = {}\n        private_attrs = {}\n        data = {}\n        for k, v in params.items():\n            if k in self.model_fields:\n                fields[k] = v\n            elif k in self.__private_attributes__:\n                private_attrs[k] = v\n            else:\n                data[k] = v\n        super().__init__(**fields)\n        for private_attr, value in private_attrs.items():\n            super().__setattr__(private_attr, value)\n        if data:\n            self._data.update(data)\n\n    def __getattr__(self, __name: str) -> Any:\n        if __name in self.__private_attributes__ or __name in self.model_fields:\n            return super().__getattr__(__name)  # type: ignore\n        else:\n            try:\n                return self._data[__name]\n            except KeyError:\n                raise AttributeError(\n                    f\"'{self.__class__.__name__}' object has no attribute '{__name}'\"\n                )\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        if name in self.__private_attributes__ or name in self.model_fields:\n            super().__setattr__(name, value)\n        else:\n            self._data.__setitem__(name, value)\n\n    def __getitem__(self, key: str) -> Any:\n        return self._data[key]\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self._data[key] = value\n\n    def get(self, key: str, default: Any = None) -> Any:\n        return self._data.get(key, default)\n\n    def __contains__(self, key: str) -> bool:\n        return key in self._data\n\n    def keys(self) -> \"dict_keys[str, Any]\":\n        return self._data.keys()\n\n    def values(self) -> \"dict_values[str, Any]\":\n        return self._data.values()\n\n    def items(self) -> \"dict_items[str, Any]\":\n        return self._data.items()\n\n    def __len__(self) -> int:\n        return len(self._data)\n\n    def __iter__(self) -> Any:\n        return iter(self._data)\n\n    def dict(self, *args: Any, **kwargs: Any) -> Dict[str, Any]:\n        return self._data\n\n    def __bool__(self) -> bool:\n        \"\"\"Make test `if event:` pass on Event instances.\"\"\"\n        return True\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(self, handler: Any) -> Dict[str, Any]:\n        data = handler(self)\n        # include _data in serialization\n        if self._data:\n            data[\"_data\"] = self._data\n        return data\n\n\nclass StartEvent(Event):\n    \"\"\"StartEvent is implicitly sent when a workflow runs.\"\"\"\n\n\nclass StopEvent(Event):\n    \"\"\"EndEvent signals the workflow to stop.\"\"\"\n\n    _result: Any = PrivateAttr(default=None)\n\n    def __init__(self, result: Any = None, **kwargs: Any) -> None:\n        # forces the user to provide a result\n        super().__init__(_result=result, **kwargs)\n\n    def _get_result(self) -> Any:\n        \"\"\"This can be overridden by subclasses to return the desired result.\"\"\"\n        return self._result\n\n    @property\n    def result(self) -> Any:\n        return self._get_result()\n\n\nclass InputRequiredEvent(Event):\n    \"\"\"InputRequiredEvent is sent when an input is required for a step.\"\"\"\n\n    prefix: str = Field(\n        description=\"The prefix and description of the input that is required.\"\n    )\n\n\nclass HumanResponseEvent(Event):\n    \"\"\"HumanResponseEvent is sent when a human response is required for a step.\"\"\"\n\n    response: str = Field(description=\"The response from the human.\")\n\n\nEventType = Type[Event]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/context_serializers.py",
    "filename": "context_serializers.py",
    "relpath": "workflow/context_serializers.py",
    "start_line": 1,
    "end_line": 82,
    "length": 82,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "serialize",
      "deserialize",
      "_serialize_value",
      "serialize",
      "_deserialize_value",
      "deserialize",
      "serialize",
      "deserialize"
    ],
    "chunk_class_names": [
      "BaseSerializer",
      "JsonSerializer",
      "JsonPickleSerializer"
    ],
    "document_function_names": [
      "serialize",
      "deserialize",
      "_serialize_value",
      "serialize",
      "_deserialize_value",
      "deserialize",
      "serialize",
      "deserialize"
    ],
    "document_class_names": [
      "BaseSerializer",
      "JsonSerializer",
      "JsonPickleSerializer"
    ],
    "content": "import base64\nimport json\nimport pickle\nfrom abc import ABC, abstractmethod\nfrom typing import Any\nfrom pydantic import BaseModel\n\nfrom llama_index.core.schema import BaseComponent\nfrom .utils import import_module_from_qualified_name, get_qualified_name\n\n\nclass BaseSerializer(ABC):\n    @abstractmethod\n    def serialize(self, value: Any) -> str:\n        ...\n\n    @abstractmethod\n    def deserialize(self, value: str) -> Any:\n        ...\n\n\nclass JsonSerializer(BaseSerializer):\n    def _serialize_value(self, value: Any) -> Any:\n        \"\"\"Helper to serialize a single value.\"\"\"\n        if isinstance(value, BaseComponent):\n            return {\n                \"__is_component\": True,\n                \"value\": value.to_dict(),\n                \"qualified_name\": get_qualified_name(value),\n            }\n        elif isinstance(value, BaseModel):\n            return {\n                \"__is_pydantic\": True,\n                \"value\": value.model_dump(),\n                \"qualified_name\": get_qualified_name(value),\n            }\n        elif isinstance(value, dict):\n            return {k: self._serialize_value(v) for k, v in value.items()}\n        elif isinstance(value, list):\n            return [self._serialize_value(item) for item in value]\n        return value\n\n    def serialize(self, value: Any) -> str:\n        try:\n            serialized_value = self._serialize_value(value)\n            return json.dumps(serialized_value)\n        except Exception as e:\n            raise ValueError(f\"Failed to serialize value: {type(value)}: {value!s}\")\n\n    def _deserialize_value(self, data: Any) -> Any:\n        \"\"\"Helper to deserialize a single value.\"\"\"\n        if isinstance(data, dict):\n            if data.get(\"__is_pydantic\") and data.get(\"qualified_name\"):\n                module_class = import_module_from_qualified_name(data[\"qualified_name\"])\n                return module_class.model_validate(data[\"value\"])\n            elif data.get(\"__is_component\") and data.get(\"qualified_name\"):\n                module_class = import_module_from_qualified_name(data[\"qualified_name\"])\n                return module_class.from_dict(data[\"value\"])\n            return {k: self._deserialize_value(v) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [self._deserialize_value(item) for item in data]\n        return data\n\n    def deserialize(self, value: str) -> Any:\n        data = json.loads(value)\n        return self._deserialize_value(data)\n\n\nclass JsonPickleSerializer(JsonSerializer):\n    def serialize(self, value: Any) -> str:\n        \"\"\"Serialize while prioritizing JSON, falling back to Pickle.\"\"\"\n        try:\n            return super().serialize(value)\n        except Exception:\n            return base64.b64encode(pickle.dumps(value)).decode(\"utf-8\")\n\n    def deserialize(self, value: str) -> Any:\n        \"\"\"Deserialize while prioritizing Pickle, falling back to JSON.\"\"\"\n        try:\n            return pickle.loads(base64.b64decode(value))\n        except Exception:\n            return super().deserialize(value)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/errors.py",
    "filename": "errors.py",
    "relpath": "workflow/errors.py",
    "start_line": 1,
    "end_line": 26,
    "length": 26,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "WorkflowValidationError",
      "WorkflowTimeoutError",
      "WorkflowRuntimeError",
      "WorkflowDone",
      "WorkflowCancelledByUser",
      "WorkflowStepDoesNotExistError",
      "WorkflowConfigurationError"
    ],
    "document_function_names": [],
    "document_class_names": [
      "WorkflowValidationError",
      "WorkflowTimeoutError",
      "WorkflowRuntimeError",
      "WorkflowDone",
      "WorkflowCancelledByUser",
      "WorkflowStepDoesNotExistError",
      "WorkflowConfigurationError"
    ],
    "content": "class WorkflowValidationError(Exception):\n    pass\n\n\nclass WorkflowTimeoutError(Exception):\n    pass\n\n\nclass WorkflowRuntimeError(Exception):\n    pass\n\n\nclass WorkflowDone(Exception):\n    pass\n\n\nclass WorkflowCancelledByUser(Exception):\n    pass\n\n\nclass WorkflowStepDoesNotExistError(Exception):\n    pass\n\n\nclass WorkflowConfigurationError(Exception):\n    pass"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/__init__.py",
    "filename": "__init__.py",
    "relpath": "workflow/__init__.py",
    "start_line": 1,
    "end_line": 49,
    "length": 49,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.workflow.context import Context\nfrom llama_index.core.workflow.decorators import step\nfrom llama_index.core.workflow.drawing import (\n    draw_all_possible_flows,\n    draw_most_recent_execution,\n)\nfrom llama_index.core.workflow.errors import (\n    WorkflowRuntimeError,\n    WorkflowTimeoutError,\n    WorkflowValidationError,\n)\nfrom llama_index.core.workflow.events import (\n    Event,\n    StartEvent,\n    StopEvent,\n    InputRequiredEvent,\n    HumanResponseEvent,\n)\nfrom llama_index.core.workflow.workflow import Workflow\nfrom llama_index.core.workflow.context import Context\nfrom llama_index.core.workflow.context_serializers import (\n    JsonPickleSerializer,\n    JsonSerializer,\n)\nfrom llama_index.core.workflow.checkpointer import (\n    Checkpoint,\n    WorkflowCheckpointer,\n)\n\n__all__ = [\n    \"Context\",\n    \"Event\",\n    \"StartEvent\",\n    \"StopEvent\",\n    \"Workflow\",\n    \"WorkflowRuntimeError\",\n    \"WorkflowTimeoutError\",\n    \"WorkflowValidationError\",\n    \"draw_all_possible_flows\",\n    \"draw_most_recent_execution\",\n    \"step\",\n    \"Context\",\n    \"InputRequiredEvent\",\n    \"HumanResponseEvent\",\n    \"JsonPickleSerializer\",\n    \"JsonSerializer\",\n    \"WorkflowCheckpointer\",\n    \"Checkpoint\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/utils.py",
    "filename": "utils.py",
    "relpath": "workflow/utils.py",
    "start_line": 1,
    "end_line": 190,
    "length": 190,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "inspect_signature",
      "validate_step_signature",
      "get_steps_from_class",
      "get_steps_from_instance",
      "_get_param_types",
      "_get_return_types",
      "is_free_function",
      "get_qualified_name",
      "import_module_from_qualified_name"
    ],
    "chunk_class_names": [
      "ServiceDefinition",
      "StepSignatureSpec"
    ],
    "document_function_names": [
      "inspect_signature",
      "validate_step_signature",
      "get_steps_from_class",
      "get_steps_from_instance",
      "_get_param_types",
      "_get_return_types",
      "is_free_function",
      "get_qualified_name",
      "import_module_from_qualified_name"
    ],
    "document_class_names": [
      "ServiceDefinition",
      "StepSignatureSpec"
    ],
    "content": "import inspect\nfrom importlib import import_module\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Union,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\n# handle python version compatibility\ntry:\n    from types import UnionType  # type: ignore[attr-defined]\nexcept ImportError:  # pragma: no cover\n    from typing import Union as UnionType  # type: ignore[assignment]\n\nfrom llama_index.core.bridge.pydantic import BaseModel, ConfigDict\n\nfrom .errors import WorkflowValidationError\nfrom .events import Event, EventType\n\nBUSY_WAIT_DELAY = 0.01\n\n\nclass ServiceDefinition(BaseModel):\n    # Make the service definition hashable\n    model_config = ConfigDict(frozen=True)\n\n    name: str\n    service: Any\n    default_value: Optional[Any]\n\n\nclass StepSignatureSpec(BaseModel):\n    \"\"\"A Pydantic model representing the signature of a step function or method.\"\"\"\n\n    accepted_events: Dict[str, List[EventType]]\n    return_types: List[Any]\n    context_parameter: Optional[str]\n    requested_services: Optional[List[ServiceDefinition]]\n\n\ndef inspect_signature(fn: Callable) -> StepSignatureSpec:\n    \"\"\"Given a function, ensure the signature is compatible with a workflow step.\"\"\"\n    sig = inspect.signature(fn)\n\n    accepted_events: Dict[str, List[EventType]] = {}\n    context_parameter = None\n    requested_services = []\n\n    # Inspect function parameters\n    for name, t in sig.parameters.items():\n        # Ignore self and cls\n        if name in (\"self\", \"cls\"):\n            continue\n\n        # Get name and type of the Context param\n        if hasattr(t.annotation, \"__name__\") and t.annotation.__name__ == \"Context\":\n            context_parameter = name\n            continue\n\n        # Collect name and types of the event param\n        param_types = _get_param_types(t)\n        if all(\n            param_t == Event\n            or (inspect.isclass(param_t) and issubclass(param_t, Event))\n            for param_t in param_types\n        ):\n            accepted_events[name] = param_types\n            continue\n\n        # Everything else will be treated as a service request\n        default_value = t.default\n        if default_value is inspect.Parameter.empty:\n            default_value = None\n\n        requested_services.append(\n            ServiceDefinition(\n                name=name, service=param_types[0], default_value=default_value\n            )\n        )\n\n    return StepSignatureSpec(\n        accepted_events=accepted_events,\n        return_types=_get_return_types(fn),\n        context_parameter=context_parameter,\n        requested_services=requested_services,\n    )\n\n\ndef validate_step_signature(spec: StepSignatureSpec) -> None:\n    num_of_events = len(spec.accepted_events)\n    if num_of_events == 0:\n        msg = \"Step signature must have at least one parameter annotated as type Event\"\n        raise WorkflowValidationError(msg)\n    elif num_of_events > 1:\n        msg = f\"Step signature must contain exactly one parameter of type Event but found {num_of_events}.\"\n        raise WorkflowValidationError(msg)\n\n    if not spec.return_types:\n        msg = f\"Return types of workflows step functions must be annotated with their type.\"\n        raise WorkflowValidationError(msg)\n\n\ndef get_steps_from_class(_class: object) -> Dict[str, Callable]:\n    \"\"\"Given a class, return the list of its methods that were defined as steps.\"\"\"\n    step_methods = {}\n    all_methods = inspect.getmembers(_class, predicate=inspect.isfunction)\n\n    for name, method in all_methods:\n        if hasattr(method, \"__step_config\"):\n            step_methods[name] = method\n\n    return step_methods\n\n\ndef get_steps_from_instance(workflow: object) -> Dict[str, Callable]:\n    \"\"\"Given a workflow instance, return the list of its methods that were defined as steps.\"\"\"\n    step_methods = {}\n    all_methods = inspect.getmembers(workflow, predicate=inspect.ismethod)\n\n    for name, method in all_methods:\n        if hasattr(method, \"__step_config\"):\n            step_methods[name] = method\n\n    return step_methods\n\n\ndef _get_param_types(param: inspect.Parameter) -> List[Any]:\n    \"\"\"Extract the types of a parameter. Handles Union and Optional types.\"\"\"\n    typ = param.annotation\n    if typ is inspect.Parameter.empty:\n        return [Any]\n    if get_origin(typ) in (Union, Optional, UnionType):\n        return [t for t in get_args(typ) if t is not type(None)]\n    return [typ]\n\n\ndef _get_return_types(func: Callable) -> List[Any]:\n    \"\"\"Extract the return type hints from a function.\n\n    Handles Union, Optional, and List types.\n    \"\"\"\n    type_hints = get_type_hints(func)\n    return_hint = type_hints.get(\"return\")\n    if return_hint is None:\n        return []\n\n    origin = get_origin(return_hint)\n    if origin in (Union, UnionType):\n        # Optional is Union[type, None] so it's covered here\n        return [t for t in get_args(return_hint) if t is not type(None)]\n    else:\n        return [return_hint]\n\n\ndef is_free_function(qualname: str) -> bool:\n    \"\"\"Determines whether a certain qualified name points to a free function.\n\n    The strategy should be able to spot nested functions, for details see PEP-3155.\n    \"\"\"\n    if not qualname:\n        msg = \"The qualified name cannot be empty\"\n        raise ValueError(msg)\n\n    toks = qualname.split(\".\")\n    if len(toks) == 1:\n        # e.g. `my_function`\n        return True\n    elif \"<locals>\" not in toks:\n        # e.g. `MyClass.my_method`\n        return False\n    else:\n        return toks[-2] == \"<locals>\"\n\n\ndef get_qualified_name(value: Any) -> str:\n    \"\"\"Get the qualified name of a value.\"\"\"\n    return value.__module__ + \".\" + value.__class__.__name__\n\n\ndef import_module_from_qualified_name(qualified_name: str) -> Any:\n    \"\"\"Import a module from a qualified name.\"\"\"\n    module_path = qualified_name.rsplit(\".\", 1)\n    module = import_module(module_path[0])\n    return getattr(module, module_path[1])"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/checkpointer.py",
    "filename": "checkpointer.py",
    "relpath": "workflow/checkpointer.py",
    "start_line": 1,
    "end_line": 195,
    "length": 195,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__call__",
      "__init__",
      "enable_checkpoint",
      "disable_checkpoint",
      "run",
      "run_from",
      "checkpoints",
      "new_checkpoint_callback_for_run",
      "_create_checkpoint",
      "_checkpoint_filter_condition",
      "filter_checkpoints"
    ],
    "chunk_class_names": [
      "CheckpointCallback",
      "Checkpoint",
      "WorkflowCheckpointer"
    ],
    "document_function_names": [
      "__call__",
      "__init__",
      "enable_checkpoint",
      "disable_checkpoint",
      "run",
      "run_from",
      "checkpoints",
      "new_checkpoint_callback_for_run",
      "_create_checkpoint",
      "_checkpoint_filter_condition",
      "filter_checkpoints"
    ],
    "document_class_names": [
      "CheckpointCallback",
      "Checkpoint",
      "WorkflowCheckpointer"
    ],
    "content": "import asyncio\nimport uuid\nimport functools\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    ConfigDict,\n)\nfrom typing import (\n    Optional,\n    Dict,\n    Any,\n    List,\n    Protocol,\n    TYPE_CHECKING,\n    Type,\n    Awaitable,\n    Set,\n)\nfrom llama_index.core.workflow.context import Context\nfrom llama_index.core.workflow.context_serializers import BaseSerializer, JsonSerializer\nfrom llama_index.core.workflow.handler import WorkflowHandler\nfrom llama_index.core.workflow.events import Event\nfrom llama_index.core.workflow.errors import WorkflowStepDoesNotExistError\n\nif TYPE_CHECKING:  # pragma: no cover\n    from .workflow import Workflow\n\n\nclass CheckpointCallback(Protocol):\n    def __call__(\n        self,\n        run_id: str,\n        last_completed_step: Optional[str],\n        input_ev: Optional[Event],\n        output_ev: Event,\n        ctx: Context,\n    ) -> Awaitable[None]:\n        ...\n\n\nclass Checkpoint(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    id_: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    last_completed_step: Optional[str]\n    input_event: Optional[Event]\n    output_event: Event\n    ctx_state: Dict[str, Any]\n\n\nclass WorkflowCheckpointer:\n    \"\"\"An object that creates and maintain's checkpoints during a Workflow run.\n\n    This checkpoint manager object works with multiple run's of a Workflow instance\n    or from several different instances. Specified checkpoints can also be used\n    as the starting point for a new Workflow run. Note that checkpoints are stored\n    at the end of every step (with the exception of the _done step) for the attached\n    Workflow.\n    \"\"\"\n\n    def __init__(\n        self,\n        workflow: \"Workflow\",\n        checkpoint_serializer: Optional[BaseSerializer] = None,\n        disabled_steps: List[str] = [],\n    ):\n        \"\"\"Create a WorkflowCheckpointer object.\n\n        Args:\n            workflow (Workflow): The wrapped workflow.\n            checkpoint_serializer (Optional[BaseSerializer], optional): The serializer to use\n                for serializing associated `Context` of a Workflow run. Defaults to None.\n            disabled_steps (List[str], optional): Steps for which to disable checkpointing. Defaults to [].\n        \"\"\"\n        self._checkpoints: Dict[str, List[Checkpoint]] = {}\n        self._checkpoint_serializer = checkpoint_serializer or JsonSerializer()\n        self._lock: asyncio.Lock = asyncio.Lock()\n\n        self.workflow = workflow\n        self.enabled_checkpoints: Set[str] = {\n            k for k in workflow._get_steps() if k != \"_done\"\n        }\n        for step_name in disabled_steps:\n            self.disable_checkpoint(step_name)\n\n    def enable_checkpoint(self, step: str) -> None:\n        \"\"\"Enable checkpointing after the completion of the specified step.\"\"\"\n        if step not in self.workflow._get_steps():\n            msg = f\"This workflow does not contain a step with name {step}\"\n            raise WorkflowStepDoesNotExistError(msg)\n\n        self.enabled_checkpoints.add(step)\n\n    def disable_checkpoint(self, step: str) -> None:\n        \"\"\"Disable checkpointing after the completion of the specified step.\"\"\"\n        if step not in self.workflow._get_steps():\n            msg = f\"This workflow does not contain a step with name {step}\"\n            raise WorkflowStepDoesNotExistError(msg)\n        try:\n            self.enabled_checkpoints.remove(step)\n        except KeyError:\n            pass\n\n    def run(self, **kwargs: Any) -> WorkflowHandler:\n        \"\"\"Run the workflow with checkpointing.\"\"\"\n        return self.workflow.run(\n            checkpoint_callback=self.new_checkpoint_callback_for_run(),\n            **kwargs,\n        )\n\n    def run_from(self, checkpoint: Checkpoint, **kwargs: Any) -> WorkflowHandler:\n        \"\"\"Run the attached workflow from the specified checkpoint.\"\"\"\n        return self.workflow.run_from(\n            checkpoint=checkpoint,\n            ctx_serializer=self._checkpoint_serializer,\n            checkpoint_callback=self.new_checkpoint_callback_for_run(),\n            **kwargs,\n        )\n\n    @property\n    def checkpoints(self) -> Dict[str, List[Checkpoint]]:\n        return self._checkpoints\n\n    def new_checkpoint_callback_for_run(self) -> CheckpointCallback:\n        \"\"\"Closure to generate a new `CheckpointCallback` with a unique run-id.\"\"\"\n\n        async def _create_checkpoint(\n            run_id: str,\n            last_completed_step: Optional[str],\n            input_ev: Optional[Event],\n            output_ev: Event,\n            ctx: Context,\n        ) -> None:\n            \"\"\"Build a checkpoint around the last completed step.\"\"\"\n            if last_completed_step not in self.enabled_checkpoints:\n                return\n\n            checkpoint = Checkpoint(\n                last_completed_step=last_completed_step,\n                input_event=input_ev,\n                output_event=output_ev,\n                ctx_state=ctx.to_dict(serializer=self._checkpoint_serializer),\n            )\n            async with self._lock:\n                if run_id in self.checkpoints:\n                    self.checkpoints[run_id].append(checkpoint)\n                else:\n                    self.checkpoints[run_id] = [checkpoint]\n\n        return _create_checkpoint\n\n    def _checkpoint_filter_condition(\n        self,\n        ckpt: Checkpoint,\n        last_completed_step: Optional[str],\n        input_event_type: Optional[Type[Event]],\n        output_event_type: Optional[Type[Event]],\n    ) -> bool:\n        if last_completed_step and ckpt.last_completed_step != last_completed_step:\n            return False\n        if input_event_type and type(ckpt.input_event) != input_event_type:\n            return False\n        if output_event_type and type(ckpt.output_event) != output_event_type:\n            return False\n        return True\n\n    def filter_checkpoints(\n        self,\n        run_id: Optional[str] = None,\n        last_completed_step: Optional[str] = None,\n        input_event_type: Optional[Type[Event]] = None,\n        output_event_type: Optional[Type[Event]] = None,\n    ) -> List[Checkpoint]:\n        \"\"\"Returns a list of Checkpoint's based on user provided filters.\"\"\"\n        if (\n            not run_id\n            and not last_completed_step\n            and not input_event_type\n            and not output_event_type\n        ):\n            raise ValueError(\"Please specify a filter.\")\n\n        candidate_ckpts = (\n            self.checkpoints[run_id]\n            if run_id\n            else functools.reduce(lambda a, b: a + b, self.checkpoints.values())\n        )\n\n        return [\n            ckpt\n            for ckpt in candidate_ckpts\n            if self._checkpoint_filter_condition(\n                ckpt, last_completed_step, input_event_type, output_event_type\n            )\n        ]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/handler.py",
    "filename": "handler.py",
    "relpath": "workflow/handler.py",
    "start_line": 1,
    "end_line": 131,
    "length": 131,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "__str__",
      "is_done",
      "stream_events",
      "run_step",
      "cancel_run"
    ],
    "chunk_class_names": [
      "WorkflowHandler"
    ],
    "document_function_names": [
      "__init__",
      "__str__",
      "is_done",
      "stream_events",
      "run_step",
      "cancel_run"
    ],
    "document_class_names": [
      "WorkflowHandler"
    ],
    "content": "import asyncio\nfrom typing import Any, AsyncGenerator, Optional\n\nfrom llama_index.core.workflow.context import Context\nfrom llama_index.core.workflow.errors import WorkflowDone\nfrom llama_index.core.workflow.events import Event, StopEvent\n\nfrom .types import RunResultT\nfrom .utils import BUSY_WAIT_DELAY\n\n\nclass WorkflowHandler(asyncio.Future[RunResultT]):\n    def __init__(\n        self,\n        *args: Any,\n        ctx: Optional[Context] = None,\n        run_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        self.run_id = run_id\n        self.ctx = ctx\n\n    def __str__(self) -> str:\n        return str(self.result())\n\n    def is_done(self) -> bool:\n        return self.done()\n\n    async def stream_events(self) -> AsyncGenerator[Event, None]:\n        if not self.ctx:\n            raise ValueError(\"Context is not set!\")\n\n        while True:\n            ev = await self.ctx.streaming_queue.get()\n\n            yield ev\n\n            if type(ev) is StopEvent:\n                break\n\n    async def run_step(self) -> Optional[Event]:\n        \"\"\"Runs the next workflow step and returns the output Event.\n\n        If return is None, then the workflow is considered done.\n\n        Examples:\n            ```python\n            handler = workflow.run(stepwise=True)\n            while not handler.is_done():\n                ev = await handler.run_step()\n                handler.ctx.send_event(ev)\n\n            result = handler.result()\n            print(result)\n            ```\n        \"\"\"\n        # since event is sent before calling this method, we need to unblock the event loop\n        await asyncio.sleep(0)\n\n        if self.ctx and not self.ctx.stepwise:\n            raise ValueError(\"Stepwise context is required to run stepwise.\")\n\n        if self.ctx:\n            try:\n                # Unblock all pending steps\n                for flag in self.ctx._step_flags.values():\n                    flag.set()\n\n                # Yield back control to the event loop to give an unblocked step\n                # the chance to run (we won't actually sleep here).\n                await asyncio.sleep(0)\n\n                # check if we're done, or if a step raised error\n                we_done = False\n                exception_raised = None\n                retval = None\n                for t in self.ctx._tasks:\n                    # Check if we're done\n                    if not t.done():\n                        continue\n\n                    we_done = True\n                    e = t.exception()\n                    if type(e) is not WorkflowDone:\n                        exception_raised = e\n\n                if we_done:\n                    # Remove any reference to the tasks\n                    for t in self.ctx._tasks:\n                        t.cancel()\n                        await asyncio.sleep(0)\n\n                    # the context is no longer running\n                    self.ctx.is_running = False\n\n                    if exception_raised:\n                        raise exception_raised\n\n                    if not self.done():\n                        self.set_result(self.ctx.get_result())\n                else:\n                    # Continue with running next step. Make sure we wait for the\n                    # step function to return before proceeding.\n                    in_progress = len(await self.ctx.running_steps())\n                    while in_progress:\n                        await asyncio.sleep(BUSY_WAIT_DELAY)\n                        in_progress = len(await self.ctx.running_steps())\n\n                    # notify unblocked task that we're ready to accept next event\n                    async with self.ctx._step_condition:\n                        self.ctx._step_condition.notify()\n\n                    # Wait to be notified that the new_ev has been written\n                    async with self.ctx._step_event_written:\n                        await self.ctx._step_event_written.wait()\n                        retval = self.ctx._step_event_holding\n            except Exception as e:\n                if not self.is_done():  # Avoid InvalidStateError edge case\n                    self.set_exception(e)\n                raise\n        else:\n            raise ValueError(\"Context is not set!\")\n\n        return retval\n\n    async def cancel_run(self) -> None:\n        \"\"\"Method to cancel a Workflow execution.\"\"\"\n        if self.ctx:\n            self.ctx._cancel_flag.set()\n            await asyncio.sleep(0)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/workflow.py",
    "filename": "workflow.py",
    "relpath": "workflow/workflow.py",
    "start_line": 1,
    "end_line": 48,
    "length": 48,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__"
    ],
    "chunk_class_names": [
      "WorkflowMeta"
    ],
    "document_function_names": [
      "__init__",
      "__init__",
      "_ensure_start_event_class",
      "_ensure_stop_event_class",
      "stream_events",
      "add_step",
      "add_workflows",
      "_get_steps",
      "_start",
      "_task",
      "_cancel_workflow_task",
      "send_event",
      "_get_start_event_instance",
      "run",
      "_run_workflow",
      "run_from",
      "is_done",
      "_done",
      "_validate"
    ],
    "document_class_names": [
      "WorkflowMeta",
      "Workflow",
      "itself",
      "to",
      "not",
      "try",
      "is"
    ],
    "content": "import asyncio\nimport functools\nimport logging\nimport time\nimport uuid\nimport warnings\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Dict,\n    Optional,\n    Set,\n    Tuple,\n)\n\nfrom llama_index.core.bridge.pydantic import ValidationError\nfrom llama_index.core.instrumentation import get_dispatcher\nfrom llama_index.core.workflow.types import RunResultT\n\nfrom .checkpointer import Checkpoint, CheckpointCallback\nfrom .context import Context\nfrom .context_serializers import BaseSerializer, JsonSerializer\nfrom .decorators import StepConfig, step\nfrom .errors import *\nfrom .events import (\n    Event,\n    HumanResponseEvent,\n    InputRequiredEvent,\n    StartEvent,\n    StopEvent,\n)\nfrom .handler import WorkflowHandler\nfrom .service import ServiceManager\nfrom .utils import (\n    ServiceDefinition,\n    get_steps_from_class,\n    get_steps_from_instance,\n)\n\ndispatcher = get_dispatcher(__name__)\nlogger = logging.getLogger()\n\n\nclass WorkflowMeta(type):\n    def __init__(cls, name: str, bases: Tuple[type, ...], dct: Dict[str, Any]) -> None:\n        super().__init__(name, bases, dct)\n        cls._step_functions: Dict[str, Callable] = {}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/workflow.py",
    "filename": "workflow.py",
    "relpath": "workflow/workflow.py",
    "start_line": 48,
    "end_line": 51,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "Workflow"
    ],
    "document_function_names": [
      "__init__",
      "__init__",
      "_ensure_start_event_class",
      "_ensure_stop_event_class",
      "stream_events",
      "add_step",
      "add_workflows",
      "_get_steps",
      "_start",
      "_task",
      "_cancel_workflow_task",
      "send_event",
      "_get_start_event_instance",
      "run",
      "_run_workflow",
      "run_from",
      "is_done",
      "_done",
      "_validate"
    ],
    "document_class_names": [
      "WorkflowMeta",
      "Workflow",
      "itself",
      "to",
      "not",
      "try",
      "is"
    ],
    "content": "class Workflow(metaclass=WorkflowMeta):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/workflow.py",
    "filename": "workflow.py",
    "relpath": "workflow/workflow.py",
    "start_line": 51,
    "end_line": 219,
    "length": 169,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_ensure_start_event_class",
      "_ensure_stop_event_class",
      "stream_events",
      "add_step",
      "add_workflows",
      "_get_steps"
    ],
    "chunk_class_names": [
      "itself",
      "to"
    ],
    "document_function_names": [
      "__init__",
      "__init__",
      "_ensure_start_event_class",
      "_ensure_stop_event_class",
      "stream_events",
      "add_step",
      "add_workflows",
      "_get_steps",
      "_start",
      "_task",
      "_cancel_workflow_task",
      "send_event",
      "_get_start_event_instance",
      "run",
      "_run_workflow",
      "run_from",
      "is_done",
      "_done",
      "_validate"
    ],
    "document_class_names": [
      "WorkflowMeta",
      "Workflow",
      "itself",
      "to",
      "not",
      "try",
      "is"
    ],
    "content": "\"\"\"An event-driven abstraction used to orchestrate the execution of different components called \"steps\".\n\n    Each step is responsible for handling certain event types and possibly emitting new events. Steps can be \"bound\"\n    when they are defined as methods of the `Workflow` class itself, or \"unbound\" when they are defined as free\n    functions. To define a step, the method or function must be decorated with the `@step` decorator.\n\n    Workflows provide basic validation to catch potential runtime errors as soon as possible. Validation happens once,\n    when the workflow starts, and does not produce much overhead. It can be disabled in any case.\n\n    Use an instance of a `Workflow` class to run a workflow and stream events produced during execution. Workflows\n    can be run step-by-step, by calling the `run_step` function multiple times until completion.\n    \"\"\"\n\n    def __init__(\n        self,\n        timeout: Optional[float] = 10.0,\n        disable_validation: bool = False,\n        verbose: bool = False,\n        service_manager: Optional[ServiceManager] = None,\n        num_concurrent_runs: Optional[int] = None,\n    ) -> None:\n        \"\"\"Create an instance of the workflow.\n\n        Args:\n            timeout:\n                Number of seconds after the workflow execution will be halted, raising a `WorkflowTimeoutError`\n                exception. If set to `None`, the timeout will be disabled.\n            disable_validaton:\n                Whether or not the workflow should be validated before running. In case the workflow is\n                misconfigured, a call to `run` will raise a `WorkflowValidationError` exception explaining the details\n                of the problem.\n            verbose:\n                Whether or not the workflow should print additional informative messages during execution.\n            service_manager:\n                The instance of the `ServiceManager` used to make nested workflows available to this\n                workflow instance. The default value is the best choice unless you're customizing the workflow runtime.\n            num_concurrent_runs:\n                maximum number of .run() executions occurring simultaneously. If set to `None`, there\n                is no limit to this number.\n        \"\"\"\n        # Configuration\n        self._timeout = timeout\n        self._verbose = verbose\n        self._disable_validation = disable_validation\n        self._num_concurrent_runs = num_concurrent_runs\n        self._stop_event_class = self._ensure_stop_event_class()\n        self._start_event_class = self._ensure_start_event_class()\n        self._sem = (\n            asyncio.Semaphore(num_concurrent_runs) if num_concurrent_runs else None\n        )\n        # Broker machinery\n        self._contexts: Set[Context] = set()\n        self._stepwise_context: Optional[Context] = None\n        # Services management\n        self._service_manager = service_manager or ServiceManager()\n\n    def _ensure_start_event_class(self) -> type[StartEvent]:\n        \"\"\"Returns the StartEvent type used in this workflow.\n\n        It works by inspecting the events received by the step methods.\n        \"\"\"\n        start_events_found: set[type[StartEvent]] = set()\n        for step_func in self._get_steps().values():\n            step_config: StepConfig = getattr(step_func, \"__step_config\")\n            for event_type in step_config.accepted_events:\n                if issubclass(event_type, StartEvent):\n                    start_events_found.add(event_type)\n\n        num_found = len(start_events_found)\n        if num_found == 0:\n            msg = \"At least one Event of type StartEvent must be received by any step.\"\n            raise WorkflowConfigurationError(msg)\n        elif num_found > 1:\n            msg = f\"Only one type of StartEvent is allowed per workflow, found {num_found}: {start_events_found}.\"\n            raise WorkflowConfigurationError(msg)\n        else:\n            return start_events_found.pop()\n\n    def _ensure_stop_event_class(self) -> type[RunResultT]:\n        \"\"\"Returns the StopEvent type used in this workflow.\n\n        It works by inspecting the events returned.\n        \"\"\"\n        stop_events_found: set[type[StopEvent]] = set()\n        for step_func in self._get_steps().values():\n            step_config: StepConfig = getattr(step_func, \"__step_config\")\n            for event_type in step_config.return_types:\n                if issubclass(event_type, StopEvent):\n                    stop_events_found.add(event_type)\n\n        num_found = len(stop_events_found)\n        if num_found == 0:\n            msg = \"At least one Event of type StopEvent must be returned by any step.\"\n            raise WorkflowConfigurationError(msg)\n        elif num_found > 1:\n            msg = f\"Only one type of StopEvent is allowed per workflow, found {num_found}: {stop_events_found}.\"\n            raise WorkflowConfigurationError(msg)\n        else:\n            return stop_events_found.pop()\n\n    async def stream_events(self) -> AsyncGenerator[Event, None]:\n        \"\"\"Returns an async generator to consume any event that workflow steps decide to stream.\n\n        To be able to use this generator, the usual pattern is to wrap the `run` call in a background task using\n        `asyncio.create_task`, then enter a for loop like this:\n\n            wf = StreamingWorkflow()\n            r = asyncio.create_task(wf.run())\n\n            async for ev in wf.stream_events():\n                print(ev)\n\n            await r\n        \"\"\"\n        # In the typical streaming use case, `run()` is not awaited but wrapped in a asyncio.Task. Since we'll be\n        # consuming events produced by `run()`, we must give its Task the chance to run before entering the dequeueing\n        # loop.\n        await asyncio.sleep(0)\n\n        if len(self._contexts) > 1:\n            # We can't possibly know from what session we should stream events, raise an error.\n            msg = (\n                \"This workflow has multiple concurrent runs in progress and cannot stream events. \"\n                \"To be able to stream events, make sure you call `run()` on this workflow only once.\"\n            )\n            raise WorkflowRuntimeError(msg)\n\n        # Enter the dequeuing loop.\n        ctx = next(iter(self._contexts))\n        while True:\n            ev = await ctx.streaming_queue.get()\n            if isinstance(ev, StopEvent):\n                break\n\n            yield ev\n\n        # remove context to free up room for the next stream_events call\n        self._contexts.remove(ctx)\n\n    @classmethod\n    def add_step(cls, func: Callable) -> None:\n        \"\"\"Adds a free function as step for this workflow instance.\n\n        It raises an exception if a step with the same name was already added to the workflow.\n        \"\"\"\n        step_config: Optional[StepConfig] = getattr(func, \"__step_config\", None)\n        if not step_config:\n            msg = f\"Step function {func.__name__} is missing the `@step` decorator.\"\n            raise WorkflowValidationError(msg)\n\n        if func.__name__ in {**get_steps_from_class(cls), **cls._step_functions}:\n            msg = f\"A step {func.__name__} is already part of this workflow, please choose another name.\"\n            raise WorkflowValidationError(msg)\n\n        cls._step_functions[func.__name__] = func\n\n    def add_workflows(self, **workflows: \"Workflow\") -> None:\n        \"\"\"Adds one or more nested workflows to this workflow.\n\n        This method only accepts keyword arguments, and the name of the parameter\n        will be used as the name of the workflow.\n        \"\"\"\n        for name, wf in workflows.items():\n            self._service_manager.add(name, wf)\n\n    def _get_steps(self) -> Dict[str, Callable]:\n        \"\"\"Returns all the steps, whether defined as methods or free functions.\"\"\"\n        return {**get_steps_from_instance(self), **self._step_functions}  # type: ignore[attr-defined]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/workflow.py",
    "filename": "workflow.py",
    "relpath": "workflow/workflow.py",
    "start_line": 219,
    "end_line": 606,
    "length": 388,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_start",
      "_task",
      "_cancel_workflow_task",
      "send_event",
      "_get_start_event_instance",
      "run",
      "_run_workflow",
      "run_from",
      "is_done",
      "_done"
    ],
    "chunk_class_names": [
      "not",
      "try",
      "is"
    ],
    "document_function_names": [
      "__init__",
      "__init__",
      "_ensure_start_event_class",
      "_ensure_stop_event_class",
      "stream_events",
      "add_step",
      "add_workflows",
      "_get_steps",
      "_start",
      "_task",
      "_cancel_workflow_task",
      "send_event",
      "_get_start_event_instance",
      "run",
      "_run_workflow",
      "run_from",
      "is_done",
      "_done",
      "_validate"
    ],
    "document_class_names": [
      "WorkflowMeta",
      "Workflow",
      "itself",
      "to",
      "not",
      "try",
      "is"
    ],
    "content": "def _start(\n        self,\n        stepwise: bool = False,\n        ctx: Optional[Context] = None,\n        checkpoint_callback: Optional[CheckpointCallback] = None,\n    ) -> Tuple[Context, str]:\n        \"\"\"Sets up the queues and tasks for each declared step.\n\n        This method also launches each step as an async task.\n        \"\"\"\n        run_id = str(uuid.uuid4())\n        if ctx is None:\n            ctx = Context(self, stepwise=stepwise)\n            self._contexts.add(ctx)\n        else:\n            # clean up the context from the previous run\n            ctx._tasks = set()\n            ctx._retval = None\n            ctx._step_event_holding = None\n            ctx._cancel_flag.clear()\n\n        for name, step_func in self._get_steps().items():\n            if name not in ctx._queues:\n                ctx._queues[name] = asyncio.Queue()\n\n            if name not in ctx._step_flags:\n                ctx._step_flags[name] = asyncio.Event()\n\n            # At this point, step_func is guaranteed to have the `__step_config` attribute\n            step_config: StepConfig = getattr(step_func, \"__step_config\")\n\n            # Make the system step \"_done\" accept custom stop events\n            if (\n                name == \"_done\"\n                and self._stop_event_class not in step_config.accepted_events\n            ):\n                step_config.accepted_events.append(self._stop_event_class)\n\n            async def _task(\n                name: str,\n                queue: asyncio.Queue,\n                step: Callable,\n                config: StepConfig,\n            ) -> None:\n                while True:\n                    ev = await queue.get()\n                    if type(ev) not in config.accepted_events:\n                        continue\n\n                    # do we need to wait for the step flag?\n                    if stepwise:\n                        await ctx._step_flags[name].wait()\n\n                        # clear all flags so that we only run one step\n                        for flag in ctx._step_flags.values():\n                            flag.clear()\n\n                    if self._verbose and name != \"_done\":\n                        print(f\"Running step {name}\")\n\n                    # run step\n                    kwargs: Dict[str, Any] = {}\n                    if config.context_parameter:\n                        kwargs[config.context_parameter] = ctx\n                    for service_definition in config.requested_services:\n                        service = self._service_manager.get(\n                            service_definition.name, service_definition.default_value\n                        )\n                        kwargs[service_definition.name] = service\n                    kwargs[config.event_name] = ev\n\n                    # wrap the step with instrumentation\n                    instrumented_step = dispatcher.span(step)\n\n                    # - check if its async or not\n                    # - if not async, run it in an executor\n                    if asyncio.iscoroutinefunction(step):\n                        retry_start_at = time.time()\n                        attempts = 0\n                        while True:\n                            await ctx.mark_in_progress(name=name, ev=ev)\n                            await ctx.add_running_step(name)\n                            try:\n                                new_ev = await instrumented_step(**kwargs)\n                                break  # exit the retrying loop\n                            except WorkflowDone:\n                                await ctx.remove_from_in_progress(name=name, ev=ev)\n                                raise\n                            except Exception as e:\n                                if config.retry_policy is None:\n                                    raise WorkflowRuntimeError(\n                                        f\"Error in step '{name}': {e!s}\"\n                                    ) from e\n\n                                delay = config.retry_policy.next(\n                                    retry_start_at + time.time(), attempts, e\n                                )\n                                if delay is None:\n                                    # We're done retrying\n                                    raise WorkflowRuntimeError(\n                                        f\"Error in step '{name}': {e!s}\"\n                                    ) from e\n\n                                attempts += 1\n                                if self._verbose:\n                                    print(\n                                        f\"Step {name} produced an error, retry in {delay} seconds\"\n                                    )\n                                await asyncio.sleep(delay)\n                            finally:\n                                await ctx.remove_running_step(name)\n\n                    else:\n                        try:\n                            run_task = functools.partial(instrumented_step, **kwargs)\n                            new_ev = await asyncio.get_event_loop().run_in_executor(\n                                None, run_task\n                            )\n                        except WorkflowDone:\n                            await ctx.remove_from_in_progress(name=name, ev=ev)\n                            raise\n                        except Exception as e:\n                            raise WorkflowRuntimeError(\n                                f\"Error in step '{name}': {e!s}\"\n                            ) from e\n\n                    if self._verbose and name != \"_done\":\n                        if new_ev is not None:\n                            print(f\"Step {name} produced event {type(new_ev).__name__}\")\n                        else:\n                            print(f\"Step {name} produced no event\")\n\n                    # handle the return value\n                    if new_ev is None:\n                        await ctx.remove_from_in_progress(name=name, ev=ev)\n                        continue\n\n                    # Store the accepted event for the drawing operations\n                    ctx._accepted_events.append((name, type(ev).__name__))\n\n                    if not isinstance(new_ev, Event):\n                        warnings.warn(\n                            f\"Step function {name} returned {type(new_ev).__name__} instead of an Event instance.\"\n                        )\n                    else:\n                        if stepwise:\n                            async with ctx._step_condition:\n                                await ctx._step_condition.wait()\n                                ctx._step_event_holding = new_ev\n                                ctx._step_event_written.notify()  # shares same lock\n\n                                await ctx.remove_from_in_progress(name=name, ev=ev)\n\n                                # for stepwise Checkpoint after handler.run_step() call\n                                if checkpoint_callback:\n                                    await checkpoint_callback(\n                                        run_id=run_id,\n                                        ctx=ctx,\n                                        last_completed_step=name,\n                                        input_ev=ev,\n                                        output_ev=new_ev,\n                                    )\n                        else:\n                            # for regular execution, Checkpoint just before firing the next event\n                            await ctx.remove_from_in_progress(name=name, ev=ev)\n                            if checkpoint_callback:\n                                await checkpoint_callback(\n                                    run_id=run_id,\n                                    ctx=ctx,\n                                    last_completed_step=name,\n                                    input_ev=ev,\n                                    output_ev=new_ev,\n                                )\n\n                            # InputRequiredEvent's are special case and need to be written to the stream\n                            # this way, the user can access and respond to the event\n                            if isinstance(new_ev, InputRequiredEvent):\n                                ctx.write_event_to_stream(new_ev)\n                            else:\n                                ctx.send_event(new_ev)\n\n            for _ in range(step_config.num_workers):\n                ctx._tasks.add(\n                    asyncio.create_task(\n                        _task(name, ctx._queues[name], step_func, step_config),\n                        name=name,\n                    )\n                )\n\n            # add dedicated cancel task\n            async def _cancel_workflow_task() -> None:\n                try:\n                    await ctx._cancel_flag.wait()\n                    raise WorkflowCancelledByUser\n                except asyncio.CancelledError:\n                    return\n\n            ctx._tasks.add(\n                asyncio.create_task(\n                    _cancel_workflow_task(), name=\"cancel_workflow_task\"\n                )\n            )\n\n        return ctx, run_id\n\n    def send_event(self, message: Event, step: Optional[str] = None) -> None:\n        msg = (\n            \"Use a Context instance to send events from a step. \"\n            \"Make sure your step method or function takes a parameter of type Context like `ctx: Context` and \"\n            \"replace `self.send_event(...)` with `ctx.send_event(...)` in your code.\"\n        )\n\n        if len(self._contexts) > 1:\n            # We can't possibly know to what session we should send this event, raise an error.\n            raise WorkflowRuntimeError(msg)\n\n        # Emit a warning as this won't work for multiple run()s.\n        warnings.warn(msg)\n        ctx = next(iter(self._contexts))\n        ctx.send_event(message=message, step=step)\n\n    def _get_start_event_instance(\n        self, start_event: Optional[StartEvent], **kwargs: Any\n    ) -> StartEvent:\n        if start_event is not None:\n            # start_event was used wrong\n            if not isinstance(start_event, StartEvent):\n                msg = \"The 'start_event' argument must be an instance of 'StartEvent'.\"\n                raise ValueError(msg)\n\n            # start_event is ok but point out that additional kwargs will be ignored in this case\n            if kwargs:\n                msg = (\n                    \"Keyword arguments are not supported when 'run()' is invoked with the 'start_event' parameter.\"\n                    f\" These keyword arguments will be ignored: {kwargs}\"\n                )\n                logger.warning(msg)\n            return start_event\n\n        # Old style start event creation, with kwargs used to create an instance of self._start_event_class\n        try:\n            return self._start_event_class(**kwargs)\n        except ValidationError as e:\n            ev_name = self._start_event_class.__name__\n            msg = f\"Failed creating a start event of type '{ev_name}' with the keyword arguments: {kwargs}\"\n            logger.debug(e)\n            raise WorkflowRuntimeError(msg)\n\n    @dispatcher.span\n    def run(\n        self,\n        ctx: Optional[Context] = None,\n        stepwise: bool = False,\n        checkpoint_callback: Optional[CheckpointCallback] = None,\n        start_event: Optional[StartEvent] = None,\n        **kwargs: Any,\n    ) -> WorkflowHandler:\n        \"\"\"Runs the workflow until completion.\"\"\"\n        # Validate the workflow and determine HITL usage\n        uses_hitl = self._validate()\n        if uses_hitl and stepwise:\n            raise WorkflowRuntimeError(\n                \"Human-in-the-loop is not supported with stepwise execution\"\n            )\n\n        # Start the machinery in a new Context or use the provided one\n        ctx, run_id = self._start(\n            ctx=ctx, stepwise=stepwise, checkpoint_callback=checkpoint_callback\n        )\n\n        result = WorkflowHandler(ctx=ctx, run_id=run_id)\n\n        async def _run_workflow() -> None:\n            if self._sem:\n                await self._sem.acquire()\n            try:\n                if not ctx.is_running:\n                    # Send the first event\n                    start_event_instance = self._get_start_event_instance(\n                        start_event, **kwargs\n                    )\n                    ctx.send_event(start_event_instance)\n\n                    # the context is now running\n                    ctx.is_running = True\n\n                done, unfinished = await asyncio.wait(\n                    ctx._tasks,\n                    timeout=self._timeout,\n                    return_when=asyncio.FIRST_EXCEPTION,\n                )\n\n                we_done = False\n                exception_raised = None\n                for task in done:\n                    e = task.exception()\n                    if type(e) is WorkflowDone:\n                        we_done = True\n                    elif e is not None:\n                        exception_raised = e\n                        break\n\n                # Cancel any pending tasks\n                for t in unfinished:\n                    t.cancel()\n\n                # wait for cancelled tasks to cleanup\n                # prevents any tasks from being stuck\n                try:\n                    await asyncio.wait_for(\n                        asyncio.gather(*unfinished, return_exceptions=True),\n                        timeout=0.5,\n                    )\n                except asyncio.TimeoutError:\n                    logger.warning(\"Some tasks did not clean up within timeout\")\n\n                # the context is no longer running\n                ctx.is_running = False\n\n                if exception_raised:\n                    # cancel the stream\n                    ctx.write_event_to_stream(StopEvent())\n\n                    raise exception_raised\n\n                if not we_done:\n                    # cancel the stream\n                    ctx.write_event_to_stream(StopEvent())\n\n                    msg = f\"Operation timed out after {self._timeout} seconds\"\n                    raise WorkflowTimeoutError(msg)\n\n                result.set_result(ctx._retval)\n            except Exception as e:\n                result.set_exception(e)\n            finally:\n                if self._sem:\n                    self._sem.release()\n\n        asyncio.create_task(_run_workflow())\n        return result\n\n    @dispatcher.span\n    def run_from(\n        self,\n        checkpoint: Checkpoint,\n        ctx_serializer: Optional[BaseSerializer] = None,\n        checkpoint_callback: Optional[CheckpointCallback] = None,\n        **kwargs: Any,\n    ) -> WorkflowHandler:\n        \"\"\"Run from a specified Checkpoint.\n\n        The `Context` snapshot contained in the checkpoint is loaded and used\n        to execute the `Workflow`.\n        \"\"\"\n        # load the `Context` from the checkpoint\n        ctx_serializer = ctx_serializer or JsonSerializer()\n        ctx = Context.from_dict(self, checkpoint.ctx_state, serializer=ctx_serializer)\n        handler: WorkflowHandler = self.run(\n            ctx=ctx, checkpoint_callback=checkpoint_callback, **kwargs\n        )\n\n        # only kick off the workflow if there are no in-progress events\n        # in-progress events are already started in self.run()\n        num_in_progress = sum(len(v) for v in ctx._in_progress.values())\n        if num_in_progress == 0 and handler.ctx is not None:\n            handler.ctx.send_event(checkpoint.output_event)\n\n        return handler\n\n    def is_done(self) -> bool:\n        \"\"\"Checks if the workflow is done.\"\"\"\n        return self._stepwise_context is None\n\n    @step\n    async def _done(self, ctx: Context, ev: StopEvent) -> None:\n        \"\"\"Tears down the whole workflow and stop execution.\"\"\"\n        if self._stop_event_class is StopEvent:\n            ctx._retval = ev.result\n        else:\n            ctx._retval = ev\n\n        ctx.write_event_to_stream(ev)\n\n        # Signal we want to stop the workflow\n        raise WorkflowDone"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/workflow.py",
    "filename": "workflow.py",
    "relpath": "workflow/workflow.py",
    "start_line": 606,
    "end_line": 690,
    "length": 85,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_validate"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "__init__",
      "_ensure_start_event_class",
      "_ensure_stop_event_class",
      "stream_events",
      "add_step",
      "add_workflows",
      "_get_steps",
      "_start",
      "_task",
      "_cancel_workflow_task",
      "send_event",
      "_get_start_event_instance",
      "run",
      "_run_workflow",
      "run_from",
      "is_done",
      "_done",
      "_validate"
    ],
    "document_class_names": [
      "WorkflowMeta",
      "Workflow",
      "itself",
      "to",
      "not",
      "try",
      "is"
    ],
    "content": "def _validate(self) -> bool:\n        \"\"\"Validate the workflow to ensure it's well-formed.\n\n        Returns True if the workflow uses human-in-the-loop, False otherwise.\n        \"\"\"\n        if self._disable_validation:\n            return False\n\n        produced_events: Set[type] = {self._start_event_class}\n        consumed_events: Set[type] = set()\n        requested_services: Set[ServiceDefinition] = set()\n\n        for step_func in self._get_steps().values():\n            step_config: Optional[StepConfig] = getattr(step_func, \"__step_config\")\n            # At this point we know step config is not None, let's make the checker happy\n            assert step_config is not None\n\n            for event_type in step_config.accepted_events:\n                consumed_events.add(event_type)\n\n            for event_type in step_config.return_types:\n                if event_type is type(None):\n                    # some events may not trigger other events\n                    continue\n\n                produced_events.add(event_type)\n\n            requested_services.update(step_config.requested_services)\n\n        # Check if no StopEvent is produced\n        stop_ok = False\n        for ev in produced_events:\n            if issubclass(ev, StopEvent):\n                stop_ok = True\n                break\n        if not stop_ok:\n            msg = f\"No event of type StopEvent is produced.\"\n            raise WorkflowValidationError(msg)\n\n        # Check if all consumed events are produced (except specific built-in events)\n        unconsumed_events = consumed_events - produced_events\n        unconsumed_events = {\n            x\n            for x in unconsumed_events\n            if not issubclass(x, (InputRequiredEvent, HumanResponseEvent, StopEvent))\n        }\n        if unconsumed_events:\n            names = \", \".join(ev.__name__ for ev in unconsumed_events)\n            raise WorkflowValidationError(\n                f\"The following events are consumed but never produced: {names}\"\n            )\n\n        # Check if there are any unused produced events (except specific built-in events)\n        unused_events = produced_events - consumed_events\n        unused_events = {\n            x\n            for x in unused_events\n            if not issubclass(\n                x, (InputRequiredEvent, HumanResponseEvent, self._stop_event_class)\n            )\n        }\n        if unused_events:\n            names = \", \".join(ev.__name__ for ev in unused_events)\n            raise WorkflowValidationError(\n                f\"The following events are produced but never consumed: {names}\"\n            )\n\n        # Check all the requested services are available\n        required_service_names = {\n            sd.name for sd in requested_services if sd.default_value is None\n        }\n        if required_service_names:\n            avail_service_names = set(self._service_manager._services.keys())\n            missing = required_service_names - avail_service_names\n            if missing:\n                msg = f\"The following services are not available: {', '.join(str(m) for m in missing)}\"\n                raise WorkflowValidationError(msg)\n\n        # Check if the workflow uses human-in-the-loop\n        return (\n            InputRequiredEvent in produced_events\n            or HumanResponseEvent in consumed_events\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/context.py",
    "filename": "context.py",
    "relpath": "workflow/context.py",
    "start_line": 1,
    "end_line": 369,
    "length": 369,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_serialize_queue",
      "_deserialize_queue",
      "_serialize_globals",
      "_deserialize_globals",
      "to_dict",
      "from_dict",
      "set",
      "mark_in_progress",
      "remove_from_in_progress",
      "add_running_step",
      "remove_running_step",
      "running_steps",
      "get",
      "data",
      "lock",
      "session",
      "_get_full_path",
      "collect_events",
      "send_event",
      "wait_for_event",
      "write_event_to_stream",
      "get_result",
      "streaming_queue"
    ],
    "chunk_class_names": [
      "Context"
    ],
    "document_function_names": [
      "__init__",
      "_serialize_queue",
      "_deserialize_queue",
      "_serialize_globals",
      "_deserialize_globals",
      "to_dict",
      "from_dict",
      "set",
      "mark_in_progress",
      "remove_from_in_progress",
      "add_running_step",
      "remove_running_step",
      "running_steps",
      "get",
      "data",
      "lock",
      "session",
      "_get_full_path",
      "collect_events",
      "send_event",
      "wait_for_event",
      "write_event_to_stream",
      "get_result",
      "streaming_queue"
    ],
    "document_class_names": [
      "Context"
    ],
    "content": "import asyncio\nimport json\nimport uuid\nimport warnings\nfrom collections import defaultdict\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    DefaultDict,\n    Dict,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n)\n\nfrom .context_serializers import BaseSerializer, JsonSerializer\nfrom .decorators import StepConfig\nfrom .errors import WorkflowRuntimeError\nfrom .events import Event\nfrom .types import RunResultT\n\nif TYPE_CHECKING:  # pragma: no cover\n    from .workflow import Workflow\n\nT = TypeVar(\"T\", bound=Event)\n\n\nclass Context:\n    \"\"\"A global object representing a context for a given workflow run.\n\n    The Context object can be used to store data that needs to be available across iterations during a workflow\n    execution, and across multiple workflow runs.\n    Every context instance offers two type of data storage: a global one, that's shared among all the steps within a\n    workflow, and private one, that's only accessible from a single step.\n\n    Both `set` and `get` operations on global data are governed by a lock, and considered coroutine-safe.\n    \"\"\"\n\n    def __init__(\n        self,\n        workflow: \"Workflow\",\n        stepwise: bool = False,\n    ) -> None:\n        self.stepwise = stepwise\n        self.is_running = False\n\n        self._workflow = workflow\n        # Broker machinery\n        self._waiter_id = str(uuid.uuid4())\n        self._queues: Dict[str, asyncio.Queue] = {self._waiter_id: asyncio.Queue()}\n        self._tasks: Set[asyncio.Task] = set()\n        self._broker_log: List[Event] = []\n        self._cancel_flag: asyncio.Event = asyncio.Event()\n        self._step_flags: Dict[str, asyncio.Event] = {}\n        self._step_event_holding: Optional[Event] = None\n        self._step_lock: asyncio.Lock = asyncio.Lock()\n        self._step_condition: asyncio.Condition = asyncio.Condition(\n            lock=self._step_lock\n        )\n        self._step_event_written: asyncio.Condition = asyncio.Condition(\n            lock=self._step_lock\n        )\n        self._accepted_events: List[Tuple[str, str]] = []\n        self._retval: RunResultT = None\n\n        # Map the step names that were executed to a list of events they received.\n        # This will be serialized, and is needed to resume a Workflow run passing\n        # an existing context.\n        self._in_progress: Dict[str, List[Event]] = defaultdict(list)\n        # Keep track of the steps currently running. This is only valid when a\n        # workflow is running and won't be serialized. Note that a single step\n        # might have multiple workers, so we keep a counter.\n        self._currently_running_steps: DefaultDict[str, int] = defaultdict(int)\n        # Streaming machinery\n        self._streaming_queue: asyncio.Queue = asyncio.Queue()\n        # Global data storage\n        self._lock = asyncio.Lock()\n        self._globals: Dict[str, Any] = {}\n        # Step-specific instance\n        self._events_buffer: Dict[str, List[Event]] = defaultdict(list)\n\n    def _serialize_queue(self, queue: asyncio.Queue, serializer: BaseSerializer) -> str:\n        queue_items = list(queue._queue)  # type: ignore\n        queue_objs = [serializer.serialize(obj) for obj in queue_items]\n        return json.dumps(queue_objs)  # type: ignore\n\n    def _deserialize_queue(\n        self,\n        queue_str: str,\n        serializer: BaseSerializer,\n        prefix_queue_objs: List[Any] = [],\n    ) -> asyncio.Queue:\n        queue_objs = json.loads(queue_str)\n        queue_objs = prefix_queue_objs + queue_objs\n        queue = asyncio.Queue()  # type: ignore\n        for obj in queue_objs:\n            event_obj = serializer.deserialize(obj)\n            queue.put_nowait(event_obj)\n        return queue\n\n    def _serialize_globals(self, serializer: BaseSerializer) -> Dict[str, Any]:\n        serialized_globals = {}\n        for key, value in self._globals.items():\n            try:\n                serialized_globals[key] = serializer.serialize(value)\n            except Exception as e:\n                raise ValueError(f\"Failed to serialize value for key {key}: {e}\")\n        return serialized_globals\n\n    def _deserialize_globals(\n        self, serialized_globals: Dict[str, Any], serializer: BaseSerializer\n    ) -> Dict[str, Any]:\n        deserialized_globals = {}\n        for key, value in serialized_globals.items():\n            try:\n                deserialized_globals[key] = serializer.deserialize(value)\n            except Exception as e:\n                raise ValueError(f\"Failed to deserialize value for key {key}: {e}\")\n        return deserialized_globals\n\n    def to_dict(self, serializer: Optional[BaseSerializer] = None) -> Dict[str, Any]:\n        serializer = serializer or JsonSerializer()\n\n        return {\n            \"globals\": self._serialize_globals(serializer),\n            \"streaming_queue\": self._serialize_queue(self._streaming_queue, serializer),\n            \"queues\": {\n                k: self._serialize_queue(v, serializer) for k, v in self._queues.items()\n            },\n            \"stepwise\": self.stepwise,\n            \"events_buffer\": {\n                k: [serializer.serialize(ev) for ev in v]\n                for k, v in self._events_buffer.items()\n            },\n            \"in_progress\": {\n                k: [serializer.serialize(ev) for ev in v]\n                for k, v in self._in_progress.items()\n            },\n            \"accepted_events\": self._accepted_events,\n            \"broker_log\": [serializer.serialize(ev) for ev in self._broker_log],\n            \"waiter_id\": self._waiter_id,\n            \"is_running\": self.is_running,\n        }\n\n    @classmethod\n    def from_dict(\n        cls,\n        workflow: \"Workflow\",\n        data: Dict[str, Any],\n        serializer: Optional[BaseSerializer] = None,\n    ) -> \"Context\":\n        serializer = serializer or JsonSerializer()\n\n        context = cls(workflow, stepwise=data[\"stepwise\"])\n        context._globals = context._deserialize_globals(data[\"globals\"], serializer)\n        context._streaming_queue = context._deserialize_queue(\n            data[\"streaming_queue\"], serializer\n        )\n        context._events_buffer = {\n            k: [serializer.deserialize(ev) for ev in v]\n            for k, v in data[\"events_buffer\"].items()\n        }\n        if len(context._events_buffer) == 0:\n            context._events_buffer = defaultdict(list)\n        context._accepted_events = data[\"accepted_events\"]\n        context._waiter_id = data.get(\"waiter_id\", str(uuid.uuid4()))\n        context._broker_log = [serializer.deserialize(ev) for ev in data[\"broker_log\"]]\n        context.is_running = data[\"is_running\"]\n        # load back up whatever was in the queue as well as the events whose steps\n        # were in progress when the serialization of the Context took place\n        context._queues = {\n            k: context._deserialize_queue(\n                v, serializer, prefix_queue_objs=data[\"in_progress\"].get(k, [])\n            )\n            for k, v in data[\"queues\"].items()\n        }\n        context._in_progress = defaultdict(list)\n        return context\n\n    async def set(self, key: str, value: Any, make_private: bool = False) -> None:\n        \"\"\"Store `value` into the Context under `key`.\n\n        Args:\n            key: A unique string to identify the value stored.\n            value: The data to be stored.\n\n        Raises:\n            ValueError: When make_private is True but a key already exists in the global storage.\n        \"\"\"\n        if make_private:\n            warnings.warn(\n                \"`make_private` is deprecated and will be ignored\", DeprecationWarning\n            )\n\n        async with self.lock:\n            self._globals[key] = value\n\n    async def mark_in_progress(self, name: str, ev: Event) -> None:\n        \"\"\"Add input event to in_progress dict.\n\n        Args:\n            name (str): The name of the step that is in progress.\n            ev (Event): The input event that kicked off this step.\n        \"\"\"\n        async with self.lock:\n            self._in_progress[name].append(ev)\n\n    async def remove_from_in_progress(self, name: str, ev: Event) -> None:\n        \"\"\"Remove input event from active steps.\n\n        Args:\n            name (str): The name of the step that has been completed.\n            ev (Event): The associated input event that kicked of this completed step.\n        \"\"\"\n        async with self.lock:\n            events = [e for e in self._in_progress[name] if e != ev]\n            self._in_progress[name] = events\n\n    async def add_running_step(self, name: str) -> None:\n        async with self.lock:\n            self._currently_running_steps[name] += 1\n\n    async def remove_running_step(self, name: str) -> None:\n        async with self.lock:\n            self._currently_running_steps[name] -= 1\n            if self._currently_running_steps[name] == 0:\n                del self._currently_running_steps[name]\n\n    async def running_steps(self) -> List[str]:\n        async with self.lock:\n            return list(self._currently_running_steps)\n\n    async def get(self, key: str, default: Optional[Any] = Ellipsis) -> Any:\n        \"\"\"Get the value corresponding to `key` from the Context.\n\n        Args:\n            key: A unique string to identify the value stored.\n            default: The value to return when `key` is missing instead of raising an exception.\n\n        Raises:\n            ValueError: When there's not value accessible corresponding to `key`.\n        \"\"\"\n        async with self.lock:\n            if key in self._globals:\n                return self._globals[key]\n            elif default is not Ellipsis:\n                return default\n\n        msg = f\"Key '{key}' not found in Context\"\n        raise ValueError(msg)\n\n    @property\n    def data(self) -> Dict[str, Any]:\n        \"\"\"This property is provided for backward compatibility.\n\n        Use `get` and `set` instead.\n        \"\"\"\n        msg = \"`data` is deprecated, please use the `get` and `set` method to store data into the Context.\"\n        warnings.warn(msg, DeprecationWarning, stacklevel=2)\n        return self._globals\n\n    @property\n    def lock(self) -> asyncio.Lock:\n        \"\"\"Returns a mutex to lock the Context.\"\"\"\n        return self._lock\n\n    @property\n    def session(self) -> \"Context\":\n        \"\"\"This property is provided for backward compatibility.\"\"\"\n        msg = \"`session` is deprecated, please use the Context instance directly.\"\n        warnings.warn(msg, DeprecationWarning, stacklevel=2)\n        return self\n\n    def _get_full_path(self, ev_type: Type[Event]) -> str:\n        return f\"{ev_type.__module__}.{ev_type.__name__}\"\n\n    def collect_events(\n        self, ev: Event, expected: List[Type[Event]]\n    ) -> Optional[List[Event]]:\n        self._events_buffer[self._get_full_path(type(ev))].append(ev)\n\n        retval: List[Event] = []\n        for e_type in expected:\n            e_instance_list = self._events_buffer.get(self._get_full_path(e_type))\n            if e_instance_list:\n                retval.append(e_instance_list.pop(0))\n\n        if len(retval) == len(expected):\n            return retval\n\n        # put back the events if unable to collect all\n        for ev in retval:\n            self._events_buffer[self._get_full_path(type(ev))].append(ev)\n\n        return None\n\n    def send_event(self, message: Event, step: Optional[str] = None) -> None:\n        \"\"\"Sends an event to a specific step in the workflow.\n\n        If step is None, the event is sent to all the receivers and we let\n        them discard events they don't want.\n        \"\"\"\n        if step is None:\n            for queue in self._queues.values():\n                queue.put_nowait(message)\n        else:\n            if step not in self._workflow._get_steps():\n                raise WorkflowRuntimeError(f\"Step {step} does not exist\")\n\n            step_func = self._workflow._get_steps()[step]\n            step_config: Optional[StepConfig] = getattr(\n                step_func, \"__step_config\", None\n            )\n\n            if step_config and type(message) in step_config.accepted_events:\n                self._queues[step].put_nowait(message)\n            else:\n                raise WorkflowRuntimeError(\n                    f\"Step {step} does not accept event of type {type(message)}\"\n                )\n\n        self._broker_log.append(message)\n\n    async def wait_for_event(\n        self,\n        event_type: Type[T],\n        requirements: Optional[Dict[str, Any]] = None,\n        timeout: Optional[float] = 2000,\n    ) -> T:\n        \"\"\"Asynchronously wait for a specific event type to be received.\n\n        Args:\n            event_type: The type of event to wait for\n            requirements: Optional dict of requirements the event must match\n            timeout: Optional timeout in seconds. Defaults to 2000s.\n\n        Returns:\n            The event type that was requested.\n\n        Raises:\n            asyncio.TimeoutError: If the timeout is reached before receiving matching event\n        \"\"\"\n        requirements = requirements or {}\n\n        while True:\n            event = await asyncio.wait_for(\n                self._queues[self._waiter_id].get(), timeout=timeout\n            )\n            if type(event) is event_type:\n                if all(\n                    event.get(k, default=None) == v for k, v in requirements.items()\n                ):\n                    return event\n                else:\n                    continue\n\n    def write_event_to_stream(self, ev: Optional[Event]) -> None:\n        self._streaming_queue.put_nowait(ev)\n\n    def get_result(self) -> RunResultT:\n        \"\"\"Returns the result of the workflow.\"\"\"\n        return self._retval\n\n    @property\n    def streaming_queue(self) -> asyncio.Queue:\n        return self._streaming_queue"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/workflow/service.py",
    "filename": "service.py",
    "relpath": "workflow/service.py",
    "start_line": 1,
    "end_line": 33,
    "length": 33,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "get",
      "add"
    ],
    "chunk_class_names": [
      "ServiceNotFoundError",
      "ServiceManager",
      "to"
    ],
    "document_function_names": [
      "__init__",
      "get",
      "add"
    ],
    "document_class_names": [
      "ServiceNotFoundError",
      "ServiceManager",
      "to"
    ],
    "content": "from typing import Dict, TYPE_CHECKING, Optional\n\n\nif TYPE_CHECKING:  # pragma: no cover\n    from .workflow import Workflow\n\n\nclass ServiceNotFoundError(Exception):\n    \"\"\"An error raised when the service manager couldn't find a certain service name.\"\"\"\n\n\nclass ServiceManager:\n    \"\"\"An helper class to decouple how services are managed from the Workflow class.\n\n    A Service is nothing more than a workflow instance attached to another workflow.\n    The service is made available to the steps of the main workflow.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._services: Dict[str, \"Workflow\"] = {}\n\n    def get(self, name: str, default: Optional[\"Workflow\"] = None) -> \"Workflow\":\n        try:\n            return self._services[name]\n        except KeyError as e:\n            if default:\n                return default\n\n            msg = f\"Service {name} not found\"\n            raise ServiceNotFoundError(msg)\n\n    def add(self, name: str, service: \"Workflow\") -> None:\n        self._services[name] = service"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/structured_llm.py",
    "filename": "structured_llm.py",
    "relpath": "llms/structured_llm.py",
    "start_line": 1,
    "end_line": 238,
    "length": 238,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "metadata",
      "chat",
      "stream_chat",
      "complete",
      "stream_complete",
      "achat",
      "astream_chat",
      "gen",
      "acomplete",
      "astream_complete",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "StructuredLLM",
      "for",
      "StructuredLLMComponent"
    ],
    "document_function_names": [
      "class_name",
      "metadata",
      "chat",
      "stream_chat",
      "complete",
      "stream_complete",
      "achat",
      "astream_chat",
      "gen",
      "acomplete",
      "astream_complete",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "StructuredLLM",
      "for",
      "StructuredLLMComponent"
    ],
    "content": "from typing import Any, Type, Sequence, Dict\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.llms.llm import (\n    LLM,\n    BaseLLMComponent,\n    LLMChatComponent,\n    LLMCompleteComponent,\n)\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseGen,\n    LLMMetadata,\n    MessageRole,\n)\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    SerializeAsAny,\n    ConfigDict,\n)\nfrom llama_index.core.base.llms.types import LLMMetadata\nfrom llama_index.core.llms.callbacks import (\n    llm_chat_callback,\n    llm_completion_callback,\n)\nfrom llama_index.core.prompts.base import ChatPromptTemplate\nfrom llama_index.core.base.llms.generic_utils import (\n    achat_to_completion_decorator,\n    chat_to_completion_decorator,\n)\nfrom llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n)\n\n\nclass StructuredLLM(LLM):\n    \"\"\"\n    A structured LLM takes in an inner LLM along with a designated output class,\n    and all methods will return outputs in that structure.\n\n    \"\"\"\n\n    llm: SerializeAsAny[LLM]\n    output_cls: Type[BaseModel] = Field(\n        ..., description=\"Output class for the structured LLM.\", exclude=True\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"structured_llm\"\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        return self.llm.metadata\n\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat endpoint for LLM.\"\"\"\n        # TODO:\n\n        # NOTE: we are wrapping existing messages in a ChatPromptTemplate to\n        # make this work with our FunctionCallingProgram, even though\n        # the messages don't technically have any variables (they are already formatted)\n\n        chat_prompt = ChatPromptTemplate(message_templates=messages)\n\n        output = self.llm.structured_predict(\n            output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n        )\n        return ChatResponse(\n            message=ChatMessage(\n                role=MessageRole.ASSISTANT, content=output.model_dump_json()\n            ),\n            raw=output,\n        )\n\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        chat_prompt = ChatPromptTemplate(message_templates=messages)\n\n        stream_output = self.llm.stream_structured_predict(\n            output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n        )\n        for partial_output in stream_output:\n            yield ChatResponse(\n                message=ChatMessage(\n                    role=MessageRole.ASSISTANT, content=partial_output.json()\n                ),\n                raw=partial_output,\n            )\n\n    @llm_completion_callback()\n    def complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n        complete_fn = chat_to_completion_decorator(self.chat)\n        return complete_fn(prompt, **kwargs)\n\n    @llm_completion_callback()\n    def stream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseGen:\n        \"\"\"Stream completion endpoint for LLM.\"\"\"\n        raise NotImplementedError(\"stream_complete is not supported by default.\")\n\n    # ===== Async Endpoints =====\n    @llm_chat_callback()\n    async def achat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponse:\n        # NOTE: we are wrapping existing messages in a ChatPromptTemplate to\n        # make this work with our FunctionCallingProgram, even though\n        # the messages don't technically have any variables (they are already formatted)\n\n        chat_prompt = ChatPromptTemplate(message_templates=messages)\n\n        output = await self.llm.astructured_predict(\n            output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n        )\n        return ChatResponse(\n            message=ChatMessage(\n                role=MessageRole.ASSISTANT, content=output.model_dump_json()\n            ),\n            raw=output,\n        )\n\n    @llm_chat_callback()\n    async def astream_chat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponseAsyncGen:\n        \"\"\"Async stream chat endpoint for LLM.\"\"\"\n\n        async def gen() -> ChatResponseAsyncGen:\n            chat_prompt = ChatPromptTemplate(message_templates=messages)\n\n            stream_output = await self.llm.astream_structured_predict(\n                output_cls=self.output_cls, prompt=chat_prompt, llm_kwargs=kwargs\n            )\n            async for partial_output in stream_output:\n                yield ChatResponse(\n                    message=ChatMessage(\n                        role=MessageRole.ASSISTANT, content=partial_output.json()\n                    ),\n                    raw=partial_output,\n                )\n\n        return gen()\n\n    @llm_completion_callback()\n    async def acomplete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n        complete_fn = achat_to_completion_decorator(self.achat)\n        return await complete_fn(prompt, **kwargs)\n\n    @llm_completion_callback()\n    async def astream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseGen:\n        \"\"\"Async stream completion endpoint for LLM.\"\"\"\n        raise NotImplementedError(\"astream_complete is not supported by default.\")\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"Return query component.\"\"\"\n        base_component: BaseLLMComponent\n        if self.metadata.is_chat_model:\n            base_component = LLMChatComponent(llm=self, **kwargs)\n        else:\n            base_component = LLMCompleteComponent(llm=self, **kwargs)\n\n        return StructuredLLMComponent(llm_component=base_component)\n\n\nclass StructuredLLMComponent(QueryComponent):\n    \"\"\"Structured LLM component.\n\n    Wraps an existing LLM component, directly returns structured output.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    llm_component: SerializeAsAny[BaseLLMComponent]\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.llm_component.set_callback_manager(callback_manager)\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        return self.llm_component.validate_component_inputs(input)\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = self.llm_component.run_component(**kwargs)[\"output\"]\n        # NOTE: can either be a CompletionResponse or ChatResponse\n        # other types are not supported at the moment\n        if isinstance(output, CompletionResponse):\n            return {\"output\": output.raw}\n        elif isinstance(output, ChatResponse):\n            return {\"output\": output.raw}\n        else:\n            raise ValueError(\"Unsupported output type from LLM component.\")\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = (await self.llm_component.arun_component(**kwargs))[\"output\"]\n        # NOTE: can either be a CompletionResponse or ChatResponse\n        # other types are not supported at the moment\n        if isinstance(output, CompletionResponse):\n            return {\"output\": output.raw}\n        elif isinstance(output, ChatResponse):\n            return {\"output\": output.raw}\n        else:\n            raise ValueError(\"Unsupported output type from LLM component.\")\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return self.llm_component.input_keys\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/chatml_utils.py",
    "filename": "chatml_utils.py",
    "relpath": "llms/chatml_utils.py",
    "start_line": 1,
    "end_line": 64,
    "length": 64,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "messages_to_prompt",
      "completion_to_prompt"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "messages_to_prompt",
      "completion_to_prompt"
    ],
    "document_class_names": [],
    "content": "from typing import List, Optional, Sequence\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\n\n# Create a prompt that matches ChatML instructions\n\n# <|im_start|>system\n# You are Dolphin, a helpful AI assistant.<|im_end|>\n# <|im_start|>user\n# {prompt}<|im_end|>\n# <|im_start|>assistant\n\nB_SYS = \"<|im_start|>system\\n\"\nB_USER = \"<|im_start|>user\\n\"\nB_ASSISTANT = \"<|im_start|>assistant\\n\"\nEND = \"<|im_end|>\\n\"\nDEFAULT_SYSTEM_PROMPT = \"\"\"\\\nYou are a helpful, respectful and honest assistant. \\\nAlways answer as helpfully as possible and follow ALL given instructions. \\\nDo not speculate or make up information. \\\nDo not reference any given instructions or context. \\\n\"\"\"\n\n\ndef messages_to_prompt(\n    messages: Sequence[ChatMessage], system_prompt: Optional[str] = None\n) -> str:\n    if len(messages) == 0:\n        raise ValueError(\n            \"At least one message is required to construct the ChatML prompt\"\n        )\n\n    string_messages: List[str] = []\n    if messages[0].role == MessageRole.SYSTEM:\n        # pull out the system message (if it exists in messages)\n        system_message_str = messages[0].content or \"\"\n        messages = messages[1:]\n    else:\n        system_message_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n\n    string_messages.append(f\"{B_SYS}{system_message_str.strip()} {END}\")\n\n    for message in messages:\n        role = message.role\n        content = message.content\n\n        if role == MessageRole.USER:\n            string_messages.append(f\"{B_USER}{content} {END}\")\n        elif role == MessageRole.ASSISTANT:\n            string_messages.append(f\"{B_ASSISTANT}{content} {END}\")\n\n    string_messages.append(f\"{B_ASSISTANT}\")\n\n    return \"\".join(string_messages)\n\n\ndef completion_to_prompt(completion: str, system_prompt: Optional[str] = None) -> str:\n    system_prompt_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n\n    return (\n        f\"{B_SYS}{system_prompt_str.strip()} {END}\"\n        f\"{B_USER}{completion.strip()} {END}\"\n        f\"{B_ASSISTANT}\"\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/loading.py",
    "filename": "loading.py",
    "relpath": "llms/loading.py",
    "start_line": 1,
    "end_line": 46,
    "length": 46,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_llm"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "load_llm"
    ],
    "document_class_names": [],
    "content": "from typing import Dict, Type\n\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.llms.mock import MockLLM\n\nRECOGNIZED_LLMS: Dict[str, Type[LLM]] = {\n    MockLLM.class_name(): MockLLM,\n}\n\n# Conditionals for llama-cloud support\ntry:\n    from llama_index.llms.openai import OpenAI  # pants: no-infer-dep\n\n    RECOGNIZED_LLMS[OpenAI.class_name()] = OpenAI  # pants: no-infer-dep\nexcept ImportError:\n    pass\n\ntry:\n    from llama_index.llms.azure_openai import AzureOpenAI  # pants: no-infer-dep\n\n    RECOGNIZED_LLMS[AzureOpenAI.class_name()] = AzureOpenAI  # pants: no-infer-dep\nexcept ImportError:\n    pass\n\ntry:\n    from llama_index.llms.huggingface_api import (\n        HuggingFaceInferenceAPI,\n    )  # pants: no-infer-dep\n\n    RECOGNIZED_LLMS[HuggingFaceInferenceAPI.class_name()] = HuggingFaceInferenceAPI\nexcept ImportError:\n    pass\n\n\ndef load_llm(data: dict) -> LLM:\n    \"\"\"Load LLM by name.\"\"\"\n    if isinstance(data, LLM):\n        return data\n    llm_name = data.get(\"class_name\", None)\n    if llm_name is None:\n        raise ValueError(\"LLM loading requires a class_name\")\n\n    if llm_name not in RECOGNIZED_LLMS:\n        raise ValueError(f\"Invalid LLM name: {llm_name}\")\n\n    return RECOGNIZED_LLMS[llm_name].from_dict(data)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/function_calling.py",
    "filename": "function_calling.py",
    "relpath": "llms/function_calling.py",
    "start_line": 1,
    "end_line": 288,
    "length": 288,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "chat_with_tools",
      "achat_with_tools",
      "stream_chat_with_tools",
      "astream_chat_with_tools",
      "_prepare_chat_with_tools",
      "_validate_chat_with_tools_response",
      "get_tool_calls_from_response",
      "predict_and_call",
      "apredict_and_call"
    ],
    "chunk_class_names": [
      "FunctionCallingLLM",
      "hierarchy"
    ],
    "document_function_names": [
      "__init__",
      "chat_with_tools",
      "achat_with_tools",
      "stream_chat_with_tools",
      "astream_chat_with_tools",
      "_prepare_chat_with_tools",
      "_validate_chat_with_tools_response",
      "get_tool_calls_from_response",
      "predict_and_call",
      "apredict_and_call"
    ],
    "document_class_names": [
      "FunctionCallingLLM",
      "hierarchy"
    ],
    "content": "import asyncio\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Union\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n)\nfrom llama_index.core.llms.llm import LLM, ToolSelection\n\nif TYPE_CHECKING:\n    from llama_index.core.chat_engine.types import AgentChatResponse\n    from llama_index.core.tools.types import BaseTool\n\n\nclass FunctionCallingLLM(LLM):\n    \"\"\"\n    Function calling LLMs are LLMs that support function calling.\n    They support an expanded range of capabilities.\n\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        # Help static checkers understand this class hierarchy\n        super().__init__(*args, **kwargs)\n\n    def chat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Chat with function calling.\"\"\"\n        chat_kwargs = self._prepare_chat_with_tools(\n            tools,\n            user_msg=user_msg,\n            chat_history=chat_history,\n            verbose=verbose,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n        response = self.chat(**chat_kwargs)\n        return self._validate_chat_with_tools_response(\n            response,\n            tools,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n\n    async def achat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Async chat with function calling.\"\"\"\n        chat_kwargs = self._prepare_chat_with_tools(\n            tools,\n            user_msg=user_msg,\n            chat_history=chat_history,\n            verbose=verbose,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n        response = await self.achat(**chat_kwargs)\n        return self._validate_chat_with_tools_response(\n            response,\n            tools,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n\n    def stream_chat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> ChatResponseGen:\n        \"\"\"Stream chat with function calling.\"\"\"\n        chat_kwargs = self._prepare_chat_with_tools(\n            tools,\n            user_msg=user_msg,\n            chat_history=chat_history,\n            verbose=verbose,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n        # TODO: no validation for streaming outputs\n        return self.stream_chat(**chat_kwargs)\n\n    async def astream_chat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> ChatResponseAsyncGen:\n        \"\"\"Async stream chat with function calling.\"\"\"\n        chat_kwargs = self._prepare_chat_with_tools(\n            tools,\n            user_msg=user_msg,\n            chat_history=chat_history,\n            verbose=verbose,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n        # TODO: no validation for streaming outputs\n        return await self.astream_chat(**chat_kwargs)\n\n    @abstractmethod\n    def _prepare_chat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> Dict[str, Any]:\n        \"\"\"Prepare the arguments needed to let the LLM chat with tools.\"\"\"\n\n    def _validate_chat_with_tools_response(\n        self,\n        response: ChatResponse,\n        tools: Sequence[\"BaseTool\"],\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Validate the response from chat_with_tools.\"\"\"\n        return response\n\n    def get_tool_calls_from_response(\n        self,\n        response: ChatResponse,\n        error_on_no_tool_call: bool = True,\n        **kwargs: Any,\n    ) -> List[ToolSelection]:\n        \"\"\"Predict and call the tool.\"\"\"\n        raise NotImplementedError(\n            \"get_tool_calls_from_response is not supported by default.\"\n        )\n\n    def predict_and_call(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        error_on_no_tool_call: bool = True,\n        error_on_tool_error: bool = False,\n        **kwargs: Any,\n    ) -> \"AgentChatResponse\":\n        \"\"\"Predict and call the tool.\"\"\"\n        from llama_index.core.chat_engine.types import AgentChatResponse\n        from llama_index.core.tools.calling import (\n            call_tool_with_selection,\n        )\n\n        if not self.metadata.is_function_calling_model:\n            return super().predict_and_call(\n                tools,\n                user_msg=user_msg,\n                chat_history=chat_history,\n                verbose=verbose,\n                **kwargs,\n            )\n\n        response = self.chat_with_tools(\n            tools,\n            user_msg=user_msg,\n            chat_history=chat_history,\n            verbose=verbose,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n        tool_calls = self.get_tool_calls_from_response(\n            response, error_on_no_tool_call=error_on_no_tool_call\n        )\n        tool_outputs = [\n            call_tool_with_selection(tool_call, tools, verbose=verbose)\n            for tool_call in tool_calls\n        ]\n        tool_outputs_with_error = [\n            tool_output for tool_output in tool_outputs if tool_output.is_error\n        ]\n        if error_on_tool_error and len(tool_outputs_with_error) > 0:\n            error_text = \"\\n\\n\".join(\n                [tool_output.content for tool_output in tool_outputs]\n            )\n            raise ValueError(error_text)\n        elif allow_parallel_tool_calls:\n            output_text = \"\\n\\n\".join(\n                [tool_output.content for tool_output in tool_outputs]\n            )\n            return AgentChatResponse(response=output_text, sources=tool_outputs)\n        else:\n            if len(tool_outputs) > 1:\n                raise ValueError(\"Invalid\")\n            elif len(tool_outputs) == 0:\n                return AgentChatResponse(\n                    response=response.message.content or \"\", sources=tool_outputs\n                )\n\n            return AgentChatResponse(\n                response=tool_outputs[0].content, sources=tool_outputs\n            )\n\n    async def apredict_and_call(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        error_on_no_tool_call: bool = True,\n        error_on_tool_error: bool = False,\n        **kwargs: Any,\n    ) -> \"AgentChatResponse\":\n        \"\"\"Predict and call the tool.\"\"\"\n        from llama_index.core.chat_engine.types import AgentChatResponse\n        from llama_index.core.tools.calling import (\n            acall_tool_with_selection,\n        )\n\n        if not self.metadata.is_function_calling_model:\n            return await super().apredict_and_call(\n                tools,\n                user_msg=user_msg,\n                chat_history=chat_history,\n                verbose=verbose,\n                **kwargs,\n            )\n\n        response = await self.achat_with_tools(\n            tools,\n            user_msg=user_msg,\n            chat_history=chat_history,\n            verbose=verbose,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n\n        tool_calls = self.get_tool_calls_from_response(\n            response, error_on_no_tool_call=error_on_no_tool_call\n        )\n        tool_tasks = [\n            acall_tool_with_selection(tool_call, tools, verbose=verbose)\n            for tool_call in tool_calls\n        ]\n        tool_outputs = await asyncio.gather(*tool_tasks)\n        tool_outputs_with_error = [\n            tool_output for tool_output in tool_outputs if tool_output.is_error\n        ]\n        if error_on_tool_error and len(tool_outputs_with_error) > 0:\n            error_text = \"\\n\\n\".join(\n                [tool_output.content for tool_output in tool_outputs]\n            )\n            raise ValueError(error_text)\n        elif allow_parallel_tool_calls:\n            output_text = \"\\n\\n\".join(\n                [tool_output.content for tool_output in tool_outputs]\n            )\n            return AgentChatResponse(response=output_text, sources=tool_outputs)\n        else:\n            if len(tool_outputs) > 1:\n                raise ValueError(\"Invalid\")\n            elif len(tool_outputs) == 0:\n                return AgentChatResponse(\n                    response=response.message.content or \"\", sources=tool_outputs\n                )\n\n            return AgentChatResponse(\n                response=tool_outputs[0].content, sources=tool_outputs\n            )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/callbacks.py",
    "filename": "callbacks.py",
    "relpath": "llms/callbacks.py",
    "start_line": 1,
    "end_line": 283,
    "length": 283,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "llm_chat_callback",
      "wrap",
      "wrapper_logic",
      "wrapped_async_llm_chat",
      "wrapped_gen",
      "wrapped_llm_chat",
      "wrapped_gen",
      "async_dummy_wrapper",
      "dummy_wrapper"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "llm_chat_callback",
      "wrap",
      "wrapper_logic",
      "wrapped_async_llm_chat",
      "wrapped_gen",
      "wrapped_llm_chat",
      "wrapped_gen",
      "async_dummy_wrapper",
      "dummy_wrapper",
      "llm_completion_callback",
      "wrap",
      "wrapper_logic",
      "extract_prompt",
      "wrapped_async_llm_predict",
      "wrapped_gen",
      "wrapped_llm_predict",
      "wrapped_gen",
      "async_dummy_wrapper",
      "dummy_wrapper"
    ],
    "document_class_names": [],
    "content": "import asyncio\nfrom contextlib import contextmanager\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Callable,\n    Generator,\n    Sequence,\n    cast,\n)\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseAsyncGen,\n    CompletionResponseGen,\n)\nfrom llama_index.core.callbacks import CallbackManager, CBEventType, EventPayload\n\n# dispatcher setup\nfrom llama_index.core.instrumentation import get_dispatcher\nfrom llama_index.core.instrumentation.events.exception import ExceptionEvent\nfrom llama_index.core.instrumentation.span import active_span_id\nfrom llama_index.core.instrumentation.events.llm import (\n    LLMCompletionEndEvent,\n    LLMCompletionStartEvent,\n    LLMChatEndEvent,\n    LLMChatStartEvent,\n    LLMChatInProgressEvent,\n    LLMCompletionInProgressEvent,\n)\n\ndispatcher = get_dispatcher(__name__)\n\n\ndef llm_chat_callback() -> Callable:\n    def wrap(f: Callable) -> Callable:\n        @contextmanager\n        def wrapper_logic(_self: Any) -> Generator[CallbackManager, None, None]:\n            callback_manager = getattr(_self, \"callback_manager\", None)\n            if not isinstance(callback_manager, CallbackManager):\n                _self.callback_manager = CallbackManager()\n\n            yield _self.callback_manager  # type: ignore\n\n        async def wrapped_async_llm_chat(\n            _self: Any, messages: Sequence[ChatMessage], **kwargs: Any\n        ) -> Any:\n            with wrapper_logic(_self) as callback_manager, callback_manager.as_trace(\n                \"chat\"\n            ):\n                span_id = active_span_id.get()\n                model_dict = _self.to_dict()\n                model_dict.pop(\"api_key\", None)\n                dispatcher.event(\n                    LLMChatStartEvent(\n                        model_dict=model_dict,\n                        messages=messages,\n                        additional_kwargs=kwargs,\n                        span_id=span_id,\n                    )\n                )\n                event_id = callback_manager.on_event_start(\n                    CBEventType.LLM,\n                    payload={\n                        EventPayload.MESSAGES: messages,\n                        EventPayload.ADDITIONAL_KWARGS: kwargs,\n                        EventPayload.SERIALIZED: _self.to_dict(),\n                    },\n                )\n                try:\n                    f_return_val = await f(_self, messages, **kwargs)\n                except BaseException as e:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={EventPayload.EXCEPTION: e},\n                        event_id=event_id,\n                    )\n                    raise\n                if isinstance(f_return_val, AsyncGenerator):\n                    # intercept the generator and add a callback to the end\n                    async def wrapped_gen() -> ChatResponseAsyncGen:\n                        last_response = None\n                        try:\n                            async for x in f_return_val:\n                                dispatcher.event(\n                                    LLMChatInProgressEvent(\n                                        messages=messages,\n                                        response=x,\n                                        span_id=span_id,\n                                    )\n                                )\n                                yield cast(ChatResponse, x)\n                                last_response = x\n                        except BaseException as exception:\n                            callback_manager.on_event_end(\n                                CBEventType.LLM,\n                                payload={EventPayload.EXCEPTION: exception},\n                                event_id=event_id,\n                            )\n                            dispatcher.event(\n                                ExceptionEvent(\n                                    exception=exception,\n                                    span_id=span_id,\n                                )\n                            )\n                            raise\n                        callback_manager.on_event_end(\n                            CBEventType.LLM,\n                            payload={\n                                EventPayload.MESSAGES: messages,\n                                EventPayload.RESPONSE: last_response,\n                            },\n                            event_id=event_id,\n                        )\n                        dispatcher.event(\n                            LLMChatEndEvent(\n                                messages=messages,\n                                response=last_response,\n                                span_id=span_id,\n                            )\n                        )\n\n                    return wrapped_gen()\n                else:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={\n                            EventPayload.MESSAGES: messages,\n                            EventPayload.RESPONSE: f_return_val,\n                        },\n                        event_id=event_id,\n                    )\n                    dispatcher.event(\n                        LLMChatEndEvent(\n                            messages=messages,\n                            response=f_return_val,\n                            span_id=span_id,\n                        )\n                    )\n\n            return f_return_val\n\n        def wrapped_llm_chat(\n            _self: Any, messages: Sequence[ChatMessage], **kwargs: Any\n        ) -> Any:\n            with wrapper_logic(_self) as callback_manager, callback_manager.as_trace(\n                \"chat\"\n            ):\n                span_id = active_span_id.get()\n                model_dict = _self.to_dict()\n                model_dict.pop(\"api_key\", None)\n                dispatcher.event(\n                    LLMChatStartEvent(\n                        model_dict=model_dict,\n                        messages=messages,\n                        additional_kwargs=kwargs,\n                        span_id=span_id,\n                    )\n                )\n                event_id = callback_manager.on_event_start(\n                    CBEventType.LLM,\n                    payload={\n                        EventPayload.MESSAGES: messages,\n                        EventPayload.ADDITIONAL_KWARGS: kwargs,\n                        EventPayload.SERIALIZED: _self.to_dict(),\n                    },\n                )\n                try:\n                    f_return_val = f(_self, messages, **kwargs)\n                except BaseException as e:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={EventPayload.EXCEPTION: e},\n                        event_id=event_id,\n                    )\n                    raise\n                if isinstance(f_return_val, Generator):\n                    # intercept the generator and add a callback to the end\n                    def wrapped_gen() -> ChatResponseGen:\n                        last_response = None\n                        try:\n                            for x in f_return_val:\n                                dispatcher.event(\n                                    LLMChatInProgressEvent(\n                                        messages=messages,\n                                        response=x,\n                                        span_id=span_id,\n                                    )\n                                )\n                                yield cast(ChatResponse, x)\n                                last_response = x\n                        except BaseException as exception:\n                            callback_manager.on_event_end(\n                                CBEventType.LLM,\n                                payload={EventPayload.EXCEPTION: exception},\n                                event_id=event_id,\n                            )\n                            dispatcher.event(\n                                ExceptionEvent(\n                                    exception=exception,\n                                    span_id=span_id,\n                                )\n                            )\n                            raise\n                        callback_manager.on_event_end(\n                            CBEventType.LLM,\n                            payload={\n                                EventPayload.MESSAGES: messages,\n                                EventPayload.RESPONSE: last_response,\n                            },\n                            event_id=event_id,\n                        )\n                        dispatcher.event(\n                            LLMChatEndEvent(\n                                messages=messages,\n                                response=last_response,\n                                span_id=span_id,\n                            )\n                        )\n\n                    return wrapped_gen()\n                else:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={\n                            EventPayload.MESSAGES: messages,\n                            EventPayload.RESPONSE: f_return_val,\n                        },\n                        event_id=event_id,\n                    )\n                    dispatcher.event(\n                        LLMChatEndEvent(\n                            messages=messages,\n                            response=f_return_val,\n                            span_id=span_id,\n                        )\n                    )\n\n            return f_return_val\n\n        async def async_dummy_wrapper(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            return await f(_self, *args, **kwargs)\n\n        def dummy_wrapper(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            return f(_self, *args, **kwargs)\n\n        # check if already wrapped\n        is_wrapped = getattr(f, \"__wrapped__\", False)\n        if not is_wrapped:\n            f.__wrapped__ = True  # type: ignore\n\n        # Update the wrapper function to look like the wrapped function.\n        # See e.g. https://github.com/python/cpython/blob/0abf997e75bd3a8b76d920d33cc64d5e6c2d380f/Lib/functools.py#L57\n        for attr in (\n            \"__module__\",\n            \"__name__\",\n            \"__qualname__\",\n            \"__doc__\",\n            \"__annotations__\",\n            \"__type_params__\",\n        ):\n            if v := getattr(f, attr, None):\n                setattr(async_dummy_wrapper, attr, v)\n                setattr(wrapped_async_llm_chat, attr, v)\n                setattr(dummy_wrapper, attr, v)\n                setattr(wrapped_llm_chat, attr, v)\n\n        if asyncio.iscoroutinefunction(f):\n            if is_wrapped:\n                return async_dummy_wrapper\n            else:\n                return wrapped_async_llm_chat\n        else:\n            if is_wrapped:\n                return dummy_wrapper\n            else:\n                return wrapped_llm_chat\n\n    return wrap"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/callbacks.py",
    "filename": "callbacks.py",
    "relpath": "llms/callbacks.py",
    "start_line": 283,
    "end_line": 541,
    "length": 259,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "llm_completion_callback",
      "wrap",
      "wrapper_logic",
      "extract_prompt",
      "wrapped_async_llm_predict",
      "wrapped_gen",
      "wrapped_llm_predict",
      "wrapped_gen",
      "async_dummy_wrapper",
      "dummy_wrapper"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "llm_chat_callback",
      "wrap",
      "wrapper_logic",
      "wrapped_async_llm_chat",
      "wrapped_gen",
      "wrapped_llm_chat",
      "wrapped_gen",
      "async_dummy_wrapper",
      "dummy_wrapper",
      "llm_completion_callback",
      "wrap",
      "wrapper_logic",
      "extract_prompt",
      "wrapped_async_llm_predict",
      "wrapped_gen",
      "wrapped_llm_predict",
      "wrapped_gen",
      "async_dummy_wrapper",
      "dummy_wrapper"
    ],
    "document_class_names": [],
    "content": "def llm_completion_callback() -> Callable:\n    def wrap(f: Callable) -> Callable:\n        @contextmanager\n        def wrapper_logic(_self: Any) -> Generator[CallbackManager, None, None]:\n            callback_manager = getattr(_self, \"callback_manager\", None)\n            if not isinstance(callback_manager, CallbackManager):\n                _self.callback_manager = CallbackManager()\n\n            yield _self.callback_manager\n\n        def extract_prompt(*args: Any, **kwargs: Any) -> str:\n            if len(args) > 0:\n                return str(args[0])\n            elif \"prompt\" in kwargs:\n                return kwargs[\"prompt\"]\n            else:\n                raise ValueError(\n                    \"No prompt provided in positional or keyword arguments\"\n                )\n\n        async def wrapped_async_llm_predict(\n            _self: Any, *args: Any, **kwargs: Any\n        ) -> Any:\n            prompt = extract_prompt(*args, **kwargs)\n            with wrapper_logic(_self) as callback_manager, callback_manager.as_trace(\n                \"completion\"\n            ):\n                span_id = active_span_id.get()\n                model_dict = _self.to_dict()\n                model_dict.pop(\"api_key\", None)\n                dispatcher.event(\n                    LLMCompletionStartEvent(\n                        model_dict=model_dict,\n                        prompt=prompt,\n                        additional_kwargs=kwargs,\n                        span_id=span_id,\n                    )\n                )\n                event_id = callback_manager.on_event_start(\n                    CBEventType.LLM,\n                    payload={\n                        EventPayload.PROMPT: prompt,\n                        EventPayload.ADDITIONAL_KWARGS: kwargs,\n                        EventPayload.SERIALIZED: _self.to_dict(),\n                    },\n                )\n\n                try:\n                    f_return_val = await f(_self, *args, **kwargs)\n                except BaseException as e:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={EventPayload.EXCEPTION: e},\n                        event_id=event_id,\n                    )\n                    raise\n                if isinstance(f_return_val, AsyncGenerator):\n                    # intercept the generator and add a callback to the end\n                    async def wrapped_gen() -> CompletionResponseAsyncGen:\n                        last_response = None\n                        try:\n                            async for x in f_return_val:\n                                dispatcher.event(\n                                    LLMCompletionInProgressEvent(\n                                        prompt=prompt,\n                                        response=x,\n                                        span_id=span_id,\n                                    )\n                                )\n                                yield cast(CompletionResponse, x)\n                                last_response = x\n                        except BaseException as exception:\n                            callback_manager.on_event_end(\n                                CBEventType.LLM,\n                                payload={EventPayload.EXCEPTION: exception},\n                                event_id=event_id,\n                            )\n                            dispatcher.event(\n                                ExceptionEvent(\n                                    exception=exception,\n                                    span_id=span_id,\n                                )\n                            )\n                            raise\n                        callback_manager.on_event_end(\n                            CBEventType.LLM,\n                            payload={\n                                EventPayload.PROMPT: prompt,\n                                EventPayload.COMPLETION: last_response,\n                            },\n                            event_id=event_id,\n                        )\n                        dispatcher.event(\n                            LLMCompletionEndEvent(\n                                prompt=prompt,\n                                response=last_response,\n                                span_id=span_id,\n                            )\n                        )\n\n                    return wrapped_gen()\n                else:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={\n                            EventPayload.PROMPT: prompt,\n                            EventPayload.COMPLETION: f_return_val,\n                        },\n                        event_id=event_id,\n                    )\n                    dispatcher.event(\n                        LLMCompletionEndEvent(\n                            prompt=prompt,\n                            response=f_return_val,\n                            span_id=span_id,\n                        )\n                    )\n\n            return f_return_val\n\n        def wrapped_llm_predict(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            prompt = extract_prompt(*args, **kwargs)\n            with wrapper_logic(_self) as callback_manager, callback_manager.as_trace(\n                \"completion\"\n            ):\n                span_id = active_span_id.get()\n                model_dict = _self.to_dict()\n                model_dict.pop(\"api_key\", None)\n                dispatcher.event(\n                    LLMCompletionStartEvent(\n                        model_dict=model_dict,\n                        prompt=prompt,\n                        additional_kwargs=kwargs,\n                        span_id=span_id,\n                    )\n                )\n                event_id = callback_manager.on_event_start(\n                    CBEventType.LLM,\n                    payload={\n                        EventPayload.PROMPT: prompt,\n                        EventPayload.ADDITIONAL_KWARGS: kwargs,\n                        EventPayload.SERIALIZED: _self.to_dict(),\n                    },\n                )\n                try:\n                    f_return_val = f(_self, *args, **kwargs)\n                except BaseException as e:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={EventPayload.EXCEPTION: e},\n                        event_id=event_id,\n                    )\n                    raise\n                if isinstance(f_return_val, Generator):\n                    # intercept the generator and add a callback to the end\n                    def wrapped_gen() -> CompletionResponseGen:\n                        last_response = None\n                        try:\n                            for x in f_return_val:\n                                dispatcher.event(\n                                    LLMCompletionInProgressEvent(\n                                        prompt=prompt,\n                                        response=x,\n                                        span_id=span_id,\n                                    )\n                                )\n                                yield cast(CompletionResponse, x)\n                                last_response = x\n                        except BaseException as exception:\n                            callback_manager.on_event_end(\n                                CBEventType.LLM,\n                                payload={EventPayload.EXCEPTION: exception},\n                                event_id=event_id,\n                            )\n                            dispatcher.event(\n                                ExceptionEvent(\n                                    exception=exception,\n                                    span_id=span_id,\n                                )\n                            )\n                            raise\n                        callback_manager.on_event_end(\n                            CBEventType.LLM,\n                            payload={\n                                EventPayload.PROMPT: prompt,\n                                EventPayload.COMPLETION: last_response,\n                            },\n                            event_id=event_id,\n                        )\n                        dispatcher.event(\n                            LLMCompletionEndEvent(\n                                prompt=prompt,\n                                response=last_response,\n                                span_id=span_id,\n                            )\n                        )\n\n                    return wrapped_gen()\n                else:\n                    callback_manager.on_event_end(\n                        CBEventType.LLM,\n                        payload={\n                            EventPayload.PROMPT: prompt,\n                            EventPayload.COMPLETION: f_return_val,\n                        },\n                        event_id=event_id,\n                    )\n                    dispatcher.event(\n                        LLMCompletionEndEvent(\n                            prompt=prompt,\n                            response=f_return_val,\n                            span_id=span_id,\n                        )\n                    )\n\n            return f_return_val\n\n        async def async_dummy_wrapper(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            return await f(_self, *args, **kwargs)\n\n        def dummy_wrapper(_self: Any, *args: Any, **kwargs: Any) -> Any:\n            return f(_self, *args, **kwargs)\n\n        # check if already wrapped\n        is_wrapped = getattr(f, \"__wrapped__\", False)\n        if not is_wrapped:\n            f.__wrapped__ = True  # type: ignore\n\n        # Update the wrapper function to look like the wrapped function.\n        # See e.g. https://github.com/python/cpython/blob/0abf997e75bd3a8b76d920d33cc64d5e6c2d380f/Lib/functools.py#L57\n        for attr in (\n            \"__module__\",\n            \"__name__\",\n            \"__qualname__\",\n            \"__doc__\",\n            \"__annotations__\",\n            \"__type_params__\",\n        ):\n            if v := getattr(f, attr, None):\n                setattr(async_dummy_wrapper, attr, v)\n                setattr(wrapped_async_llm_predict, attr, v)\n                setattr(dummy_wrapper, attr, v)\n                setattr(wrapped_llm_predict, attr, v)\n\n        if asyncio.iscoroutinefunction(f):\n            if is_wrapped:\n                return async_dummy_wrapper\n            else:\n                return wrapped_async_llm_predict\n        else:\n            if is_wrapped:\n                return dummy_wrapper\n            else:\n                return wrapped_llm_predict\n\n    return wrap"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/__init__.py",
    "filename": "__init__.py",
    "relpath": "llms/__init__.py",
    "start_line": 1,
    "end_line": 35,
    "length": 35,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseAsyncGen,\n    CompletionResponseGen,\n    ImageBlock,\n    LLMMetadata,\n    MessageRole,\n    TextBlock,\n    AudioBlock,\n)\nfrom llama_index.core.llms.custom import CustomLLM\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.llms.mock import MockLLM\n\n__all__ = [\n    \"CustomLLM\",\n    \"LLM\",\n    \"ChatMessage\",\n    \"ChatResponse\",\n    \"ChatResponseAsyncGen\",\n    \"ChatResponseGen\",\n    \"CompletionResponse\",\n    \"CompletionResponseAsyncGen\",\n    \"CompletionResponseGen\",\n    \"LLMMetadata\",\n    \"MessageRole\",\n    \"MockLLM\",\n    \"ImageBlock\",\n    \"TextBlock\",\n    \"AudioBlock\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/utils.py",
    "filename": "utils.py",
    "relpath": "llms/utils.py",
    "start_line": 1,
    "end_line": 179,
    "length": 179,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "resolve_llm",
      "parse_partial_json"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "resolve_llm",
      "parse_partial_json"
    ],
    "document_class_names": [],
    "content": "from typing import TYPE_CHECKING, Optional, Union, Dict\nimport json\n\nif TYPE_CHECKING:\n    from langchain.base_language import BaseLanguageModel  # pants: no-infer-dep\n\nimport os\n\nfrom llama_index.core.llms.callbacks import CallbackManager\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.llms.mock import MockLLM\n\nLLMType = Union[str, LLM, \"BaseLanguageModel\"]\n\n\ndef resolve_llm(\n    llm: Optional[LLMType] = None, callback_manager: Optional[CallbackManager] = None\n) -> LLM:\n    \"\"\"Resolve LLM from string or LLM instance.\"\"\"\n    from llama_index.core.settings import Settings\n\n    try:\n        from langchain.base_language import BaseLanguageModel  # pants: no-infer-dep\n    except ImportError:\n        BaseLanguageModel = None  # type: ignore\n\n    if llm == \"default\":\n        # if testing return mock llm\n        if os.getenv(\"IS_TESTING\"):\n            llm = MockLLM()\n            llm.callback_manager = callback_manager or Settings.callback_manager\n            return llm\n\n        # return default OpenAI model. If it fails, return LlamaCPP\n        try:\n            from llama_index.llms.openai import OpenAI  # pants: no-infer-dep\n            from llama_index.llms.openai.utils import (\n                validate_openai_api_key,\n            )  # pants: no-infer-dep\n\n            llm = OpenAI()\n            validate_openai_api_key(llm.api_key)  # type: ignore\n        except ImportError:\n            raise ImportError(\n                \"`llama-index-llms-openai` package not found, \"\n                \"please run `pip install llama-index-llms-openai`\"\n            )\n        except ValueError as e:\n            raise ValueError(\n                \"\\n******\\n\"\n                \"Could not load OpenAI model. \"\n                \"If you intended to use OpenAI, please check your OPENAI_API_KEY.\\n\"\n                \"Original error:\\n\"\n                f\"{e!s}\"\n                \"\\nTo disable the LLM entirely, set llm=None.\"\n                \"\\n******\"\n            )\n\n    if isinstance(llm, str):\n        splits = llm.split(\":\", 1)\n        is_local = splits[0]\n        model_path = splits[1] if len(splits) > 1 else None\n        if is_local != \"local\":\n            raise ValueError(\n                \"llm must start with str 'local' or of type LLM or BaseLanguageModel\"\n            )\n        try:\n            from llama_index.llms.llama_cpp.llama_utils import (\n                completion_to_prompt,\n                messages_to_prompt,\n            )  # pants: no-infer-dep\n\n            from llama_index.llms.llama_cpp import LlamaCPP  # pants: no-infer-dep\n\n            llm = LlamaCPP(\n                model_path=model_path,\n                messages_to_prompt=messages_to_prompt,\n                completion_to_prompt=completion_to_prompt,\n                model_kwargs={\"n_gpu_layers\": 1},\n            )\n        except ImportError:\n            raise ImportError(\n                \"`llama-index-llms-llama-cpp` package not found, \"\n                \"please run `pip install llama-index-llms-llama-cpp`\"\n            )\n\n    elif BaseLanguageModel is not None and isinstance(llm, BaseLanguageModel):\n        # NOTE: if it's a langchain model, wrap it in a LangChainLLM\n        try:\n            from llama_index.llms.langchain import LangChainLLM  # pants: no-infer-dep\n\n            llm = LangChainLLM(llm=llm)\n        except ImportError:\n            raise ImportError(\n                \"`llama-index-llms-langchain` package not found, \"\n                \"please run `pip install llama-index-llms-langchain`\"\n            )\n    elif llm is None:\n        print(\"LLM is explicitly disabled. Using MockLLM.\")\n        llm = MockLLM()\n\n    assert isinstance(llm, LLM)\n\n    llm.callback_manager = callback_manager or Settings.callback_manager\n\n    return llm\n\n\ndef parse_partial_json(s: str) -> Dict:\n    \"\"\"Parse an incomplete JSON string into a valid python dictionary.\n\n    NOTE: This is adapted from\n    https://github.com/OpenInterpreter/open-interpreter/blob/5b6080fae1f8c68938a1e4fa8667e3744084ee21/interpreter/utils/parse_partial_json.py\n    \"\"\"\n    # Attempt to parse the string as-is.\n    try:\n        return json.loads(s)\n    except json.JSONDecodeError:\n        pass\n\n    # Initialize variables.\n    new_s = \"\"\n    stack = []\n    is_inside_string = False\n    escaped = False\n\n    # Process each character in the string one at a time.\n    for char in s:\n        if is_inside_string:\n            if char == '\"' and not escaped:\n                is_inside_string = False\n            elif char == \"\\n\" and not escaped:\n                char = \"\\\\n\"  # Replace the newline character with the escape sequence.\n            elif char == \"\\\\\":\n                escaped = not escaped\n            else:\n                escaped = False\n        else:\n            if char == '\"':\n                is_inside_string = True\n                escaped = False\n            elif char == \"{\":\n                stack.append(\"}\")\n            elif char == \"[\":\n                stack.append(\"]\")\n            elif char == \"}\" or char == \"]\":\n                if stack and stack[-1] == char:\n                    stack.pop()\n                else:\n                    # Mismatched closing character; the input is malformed.\n                    raise ValueError(\"Malformed partial JSON encountered.\")\n\n        # Append the processed character to the new string.\n        new_s += char\n\n    # If we're still inside a string at the end of processing and no colon was found after the opening quote,\n    # this is an incomplete key - remove it\n    if is_inside_string and '\"' in new_s and \":\" not in new_s[new_s.rindex('\"') :]:\n        new_s = new_s[: new_s.rindex('\"')]\n    elif is_inside_string:\n        new_s += '\"'\n\n    # Check if we have an incomplete key-value pair\n    new_s = new_s.rstrip()\n    if new_s.endswith(\":\"):\n        new_s += \" null\"  # Add a default value for incomplete value\n    elif new_s.endswith(\",\"):\n        new_s = new_s[:-1]  # Remove the trailing comma\n\n    # Close any remaining open structures in the reverse order that they were opened.\n    for closing_char in reversed(stack):\n        new_s += closing_char\n\n    # Attempt to parse the modified string as JSON.\n    try:\n        return json.loads(new_s)\n    except json.JSONDecodeError:\n        # If we still can't parse the string as JSON, raise error to indicate failure.\n        raise ValueError(\"Malformed partial JSON encountered.\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/mock.py",
    "filename": "mock.py",
    "relpath": "llms/mock.py",
    "start_line": 1,
    "end_line": 87,
    "length": 87,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "class_name",
      "metadata",
      "_generate_text",
      "complete",
      "stream_complete",
      "gen_prompt",
      "gen_response",
      "stream_chat"
    ],
    "chunk_class_names": [
      "MockLLM",
      "MockLLMWithNonyieldingChatStream"
    ],
    "document_function_names": [
      "__init__",
      "class_name",
      "metadata",
      "_generate_text",
      "complete",
      "stream_complete",
      "gen_prompt",
      "gen_response",
      "stream_chat"
    ],
    "document_class_names": [
      "MockLLM",
      "MockLLMWithNonyieldingChatStream"
    ],
    "content": "from typing import Any, Optional, Sequence\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseGen,\n    LLMMetadata,\n)\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.llms.callbacks import llm_chat_callback, llm_completion_callback\nfrom llama_index.core.llms.custom import CustomLLM\nfrom llama_index.core.llms.llm import MessagesToPromptType, CompletionToPromptType\nfrom llama_index.core.types import PydanticProgramMode\n\n\nclass MockLLM(CustomLLM):\n    max_tokens: Optional[int]\n\n    def __init__(\n        self,\n        max_tokens: Optional[int] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        system_prompt: Optional[str] = None,\n        messages_to_prompt: Optional[MessagesToPromptType] = None,\n        completion_to_prompt: Optional[CompletionToPromptType] = None,\n        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,\n    ) -> None:\n        super().__init__(\n            max_tokens=max_tokens,\n            callback_manager=callback_manager or CallbackManager([]),\n            system_prompt=system_prompt,\n            messages_to_prompt=messages_to_prompt,\n            completion_to_prompt=completion_to_prompt,\n            pydantic_program_mode=pydantic_program_mode,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"MockLLM\"\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        return LLMMetadata(num_output=self.max_tokens or -1)\n\n    def _generate_text(self, length: int) -> str:\n        return \" \".join([\"text\" for _ in range(length)])\n\n    @llm_completion_callback()\n    def complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n        response_text = (\n            self._generate_text(self.max_tokens) if self.max_tokens else prompt\n        )\n\n        return CompletionResponse(\n            text=response_text,\n        )\n\n    @llm_completion_callback()\n    def stream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseGen:\n        def gen_prompt() -> CompletionResponseGen:\n            for ch in prompt:\n                yield CompletionResponse(\n                    text=prompt,\n                    delta=ch,\n                )\n\n        def gen_response(max_tokens: int) -> CompletionResponseGen:\n            for i in range(max_tokens):\n                response_text = self._generate_text(i)\n                yield CompletionResponse(\n                    text=response_text,\n                    delta=\"text \",\n                )\n\n        return gen_response(self.max_tokens) if self.max_tokens else gen_prompt()\n\n\nclass MockLLMWithNonyieldingChatStream(MockLLM):\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        yield from []"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/custom.py",
    "filename": "custom.py",
    "relpath": "llms/custom.py",
    "start_line": 1,
    "end_line": 90,
    "length": 90,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "gen",
      "acomplete",
      "astream_complete",
      "gen",
      "class_name"
    ],
    "chunk_class_names": [
      "CustomLLM",
      "for"
    ],
    "document_function_names": [
      "__init__",
      "chat",
      "stream_chat",
      "achat",
      "astream_chat",
      "gen",
      "acomplete",
      "astream_complete",
      "gen",
      "class_name"
    ],
    "document_class_names": [
      "CustomLLM",
      "for"
    ],
    "content": "from typing import Any, Sequence\n\nfrom llama_index.core.base.llms.generic_utils import (\n    completion_response_to_chat_response,\n    stream_completion_response_to_chat_response,\n)\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseAsyncGen,\n)\nfrom llama_index.core.llms.callbacks import (\n    llm_chat_callback,\n    llm_completion_callback,\n)\nfrom llama_index.core.llms.llm import LLM\n\n\nclass CustomLLM(LLM):\n    \"\"\"Simple abstract base class for custom LLMs.\n\n    Subclasses must implement the `__init__`, `_complete`,\n        `_stream_complete`, and `metadata` methods.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(*args, **kwargs)\n\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        assert self.messages_to_prompt is not None\n\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)\n\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        assert self.messages_to_prompt is not None\n\n        prompt = self.messages_to_prompt(messages)\n        completion_response_gen = self.stream_complete(prompt, formatted=True, **kwargs)\n        return stream_completion_response_to_chat_response(completion_response_gen)\n\n    @llm_chat_callback()\n    async def achat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponse:\n        return self.chat(messages, **kwargs)\n\n    @llm_chat_callback()\n    async def astream_chat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponseAsyncGen:\n        async def gen() -> ChatResponseAsyncGen:\n            for message in self.stream_chat(messages, **kwargs):\n                yield message\n\n        # NOTE: convert generator to async generator\n        return gen()\n\n    @llm_completion_callback()\n    async def acomplete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n        return self.complete(prompt, formatted=formatted, **kwargs)\n\n    @llm_completion_callback()\n    async def astream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseAsyncGen:\n        async def gen() -> CompletionResponseAsyncGen:\n            for message in self.stream_complete(prompt, formatted=formatted, **kwargs):\n                yield message\n\n        # NOTE: convert generator to async generator\n        return gen()\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"custom_llm\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/llm.py",
    "filename": "llm.py",
    "relpath": "llms/llm.py",
    "start_line": 1,
    "end_line": 171,
    "length": 171,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "ignore_non_dict_arguments",
      "__call__",
      "__call__",
      "stream_completion_response_to_tokens",
      "gen",
      "stream_chat_response_to_tokens",
      "gen",
      "astream_completion_response_to_tokens",
      "gen",
      "astream_chat_response_to_tokens",
      "gen",
      "default_completion_to_prompt"
    ],
    "chunk_class_names": [
      "ToolSelection",
      "MessagesToPromptType",
      "CompletionToPromptType"
    ],
    "document_function_names": [
      "ignore_non_dict_arguments",
      "__call__",
      "__call__",
      "stream_completion_response_to_tokens",
      "gen",
      "stream_chat_response_to_tokens",
      "gen",
      "astream_completion_response_to_tokens",
      "gen",
      "astream_chat_response_to_tokens",
      "gen",
      "default_completion_to_prompt",
      "set_messages_to_prompt",
      "set_completion_to_prompt",
      "check_prompts",
      "_log_template_data",
      "_get_prompt",
      "_get_messages",
      "_parse_output",
      "_extend_prompt",
      "_extend_messages",
      "_as_query_component",
      "structured_predict",
      "astructured_predict",
      "stream_structured_predict",
      "astream_structured_predict",
      "gen",
      "predict",
      "stream",
      "apredict",
      "astream",
      "predict_and_call",
      "apredict_and_call",
      "as_structured_llm",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "ToolSelection",
      "MessagesToPromptType",
      "CompletionToPromptType",
      "LLM",
      "is",
      "for",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "BaseLLMComponent",
      "LLMCompleteComponent",
      "LLMChatComponent"
    ],
    "content": "from collections import ChainMap\nfrom typing import (\n    Any,\n    Dict,\n    List,\n    Generator,\n    AsyncGenerator,\n    Optional,\n    Protocol,\n    Sequence,\n    Union,\n    get_args,\n    runtime_checkable,\n    TYPE_CHECKING,\n    Type,\n)\nfrom typing_extensions import Annotated\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponseAsyncGen,\n    CompletionResponseGen,\n    MessageRole,\n)\nfrom llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    StringableInput,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    WithJsonSchema,\n    Field,\n    field_validator,\n    model_validator,\n    ConfigDict,\n    ValidationError,\n)\nfrom llama_index.core.callbacks import CBEventType, EventPayload\nfrom llama_index.core.base.llms.base import BaseLLM\nfrom llama_index.core.base.llms.generic_utils import (\n    messages_to_prompt as generic_messages_to_prompt,\n)\nfrom llama_index.core.base.llms.generic_utils import (\n    prompt_to_messages,\n)\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.types import (\n    BaseOutputParser,\n    PydanticProgramMode,\n    TokenAsyncGen,\n    TokenGen,\n    Model,\n)\nfrom llama_index.core.instrumentation.events.llm import (\n    LLMPredictEndEvent,\n    LLMPredictStartEvent,\n    LLMStructuredPredictInProgressEvent,\n    LLMStructuredPredictEndEvent,\n    LLMStructuredPredictStartEvent,\n)\n\nimport llama_index.core.instrumentation as instrument\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n)\n\ndispatcher = instrument.get_dispatcher(__name__)\n\nif TYPE_CHECKING:\n    from llama_index.core.chat_engine.types import AgentChatResponse\n    from llama_index.core.program.utils import FlexibleModel\n    from llama_index.core.tools.types import BaseTool\n    from llama_index.core.llms.structured_llm import StructuredLLM\n\n\nclass ToolSelection(BaseModel):\n    \"\"\"Tool selection.\"\"\"\n\n    tool_id: str = Field(description=\"Tool ID to select.\")\n    tool_name: str = Field(description=\"Tool name to select.\")\n    tool_kwargs: Dict[str, Any] = Field(description=\"Keyword arguments for the tool.\")\n\n    @field_validator(\"tool_kwargs\", mode=\"wrap\")\n    @classmethod\n    def ignore_non_dict_arguments(cls, v: Any, handler: Any) -> Dict[str, Any]:\n        try:\n            return handler(v)\n        except ValidationError:\n            return handler({})\n\n\n# NOTE: These two protocols are needed to appease mypy\n@runtime_checkable\nclass MessagesToPromptType(Protocol):\n    def __call__(self, messages: Sequence[ChatMessage]) -> str:\n        pass\n\n\n@runtime_checkable\nclass CompletionToPromptType(Protocol):\n    def __call__(self, prompt: str) -> str:\n        pass\n\n\ndef stream_completion_response_to_tokens(\n    completion_response_gen: CompletionResponseGen,\n) -> TokenGen:\n    \"\"\"Convert a stream completion response to a stream of tokens.\"\"\"\n\n    def gen() -> TokenGen:\n        for response in completion_response_gen:\n            yield response.delta or \"\"\n\n    return gen()\n\n\ndef stream_chat_response_to_tokens(\n    chat_response_gen: ChatResponseGen,\n) -> TokenGen:\n    \"\"\"Convert a stream completion response to a stream of tokens.\"\"\"\n\n    def gen() -> TokenGen:\n        for response in chat_response_gen:\n            yield response.delta or \"\"\n\n    return gen()\n\n\nasync def astream_completion_response_to_tokens(\n    completion_response_gen: CompletionResponseAsyncGen,\n) -> TokenAsyncGen:\n    \"\"\"Convert a stream completion response to a stream of tokens.\"\"\"\n\n    async def gen() -> TokenAsyncGen:\n        async for response in completion_response_gen:\n            yield response.delta or \"\"\n\n    return gen()\n\n\nasync def astream_chat_response_to_tokens(\n    chat_response_gen: ChatResponseAsyncGen,\n) -> TokenAsyncGen:\n    \"\"\"Convert a stream completion response to a stream of tokens.\"\"\"\n\n    async def gen() -> TokenAsyncGen:\n        async for response in chat_response_gen:\n            yield response.delta or \"\"\n\n    return gen()\n\n\ndef default_completion_to_prompt(prompt: str) -> str:\n    return prompt\n\n\nMessagesToPromptCallable = Annotated[\n    Optional[MessagesToPromptType],\n    WithJsonSchema({\"type\": \"string\"}),\n]\n\n\nCompletionToPromptCallable = Annotated[\n    Optional[CompletionToPromptType],\n    WithJsonSchema({\"type\": \"string\"}),\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/llm.py",
    "filename": "llm.py",
    "relpath": "llms/llm.py",
    "start_line": 171,
    "end_line": 174,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "LLM"
    ],
    "document_function_names": [
      "ignore_non_dict_arguments",
      "__call__",
      "__call__",
      "stream_completion_response_to_tokens",
      "gen",
      "stream_chat_response_to_tokens",
      "gen",
      "astream_completion_response_to_tokens",
      "gen",
      "astream_chat_response_to_tokens",
      "gen",
      "default_completion_to_prompt",
      "set_messages_to_prompt",
      "set_completion_to_prompt",
      "check_prompts",
      "_log_template_data",
      "_get_prompt",
      "_get_messages",
      "_parse_output",
      "_extend_prompt",
      "_extend_messages",
      "_as_query_component",
      "structured_predict",
      "astructured_predict",
      "stream_structured_predict",
      "astream_structured_predict",
      "gen",
      "predict",
      "stream",
      "apredict",
      "astream",
      "predict_and_call",
      "apredict_and_call",
      "as_structured_llm",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "ToolSelection",
      "MessagesToPromptType",
      "CompletionToPromptType",
      "LLM",
      "is",
      "for",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "BaseLLMComponent",
      "LLMCompleteComponent",
      "LLMChatComponent"
    ],
    "content": "class LLM(BaseLLM):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/llm.py",
    "filename": "llm.py",
    "relpath": "llms/llm.py",
    "start_line": 174,
    "end_line": 613,
    "length": 440,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "set_messages_to_prompt",
      "set_completion_to_prompt",
      "check_prompts",
      "_log_template_data",
      "_get_prompt",
      "_get_messages",
      "_parse_output",
      "_extend_prompt",
      "_extend_messages",
      "_as_query_component",
      "structured_predict",
      "astructured_predict",
      "stream_structured_predict",
      "astream_structured_predict",
      "gen",
      "predict"
    ],
    "chunk_class_names": [
      "is",
      "for",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test"
    ],
    "document_function_names": [
      "ignore_non_dict_arguments",
      "__call__",
      "__call__",
      "stream_completion_response_to_tokens",
      "gen",
      "stream_chat_response_to_tokens",
      "gen",
      "astream_completion_response_to_tokens",
      "gen",
      "astream_chat_response_to_tokens",
      "gen",
      "default_completion_to_prompt",
      "set_messages_to_prompt",
      "set_completion_to_prompt",
      "check_prompts",
      "_log_template_data",
      "_get_prompt",
      "_get_messages",
      "_parse_output",
      "_extend_prompt",
      "_extend_messages",
      "_as_query_component",
      "structured_predict",
      "astructured_predict",
      "stream_structured_predict",
      "astream_structured_predict",
      "gen",
      "predict",
      "stream",
      "apredict",
      "astream",
      "predict_and_call",
      "apredict_and_call",
      "as_structured_llm",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "ToolSelection",
      "MessagesToPromptType",
      "CompletionToPromptType",
      "LLM",
      "is",
      "for",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "BaseLLMComponent",
      "LLMCompleteComponent",
      "LLMChatComponent"
    ],
    "content": "\"\"\"\n    The LLM class is the main class for interacting with language models.\n\n    Attributes:\n        system_prompt (Optional[str]):\n            System prompt for LLM calls.\n        messages_to_prompt (Callable):\n            Function to convert a list of messages to an LLM prompt.\n        completion_to_prompt (Callable):\n            Function to convert a completion to an LLM prompt.\n        output_parser (Optional[BaseOutputParser]):\n            Output parser to parse, validate, and correct errors programmatically.\n        pydantic_program_mode (PydanticProgramMode):\n            Pydantic program mode to use for structured prediction.\n    \"\"\"\n\n    system_prompt: Optional[str] = Field(\n        default=None, description=\"System prompt for LLM calls.\"\n    )\n    messages_to_prompt: MessagesToPromptCallable = Field(\n        description=\"Function to convert a list of messages to an LLM prompt.\",\n        default=None,\n        exclude=True,\n    )\n    completion_to_prompt: CompletionToPromptCallable = Field(\n        description=\"Function to convert a completion to an LLM prompt.\",\n        default=None,\n        exclude=True,\n    )\n    output_parser: Optional[BaseOutputParser] = Field(\n        description=\"Output parser to parse, validate, and correct errors programmatically.\",\n        default=None,\n        exclude=True,\n    )\n    pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT\n\n    # deprecated\n    query_wrapper_prompt: Optional[BasePromptTemplate] = Field(\n        description=\"Query wrapper prompt for LLM calls.\",\n        default=None,\n        exclude=True,\n    )\n\n    # -- Pydantic Configs --\n\n    @field_validator(\"messages_to_prompt\")\n    @classmethod\n    def set_messages_to_prompt(\n        cls, messages_to_prompt: Optional[MessagesToPromptType]\n    ) -> MessagesToPromptType:\n        return messages_to_prompt or generic_messages_to_prompt\n\n    @field_validator(\"completion_to_prompt\")\n    @classmethod\n    def set_completion_to_prompt(\n        cls, completion_to_prompt: Optional[CompletionToPromptType]\n    ) -> CompletionToPromptType:\n        return completion_to_prompt or default_completion_to_prompt\n\n    @model_validator(mode=\"after\")\n    def check_prompts(self) -> \"LLM\":\n        if self.completion_to_prompt is None:\n            self.completion_to_prompt = default_completion_to_prompt\n        if self.messages_to_prompt is None:\n            self.messages_to_prompt = generic_messages_to_prompt\n        return self\n\n    # -- Utils --\n\n    def _log_template_data(\n        self, prompt: BasePromptTemplate, **prompt_args: Any\n    ) -> None:\n        template_vars = {\n            k: v\n            for k, v in ChainMap(prompt.kwargs, prompt_args).items()\n            if k in prompt.template_vars\n        }\n        with self.callback_manager.event(\n            CBEventType.TEMPLATING,\n            payload={\n                EventPayload.TEMPLATE: prompt.get_template(llm=self),\n                EventPayload.TEMPLATE_VARS: template_vars,\n                EventPayload.SYSTEM_PROMPT: self.system_prompt,\n                EventPayload.QUERY_WRAPPER_PROMPT: self.query_wrapper_prompt,\n            },\n        ):\n            pass\n\n    def _get_prompt(self, prompt: BasePromptTemplate, **prompt_args: Any) -> str:\n        formatted_prompt = prompt.format(\n            llm=self,\n            messages_to_prompt=self.messages_to_prompt,\n            completion_to_prompt=self.completion_to_prompt,\n            **prompt_args,\n        )\n        if self.output_parser is not None:\n            formatted_prompt = self.output_parser.format(formatted_prompt)\n        return self._extend_prompt(formatted_prompt)\n\n    def _get_messages(\n        self, prompt: BasePromptTemplate, **prompt_args: Any\n    ) -> List[ChatMessage]:\n        messages = prompt.format_messages(llm=self, **prompt_args)\n        if self.output_parser is not None:\n            messages = self.output_parser.format_messages(messages)\n        return self._extend_messages(messages)\n\n    def _parse_output(self, output: str) -> str:\n        if self.output_parser is not None:\n            return str(self.output_parser.parse(output))\n\n        return output\n\n    def _extend_prompt(\n        self,\n        formatted_prompt: str,\n    ) -> str:\n        \"\"\"Add system and query wrapper prompts to base prompt.\"\"\"\n        extended_prompt = formatted_prompt\n\n        if self.system_prompt:\n            extended_prompt = self.system_prompt + \"\\n\\n\" + extended_prompt\n\n        if self.query_wrapper_prompt:\n            extended_prompt = self.query_wrapper_prompt.format(\n                query_str=extended_prompt\n            )\n\n        return extended_prompt\n\n    def _extend_messages(self, messages: List[ChatMessage]) -> List[ChatMessage]:\n        \"\"\"Add system prompt to chat message list.\"\"\"\n        if self.system_prompt:\n            messages = [\n                ChatMessage(role=MessageRole.SYSTEM, content=self.system_prompt),\n                *messages,\n            ]\n        return messages\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"Return query component.\"\"\"\n        if self.metadata.is_chat_model:\n            return LLMChatComponent(llm=self, **kwargs)\n        else:\n            return LLMCompleteComponent(llm=self, **kwargs)\n\n    # -- Structured outputs --\n\n    @dispatcher.span\n    def structured_predict(\n        self,\n        output_cls: Type[Model],\n        prompt: PromptTemplate,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        **prompt_args: Any,\n    ) -> Model:\n        r\"\"\"Structured predict.\n\n        Args:\n            output_cls (BaseModel):\n                Output class to use for structured prediction.\n            prompt (PromptTemplate):\n                Prompt template to use for structured prediction.\n            llm_kwargs (Optional[Dict[str, Any]]):\n                Arguments that are passed down to the LLM invoked by the program.\n            prompt_args (Any):\n                Additional arguments to format the prompt with.\n\n        Returns:\n            BaseModel: The structured prediction output.\n\n        Examples:\n            ```python\n            from pydantic import BaseModel\n\n            class Test(BaseModel):\n                \\\"\\\"\\\"My test class.\\\"\\\"\\\"\n                name: str\n\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please predict a Test with a random name related to {topic}.\")\n            output = llm.structured_predict(Test, prompt, topic=\"cats\")\n            print(output.name)\n            ```\n        \"\"\"\n        from llama_index.core.program.utils import get_program_for_llm\n\n        dispatcher.event(\n            LLMStructuredPredictStartEvent(\n                output_cls=output_cls, template=prompt, template_args=prompt_args\n            )\n        )\n        program = get_program_for_llm(\n            output_cls,\n            prompt,\n            self,\n            pydantic_program_mode=self.pydantic_program_mode,\n        )\n\n        result = program(llm_kwargs=llm_kwargs, **prompt_args)\n        assert not isinstance(result, list)\n\n        dispatcher.event(LLMStructuredPredictEndEvent(output=result))\n        return result\n\n    @dispatcher.span\n    async def astructured_predict(\n        self,\n        output_cls: Type[Model],\n        prompt: PromptTemplate,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        **prompt_args: Any,\n    ) -> Model:\n        r\"\"\"Async Structured predict.\n\n        Args:\n            output_cls (BaseModel):\n                Output class to use for structured prediction.\n            prompt (PromptTemplate):\n                Prompt template to use for structured prediction.\n            llm_kwargs (Optional[Dict[str, Any]]):\n                Arguments that are passed down to the LLM invoked by the program.\n            prompt_args (Any):\n                Additional arguments to format the prompt with.\n\n        Returns:\n            BaseModel: The structured prediction output.\n\n        Examples:\n            ```python\n            from pydantic import BaseModel\n\n            class Test(BaseModel):\n                \\\"\\\"\\\"My test class.\\\"\\\"\\\"\n                name: str\n\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please predict a Test with a random name related to {topic}.\")\n            output = await llm.astructured_predict(Test, prompt, topic=\"cats\")\n            print(output.name)\n            ```\n        \"\"\"\n        from llama_index.core.program.utils import get_program_for_llm\n\n        dispatcher.event(\n            LLMStructuredPredictStartEvent(\n                output_cls=output_cls, template=prompt, template_args=prompt_args\n            )\n        )\n\n        program = get_program_for_llm(\n            output_cls,\n            prompt,\n            self,\n            pydantic_program_mode=self.pydantic_program_mode,\n        )\n\n        result = await program.acall(llm_kwargs=llm_kwargs, **prompt_args)\n        assert not isinstance(result, list)\n\n        dispatcher.event(LLMStructuredPredictEndEvent(output=result))\n        return result\n\n    @dispatcher.span\n    def stream_structured_predict(\n        self,\n        output_cls: Type[Model],\n        prompt: PromptTemplate,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        **prompt_args: Any,\n    ) -> Generator[Union[Model, \"FlexibleModel\"], None, None]:\n        r\"\"\"Stream Structured predict.\n\n        Args:\n            output_cls (BaseModel):\n                Output class to use for structured prediction.\n            prompt (PromptTemplate):\n                Prompt template to use for structured prediction.\n            llm_kwargs (Optional[Dict[str, Any]]):\n                Arguments that are passed down to the LLM invoked by the program.\n            prompt_args (Any):\n                Additional arguments to format the prompt with.\n\n        Returns:\n            Generator: A generator returning partial copies of the model or list of models.\n\n        Examples:\n            ```python\n            from pydantic import BaseModel\n\n            class Test(BaseModel):\n                \\\"\\\"\\\"My test class.\\\"\\\"\\\"\n                name: str\n\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please predict a Test with a random name related to {topic}.\")\n            stream_output = llm.stream_structured_predict(Test, prompt, topic=\"cats\")\n            for partial_output in stream_output:\n                # stream partial outputs until completion\n                print(partial_output.name)\n            ```\n        \"\"\"\n        from llama_index.core.program.utils import get_program_for_llm\n\n        dispatcher.event(\n            LLMStructuredPredictStartEvent(\n                output_cls=output_cls, template=prompt, template_args=prompt_args\n            )\n        )\n        program = get_program_for_llm(\n            output_cls,\n            prompt,\n            self,\n            pydantic_program_mode=self.pydantic_program_mode,\n        )\n\n        result = program.stream_call(llm_kwargs=llm_kwargs, **prompt_args)\n        for r in result:\n            dispatcher.event(LLMStructuredPredictInProgressEvent(output=r))\n            assert not isinstance(r, list)\n            yield r\n\n        dispatcher.event(LLMStructuredPredictEndEvent(output=r))\n\n    @dispatcher.span\n    async def astream_structured_predict(\n        self,\n        output_cls: Type[Model],\n        prompt: PromptTemplate,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        **prompt_args: Any,\n    ) -> AsyncGenerator[Union[Model, \"FlexibleModel\"], None]:\n        r\"\"\"Async Stream Structured predict.\n\n        Args:\n            output_cls (BaseModel):\n                Output class to use for structured prediction.\n            prompt (PromptTemplate):\n                Prompt template to use for structured prediction.\n            llm_kwargs (Optional[Dict[str, Any]]):\n                Arguments that are passed down to the LLM invoked by the program.\n            prompt_args (Any):\n                Additional arguments to format the prompt with.\n\n        Returns:\n            Generator: A generator returning partial copies of the model or list of models.\n\n        Examples:\n            ```python\n            from pydantic import BaseModel\n\n            class Test(BaseModel):\n                \\\"\\\"\\\"My test class.\\\"\\\"\\\"\n                name: str\n\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please predict a Test with a random name related to {topic}.\")\n            stream_output = await llm.astream_structured_predict(Test, prompt, topic=\"cats\")\n            async for partial_output in stream_output:\n                # stream partial outputs until completion\n                print(partial_output.name)\n            ```\n        \"\"\"\n\n        async def gen() -> AsyncGenerator[Union[Model, \"FlexibleModel\"], None]:\n            from llama_index.core.program.utils import (\n                get_program_for_llm,\n            )\n\n            dispatcher.event(\n                LLMStructuredPredictStartEvent(\n                    output_cls=output_cls, template=prompt, template_args=prompt_args\n                )\n            )\n            program = get_program_for_llm(\n                output_cls,\n                prompt,\n                self,\n                pydantic_program_mode=self.pydantic_program_mode,\n            )\n\n            result = await program.astream_call(llm_kwargs=llm_kwargs, **prompt_args)\n            async for r in result:\n                dispatcher.event(LLMStructuredPredictInProgressEvent(output=r))\n                assert not isinstance(r, list)\n                yield r\n\n            dispatcher.event(LLMStructuredPredictEndEvent(output=r))\n\n        return gen()\n\n    # -- Prompt Chaining --\n\n    @dispatcher.span\n    def predict(\n        self,\n        prompt: BasePromptTemplate,\n        **prompt_args: Any,\n    ) -> str:\n        \"\"\"Predict for a given prompt.\n\n        Args:\n            prompt (BasePromptTemplate):\n                The prompt to use for prediction.\n            prompt_args (Any):\n                Additional arguments to format the prompt with.\n\n        Returns:\n            str: The prediction output.\n\n        Examples:\n            ```python\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n            output = llm.predict(prompt, topic=\"cats\")\n            print(output)\n            ```\n        \"\"\"\n        dispatcher.event(\n            LLMPredictStartEvent(template=prompt, template_args=prompt_args)\n        )\n        self._log_template_data(prompt, **prompt_args)\n\n        if self.metadata.is_chat_model:\n            messages = self._get_messages(prompt, **prompt_args)\n            chat_response = self.chat(messages)\n            output = chat_response.message.content or \"\"\n        else:\n            formatted_prompt = self._get_prompt(prompt, **prompt_args)\n            response = self.complete(formatted_prompt, formatted=True)\n            output = response.text\n        parsed_output = self._parse_output(output)\n        dispatcher.event(LLMPredictEndEvent(output=parsed_output))\n        return parsed_output"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/llm.py",
    "filename": "llm.py",
    "relpath": "llms/llm.py",
    "start_line": 613,
    "end_line": 892,
    "length": 280,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "stream",
      "apredict",
      "astream",
      "predict_and_call",
      "apredict_and_call",
      "as_structured_llm"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "ignore_non_dict_arguments",
      "__call__",
      "__call__",
      "stream_completion_response_to_tokens",
      "gen",
      "stream_chat_response_to_tokens",
      "gen",
      "astream_completion_response_to_tokens",
      "gen",
      "astream_chat_response_to_tokens",
      "gen",
      "default_completion_to_prompt",
      "set_messages_to_prompt",
      "set_completion_to_prompt",
      "check_prompts",
      "_log_template_data",
      "_get_prompt",
      "_get_messages",
      "_parse_output",
      "_extend_prompt",
      "_extend_messages",
      "_as_query_component",
      "structured_predict",
      "astructured_predict",
      "stream_structured_predict",
      "astream_structured_predict",
      "gen",
      "predict",
      "stream",
      "apredict",
      "astream",
      "predict_and_call",
      "apredict_and_call",
      "as_structured_llm",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "ToolSelection",
      "MessagesToPromptType",
      "CompletionToPromptType",
      "LLM",
      "is",
      "for",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "BaseLLMComponent",
      "LLMCompleteComponent",
      "LLMChatComponent"
    ],
    "content": "@dispatcher.span\n    def stream(\n        self,\n        prompt: BasePromptTemplate,\n        **prompt_args: Any,\n    ) -> TokenGen:\n        \"\"\"Stream predict for a given prompt.\n\n        Args:\n            prompt (BasePromptTemplate):\n                The prompt to use for prediction.\n            prompt_args (Any):\n                Additional arguments to format the prompt with.\n\n        Yields:\n            str: Each streamed token.\n\n        Examples:\n            ```python\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n            gen = llm.stream_predict(prompt, topic=\"cats\")\n            for token in gen:\n                print(token, end=\"\", flush=True)\n            ```\n        \"\"\"\n        self._log_template_data(prompt, **prompt_args)\n\n        dispatcher.event(\n            LLMPredictStartEvent(template=prompt, template_args=prompt_args)\n        )\n        if self.metadata.is_chat_model:\n            messages = self._get_messages(prompt, **prompt_args)\n            chat_response = self.stream_chat(messages)\n            stream_tokens = stream_chat_response_to_tokens(chat_response)\n        else:\n            formatted_prompt = self._get_prompt(prompt, **prompt_args)\n            stream_response = self.stream_complete(formatted_prompt, formatted=True)\n            stream_tokens = stream_completion_response_to_tokens(stream_response)\n\n        if prompt.output_parser is not None or self.output_parser is not None:\n            raise NotImplementedError(\"Output parser is not supported for streaming.\")\n\n        return stream_tokens\n\n    @dispatcher.span\n    async def apredict(\n        self,\n        prompt: BasePromptTemplate,\n        **prompt_args: Any,\n    ) -> str:\n        \"\"\"Async Predict for a given prompt.\n\n        Args:\n            prompt (BasePromptTemplate):\n                The prompt to use for prediction.\n            prompt_args (Any):\n                Additional arguments to format the prompt with.\n\n        Returns:\n            str: The prediction output.\n\n        Examples:\n            ```python\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n            output = await llm.apredict(prompt, topic=\"cats\")\n            print(output)\n            ```\n        \"\"\"\n        dispatcher.event(\n            LLMPredictStartEvent(template=prompt, template_args=prompt_args)\n        )\n        self._log_template_data(prompt, **prompt_args)\n\n        if self.metadata.is_chat_model:\n            messages = self._get_messages(prompt, **prompt_args)\n            chat_response = await self.achat(messages)\n            output = chat_response.message.content or \"\"\n        else:\n            formatted_prompt = self._get_prompt(prompt, **prompt_args)\n            response = await self.acomplete(formatted_prompt, formatted=True)\n            output = response.text\n\n        parsed_output = self._parse_output(output)\n        dispatcher.event(LLMPredictEndEvent(output=parsed_output))\n        return parsed_output\n\n    @dispatcher.span\n    async def astream(\n        self,\n        prompt: BasePromptTemplate,\n        **prompt_args: Any,\n    ) -> TokenAsyncGen:\n        \"\"\"Async stream predict for a given prompt.\n\n        Args:\n        prompt (BasePromptTemplate):\n            The prompt to use for prediction.\n        prompt_args (Any):\n            Additional arguments to format the prompt with.\n\n        Yields:\n            str: An async generator that yields strings of tokens.\n\n        Examples:\n            ```python\n            from llama_index.core.prompts import PromptTemplate\n\n            prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\n            gen = await llm.astream_predict(prompt, topic=\"cats\")\n            async for token in gen:\n                print(token, end=\"\", flush=True)\n            ```\n        \"\"\"\n        self._log_template_data(prompt, **prompt_args)\n\n        dispatcher.event(\n            LLMPredictStartEvent(template=prompt, template_args=prompt_args)\n        )\n        if self.metadata.is_chat_model:\n            messages = self._get_messages(prompt, **prompt_args)\n            chat_response = await self.astream_chat(messages)\n            stream_tokens = await astream_chat_response_to_tokens(chat_response)\n        else:\n            formatted_prompt = self._get_prompt(prompt, **prompt_args)\n            stream_response = await self.astream_complete(\n                formatted_prompt, formatted=True\n            )\n            stream_tokens = await astream_completion_response_to_tokens(stream_response)\n\n        if prompt.output_parser is not None or self.output_parser is not None:\n            raise NotImplementedError(\"Output parser is not supported for streaming.\")\n\n        return stream_tokens\n\n    @dispatcher.span\n    def predict_and_call(\n        self,\n        tools: List[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"AgentChatResponse\":\n        \"\"\"Predict and call the tool.\n\n        By default uses a ReAct agent to do tool calling (through text prompting),\n        but function calling LLMs will implement this differently.\n\n        \"\"\"\n        from llama_index.core.agent.react import ReActAgentWorker\n        from llama_index.core.agent.types import Task\n        from llama_index.core.chat_engine.types import AgentChatResponse\n        from llama_index.core.memory import ChatMemoryBuffer\n\n        worker = ReActAgentWorker(\n            tools,\n            llm=self,\n            callback_manager=self.callback_manager,\n            verbose=verbose,\n            max_iterations=kwargs.get(\"max_iterations\", 10),\n            react_chat_formatter=kwargs.get(\"react_chat_formatter\", None),\n            output_parser=kwargs.get(\"output_parser\", None),\n            tool_retriever=kwargs.get(\"tool_retriever\", None),\n            handle_reasoning_failure_fn=kwargs.get(\"handle_reasoning_failure_fn\", None),\n        )\n\n        if isinstance(user_msg, ChatMessage) and isinstance(user_msg.content, str):\n            user_msg = user_msg.content\n        elif isinstance(user_msg, str):\n            pass\n        elif (\n            not user_msg\n            and chat_history is not None\n            and len(chat_history) > 0\n            and isinstance(chat_history[-1].content, str)\n        ):\n            user_msg = chat_history[-1].content\n        else:\n            raise ValueError(\"No user message provided or found in chat history.\")\n\n        task = Task(\n            input=user_msg,\n            memory=ChatMemoryBuffer.from_defaults(chat_history=chat_history),\n            extra_state={},\n            callback_manager=self.callback_manager,\n        )\n        step = worker.initialize_step(task)\n\n        try:\n            output = worker.run_step(step, task).output\n\n            # react agent worker inserts a \"Observation: \" prefix to the response\n            if output.response and output.response.startswith(\"Observation: \"):\n                output.response = output.response.replace(\"Observation: \", \"\")\n        except Exception as e:\n            output = AgentChatResponse(\n                response=\"An error occurred while running the tool: \" + str(e),\n                sources=[],\n            )\n\n        return output\n\n    @dispatcher.span\n    async def apredict_and_call(\n        self,\n        tools: List[\"BaseTool\"],\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"AgentChatResponse\":\n        \"\"\"Predict and call the tool.\"\"\"\n        from llama_index.core.agent.react import ReActAgentWorker\n        from llama_index.core.agent.types import Task\n        from llama_index.core.chat_engine.types import AgentChatResponse\n        from llama_index.core.memory import ChatMemoryBuffer\n\n        worker = ReActAgentWorker(\n            tools,\n            llm=self,\n            callback_manager=self.callback_manager,\n            verbose=verbose,\n            max_iterations=kwargs.get(\"max_iterations\", 10),\n            react_chat_formatter=kwargs.get(\"react_chat_formatter\", None),\n            output_parser=kwargs.get(\"output_parser\", None),\n            tool_retriever=kwargs.get(\"tool_retriever\", None),\n            handle_reasoning_failure_fn=kwargs.get(\"handle_reasoning_failure_fn\", None),\n        )\n\n        if isinstance(user_msg, ChatMessage) and isinstance(user_msg.content, str):\n            user_msg = user_msg.content\n        elif isinstance(user_msg, str):\n            pass\n        elif (\n            not user_msg\n            and chat_history is not None\n            and len(chat_history) > 0\n            and isinstance(chat_history[-1].content, str)\n        ):\n            user_msg = chat_history[-1].content\n        else:\n            raise ValueError(\"No user message provided or found in chat history.\")\n\n        task = Task(\n            input=user_msg,\n            memory=ChatMemoryBuffer.from_defaults(chat_history=chat_history),\n            extra_state={},\n            callback_manager=self.callback_manager,\n        )\n        step = worker.initialize_step(task)\n\n        try:\n            output = (await worker.arun_step(step, task)).output\n\n            # react agent worker inserts a \"Observation: \" prefix to the response\n            if output.response and output.response.startswith(\"Observation: \"):\n                output.response = output.response.replace(\"Observation: \", \"\")\n        except Exception as e:\n            output = AgentChatResponse(\n                response=\"An error occurred while running the tool: \" + str(e),\n                sources=[],\n            )\n\n        return output\n\n    def as_structured_llm(\n        self,\n        output_cls: Type[BaseModel],\n        **kwargs: Any,\n    ) -> \"StructuredLLM\":\n        \"\"\"Return a structured LLM around a given object.\"\"\"\n        from llama_index.core.llms.structured_llm import StructuredLLM\n\n        return StructuredLLM(llm=self, output_cls=output_cls, **kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/llms/llm.py",
    "filename": "llm.py",
    "relpath": "llms/llm.py",
    "start_line": 171,
    "end_line": 1015,
    "length": 845,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "BaseLLMComponent",
      "LLMCompleteComponent",
      "LLMChatComponent"
    ],
    "document_function_names": [
      "ignore_non_dict_arguments",
      "__call__",
      "__call__",
      "stream_completion_response_to_tokens",
      "gen",
      "stream_chat_response_to_tokens",
      "gen",
      "astream_completion_response_to_tokens",
      "gen",
      "astream_chat_response_to_tokens",
      "gen",
      "default_completion_to_prompt",
      "set_messages_to_prompt",
      "set_completion_to_prompt",
      "check_prompts",
      "_log_template_data",
      "_get_prompt",
      "_get_messages",
      "_parse_output",
      "_extend_prompt",
      "_extend_messages",
      "_as_query_component",
      "structured_predict",
      "astructured_predict",
      "stream_structured_predict",
      "astream_structured_predict",
      "gen",
      "predict",
      "stream",
      "apredict",
      "astream",
      "predict_and_call",
      "apredict_and_call",
      "as_structured_llm",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "ToolSelection",
      "MessagesToPromptType",
      "CompletionToPromptType",
      "LLM",
      "is",
      "for",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "to",
      "Test",
      "BaseLLMComponent",
      "LLMCompleteComponent",
      "LLMChatComponent"
    ],
    "content": "class BaseLLMComponent(QueryComponent):\n    \"\"\"Base LLM component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    llm: LLM = Field(..., description=\"LLM\")\n    streaming: bool = Field(default=False, description=\"Streaming mode\")\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.llm.callback_manager = callback_manager\n\n\nclass LLMCompleteComponent(BaseLLMComponent):\n    \"\"\"LLM completion component.\"\"\"\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        if \"prompt\" not in input:\n            raise ValueError(\"Prompt must be in input dict.\")\n\n        # do special check to see if prompt is a list of chat messages\n        if isinstance(input[\"prompt\"], get_args(List[ChatMessage])):\n            if self.llm.messages_to_prompt:\n                input[\"prompt\"] = self.llm.messages_to_prompt(input[\"prompt\"])\n            input[\"prompt\"] = validate_and_convert_stringable(input[\"prompt\"])\n        else:\n            input[\"prompt\"] = validate_and_convert_stringable(input[\"prompt\"])\n            if self.llm.completion_to_prompt:\n                input[\"prompt\"] = self.llm.completion_to_prompt(input[\"prompt\"])\n\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # TODO: support only complete for now\n        # non-trivial to figure how to support chat/complete/etc.\n        prompt = kwargs[\"prompt\"]\n        # ignore all other kwargs for now\n\n        response: Any\n        if self.streaming:\n            response = self.llm.stream_complete(prompt, formatted=True)\n        else:\n            response = self.llm.complete(prompt, formatted=True)\n        return {\"output\": response}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # TODO: support only complete for now\n        # non-trivial to figure how to support chat/complete/etc.\n        prompt = kwargs[\"prompt\"]\n        # ignore all other kwargs for now\n        response = await self.llm.acomplete(prompt, formatted=True)\n        return {\"output\": response}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # TODO: support only complete for now\n        return InputKeys.from_keys({\"prompt\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})\n\n\nclass LLMChatComponent(BaseLLMComponent):\n    \"\"\"LLM chat component.\"\"\"\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        if \"messages\" not in input:\n            raise ValueError(\"Messages must be in input dict.\")\n\n        # if `messages` is a string, convert to a list of chat message\n        if isinstance(input[\"messages\"], get_args(StringableInput)):\n            input[\"messages\"] = validate_and_convert_stringable(input[\"messages\"])\n            input[\"messages\"] = prompt_to_messages(str(input[\"messages\"]))\n\n        for message in input[\"messages\"]:\n            if not isinstance(message, ChatMessage):\n                raise ValueError(\"Messages must be a list of ChatMessage\")\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # TODO: support only complete for now\n        # non-trivial to figure how to support chat/complete/etc.\n        messages = kwargs[\"messages\"]\n\n        response: Any\n        if self.streaming:\n            response = self.llm.stream_chat(messages)\n        else:\n            response = self.llm.chat(messages)\n        return {\"output\": response}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # TODO: support only complete for now\n        # non-trivial to figure how to support chat/complete/etc.\n        messages = kwargs[\"messages\"]\n\n        response: Any\n        if self.streaming:\n            response = await self.llm.astream_chat(messages)\n        else:\n            response = await self.llm.achat(messages)\n        return {\"output\": response}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # TODO: support only complete for now\n        return InputKeys.from_keys({\"messages\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/__init__.py",
    "filename": "__init__.py",
    "relpath": "query_pipeline/__init__.py",
    "start_line": 1,
    "end_line": 48,
    "length": 48,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init file.\"\"\"\n\nfrom llama_index.core.query_pipeline.components.agent import (\n    AgentFnComponent,\n    AgentInputComponent,\n    CustomAgentComponent,\n    QueryComponent,\n)\nfrom llama_index.core.query_pipeline.components.argpacks import ArgPackComponent\nfrom llama_index.core.query_pipeline.components.function import (\n    FnComponent,\n    FunctionComponent,\n)\nfrom llama_index.core.query_pipeline.components.input import InputComponent\nfrom llama_index.core.query_pipeline.components.router import RouterComponent\nfrom llama_index.core.query_pipeline.components.tool_runner import ToolRunnerComponent\nfrom llama_index.core.query_pipeline.query import (\n    QueryPipeline,\n    Link,\n    ChainableMixin,\n    QueryComponent,\n)\nfrom llama_index.core.query_pipeline.components.stateful import StatefulFnComponent\nfrom llama_index.core.query_pipeline.components.loop import LoopComponent\n\nfrom llama_index.core.base.query_pipeline.query import (\n    CustomQueryComponent,\n)\n\n__all__ = [\n    \"AgentFnComponent\",\n    \"AgentInputComponent\",\n    \"ArgPackComponent\",\n    \"FnComponent\",\n    \"FunctionComponent\",\n    \"InputComponent\",\n    \"RouterComponent\",\n    \"ToolRunnerComponent\",\n    \"QueryPipeline\",\n    \"CustomAgentComponent\",\n    \"QueryComponent\",\n    \"Link\",\n    \"ChainableMixin\",\n    \"QueryComponent\",\n    \"CustomQueryComponent\",\n    \"StatefulFnComponent\",\n    \"LoopComponent\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/query.py",
    "filename": "query.py",
    "relpath": "query_pipeline/query.py",
    "start_line": 1,
    "end_line": 198,
    "length": 198,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "get_output",
      "add_output_to_module_inputs",
      "print_debug_input",
      "print_debug_input_multi",
      "clean_graph_attributes_copy",
      "get_stateful_components",
      "update_stateful_components",
      "get_and_update_stateful_components"
    ],
    "chunk_class_names": [
      "RunState"
    ],
    "document_function_names": [
      "__init__",
      "get_output",
      "add_output_to_module_inputs",
      "print_debug_input",
      "print_debug_input_multi",
      "clean_graph_attributes_copy",
      "get_stateful_components",
      "update_stateful_components",
      "get_and_update_stateful_components",
      "__init__",
      "set_state",
      "update_state",
      "reset_state",
      "_init_graph",
      "add_chain",
      "stateful_components",
      "add_links",
      "add_modules",
      "add",
      "add_link",
      "get_root_keys",
      "get_leaf_keys",
      "_get_root_keys",
      "_get_leaf_keys",
      "set_callback_manager",
      "run",
      "run_with_intermediates",
      "merge_dicts",
      "run_multi",
      "run_multi_with_intermediates",
      "arun",
      "arun_with_intermediates",
      "arun_multi",
      "arun_multi_with_intermediates",
      "_get_root_key_and_kwargs",
      "get_input_dict",
      "_get_single_result_output",
      "_run",
      "_arun",
      "_validate_inputs",
      "process_component_output",
      "get_next_module_keys",
      "get_run_state",
      "_run_multi",
      "_arun_multi",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components",
      "clean_dag"
    ],
    "document_class_names": [
      "RunState",
      "QueryPipeline"
    ],
    "content": "\"\"\"Query Pipeline.\"\"\"\n\nimport json\nimport uuid\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n    cast,\n    get_args,\n)\n\nimport networkx\n\nfrom llama_index.core.async_utils import asyncio_run, run_jobs\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.base.query_pipeline.query import (\n    QUERY_COMPONENT_TYPE,\n    ChainableMixin,\n    InputKeys,\n    Link,\n    OutputKeys,\n    QueryComponent,\n    ComponentIntermediates,\n)\nfrom llama_index.core.utils import print_text\nfrom llama_index.core.query_pipeline.components.stateful import BaseStatefulComponent\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\n# TODO: Make this (safely) pydantic?\nclass RunState:\n    def __init__(\n        self,\n        module_dict: Dict[str, QueryComponent],\n        module_input_dict: Dict[str, Dict[str, Any]],\n    ):\n        self.all_module_inputs: Dict[str, Dict[str, Any]] = {\n            module_key: {} for module_key in module_dict\n        }\n\n        for module_key, input_dict in module_input_dict.items():\n            self.all_module_inputs[module_key] = input_dict\n\n        self.module_dict = module_dict\n        self.result_outputs: Dict[str, Any] = {}\n        self.intermediate_outputs: Dict[str, ComponentIntermediates] = {}\n        self.executed_modules: Set[str] = set()\n\n\ndef get_output(\n    src_key: Optional[str],\n    output_dict: Dict[str, Any],\n) -> Any:\n    \"\"\"Add input to module deps inputs.\"\"\"\n    # get relevant output from link\n    if src_key is None:\n        # ensure that output_dict only has one key\n        if len(output_dict) != 1:\n            raise ValueError(\"Output dict must have exactly one key.\")\n        output = next(iter(output_dict.values()))\n    else:\n        output = output_dict[src_key]\n    return output\n\n\ndef add_output_to_module_inputs(\n    dest_key: str,\n    output: Any,\n    module: QueryComponent,\n    module_inputs: Dict[str, Any],\n) -> None:\n    \"\"\"Add input to module deps inputs.\"\"\"\n    # now attach output to relevant input key for module\n    if dest_key is None:\n        free_keys = module.free_req_input_keys\n        # ensure that there is only one remaining key given partials\n        if len(free_keys) != 1:\n            raise ValueError(\n                \"Module input keys must have exactly one key if \"\n                \"dest_key is not specified. Remaining keys: \"\n                f\"in module: {free_keys}\"\n            )\n        module_inputs[next(iter(free_keys))] = output\n    else:\n        module_inputs[dest_key] = output\n\n\ndef print_debug_input(\n    module_key: str,\n    input: Dict[str, Any],\n    val_str_len: int = 200,\n) -> None:\n    \"\"\"Print debug input.\"\"\"\n    output = f\"> Running module {module_key} with input: \\n\"\n    for key, value in input.items():\n        # stringify and truncate output\n        val_str = (\n            str(value)[:val_str_len] + \"...\"\n            if len(str(value)) > val_str_len\n            else str(value)\n        )\n        output += f\"{key}: {val_str}\\n\"\n\n    print_text(output + \"\\n\", color=\"llama_lavender\")\n\n\ndef print_debug_input_multi(\n    module_keys: List[str],\n    module_inputs: List[Dict[str, Any]],\n    val_str_len: int = 200,\n) -> None:\n    \"\"\"Print debug input.\"\"\"\n    output = f\"> Running modules and inputs in parallel: \\n\"\n    for module_key, input in zip(module_keys, module_inputs):\n        cur_output = f\"Module key: {module_key}. Input: \\n\"\n        for key, value in input.items():\n            # stringify and truncate output\n            val_str = (\n                str(value)[:val_str_len] + \"...\"\n                if len(str(value)) > val_str_len\n                else str(value)\n            )\n            cur_output += f\"{key}: {val_str}\\n\"\n        output += cur_output + \"\\n\"\n\n    print_text(output + \"\\n\", color=\"llama_lavender\")\n\n\n# Function to clean non-serializable attributes and return a copy of the graph\n# https://stackoverflow.com/questions/23268421/networkx-how-to-access-attributes-of-objects-as-nodes\ndef clean_graph_attributes_copy(graph: networkx.MultiDiGraph) -> networkx.MultiDiGraph:\n    # Create a deep copy of the graph to preserve the original\n    graph_copy = graph.copy()\n\n    # Iterate over nodes and clean attributes\n    for node, attributes in graph_copy.nodes(data=True):\n        for key, value in list(attributes.items()):\n            if callable(value):  # Checks if the value is a function\n                del attributes[key]  # Remove the attribute if it's non-serializable\n\n    # Similarly, you can extend this to clean edge attributes if necessary\n    for u, v, attributes in graph_copy.edges(data=True):\n        for key, value in list(attributes.items()):\n            if callable(value):  # Checks if the value is a function\n                del attributes[key]  # Remove the attribute if it's non-serializable\n\n    return graph_copy\n\n\ndef get_stateful_components(\n    query_component: QueryComponent,\n) -> List[BaseStatefulComponent]:\n    \"\"\"Get stateful components.\"\"\"\n    stateful_components: List[BaseStatefulComponent] = []\n    for c in query_component.sub_query_components:\n        if isinstance(c, BaseStatefulComponent):\n            stateful_components.append(cast(BaseStatefulComponent, c))\n\n        if len(c.sub_query_components) > 0:\n            stateful_components.extend(get_stateful_components(c))\n\n    return stateful_components\n\n\ndef update_stateful_components(\n    stateful_components: List[BaseStatefulComponent], state: Dict[str, Any]\n) -> None:\n    \"\"\"Update stateful components.\"\"\"\n    for stateful_component in stateful_components:\n        # stateful_component.partial(state=state)\n        stateful_component.state = state\n\n\ndef get_and_update_stateful_components(\n    query_component: QueryComponent, state: Dict[str, Any]\n) -> List[BaseStatefulComponent]:\n    \"\"\"Get and update stateful components.\n\n    Assign all stateful components in the query component with the state.\n\n    \"\"\"\n    stateful_components = get_stateful_components(query_component)\n    update_stateful_components(stateful_components, state)\n    return stateful_components\n\n\nCHAIN_COMPONENT_TYPE = Union[QUERY_COMPONENT_TYPE, str]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/query.py",
    "filename": "query.py",
    "relpath": "query_pipeline/query.py",
    "start_line": 198,
    "end_line": 201,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "QueryPipeline"
    ],
    "document_function_names": [
      "__init__",
      "get_output",
      "add_output_to_module_inputs",
      "print_debug_input",
      "print_debug_input_multi",
      "clean_graph_attributes_copy",
      "get_stateful_components",
      "update_stateful_components",
      "get_and_update_stateful_components",
      "__init__",
      "set_state",
      "update_state",
      "reset_state",
      "_init_graph",
      "add_chain",
      "stateful_components",
      "add_links",
      "add_modules",
      "add",
      "add_link",
      "get_root_keys",
      "get_leaf_keys",
      "_get_root_keys",
      "_get_leaf_keys",
      "set_callback_manager",
      "run",
      "run_with_intermediates",
      "merge_dicts",
      "run_multi",
      "run_multi_with_intermediates",
      "arun",
      "arun_with_intermediates",
      "arun_multi",
      "arun_multi_with_intermediates",
      "_get_root_key_and_kwargs",
      "get_input_dict",
      "_get_single_result_output",
      "_run",
      "_arun",
      "_validate_inputs",
      "process_component_output",
      "get_next_module_keys",
      "get_run_state",
      "_run_multi",
      "_arun_multi",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components",
      "clean_dag"
    ],
    "document_class_names": [
      "RunState",
      "QueryPipeline"
    ],
    "content": "class QueryPipeline(QueryComponent):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/query.py",
    "filename": "query.py",
    "relpath": "query_pipeline/query.py",
    "start_line": 201,
    "end_line": 595,
    "length": 395,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "set_state",
      "update_state",
      "reset_state",
      "_init_graph",
      "add_chain",
      "stateful_components",
      "add_links",
      "add_modules",
      "add",
      "add_link",
      "get_root_keys",
      "get_leaf_keys",
      "_get_root_keys",
      "_get_leaf_keys",
      "set_callback_manager",
      "run",
      "run_with_intermediates",
      "merge_dicts",
      "run_multi",
      "run_multi_with_intermediates",
      "arun",
      "arun_with_intermediates"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "get_output",
      "add_output_to_module_inputs",
      "print_debug_input",
      "print_debug_input_multi",
      "clean_graph_attributes_copy",
      "get_stateful_components",
      "update_stateful_components",
      "get_and_update_stateful_components",
      "__init__",
      "set_state",
      "update_state",
      "reset_state",
      "_init_graph",
      "add_chain",
      "stateful_components",
      "add_links",
      "add_modules",
      "add",
      "add_link",
      "get_root_keys",
      "get_leaf_keys",
      "_get_root_keys",
      "_get_leaf_keys",
      "set_callback_manager",
      "run",
      "run_with_intermediates",
      "merge_dicts",
      "run_multi",
      "run_multi_with_intermediates",
      "arun",
      "arun_with_intermediates",
      "arun_multi",
      "arun_multi_with_intermediates",
      "_get_root_key_and_kwargs",
      "get_input_dict",
      "_get_single_result_output",
      "_run",
      "_arun",
      "_validate_inputs",
      "process_component_output",
      "get_next_module_keys",
      "get_run_state",
      "_run_multi",
      "_arun_multi",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components",
      "clean_dag"
    ],
    "document_class_names": [
      "RunState",
      "QueryPipeline"
    ],
    "content": "\"\"\"A query pipeline that can allow arbitrary chaining of different modules.\n\n    A pipeline itself is a query component, and can be used as a module in another pipeline.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n\n    module_dict: Dict[str, QueryComponent] = Field(\n        default_factory=dict, description=\"The modules in the pipeline.\"\n    )\n    dag: networkx.MultiDiGraph = Field(\n        default_factory=networkx.MultiDiGraph, description=\"The DAG of the pipeline.\"\n    )\n    verbose: bool = Field(\n        default=False, description=\"Whether to print intermediate steps.\"\n    )\n    show_progress: bool = Field(\n        default=False,\n        description=\"Whether to show progress bar (currently async only).\",\n    )\n    num_workers: int = Field(\n        default=4, description=\"Number of workers to use (currently async only).\"\n    )\n    state: Dict[str, Any] = Field(\n        default_factory=dict, description=\"State of the pipeline.\"\n    )\n\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager] = None,\n        chain: Optional[Sequence[CHAIN_COMPONENT_TYPE]] = None,\n        modules: Optional[Dict[str, QUERY_COMPONENT_TYPE]] = None,\n        links: Optional[List[Link]] = None,\n        state: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ):\n        state = state or {}\n        super().__init__(\n            callback_manager=callback_manager or CallbackManager([]),\n            state=state,\n            **kwargs,\n        )\n\n        self._init_graph(chain=chain, modules=modules, links=links)\n        # Pydantic validator isn't called for __init__ so we need to call it manually\n        get_and_update_stateful_components(self, state)\n\n    def set_state(self, state: Dict[str, Any]) -> None:\n        \"\"\"Set state.\"\"\"\n        self.state = state\n        get_and_update_stateful_components(self, state)\n\n    def update_state(self, state: Dict[str, Any]) -> None:\n        \"\"\"Update state.\"\"\"\n        self.state.update(state)\n        get_and_update_stateful_components(self, state)\n\n    def reset_state(self) -> None:\n        \"\"\"Reset state.\"\"\"\n        # use pydantic validator to update state\n        self.set_state({})\n\n    def _init_graph(\n        self,\n        chain: Optional[Sequence[CHAIN_COMPONENT_TYPE]] = None,\n        modules: Optional[Dict[str, QUERY_COMPONENT_TYPE]] = None,\n        links: Optional[List[Link]] = None,\n    ) -> None:\n        \"\"\"Initialize graph.\"\"\"\n        if chain is not None:\n            if modules is not None or links is not None:\n                raise ValueError(\"Cannot specify both chain and modules/links in init.\")\n            self.add_chain(chain)\n        elif modules is not None:\n            self.add_modules(modules)\n            if links is not None:\n                for link in links:\n                    self.add_link(**link.model_dump())\n\n    def add_chain(self, chain: Sequence[CHAIN_COMPONENT_TYPE]) -> None:\n        \"\"\"Add a chain of modules to the pipeline.\n\n        This is a special form of pipeline that is purely sequential/linear.\n        This allows a more concise way of specifying a pipeline.\n\n        \"\"\"\n        # first add all modules\n        module_keys: List[str] = []\n        for module in chain:\n            if isinstance(module, get_args(QUERY_COMPONENT_TYPE)):\n                module_key = str(uuid.uuid4())\n                self.add(module_key, cast(QUERY_COMPONENT_TYPE, module))\n                module_keys.append(module_key)\n            elif isinstance(module, str):\n                module_keys.append(module)\n            else:\n                raise ValueError(\"Chain must be a sequence of modules or module keys.\")\n\n        # then add all links\n        for i in range(len(chain) - 1):\n            self.add_link(src=module_keys[i], dest=module_keys[i + 1])\n\n    @property\n    def stateful_components(self) -> List[BaseStatefulComponent]:\n        \"\"\"Get stateful component.\"\"\"\n        return get_stateful_components(self)\n\n    def add_links(\n        self,\n        links: List[Link],\n    ) -> None:\n        \"\"\"Add links to the pipeline.\"\"\"\n        for link in links:\n            if isinstance(link, Link):\n                self.add_link(**link.model_dump())\n            else:\n                raise ValueError(\"Link must be of type `Link` or `ConditionalLinks`.\")\n\n    def add_modules(self, module_dict: Dict[str, QUERY_COMPONENT_TYPE]) -> None:\n        \"\"\"Add modules to the pipeline.\"\"\"\n        for module_key, module in module_dict.items():\n            self.add(module_key, module)\n\n    def add(self, module_key: str, module: QUERY_COMPONENT_TYPE) -> None:\n        \"\"\"Add a module to the pipeline.\"\"\"\n        # if already exists, raise error\n        if module_key in self.module_dict:\n            raise ValueError(f\"Module {module_key} already exists in pipeline.\")\n\n        if isinstance(module, ChainableMixin):\n            module = module.as_query_component()\n        else:\n            pass\n\n        self.module_dict[module_key] = cast(QueryComponent, module)\n        self.dag.add_node(module_key)\n        # propagate state to new modules added\n        # TODO: there's more efficient ways to do this\n        get_and_update_stateful_components(self, self.state)\n\n    def add_link(\n        self,\n        src: str,\n        dest: str,\n        src_key: Optional[str] = None,\n        dest_key: Optional[str] = None,\n        condition_fn: Optional[Callable] = None,\n        input_fn: Optional[Callable] = None,\n    ) -> None:\n        \"\"\"Add a link between two modules.\"\"\"\n        if src not in self.module_dict:\n            raise ValueError(f\"Module {src} does not exist in pipeline.\")\n        self.dag.add_edge(\n            src,\n            dest,\n            src_key=src_key,\n            dest_key=dest_key,\n            condition_fn=condition_fn,\n            input_fn=input_fn,\n        )\n\n    def get_root_keys(self) -> List[str]:\n        \"\"\"Get root keys.\"\"\"\n        return self._get_root_keys()\n\n    def get_leaf_keys(self) -> List[str]:\n        \"\"\"Get leaf keys.\"\"\"\n        return self._get_leaf_keys()\n\n    def _get_root_keys(self) -> List[str]:\n        \"\"\"Get root keys.\"\"\"\n        return [v for v, d in self.dag.in_degree() if d == 0]\n\n    def _get_leaf_keys(self) -> List[str]:\n        \"\"\"Get leaf keys.\"\"\"\n        # get all modules without downstream dependencies\n        return [v for v, d in self.dag.out_degree() if d == 0]\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # go through every module in module dict and set callback manager\n        self.callback_manager = callback_manager\n        for module in self.module_dict.values():\n            module.set_callback_manager(callback_manager)\n\n    @dispatcher.span\n    def run(\n        self,\n        *args: Any,\n        return_values_direct: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        batch: bool = False,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the pipeline.\"\"\"\n        # first set callback manager\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            # try to get query payload\n            try:\n                query_payload = json.dumps(kwargs)\n            except TypeError:\n                query_payload = json.dumps(str(kwargs))\n            with self.callback_manager.event(\n                CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_payload}\n            ) as query_event:\n                outputs, _ = self._run(\n                    *args,\n                    return_values_direct=return_values_direct,\n                    show_intermediates=False,\n                    batch=batch,\n                    **kwargs,\n                )\n\n                return outputs\n\n    def run_with_intermediates(\n        self,\n        *args: Any,\n        return_values_direct: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        batch: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Tuple[Any, Dict[str, ComponentIntermediates]]:\n        \"\"\"Run the pipeline.\"\"\"\n        if batch is not None:\n            raise ValueError(\"Batch is not supported for run_with_intermediates.\")\n\n        # first set callback manager\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            # try to get query payload\n            try:\n                query_payload = json.dumps(kwargs)\n            except TypeError:\n                query_payload = json.dumps(str(kwargs))\n            with self.callback_manager.event(\n                CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_payload}\n            ) as query_event:\n                return self._run(\n                    *args,\n                    return_values_direct=return_values_direct,\n                    show_intermediates=True,\n                    **kwargs,\n                )\n\n    def merge_dicts(self, d1: Dict[str, Any], d2: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Merge two dictionaries recursively, combining values of the same key into a list.\"\"\"\n        merged = {}\n        for key in set(d1).union(d2):\n            if key in d1 and key in d2:\n                if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                    merged[key] = self.merge_dicts(d1[key], d2[key])\n                else:\n                    new_val = [d1[key]] if not isinstance(d1[key], list) else d1[key]\n                    assert isinstance(new_val, list)\n\n                    new_val.append(d2[key])\n                    merged[key] = new_val  # type: ignore[assignment]\n            else:\n                merged[key] = d1.get(key, d2.get(key))\n        return merged\n\n    def run_multi(\n        self,\n        module_input_dict: Dict[str, Any],\n        callback_manager: Optional[CallbackManager] = None,\n        batch: bool = False,\n    ) -> Dict[str, Any]:\n        \"\"\"Run the pipeline for multiple roots.\"\"\"\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            with self.callback_manager.event(\n                CBEventType.QUERY,\n                payload={EventPayload.QUERY_STR: json.dumps(module_input_dict)},\n            ) as query_event:\n                if batch:\n                    outputs: Dict[str, Any] = {}\n\n                    batch_lengths = {\n                        len(values)\n                        for subdict in module_input_dict.values()\n                        for values in subdict.values()\n                    }\n\n                    if len(batch_lengths) != 1:\n                        raise ValueError(\"Length of batch inputs must be the same.\")\n\n                    batch_size = next(iter(batch_lengths))\n\n                    # List individual outputs from batch multi input.\n                    inputs = [\n                        {\n                            key: {\n                                inner_key: inner_val[i]\n                                for inner_key, inner_val in value.items()\n                            }\n                            for key, value in module_input_dict.items()\n                        }\n                        for i in range(batch_size)\n                    ]\n                    jobs = [self._arun_multi(input) for input in inputs]\n                    results = asyncio_run(run_jobs(jobs, workers=len(jobs)))\n\n                    for result in results:\n                        outputs = self.merge_dicts(outputs, result[0])\n\n                    return outputs\n                else:\n                    outputs, _ = self._run_multi(module_input_dict)\n                    return outputs\n\n    def run_multi_with_intermediates(\n        self,\n        module_input_dict: Dict[str, Any],\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> Tuple[Dict[str, Any], Dict[str, ComponentIntermediates]]:\n        \"\"\"Run the pipeline for multiple roots.\"\"\"\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            with self.callback_manager.event(\n                CBEventType.QUERY,\n                payload={EventPayload.QUERY_STR: json.dumps(module_input_dict)},\n            ) as query_event:\n                return self._run_multi(module_input_dict, show_intermediates=True)\n\n    @dispatcher.span\n    async def arun(\n        self,\n        *args: Any,\n        return_values_direct: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        batch: bool = False,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the pipeline.\"\"\"\n        # first set callback manager\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            try:\n                query_payload = json.dumps(kwargs)\n            except TypeError:\n                query_payload = json.dumps(str(kwargs))\n            with self.callback_manager.event(\n                CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_payload}\n            ) as query_event:\n                outputs, _ = await self._arun(\n                    *args,\n                    return_values_direct=return_values_direct,\n                    show_intermediates=False,\n                    batch=batch,\n                    **kwargs,\n                )\n\n                return outputs\n\n    async def arun_with_intermediates(\n        self,\n        *args: Any,\n        return_values_direct: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        batch: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Tuple[Any, Dict[str, ComponentIntermediates]]:\n        \"\"\"Run the pipeline.\"\"\"\n        if batch is not None:\n            raise ValueError(\"Batch is not supported for run_with_intermediates.\")\n\n        # first set callback manager\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            try:\n                query_payload = json.dumps(kwargs)\n            except TypeError:\n                query_payload = json.dumps(str(kwargs))\n            with self.callback_manager.event(\n                CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_payload}\n            ) as query_event:\n                return await self._arun(\n                    *args,\n                    return_values_direct=return_values_direct,\n                    show_intermediates=True,\n                    **kwargs,\n                )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/query.py",
    "filename": "query.py",
    "relpath": "query_pipeline/query.py",
    "start_line": 595,
    "end_line": 1038,
    "length": 444,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "arun_multi",
      "arun_multi_with_intermediates",
      "_get_root_key_and_kwargs",
      "get_input_dict",
      "_get_single_result_output",
      "_run",
      "_arun",
      "_validate_inputs",
      "process_component_output",
      "get_next_module_keys",
      "get_run_state",
      "_run_multi",
      "_arun_multi",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_outputs",
      "_run_component",
      "_arun_component"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "get_output",
      "add_output_to_module_inputs",
      "print_debug_input",
      "print_debug_input_multi",
      "clean_graph_attributes_copy",
      "get_stateful_components",
      "update_stateful_components",
      "get_and_update_stateful_components",
      "__init__",
      "set_state",
      "update_state",
      "reset_state",
      "_init_graph",
      "add_chain",
      "stateful_components",
      "add_links",
      "add_modules",
      "add",
      "add_link",
      "get_root_keys",
      "get_leaf_keys",
      "_get_root_keys",
      "_get_leaf_keys",
      "set_callback_manager",
      "run",
      "run_with_intermediates",
      "merge_dicts",
      "run_multi",
      "run_multi_with_intermediates",
      "arun",
      "arun_with_intermediates",
      "arun_multi",
      "arun_multi_with_intermediates",
      "_get_root_key_and_kwargs",
      "get_input_dict",
      "_get_single_result_output",
      "_run",
      "_arun",
      "_validate_inputs",
      "process_component_output",
      "get_next_module_keys",
      "get_run_state",
      "_run_multi",
      "_arun_multi",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components",
      "clean_dag"
    ],
    "document_class_names": [
      "RunState",
      "QueryPipeline"
    ],
    "content": "async def arun_multi(\n        self,\n        module_input_dict: Dict[str, Any],\n        callback_manager: Optional[CallbackManager] = None,\n        batch: bool = False,\n    ) -> Dict[str, Any]:\n        \"\"\"Run the pipeline for multiple roots.\"\"\"\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            with self.callback_manager.event(\n                CBEventType.QUERY,\n                payload={EventPayload.QUERY_STR: json.dumps(module_input_dict)},\n            ) as query_event:\n                if batch:\n                    outputs: Dict[str, Any] = {}\n\n                    batch_lengths = {\n                        len(values)\n                        for subdict in module_input_dict.values()\n                        for values in subdict.values()\n                    }\n\n                    if len(batch_lengths) != 1:\n                        raise ValueError(\"Length of batch inputs must be the same.\")\n\n                    batch_size = next(iter(batch_lengths))\n\n                    # List individual outputs from batch multi input.\n                    inputs = [\n                        {\n                            key: {\n                                inner_key: inner_val[i]\n                                for inner_key, inner_val in value.items()\n                            }\n                            for key, value in module_input_dict.items()\n                        }\n                        for i in range(batch_size)\n                    ]\n\n                    jobs = [self._arun_multi(input) for input in inputs]\n                    results = await run_jobs(jobs, workers=len(jobs))\n\n                    for result in results:\n                        outputs = self.merge_dicts(outputs, result[0])\n\n                    return outputs\n                else:\n                    outputs, _ = await self._arun_multi(module_input_dict)\n                    return outputs\n\n    async def arun_multi_with_intermediates(\n        self,\n        module_input_dict: Dict[str, Any],\n        callback_manager: Optional[CallbackManager] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Run the pipeline for multiple roots.\"\"\"\n        callback_manager = callback_manager or self.callback_manager\n        self.set_callback_manager(callback_manager)\n        with self.callback_manager.as_trace(\"query\"):\n            with self.callback_manager.event(\n                CBEventType.QUERY,\n                payload={EventPayload.QUERY_STR: json.dumps(module_input_dict)},\n            ) as query_event:\n                outputs, _ = await self._arun_multi(\n                    module_input_dict, show_intermediates=True\n                )\n                return outputs\n\n    def _get_root_key_and_kwargs(\n        self, *args: Any, **kwargs: Any\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Get root key and kwargs.\n\n        This is for `_run`.\n\n        \"\"\"\n        ## run pipeline\n        ## assume there is only one root - for multiple roots, need to specify `run_multi`\n        root_keys = self._get_root_keys()\n        if len(root_keys) != 1:\n            raise ValueError(\"Only one root is supported.\")\n        root_key = root_keys[0]\n\n        root_module = self.module_dict[root_key]\n        if len(args) > 0:\n            # if args is specified, validate. only one arg is allowed, and there can only be one free\n            # input key in the module\n            if len(args) > 1:\n                raise ValueError(\"Only one arg is allowed.\")\n            if len(kwargs) > 0:\n                raise ValueError(\"No kwargs allowed if args is specified.\")\n            if len(root_module.free_req_input_keys) != 1:\n                raise ValueError(\"Only one free input key is allowed.\")\n            # set kwargs\n            kwargs[next(iter(root_module.free_req_input_keys))] = args[0]\n\n        # if one kwarg and module only needs one kwarg, align them\n        if len(root_module.free_req_input_keys) == 1 and len(kwargs) == 1:\n            module_input_key = next(iter(root_module.free_req_input_keys))\n            kwarg = next(iter(kwargs.values()))\n            kwargs = {module_input_key: kwarg}\n\n        return root_key, kwargs\n\n    def get_input_dict(self, *args: Any, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Get input dict.\"\"\"\n        root_key, kwargs = self._get_root_key_and_kwargs(*args, **kwargs)\n        return {root_key: kwargs}\n\n    def _get_single_result_output(\n        self,\n        result_outputs: Dict[str, Any],\n        return_values_direct: bool,\n    ) -> Any:\n        \"\"\"Get result output from a single module.\n\n        If output dict is a single key, return the value directly\n        if return_values_direct is True.\n\n        \"\"\"\n        if len(result_outputs) != 1:\n            raise ValueError(\"Only one output is supported.\")\n\n        result_output = next(iter(result_outputs.values()))\n        # return_values_direct: if True, return the value directly\n        # without the key\n        # if it's a dict with one key, return the value\n        if (\n            isinstance(result_output, dict)\n            and len(result_output) == 1\n            and return_values_direct\n        ):\n            return next(iter(result_output.values()))\n        else:\n            return result_output\n\n    @dispatcher.span\n    def _run(\n        self,\n        *args: Any,\n        return_values_direct: bool = True,\n        show_intermediates: bool = False,\n        batch: bool = False,\n        **kwargs: Any,\n    ) -> Tuple[Any, Dict[str, ComponentIntermediates]]:\n        \"\"\"Run the pipeline.\n\n        Assume that there is a single root module and a single output module.\n\n        For multi-input and multi-outputs, please see `run_multi`.\n\n        \"\"\"\n        root_key, kwargs = self._get_root_key_and_kwargs(*args, **kwargs)\n\n        if batch:\n            result_outputs = []\n            intermediates = []\n\n            if len({len(value) for value in kwargs.values()}) != 1:\n                raise ValueError(\"Length of batch inputs must be the same.\")\n\n            # List of individual inputs from batch input\n            kwargs_list = [\n                dict(zip(kwargs.keys(), values)) for values in zip(*kwargs.values())\n            ]\n\n            jobs = [\n                self._arun_multi(\n                    {root_key: kwarg}, show_intermediates=show_intermediates\n                )\n                for kwarg in kwargs_list\n            ]\n\n            results = asyncio_run(run_jobs(jobs, workers=len(jobs)))\n\n            for result in results:\n                result_outputs.append(\n                    self._get_single_result_output(result[0], return_values_direct)\n                )\n                intermediates.append(result[1])\n\n            return result_outputs, intermediates  # type: ignore[return-value]\n        else:\n            result_output_dicts, intermediate_dicts = self._run_multi(\n                {root_key: kwargs}, show_intermediates=show_intermediates\n            )\n\n            return (\n                self._get_single_result_output(\n                    result_output_dicts, return_values_direct\n                ),\n                intermediate_dicts,\n            )\n\n    @dispatcher.span\n    async def _arun(\n        self,\n        *args: Any,\n        return_values_direct: bool = True,\n        show_intermediates: bool = False,\n        batch: bool = False,\n        **kwargs: Any,\n    ) -> Tuple[Any, Dict[str, ComponentIntermediates]]:\n        \"\"\"Run the pipeline.\n\n        Assume that there is a single root module and a single output module.\n\n        For multi-input and multi-outputs, please see `run_multi`.\n\n        \"\"\"\n        root_key, kwargs = self._get_root_key_and_kwargs(*args, **kwargs)\n\n        if batch:\n            result_outputs = []\n            intermediates = []\n\n            if len({len(value) for value in kwargs.values()}) != 1:\n                raise ValueError(\"Length of batch inputs must be the same.\")\n\n            # List of individual inputs from batch input\n            kwargs_list = [\n                dict(zip(kwargs.keys(), values)) for values in zip(*kwargs.values())\n            ]\n\n            jobs = [\n                self._arun_multi(\n                    {root_key: kwarg}, show_intermediates=show_intermediates\n                )\n                for kwarg in kwargs_list\n            ]\n\n            results = await run_jobs(jobs, workers=len(jobs))\n\n            for result in results:\n                result_outputs.append(\n                    self._get_single_result_output(result[0], return_values_direct)\n                )\n                intermediates.append(result[1])\n\n            return result_outputs, intermediates  # type: ignore[return-value]\n        else:\n            result_output_dicts, intermediate_dicts = await self._arun_multi(\n                {root_key: kwargs}, show_intermediates=show_intermediates\n            )\n\n            return (\n                self._get_single_result_output(\n                    result_output_dicts, return_values_direct\n                ),\n                intermediate_dicts,\n            )\n\n    def _validate_inputs(self, module_input_dict: Dict[str, Any]) -> None:\n        root_keys = self._get_root_keys()\n        # if root keys don't match up with kwargs keys, raise error\n        if set(root_keys) != set(module_input_dict.keys()):\n            raise ValueError(\n                \"Expected root keys do not match up with input keys.\\n\"\n                f\"Expected root keys: {root_keys}\\n\"\n                f\"Input keys: {module_input_dict.keys()}\\n\"\n            )\n\n    def process_component_output(\n        self,\n        output_dict: Dict[str, Any],\n        module_key: str,\n        run_state: RunState,\n    ) -> None:\n        \"\"\"Process component output.\"\"\"\n        if module_key in self._get_leaf_keys():\n            run_state.result_outputs[module_key] = output_dict\n        else:\n            edge_list = list(self.dag.edges(module_key, data=True))\n\n            for _, dest, attr in edge_list:\n                if dest in run_state.executed_modules:\n                    continue  # Skip already executed modules\n\n                output = get_output(attr.get(\"src_key\"), output_dict)\n\n                if attr[\"input_fn\"] is not None:\n                    dest_output = attr[\"input_fn\"](output)\n                else:\n                    dest_output = output\n\n                if attr[\"condition_fn\"] is None or attr[\"condition_fn\"](output):\n                    add_output_to_module_inputs(\n                        attr.get(\"dest_key\"),\n                        dest_output,\n                        self.module_dict[dest],\n                        run_state.all_module_inputs[dest],\n                    )\n\n        run_state.executed_modules.add(module_key)\n\n    def get_next_module_keys(self, run_state: RunState) -> List[str]:\n        \"\"\"Determine the next module keys to run based on the current state.\"\"\"\n        next_module_keys = []\n\n        for module_key, module_input in run_state.all_module_inputs.items():\n            if module_key in run_state.executed_modules:\n                continue  # Module already executed\n\n            if all(\n                key in module_input\n                for key in self.module_dict[module_key].free_req_input_keys\n            ):\n                next_module_keys.append(module_key)\n\n        return next_module_keys\n\n    def get_run_state(\n        self, module_input_dict: Optional[Dict[str, Any]] = None, **pipeline_inputs: Any\n    ) -> RunState:\n        \"\"\"Get run state.\"\"\"\n        if module_input_dict is not None:\n            return RunState(self.module_dict, module_input_dict)\n        else:\n            root_key, kwargs = self._get_root_key_and_kwargs(**pipeline_inputs)\n            return RunState(self.module_dict, {root_key: kwargs})\n\n    @dispatcher.span\n    def _run_multi(\n        self, module_input_dict: Dict[str, Any], show_intermediates: bool = False\n    ) -> Tuple[Dict[str, Any], Dict[str, ComponentIntermediates]]:\n        \"\"\"Run the pipeline for multiple roots.\"\"\"\n        self._validate_inputs(module_input_dict)\n\n        run_state = self.get_run_state(module_input_dict)\n\n        # Add root inputs to all_module_inputs\n        next_module_keys = self.get_next_module_keys(run_state)\n\n        while True:\n            for module_key in next_module_keys:\n                module = run_state.module_dict[module_key]\n                module_input = run_state.all_module_inputs[module_key]\n\n                if self.verbose:\n                    print_debug_input(module_key, module_input)\n                output_dict = module.run_component(**module_input)\n\n                if (\n                    show_intermediates\n                    and module_key not in run_state.intermediate_outputs\n                ):\n                    run_state.intermediate_outputs[module_key] = ComponentIntermediates(\n                        inputs=module_input, outputs=output_dict\n                    )\n\n                self.process_component_output(\n                    output_dict,\n                    module_key,\n                    run_state,\n                )\n\n            next_module_keys = self.get_next_module_keys(\n                run_state,\n            )\n            if not next_module_keys:\n                run_state.result_outputs[module_key] = output_dict\n                break\n\n        return run_state.result_outputs, run_state.intermediate_outputs\n\n    @dispatcher.span\n    async def _arun_multi(\n        self, module_input_dict: Dict[str, Any], show_intermediates: bool = False\n    ) -> Tuple[Dict[str, Any], Dict[str, ComponentIntermediates]]:\n        \"\"\"Run the pipeline for multiple roots.\n\n        kwargs is in the form of module_dict -> input_dict\n        input_dict is in the form of input_key -> input\n\n        \"\"\"\n        self._validate_inputs(module_input_dict)\n\n        run_state = self.get_run_state(module_input_dict)\n\n        # Add root inputs to all_module_inputs\n        next_module_keys = self.get_next_module_keys(run_state)\n\n        while True:\n            jobs = []\n            for module_key in next_module_keys:\n                module = run_state.module_dict[module_key]\n                module_input = run_state.all_module_inputs[module_key]\n\n                if self.verbose:\n                    print_debug_input(module_key, module_input)\n\n                jobs.append(module.arun_component(**module_input))\n\n            output_dicts = await run_jobs(jobs, show_progress=self.show_progress)\n            for module_key, output_dict in zip(next_module_keys, output_dicts):\n                if (\n                    show_intermediates\n                    and module_key not in run_state.intermediate_outputs\n                ):\n                    run_state.intermediate_outputs[module_key] = ComponentIntermediates(\n                        inputs=module_input, outputs=output_dict\n                    )\n\n                self.process_component_output(\n                    output_dict,\n                    module_key,\n                    run_state,\n                )\n\n            next_module_keys = self.get_next_module_keys(\n                run_state,\n            )\n            if not next_module_keys:\n                run_state.result_outputs[module_key] = output_dicts[-1]\n                break\n\n        return run_state.result_outputs, run_state.intermediate_outputs\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        raise NotImplementedError\n\n    def validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs.\"\"\"\n        return input\n\n    def _validate_component_outputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs.\"\"\"\n        # NOTE: we override this to do nothing\n        return output\n\n    def _run_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        return self.run(return_values_direct=False, **kwargs)\n\n    async def _arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        return await self.arun(return_values_direct=False, **kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/query.py",
    "filename": "query.py",
    "relpath": "query_pipeline/query.py",
    "start_line": 1038,
    "end_line": 1068,
    "length": 31,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "input_keys",
      "output_keys",
      "sub_query_components",
      "clean_dag"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "get_output",
      "add_output_to_module_inputs",
      "print_debug_input",
      "print_debug_input_multi",
      "clean_graph_attributes_copy",
      "get_stateful_components",
      "update_stateful_components",
      "get_and_update_stateful_components",
      "__init__",
      "set_state",
      "update_state",
      "reset_state",
      "_init_graph",
      "add_chain",
      "stateful_components",
      "add_links",
      "add_modules",
      "add",
      "add_link",
      "get_root_keys",
      "get_leaf_keys",
      "_get_root_keys",
      "_get_leaf_keys",
      "set_callback_manager",
      "run",
      "run_with_intermediates",
      "merge_dicts",
      "run_multi",
      "run_multi_with_intermediates",
      "arun",
      "arun_with_intermediates",
      "arun_multi",
      "arun_multi_with_intermediates",
      "_get_root_key_and_kwargs",
      "get_input_dict",
      "_get_single_result_output",
      "_run",
      "_arun",
      "_validate_inputs",
      "process_component_output",
      "get_next_module_keys",
      "get_run_state",
      "_run_multi",
      "_arun_multi",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components",
      "clean_dag"
    ],
    "document_class_names": [
      "RunState",
      "QueryPipeline"
    ],
    "content": "@property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # get input key of first module\n        root_keys = self._get_root_keys()\n        if len(root_keys) != 1:\n            raise ValueError(\"Only one root is supported.\")\n        root_module = self.module_dict[root_keys[0]]\n        return root_module.input_keys\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        # get output key of last module\n        leaf_keys = self._get_leaf_keys()\n        if len(leaf_keys) != 1:\n            raise ValueError(\"Only one leaf is supported.\")\n        leaf_module = self.module_dict[leaf_keys[0]]\n        return leaf_module.output_keys\n\n    @property\n    def sub_query_components(self) -> List[QueryComponent]:\n        \"\"\"Sub query components.\"\"\"\n        return list(self.module_dict.values())\n\n    @property\n    def clean_dag(self) -> networkx.DiGraph:\n        \"\"\"Clean dag.\"\"\"\n        return clean_graph_attributes_copy(self.dag)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/tool_runner.py",
    "filename": "tool_runner.py",
    "relpath": "query_pipeline/components/tool_runner.py",
    "start_line": 1,
    "end_line": 106,
    "length": 106,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "ToolRunnerComponent"
    ],
    "document_function_names": [
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "ToolRunnerComponent"
    ],
    "content": "\"\"\"Tool runner component.\"\"\"\n\nfrom typing import Any, Dict, Optional, Sequence, cast\n\nfrom llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    CBEventType,\n    EventPayload,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.tools import AsyncBaseTool, adapt_to_async_tool\n\n\nclass ToolRunnerComponent(QueryComponent):\n    \"\"\"Tool runner component that takes in a set of tools.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    tool_dict: Dict[str, AsyncBaseTool] = Field(\n        ..., description=\"Dictionary of tool names to tools.\"\n    )\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n\n    def __init__(\n        self,\n        tools: Sequence[AsyncBaseTool],\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        # determine parameters\n        tool_dict = {tool.metadata.name: adapt_to_async_tool(tool) for tool in tools}\n        callback_manager = callback_manager or CallbackManager([])\n        super().__init__(\n            tool_dict=tool_dict, callback_manager=callback_manager, **kwargs\n        )\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.callback_manager = callback_manager\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        if \"tool_name\" not in input:\n            raise ValueError(\"tool_name must be provided in input\")\n\n        input[\"tool_name\"] = validate_and_convert_stringable(input[\"tool_name\"])\n\n        if \"tool_input\" not in input:\n            raise ValueError(\"tool_input must be provided in input\")\n        # make sure tool_input is a dictionary\n        if not isinstance(input[\"tool_input\"], dict):\n            raise ValueError(\"tool_input must be a dictionary\")\n\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Dict:\n        \"\"\"Run component.\"\"\"\n        tool_name = kwargs[\"tool_name\"]\n        tool_input = kwargs[\"tool_input\"]\n        tool = cast(AsyncBaseTool, self.tool_dict[tool_name])\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: tool_input,\n                EventPayload.TOOL: tool.metadata,\n            },\n        ) as event:\n            tool_output = tool(**tool_input)\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n\n        return {\"output\": tool_output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        tool_name = kwargs[\"tool_name\"]\n        tool_input = kwargs[\"tool_input\"]\n        tool = cast(AsyncBaseTool, self.tool_dict[tool_name])\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: tool_input,\n                EventPayload.TOOL: tool.metadata,\n            },\n        ) as event:\n            tool_output = await tool.acall(**tool_input)\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n        return {\"output\": tool_output}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"tool_name\", \"tool_input\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/function.py",
    "filename": "function.py",
    "relpath": "query_pipeline/components/function.py",
    "start_line": 1,
    "end_line": 130,
    "length": 130,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_parameters",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "FnComponent"
    ],
    "document_function_names": [
      "get_parameters",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "FnComponent"
    ],
    "content": "\"\"\"Function components.\"\"\"\n\nfrom inspect import signature\nfrom typing import Any, Callable, Dict, Optional, Set, Tuple\nfrom typing_extensions import Annotated\n\nfrom llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n)\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    PrivateAttr,\n    ConfigDict,\n    WithJsonSchema,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\n\nAnnotatedCallable = Annotated[\n    Callable,\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n]\n\n\ndef get_parameters(fn: Callable) -> Tuple[Set[str], Set[str]]:\n    \"\"\"Get parameters from function.\n\n    Returns:\n        Tuple[Set[str], Set[str]]: required and optional parameters\n\n    \"\"\"\n    # please write function below\n    params = signature(fn).parameters\n    required_params = set()\n    optional_params = set()\n    for param_name in params:\n        param_default = params[param_name].default\n        if param_default is params[param_name].empty:\n            required_params.add(param_name)\n        else:\n            optional_params.add(param_name)\n    return required_params, optional_params\n\n\nclass FnComponent(QueryComponent):\n    \"\"\"Query component that takes in an arbitrary function.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    fn: AnnotatedCallable = Field(..., description=\"Function to run.\")\n    async_fn: Optional[AnnotatedCallable] = Field(\n        None, description=\"Async function to run. If not provided, will run `fn`.\"\n    )\n    output_key: str = Field(\n        default=\"output\", description=\"Output key for component output.\"\n    )\n\n    _req_params: Set[str] = PrivateAttr()\n    _opt_params: Set[str] = PrivateAttr()\n\n    def __init__(\n        self,\n        fn: Callable,\n        async_fn: Optional[Callable] = None,\n        req_params: Optional[Set[str]] = None,\n        opt_params: Optional[Set[str]] = None,\n        output_key: str = \"output\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        # determine parameters\n        super().__init__(fn=fn, async_fn=async_fn, output_key=output_key, **kwargs)\n        default_req_params, default_opt_params = get_parameters(fn)\n        if req_params is None:\n            req_params = default_req_params\n        if opt_params is None:\n            opt_params = default_opt_params\n\n        self._req_params = req_params\n        self._opt_params = opt_params\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: implement\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # check that all required parameters are present\n        missing_params = self._req_params - set(input.keys())\n        if missing_params:\n            raise ValueError(\n                f\"Missing required parameters: {missing_params}. \"\n                f\"Input keys: {input.keys()}\"\n            )\n\n        # check that no extra parameters are present\n        extra_params = set(input.keys()) - self._req_params - self._opt_params\n        if extra_params:\n            raise ValueError(\n                f\"Extra parameters: {extra_params}. \" f\"Input keys: {input.keys()}\"\n            )\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Dict:\n        \"\"\"Run component.\"\"\"\n        return {self.output_key: self.fn(**kwargs)}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        if self.async_fn is None:\n            return self._run_component(**kwargs)\n        else:\n            return {self.output_key: await self.async_fn(**kwargs)}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys(\n            required_keys=self._req_params, optional_keys=self._opt_params\n        )\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({self.output_key})\n\n\n# alias\nFunctionComponent = FnComponent"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/input.py",
    "filename": "input.py",
    "relpath": "query_pipeline/components/input.py",
    "start_line": 1,
    "end_line": 52,
    "length": 52,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_inputs",
      "validate_component_outputs",
      "set_callback_manager",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "InputComponent"
    ],
    "document_function_names": [
      "_validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_inputs",
      "validate_component_outputs",
      "set_callback_manager",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "InputComponent"
    ],
    "content": "\"\"\"Input components.\"\"\"\n\nfrom typing import Any, Dict\n\nfrom llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n)\n\n\nclass InputComponent(QueryComponent):\n    \"\"\"Input component.\"\"\"\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        return input\n\n    def _validate_component_outputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        return input\n\n    def validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs.\"\"\"\n        # NOTE: we override this to do nothing\n        return input\n\n    def validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs.\"\"\"\n        # NOTE: we override this to do nothing\n        return output\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        return kwargs\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        return self._run_component(**kwargs)\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # NOTE: this shouldn't be used\n        return InputKeys.from_keys(set(), optional_keys=set())\n        # return InputComponentKeys.from_keys(set(), optional_keys=set())\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys(set())"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/agent.py",
    "filename": "agent.py",
    "relpath": "query_pipeline/components/agent.py",
    "start_line": 1,
    "end_line": 332,
    "length": 332,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_parameters",
      "default_agent_input_fn",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "validate_component_outputs",
      "_validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "validate_component_outputs",
      "_validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "set_callback_manager",
      "_validate_component_inputs",
      "_arun_component",
      "_input_keys",
      "_optional_input_keys",
      "_output_keys",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "AgentInputComponent",
      "BaseAgentComponent",
      "used",
      "AgentFnComponent",
      "CustomAgentComponent",
      "for"
    ],
    "document_function_names": [
      "get_parameters",
      "default_agent_input_fn",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "validate_component_outputs",
      "_validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "validate_component_outputs",
      "_validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "set_callback_manager",
      "_validate_component_inputs",
      "_arun_component",
      "_input_keys",
      "_optional_input_keys",
      "_output_keys",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "AgentInputComponent",
      "BaseAgentComponent",
      "used",
      "AgentFnComponent",
      "CustomAgentComponent",
      "for"
    ],
    "content": "\"\"\"Agent components.\"\"\"\n\nfrom inspect import signature\nfrom typing import Any, Callable, Dict, Optional, Set, Tuple, cast\nfrom typing_extensions import Annotated\n\nfrom llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n)\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    PrivateAttr,\n    ConfigDict,\n    WithJsonSchema,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\n\n\nAnnotatedCallable = Annotated[\n    Callable,\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n]\n\n\ndef get_parameters(fn: Callable) -> Tuple[Set[str], Set[str]]:\n    \"\"\"Get parameters from function.\n\n    Returns:\n        Tuple[Set[str], Set[str]]: required and optional parameters\n\n    \"\"\"\n    # please write function below\n    params = signature(fn).parameters\n    required_params = set()\n    optional_params = set()\n    for param_name in params:\n        param_default = params[param_name].default\n        if param_default is params[param_name].empty:\n            required_params.add(param_name)\n        else:\n            optional_params.add(param_name)\n    return required_params, optional_params\n\n\ndef default_agent_input_fn(task: Any, state: dict) -> dict:\n    \"\"\"Default agent input function.\"\"\"\n    from llama_index.core.agent.types import Task\n\n    task = cast(Task, task)\n\n    return {\"input\": task.input}\n\n\nclass AgentInputComponent(QueryComponent):\n    \"\"\"Takes in agent inputs and transforms it into desired outputs.\n\n    NOTE: this is now deprecated in favor of using `StatefulFnComponent`.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    fn: AnnotatedCallable = Field(..., description=\"Function to run.\")\n    async_fn: Optional[AnnotatedCallable] = Field(\n        None, description=\"Async function to run. If not provided, will run `fn`.\"\n    )\n\n    _req_params: Set[str] = PrivateAttr()\n    _opt_params: Set[str] = PrivateAttr()\n\n    def __init__(\n        self,\n        fn: Callable,\n        async_fn: Optional[Callable] = None,\n        req_params: Optional[Set[str]] = None,\n        opt_params: Optional[Set[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        # determine parameters\n        super().__init__(fn=fn, async_fn=async_fn, **kwargs)\n        default_req_params, default_opt_params = get_parameters(fn)\n        if req_params is None:\n            req_params = default_req_params\n        if opt_params is None:\n            opt_params = default_opt_params\n\n        self._req_params = req_params\n        self._opt_params = opt_params\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: implement\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        from llama_index.core.agent.types import Task\n\n        if \"task\" not in input:\n            raise ValueError(\"Input must have key 'task'\")\n        if not isinstance(input[\"task\"], Task):\n            raise ValueError(\"Input must have key 'task' of type Task\")\n\n        if \"state\" not in input:\n            raise ValueError(\"Input must have key 'state'\")\n        if not isinstance(input[\"state\"], dict):\n            raise ValueError(\"Input must have key 'state' of type dict\")\n\n        return input\n\n    def validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs.\"\"\"\n        # NOTE: we override this to do nothing\n        return output\n\n    def _validate_component_outputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Dict:\n        \"\"\"Run component.\"\"\"\n        output = self.fn(**kwargs)\n        if not isinstance(output, dict):\n            raise ValueError(\"Output must be a dictionary\")\n\n        return output\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        if self.async_fn is None:\n            return self._run_component(**kwargs)\n        else:\n            output = await self.async_fn(**kwargs)\n            if not isinstance(output, dict):\n                raise ValueError(\"Output must be a dictionary\")\n            return output\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys(\n            required_keys={\"task\", \"state\", *self._req_params},\n            optional_keys=self._opt_params,\n        )\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        # output can be anything, overrode validate function\n        return OutputKeys.from_keys(set())\n\n\nclass BaseAgentComponent(QueryComponent):\n    \"\"\"Agent component.\n\n    Abstract class used for type checking.\n\n    \"\"\"\n\n\nclass AgentFnComponent(BaseAgentComponent):\n    \"\"\"Function component for agents.\n\n    Designed to let users easily modify state.\n\n    NOTE: this is now deprecated in favor of using `StatefulFnComponent`.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    fn: Callable = Field(..., description=\"Function to run.\")\n    async_fn: Optional[Callable] = Field(\n        None, description=\"Async function to run. If not provided, will run `fn`.\"\n    )\n\n    _req_params: Set[str] = PrivateAttr()\n    _opt_params: Set[str] = PrivateAttr()\n\n    def __init__(\n        self,\n        fn: Callable,\n        async_fn: Optional[Callable] = None,\n        req_params: Optional[Set[str]] = None,\n        opt_params: Optional[Set[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        # determine parameters\n        super().__init__(fn=fn, async_fn=async_fn, **kwargs)\n        default_req_params, default_opt_params = get_parameters(fn)\n        # make sure task and step are part of the list, and remove them from the list\n        if \"task\" not in default_req_params or \"state\" not in default_req_params:\n            raise ValueError(\n                \"AgentFnComponent must have 'task' and 'state' as required parameters\"\n            )\n\n        default_req_params = default_req_params - {\"task\", \"state\"}\n        default_opt_params = default_opt_params - {\"task\", \"state\"}\n\n        if req_params is None:\n            req_params = default_req_params\n        if opt_params is None:\n            opt_params = default_opt_params\n\n        self._req_params = req_params\n        self._opt_params = opt_params\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: implement\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        from llama_index.core.agent.types import Task\n\n        if \"task\" not in input:\n            raise ValueError(\"Input must have key 'task'\")\n        if not isinstance(input[\"task\"], Task):\n            raise ValueError(\"Input must have key 'task' of type Task\")\n\n        if \"state\" not in input:\n            raise ValueError(\"Input must have key 'state'\")\n        if not isinstance(input[\"state\"], dict):\n            raise ValueError(\"Input must have key 'state' of type dict\")\n\n        return input\n\n    def validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs.\"\"\"\n        # NOTE: we override this to do nothing\n        return output\n\n    def _validate_component_outputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Dict:\n        \"\"\"Run component.\"\"\"\n        output = self.fn(**kwargs)\n        # if not isinstance(output, dict):\n        #     raise ValueError(\"Output must be a dictionary\")\n\n        return {\"output\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        if self.async_fn is None:\n            return self._run_component(**kwargs)\n        else:\n            output = await self.async_fn(**kwargs)\n            # if not isinstance(output, dict):\n            #     raise ValueError(\"Output must be a dictionary\")\n            return {\"output\": output}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys(\n            required_keys={\"task\", \"state\", *self._req_params},\n            optional_keys=self._opt_params,\n        )\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        # output can be anything, overrode validate function\n        return OutputKeys.from_keys({\"output\"})\n\n\nclass CustomAgentComponent(BaseAgentComponent):\n    \"\"\"Custom component for agents.\n\n    Designed to let users easily modify state.\n\n    NOTE: this is now deprecated in favor of using `StatefulFnComponent`.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=CallbackManager, description=\"Callback manager\"\n    )\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.callback_manager = callback_manager\n        # TODO: refactor to put this on base class\n        for component in self.sub_query_components:\n            component.set_callback_manager(callback_manager)\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # NOTE: user can override this method to validate inputs\n        # but we do this by default for convenience\n        return input\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        raise NotImplementedError(\"This component does not support async run.\")\n\n    @property\n    def _input_keys(self) -> Set[str]:\n        \"\"\"Input keys dict.\"\"\"\n        raise NotImplementedError(\"Not implemented yet. Please override this method.\")\n\n    @property\n    def _optional_input_keys(self) -> Set[str]:\n        \"\"\"Optional input keys dict.\"\"\"\n        return set()\n\n    @property\n    def _output_keys(self) -> Set[str]:\n        \"\"\"Output keys dict.\"\"\"\n        raise NotImplementedError(\"Not implemented yet. Please override this method.\")\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # NOTE: user can override this too, but we have them implement an\n        # abstract method to make sure they do it\n\n        input_keys = self._input_keys.union({\"task\", \"state\"})\n        return InputKeys.from_keys(\n            required_keys=input_keys, optional_keys=self._optional_input_keys\n        )\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        # NOTE: user can override this too, but we have them implement an\n        # abstract method to make sure they do it\n        return OutputKeys.from_keys(self._output_keys)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/loop.py",
    "filename": "loop.py",
    "relpath": "query_pipeline/components/loop.py",
    "start_line": 1,
    "end_line": 93,
    "length": 93,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "LoopComponent"
    ],
    "document_function_names": [
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "LoopComponent"
    ],
    "content": "from llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n)\nfrom llama_index.core.query_pipeline.query import QueryPipeline\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict, WithJsonSchema\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom typing import Any, Dict, Optional, Callable\nfrom typing_extensions import Annotated\n\nAnnotatedCallable = Annotated[\n    Callable,\n    WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    WithJsonSchema({\"type\": \"string\"}, mode=\"validation\"),\n]\n\n\nclass LoopComponent(QueryComponent):\n    \"\"\"Loop component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    pipeline: QueryPipeline = Field(..., description=\"Query pipeline\")\n    should_exit_fn: Optional[AnnotatedCallable] = Field(\n        ..., description=\"Should exit function\"\n    )\n    add_output_to_input_fn: Optional[AnnotatedCallable] = Field(\n        ...,\n        description=\"Add output to input function. If not provided, will reuse the original input for the next iteration. If provided, will call the function to combine the output into the input for the next iteration.\",\n    )\n    max_iterations: int = Field(default=5, description=\"Max iterations\")\n\n    def __init__(\n        self,\n        pipeline: QueryPipeline,\n        should_exit_fn: Optional[Callable] = None,\n        add_output_to_input_fn: Optional[Callable] = None,\n        max_iterations: Optional[int] = 5,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        super().__init__(\n            pipeline=pipeline,\n            should_exit_fn=should_exit_fn,\n            add_output_to_input_fn=add_output_to_input_fn,\n            max_iterations=max_iterations,\n        )\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: implement\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Dict:\n        \"\"\"Run component.\"\"\"\n        current_input = kwargs\n        for _ in range(self.max_iterations):\n            output = self.pipeline.run_component(**current_input)\n            if self.should_exit_fn:\n                should_exit = self.should_exit_fn(output)\n                if should_exit:\n                    break\n\n            if self.add_output_to_input_fn:\n                current_input = self.add_output_to_input_fn(current_input, output)\n\n        return output\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        current_input = kwargs\n        for _ in range(self.max_iterations):\n            output = await self.pipeline.arun_component(**current_input)\n            if self.should_exit_fn:\n                should_exit = self.should_exit_fn(output)\n                if should_exit:\n                    break\n\n            if self.add_output_to_input_fn:\n                current_input = self.add_output_to_input_fn(current_input, output)\n\n        return output\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return self.pipeline.input_keys\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return self.pipeline.output_keys"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/__init__.py",
    "filename": "__init__.py",
    "relpath": "query_pipeline/components/__init__.py",
    "start_line": 1,
    "end_line": 35,
    "length": 35,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.query_pipeline.components.agent import (\n    AgentFnComponent,\n    AgentInputComponent,\n    BaseAgentComponent,\n    CustomAgentComponent,\n)\nfrom llama_index.core.query_pipeline.components.argpacks import ArgPackComponent\nfrom llama_index.core.query_pipeline.components.function import (\n    FnComponent,\n    FunctionComponent,\n)\nfrom llama_index.core.query_pipeline.components.input import InputComponent\nfrom llama_index.core.query_pipeline.components.router import (\n    RouterComponent,\n    SelectorComponent,\n)\nfrom llama_index.core.query_pipeline.components.tool_runner import ToolRunnerComponent\nfrom llama_index.core.query_pipeline.components.stateful import StatefulFnComponent\nfrom llama_index.core.query_pipeline.components.loop import LoopComponent\n\n__all__ = [\n    \"AgentFnComponent\",\n    \"AgentInputComponent\",\n    \"BaseAgentComponent\",\n    \"CustomAgentComponent\",\n    \"ArgPackComponent\",\n    \"FnComponent\",\n    \"FunctionComponent\",\n    \"InputComponent\",\n    \"RouterComponent\",\n    \"SelectorComponent\",\n    \"ToolRunnerComponent\",\n    \"StatefulFnComponent\",\n    \"LoopComponent\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/argpacks.py",
    "filename": "argpacks.py",
    "relpath": "query_pipeline/components/argpacks.py",
    "start_line": 1,
    "end_line": 121,
    "length": 121,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "set_callback_manager",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "set_callback_manager",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "ArgPackComponent",
      "KwargPackComponent"
    ],
    "document_function_names": [
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "set_callback_manager",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "_validate_component_inputs",
      "validate_component_inputs",
      "_validate_component_outputs",
      "set_callback_manager",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "ArgPackComponent",
      "KwargPackComponent"
    ],
    "content": "\"\"\"Arg pack components.\"\"\"\n\nfrom typing import Any, Callable, Dict, Optional\n\nfrom llama_index.core.base.query_pipeline.query import (\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n)\nfrom llama_index.core.bridge.pydantic import Field\n\n\nclass ArgPackComponent(QueryComponent):\n    \"\"\"Arg pack component.\n\n    Packs arbitrary number of args into a list.\n\n    \"\"\"\n\n    convert_fn: Optional[Callable] = Field(\n        default=None, description=\"Function to convert output.\"\n    )\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        raise NotImplementedError\n\n    def validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs.\"\"\"\n        return input\n\n    def _validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs.\"\"\"\n        # make sure output value is a list\n        if not isinstance(output[\"output\"], list):\n            raise ValueError(f\"Output is not a list.\")\n        return output\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # combine all lists into one\n        output = []\n        for v in kwargs.values():\n            if self.convert_fn is not None:\n                v = self.convert_fn(v)\n\n            if isinstance(v, list):\n                output.extend(v)\n            else:\n                output.append(v)\n        return {\"output\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        return self._run_component(**kwargs)\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # NOTE: this shouldn't be used\n        return InputKeys.from_keys(set(), optional_keys=set())\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})\n\n\nclass KwargPackComponent(QueryComponent):\n    \"\"\"Kwarg pack component.\n\n    Packs arbitrary number of kwargs into a dict.\n\n    \"\"\"\n\n    convert_fn: Optional[Callable] = Field(\n        default=None, description=\"Function to convert output.\"\n    )\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        raise NotImplementedError\n\n    def validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs.\"\"\"\n        return input\n\n    def _validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs.\"\"\"\n        # make sure output value is a list\n        if not isinstance(output[\"output\"], dict):\n            raise ValueError(f\"Output is not a dict.\")\n        return output\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        if self.convert_fn is not None:\n            for k, v in kwargs.items():\n                kwargs[k] = self.convert_fn(v)\n        return {\"output\": kwargs}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        return self._run_component(**kwargs)\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # NOTE: this shouldn't be used\n        return InputKeys.from_keys(set(), optional_keys=set())\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/router.py",
    "filename": "router.py",
    "relpath": "query_pipeline/components/router.py",
    "start_line": 1,
    "end_line": 196,
    "length": 196,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "validate_component_outputs",
      "_validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components"
    ],
    "chunk_class_names": [
      "SelectorComponent",
      "RouterComponent"
    ],
    "document_function_names": [
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "__init__",
      "set_callback_manager",
      "_validate_component_inputs",
      "validate_component_outputs",
      "_validate_component_outputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components"
    ],
    "document_class_names": [
      "SelectorComponent",
      "RouterComponent"
    ],
    "content": "\"\"\"Router components.\"\"\"\n\nfrom typing import Any, Dict, List\n\nfrom llama_index.core.base.base_selector import BaseSelector\nfrom llama_index.core.base.query_pipeline.query import (\n    QUERY_COMPONENT_TYPE,\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    PrivateAttr,\n    SerializeAsAny,\n    ConfigDict,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.utils import print_text\n\n\nclass SelectorComponent(QueryComponent):\n    \"\"\"Selector component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    selector: BaseSelector = Field(..., description=\"Selector\")\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        if \"choices\" not in input:\n            raise ValueError(\"Input must have key 'choices'\")\n        if not isinstance(input[\"choices\"], list):\n            raise ValueError(\"Input choices must be a list\")\n\n        for idx, choice in enumerate(input[\"choices\"]):\n            # make stringable\n            input[\"choices\"][idx] = validate_and_convert_stringable(choice)\n\n        # make sure `query` is stringable\n        if \"query\" not in input:\n            raise ValueError(\"Input must have key 'query'\")\n        input[\"query\"] = validate_and_convert_stringable(input[\"query\"])\n\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = self.selector.select(kwargs[\"choices\"], kwargs[\"query\"])\n        return {\"output\": output.selections}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        # NOTE: no native async for postprocessor\n        return self._run_component(**kwargs)\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"choices\", \"query\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})\n\n\nclass RouterComponent(QueryComponent):\n    \"\"\"Router Component.\n\n    Routes queries to different query components based on a selector.\n\n    Assumes a single query component is selected.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    selector: SerializeAsAny[BaseSelector] = Field(..., description=\"Selector\")\n    choices: List[str] = Field(\n        ..., description=\"Choices (must correspond to components)\"\n    )\n    components: List[SerializeAsAny[QueryComponent]] = Field(\n        ..., description=\"Components (must correspond to choices)\"\n    )\n    verbose: bool = Field(default=False, description=\"Verbose\")\n\n    _query_keys: List[str] = PrivateAttr()\n\n    def __init__(\n        self,\n        selector: BaseSelector,\n        choices: List[str],\n        components: List[QUERY_COMPONENT_TYPE],\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Init.\"\"\"\n        new_components = []\n        query_keys = []\n        for component in components:\n            if isinstance(component, ChainableMixin):\n                new_component = component.as_query_component()\n            else:\n                new_component = component\n\n            # validate component has one input key\n            if len(new_component.free_req_input_keys) != 1:\n                raise ValueError(\"Expected one required input key\")\n            query_keys.append(next(iter(new_component.free_req_input_keys)))\n            new_components.append(new_component)\n        super().__init__(\n            selector=selector,\n            choices=choices,\n            components=new_components,\n            verbose=verbose,\n        )\n        self._query_keys = query_keys\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        for component in self.components:\n            component.set_callback_manager(callback_manager)\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # make sure `query` is stringable\n        if \"query\" not in input:\n            raise ValueError(\"Input must have key 'query'\")\n        input[\"query\"] = validate_and_convert_stringable(input[\"query\"])\n\n        return input\n\n    def validate_component_outputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        return input\n\n    def _validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # for the output selection, run the corresponding component, aggregate into list\n        sel_output = self.selector.select(self.choices, kwargs[\"query\"])\n        # assume one selection\n        if len(sel_output.selections) != 1:\n            raise ValueError(\"Expected one selection\")\n        component = self.components[sel_output.ind]\n        log_str = f\"Selecting component {sel_output.ind}: \" f\"{sel_output.reason}.\"\n        if self.verbose:\n            print_text(log_str + \"\\n\", color=\"pink\")\n        # run component\n        # run with input_keys of component\n        return component.run_component(\n            **{self._query_keys[sel_output.ind]: kwargs[\"query\"]}\n        )\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        # for the output selection, run the corresponding component, aggregate into list\n        sel_output = await self.selector.aselect(self.choices, kwargs[\"query\"])\n        # assume one selection\n        if len(sel_output.selections) != 1:\n            raise ValueError(\"Expected one selection\")\n        component = self.components[sel_output.ind]\n        log_str = f\"Selecting component {sel_output.ind}: \" f\"{sel_output.reason}.\"\n        if self.verbose:\n            print_text(log_str + \"\\n\", color=\"pink\")\n        # run component\n        return await component.arun_component(\n            **{self._query_keys[sel_output.ind]: kwargs[\"query\"]}\n        )\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"query\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        # not used\n        return OutputKeys.from_keys(set())\n\n    @property\n    def sub_query_components(self) -> List[\"QueryComponent\"]:\n        \"\"\"Get sub query components.\n\n        Certain query components may have sub query components, e.g. a\n        query pipeline will have sub query components, and so will\n        an IfElseComponent.\n\n        \"\"\"\n        return self.components"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/query_pipeline/components/stateful.py",
    "filename": "stateful.py",
    "relpath": "query_pipeline/components/stateful.py",
    "start_line": 1,
    "end_line": 77,
    "length": 77,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "reset_state",
      "__init__",
      "_run_component",
      "_arun_component"
    ],
    "chunk_class_names": [
      "BaseStatefulComponent",
      "StatefulFnComponent"
    ],
    "document_function_names": [
      "reset_state",
      "__init__",
      "_run_component",
      "_arun_component"
    ],
    "document_class_names": [
      "BaseStatefulComponent",
      "StatefulFnComponent"
    ],
    "content": "\"\"\"Agent components.\"\"\"\n\nfrom typing import Any, Callable, Dict, Optional, Set\n\nfrom llama_index.core.base.query_pipeline.query import (\n    QueryComponent,\n)\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.query_pipeline.components.function import (\n    FnComponent,\n    get_parameters,\n)\n\n# from llama_index.core.query_pipeline.components.input import InputComponent\n\n\nclass BaseStatefulComponent(QueryComponent):\n    \"\"\"Takes in agent inputs and transforms it into desired outputs.\"\"\"\n\n    state: Dict[str, Any] = Field(\n        default_factory=dict, description=\"State of the pipeline.\"\n    )\n\n    def reset_state(self) -> None:\n        \"\"\"Reset state.\"\"\"\n        self.state = {}\n\n\nclass StatefulFnComponent(BaseStatefulComponent, FnComponent):\n    \"\"\"Query component that takes in an arbitrary function.\n\n    Stateful version of `FnComponent`. Expects functions to have `state` as the first argument.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        fn: Callable,\n        req_params: Optional[Set[str]] = None,\n        opt_params: Optional[Set[str]] = None,\n        state: Optional[Dict[str, Any]] = None,\n        **kwargs: Any\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        # determine parameters\n        default_req_params, default_opt_params = get_parameters(fn)\n        # make sure task and step are part of the list, and remove them from the list\n        if \"state\" not in default_req_params:\n            raise ValueError(\n                \"StatefulFnComponent must have 'state' as required parameters\"\n            )\n\n        default_req_params = default_req_params - {\"state\"}\n        default_opt_params = default_opt_params - {\"state\"}\n\n        if req_params is None:\n            req_params = default_req_params\n        if opt_params is None:\n            opt_params = default_opt_params\n\n        super().__init__(\n            fn=fn,\n            req_params=req_params,\n            opt_params=opt_params,\n            state=state or {},\n            **kwargs\n        )\n\n    def _run_component(self, **kwargs: Any) -> Dict:\n        \"\"\"Run component.\"\"\"\n        kwargs.update({\"state\": self.state})\n        return super()._run_component(**kwargs)\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Async run component.\"\"\"\n        kwargs.update({\"state\": self.state})\n        return await super()._arun_component(**kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/question_gen/types.py",
    "filename": "types.py",
    "relpath": "question_gen/types.py",
    "start_line": 1,
    "end_line": 40,
    "length": 40,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompt_modules",
      "generate",
      "agenerate"
    ],
    "chunk_class_names": [
      "SubQuestion",
      "SubQuestionList",
      "BaseQuestionGenerator"
    ],
    "document_function_names": [
      "_get_prompt_modules",
      "generate",
      "agenerate"
    ],
    "document_class_names": [
      "SubQuestion",
      "SubQuestionList",
      "BaseQuestionGenerator"
    ],
    "content": "from abc import abstractmethod\nfrom typing import List, Sequence\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.prompts.mixin import PromptMixin, PromptMixinType\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.tools.types import ToolMetadata\n\n\nclass SubQuestion(BaseModel):\n    sub_question: str\n    tool_name: str\n\n\nclass SubQuestionList(BaseModel):\n    \"\"\"A pydantic object wrapping a list of sub-questions.\n\n    This is mostly used to make getting a json schema easier.\n    \"\"\"\n\n    items: List[SubQuestion]\n\n\nclass BaseQuestionGenerator(PromptMixin, DispatcherSpanMixin):\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    @abstractmethod\n    def generate(\n        self, tools: Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        pass\n\n    @abstractmethod\n    async def agenerate(\n        self, tools: Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        pass"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/question_gen/output_parser.py",
    "filename": "output_parser.py",
    "relpath": "question_gen/output_parser.py",
    "start_line": 1,
    "end_line": 25,
    "length": 25,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "parse",
      "format"
    ],
    "chunk_class_names": [
      "SubQuestionOutputParser"
    ],
    "document_function_names": [
      "parse",
      "format"
    ],
    "document_class_names": [
      "SubQuestionOutputParser"
    ],
    "content": "from typing import Any\n\nfrom llama_index.core.output_parsers.base import StructuredOutput\nfrom llama_index.core.output_parsers.utils import parse_json_markdown\nfrom llama_index.core.question_gen.types import SubQuestion\nfrom llama_index.core.types import BaseOutputParser\n\n\nclass SubQuestionOutputParser(BaseOutputParser):\n    def parse(self, output: str) -> Any:\n        json_dict = parse_json_markdown(output)\n        if not json_dict:\n            raise ValueError(f\"No valid JSON found in output: {output}\")\n\n        # example code includes an 'items' key, which breaks\n        # the parsing from open-source LLMs such as Zephyr.\n        # This gets the actual subquestions and recommended tools directly\n        if \"items\" in json_dict:\n            json_dict = json_dict[\"items\"]\n\n        sub_questions = [SubQuestion.model_validate(item) for item in json_dict]\n        return StructuredOutput(raw_output=output, parsed_output=sub_questions)\n\n    def format(self, prompt_template: str) -> str:\n        return prompt_template"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/question_gen/__init__.py",
    "filename": "__init__.py",
    "relpath": "question_gen/__init__.py",
    "start_line": 1,
    "end_line": 7,
    "length": 7,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.question_gen.llm_generators import LLMQuestionGenerator\nfrom llama_index.core.question_gen.output_parser import SubQuestionOutputParser\n\n__all__ = [\n    \"LLMQuestionGenerator\",\n    \"SubQuestionOutputParser\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/question_gen/llm_generators.py",
    "filename": "llm_generators.py",
    "relpath": "question_gen/llm_generators.py",
    "start_line": 1,
    "end_line": 97,
    "length": 97,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "generate",
      "agenerate"
    ],
    "chunk_class_names": [
      "LLMQuestionGenerator"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "_get_prompts",
      "_update_prompts",
      "generate",
      "agenerate"
    ],
    "document_class_names": [
      "LLMQuestionGenerator"
    ],
    "content": "from typing import List, Optional, Sequence, cast\n\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.output_parsers.base import StructuredOutput\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.prompts.prompt_type import PromptType\nfrom llama_index.core.question_gen.output_parser import SubQuestionOutputParser\nfrom llama_index.core.question_gen.prompts import (\n    DEFAULT_SUB_QUESTION_PROMPT_TMPL,\n    build_tools_text,\n)\nfrom llama_index.core.question_gen.types import BaseQuestionGenerator, SubQuestion\nfrom llama_index.core.schema import QueryBundle\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.types import ToolMetadata\nfrom llama_index.core.types import BaseOutputParser\n\n\nclass LLMQuestionGenerator(BaseQuestionGenerator):\n    def __init__(\n        self,\n        llm: LLM,\n        prompt: BasePromptTemplate,\n    ) -> None:\n        self._llm = llm\n        self._prompt = prompt\n\n        if self._prompt.output_parser is None:\n            raise ValueError(\"Prompt should have output parser.\")\n\n    @classmethod\n    def from_defaults(\n        cls,\n        llm: Optional[LLM] = None,\n        prompt_template_str: Optional[str] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n    ) -> \"LLMQuestionGenerator\":\n        # optionally initialize defaults\n        llm = llm or Settings.llm\n        prompt_template_str = prompt_template_str or DEFAULT_SUB_QUESTION_PROMPT_TMPL\n        output_parser = output_parser or SubQuestionOutputParser()\n\n        # construct prompt\n        prompt = PromptTemplate(\n            template=prompt_template_str,\n            output_parser=output_parser,\n            prompt_type=PromptType.SUB_QUESTION,\n        )\n        return cls(llm, prompt)\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\"question_gen_prompt\": self._prompt}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"question_gen_prompt\" in prompts:\n            output_parser = prompts[\"question_gen_prompt\"].output_parser\n            if output_parser is None:\n                output_parser = SubQuestionOutputParser()\n            self._prompt = PromptTemplate(\n                prompts[\"question_gen_prompt\"].get_template(llm=self._llm),\n                output_parser=output_parser,\n            )\n\n    def generate(\n        self, tools: Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        tools_str = build_tools_text(tools)\n        query_str = query.query_str\n        prediction = self._llm.predict(\n            prompt=self._prompt,\n            tools_str=tools_str,\n            query_str=query_str,\n        )\n\n        assert self._prompt.output_parser is not None\n        parse = self._prompt.output_parser.parse(prediction)\n        parse = cast(StructuredOutput, parse)\n        return parse.parsed_output\n\n    async def agenerate(\n        self, tools: Sequence[ToolMetadata], query: QueryBundle\n    ) -> List[SubQuestion]:\n        tools_str = build_tools_text(tools)\n        query_str = query.query_str\n        prediction = await self._llm.apredict(\n            prompt=self._prompt,\n            tools_str=tools_str,\n            query_str=query_str,\n        )\n\n        assert self._prompt.output_parser is not None\n        parse = self._prompt.output_parser.parse(prediction)\n        parse = cast(StructuredOutput, parse)\n        return parse.parsed_output"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/question_gen/prompts.py",
    "filename": "prompts.py",
    "relpath": "question_gen/prompts.py",
    "start_line": 1,
    "end_line": 85,
    "length": 85,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "build_tools_text"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "build_tools_text"
    ],
    "document_class_names": [],
    "content": "import json\nfrom typing import Sequence\n\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.question_gen.types import SubQuestion\nfrom llama_index.core.tools.types import ToolMetadata\n\n# deprecated, kept for backward compatibility\nSubQuestionPrompt = PromptTemplate\n\n\ndef build_tools_text(tools: Sequence[ToolMetadata]) -> str:\n    tools_dict = {}\n    for tool in tools:\n        tools_dict[tool.name] = tool.description\n    return json.dumps(tools_dict, indent=4)\n\n\nPREFIX = \"\"\"\\\nGiven a user question, and a list of tools, output a list of relevant sub-questions \\\nin json markdown that when composed can help answer the full user question:\n\n\"\"\"\n\n\nexample_query_str = (\n    \"Compare and contrast the revenue growth and EBITDA of Uber and Lyft for year 2021\"\n)\nexample_tools = [\n    ToolMetadata(\n        name=\"uber_10k\",\n        description=\"Provides information about Uber financials for year 2021\",\n    ),\n    ToolMetadata(\n        name=\"lyft_10k\",\n        description=\"Provides information about Lyft financials for year 2021\",\n    ),\n]\nexample_tools_str = build_tools_text(example_tools)\nexample_output = [\n    SubQuestion(\n        sub_question=\"What is the revenue growth of Uber\", tool_name=\"uber_10k\"\n    ),\n    SubQuestion(sub_question=\"What is the EBITDA of Uber\", tool_name=\"uber_10k\"),\n    SubQuestion(\n        sub_question=\"What is the revenue growth of Lyft\", tool_name=\"lyft_10k\"\n    ),\n    SubQuestion(sub_question=\"What is the EBITDA of Lyft\", tool_name=\"lyft_10k\"),\n]\nexample_output_str = json.dumps(\n    {\"items\": [x.model_dump() for x in example_output]}, indent=4\n)\n\nEXAMPLES = f\"\"\"\\\n# Example 1\n<Tools>\n```json\n{example_tools_str}\n```\n\n<User Question>\n{example_query_str}\n\n\n<Output>\n```json\n{example_output_str}\n```\n\n\"\"\"\n\nSUFFIX = \"\"\"\\\n# Example 2\n<Tools>\n```json\n{tools_str}\n```\n\n<User Question>\n{query_str}\n\n<Output>\n\"\"\"\n\nDEFAULT_SUB_QUESTION_PROMPT_TMPL = PREFIX + EXAMPLES + SUFFIX"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/bridge/pydantic_settings.py",
    "filename": "pydantic_settings.py",
    "relpath": "bridge/pydantic_settings.py",
    "start_line": 1,
    "end_line": 5,
    "length": 5,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "import pydantic_settings\n\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n__all__ = [\"pydantic_settings\", \"BaseSettings\", \"SettingsConfigDict\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/bridge/langchain.py",
    "filename": "langchain.py",
    "relpath": "bridge/langchain.py",
    "start_line": 1,
    "end_line": 135,
    "length": 135,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "import langchain  # pants: no-infer-dep\nfrom langchain.agents import (\n    AgentExecutor,\n    AgentType,\n    initialize_agent,\n)  # pants: no-infer-dep\n\n# agents and tools\nfrom langchain.agents.agent_toolkits.base import BaseToolkit  # pants: no-infer-dep\nfrom langchain.base_language import BaseLanguageModel  # pants: no-infer-dep\n\n# callback\nfrom langchain.callbacks.base import (\n    BaseCallbackHandler,\n    BaseCallbackManager,\n)  # pants: no-infer-dep\nfrom langchain.chains.prompt_selector import (\n    ConditionalPromptSelector,\n    is_chat_model,\n)  # pants: no-infer-dep\nfrom langchain.chat_models.base import BaseChatModel  # pants: no-infer-dep\nfrom langchain.docstore.document import Document  # pants: no-infer-dep\nfrom langchain.memory import ConversationBufferMemory  # pants: no-infer-dep\n\n# chat and memory\nfrom langchain.memory.chat_memory import BaseChatMemory  # pants: no-infer-dep\nfrom langchain.output_parsers import ResponseSchema  # pants: no-infer-dep\n\n# prompts\nfrom langchain.prompts import PromptTemplate  # pants: no-infer-dep\nfrom langchain.prompts.chat import (  # pants: no-infer-dep\n    AIMessagePromptTemplate,  # pants: no-infer-dep\n    BaseMessagePromptTemplate,  # pants: no-infer-dep\n    ChatPromptTemplate,  # pants: no-infer-dep\n    HumanMessagePromptTemplate,  # pants: no-infer-dep\n    SystemMessagePromptTemplate,  # pants: no-infer-dep\n)  # pants: no-infer-dep\n\n# schema\nfrom langchain.schema import (  # pants: no-infer-dep\n    AIMessage,  # pants: no-infer-dep\n    BaseMemory,  # pants: no-infer-dep\n    BaseMessage,  # pants: no-infer-dep\n    BaseOutputParser,  # pants: no-infer-dep\n    ChatGeneration,  # pants: no-infer-dep\n    ChatMessage,  # pants: no-infer-dep\n    FunctionMessage,  # pants: no-infer-dep\n    HumanMessage,  # pants: no-infer-dep\n    LLMResult,  # pants: no-infer-dep\n    SystemMessage,  # pants: no-infer-dep\n)  # pants: no-infer-dep\n\n# embeddings\nfrom langchain.schema.embeddings import Embeddings  # pants: no-infer-dep\nfrom langchain.schema.prompt_template import BasePromptTemplate  # pants: no-infer-dep\n\n# input & output\nfrom langchain.text_splitter import (\n    RecursiveCharacterTextSplitter,\n    TextSplitter,\n)  # pants: no-infer-dep\nfrom langchain.tools import BaseTool, StructuredTool, Tool  # pants: no-infer-dep\nfrom langchain_community.chat_message_histories import (\n    ChatMessageHistory,\n)  # pants: no-infer-dep\nfrom langchain_community.chat_models import (\n    ChatAnyscale,\n    ChatOpenAI,\n    ChatFireworks,\n)  # pants: no-infer-dep\nfrom langchain_community.embeddings import (  # pants: no-infer-dep\n    HuggingFaceBgeEmbeddings,  # pants: no-infer-dep\n    HuggingFaceEmbeddings,  # pants: no-infer-dep\n)  # pants: no-infer-dep\n\n# LLMs\nfrom langchain_community.llms import (\n    AI21,\n    BaseLLM,\n    Cohere,\n    FakeListLLM,\n    OpenAI,\n)  # pants: no-infer-dep\n\n__all__ = [\n    \"langchain\",\n    \"BaseLLM\",\n    \"FakeListLLM\",\n    \"OpenAI\",\n    \"AI21\",\n    \"Cohere\",\n    \"BaseChatModel\",\n    \"ChatAnyscale\",\n    \"ChatOpenAI\",\n    \"ChatFireworks\",\n    \"BaseLanguageModel\",\n    \"Embeddings\",\n    \"HuggingFaceEmbeddings\",\n    \"HuggingFaceBgeEmbeddings\",\n    \"PromptTemplate\",\n    \"BasePromptTemplate\",\n    \"ConditionalPromptSelector\",\n    \"is_chat_model\",\n    \"AIMessagePromptTemplate\",\n    \"ChatPromptTemplate\",\n    \"HumanMessagePromptTemplate\",\n    \"BaseMessagePromptTemplate\",\n    \"SystemMessagePromptTemplate\",\n    \"BaseChatMemory\",\n    \"ConversationBufferMemory\",\n    \"ChatMessageHistory\",\n    \"BaseToolkit\",\n    \"AgentType\",\n    \"AgentExecutor\",\n    \"initialize_agent\",\n    \"StructuredTool\",\n    \"Tool\",\n    \"BaseTool\",\n    \"ResponseSchema\",\n    \"BaseCallbackHandler\",\n    \"BaseCallbackManager\",\n    \"AIMessage\",\n    \"FunctionMessage\",\n    \"BaseMessage\",\n    \"ChatMessage\",\n    \"HumanMessage\",\n    \"SystemMessage\",\n    \"BaseMemory\",\n    \"BaseOutputParser\",\n    \"LLMResult\",\n    \"ChatGeneration\",\n    \"Document\",\n    \"RecursiveCharacterTextSplitter\",\n    \"TextSplitter\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/bridge/pydantic.py",
    "filename": "pydantic.py",
    "relpath": "bridge/pydantic.py",
    "start_line": 1,
    "end_line": 67,
    "length": 67,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "import pydantic\nfrom pydantic import (\n    AnyUrl,\n    BaseModel,\n    BeforeValidator,\n    ConfigDict,\n    Field,\n    FilePath,\n    GetCoreSchemaHandler,\n    GetJsonSchemaHandler,\n    PlainSerializer,\n    PrivateAttr,\n    Secret,\n    SecretStr,\n    SerializationInfo,\n    SerializeAsAny,\n    SerializerFunctionWrapHandler,\n    StrictFloat,\n    StrictInt,\n    StrictStr,\n    TypeAdapter,\n    ValidationError,\n    ValidationInfo,\n    WithJsonSchema,\n    WrapSerializer,\n    create_model,\n    field_serializer,\n    field_validator,\n    model_serializer,\n    model_validator,\n)\nfrom pydantic.fields import FieldInfo\nfrom pydantic.json_schema import JsonSchemaValue\n\n__all__ = [\n    \"pydantic\",\n    \"BaseModel\",\n    \"ConfigDict\",\n    \"GetJsonSchemaHandler\",\n    \"GetCoreSchemaHandler\",\n    \"Field\",\n    \"PlainSerializer\",\n    \"PrivateAttr\",\n    \"model_validator\",\n    \"field_validator\",\n    \"create_model\",\n    \"StrictFloat\",\n    \"StrictInt\",\n    \"StrictStr\",\n    \"FieldInfo\",\n    \"ValidationInfo\",\n    \"TypeAdapter\",\n    \"ValidationError\",\n    \"WithJsonSchema\",\n    \"BeforeValidator\",\n    \"JsonSchemaValue\",\n    \"SerializeAsAny\",\n    \"WrapSerializer\",\n    \"field_serializer\",\n    \"Secret\",\n    \"SecretStr\",\n    \"model_serializer\",\n    \"AnyUrl\",\n    \"FilePath\",\n    \"SerializationInfo\",\n    \"SerializerFunctionWrapHandler\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/bridge/pydantic_core.py",
    "filename": "pydantic_core.py",
    "relpath": "bridge/pydantic_core.py",
    "start_line": 1,
    "end_line": 5,
    "length": 5,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "import pydantic_core\n\nfrom pydantic_core import CoreSchema, core_schema\n\n__all__ = [\"pydantic_core\", \"CoreSchema\", \"core_schema\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/extractors/loading.py",
    "filename": "loading.py",
    "relpath": "extractors/loading.py",
    "start_line": 1,
    "end_line": 29,
    "length": 29,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_extractor"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "load_extractor"
    ],
    "document_class_names": [],
    "content": "from llama_index.core.extractors.metadata_extractors import (\n    BaseExtractor,\n    KeywordExtractor,\n    QuestionsAnsweredExtractor,\n    SummaryExtractor,\n    TitleExtractor,\n)\n\n\ndef load_extractor(\n    data: dict,\n) -> BaseExtractor:\n    if isinstance(data, BaseExtractor):\n        return data\n\n    extractor_name = data.get(\"class_name\", None)\n    if extractor_name is None:\n        raise ValueError(\"Extractor loading requires a class_name\")\n\n    if extractor_name == SummaryExtractor.class_name():\n        return SummaryExtractor.from_dict(data)\n    elif extractor_name == QuestionsAnsweredExtractor.class_name():\n        return QuestionsAnsweredExtractor.from_dict(data)\n    elif extractor_name == TitleExtractor.class_name():\n        return TitleExtractor.from_dict(data)\n    elif extractor_name == KeywordExtractor.class_name():\n        return KeywordExtractor.from_dict(data)\n    else:\n        raise ValueError(f\"Unknown extractor name: {extractor_name}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/extractors/interface.py",
    "filename": "interface.py",
    "relpath": "extractors/interface.py",
    "start_line": 1,
    "end_line": 170,
    "length": 170,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_dict",
      "class_name",
      "aextract",
      "extract",
      "aprocess_nodes",
      "process_nodes",
      "__call__",
      "acall"
    ],
    "chunk_class_names": [
      "BaseExtractor",
      "name"
    ],
    "document_function_names": [
      "from_dict",
      "class_name",
      "aextract",
      "extract",
      "aprocess_nodes",
      "process_nodes",
      "__call__",
      "acall"
    ],
    "document_class_names": [
      "BaseExtractor",
      "name"
    ],
    "content": "\"\"\"Node parser interface.\"\"\"\n\nfrom abc import abstractmethod\nfrom copy import deepcopy\nfrom typing import Any, Dict, List, Optional, Sequence, cast\n\nfrom llama_index.core.async_utils import asyncio_run\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.schema import (\n    BaseNode,\n    MetadataMode,\n    TextNode,\n    TransformComponent,\n)\nfrom typing_extensions import Self\n\nDEFAULT_NODE_TEXT_TEMPLATE = \"\"\"\\\n[Excerpt from document]\\n{metadata_str}\\n\\\nExcerpt:\\n-----\\n{content}\\n-----\\n\"\"\"\n\n\nclass BaseExtractor(TransformComponent):\n    \"\"\"Metadata extractor.\"\"\"\n\n    is_text_node_only: bool = True\n\n    show_progress: bool = Field(default=True, description=\"Whether to show progress.\")\n\n    metadata_mode: MetadataMode = Field(\n        default=MetadataMode.ALL, description=\"Metadata mode to use when reading nodes.\"\n    )\n\n    node_text_template: str = Field(\n        default=DEFAULT_NODE_TEXT_TEMPLATE,\n        description=\"Template to represent how node text is mixed with metadata text.\",\n    )\n    disable_template_rewrite: bool = Field(\n        default=False, description=\"Disable the node template rewrite.\"\n    )\n\n    in_place: bool = Field(\n        default=True, description=\"Whether to process nodes in place.\"\n    )\n\n    num_workers: int = Field(\n        default=4,\n        description=\"Number of workers to use for concurrent async processing.\",\n    )\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], **kwargs: Any) -> Self:  # type: ignore\n        if isinstance(kwargs, dict):\n            data.update(kwargs)\n\n        data.pop(\"class_name\", None)\n\n        llm_predictor = data.get(\"llm_predictor\", None)\n        if llm_predictor:\n            from llama_index.core.llm_predictor.loading import load_predictor\n\n            llm_predictor = load_predictor(llm_predictor)\n            data[\"llm_predictor\"] = llm_predictor\n\n        llm = data.get(\"llm\", None)\n        if llm:\n            from llama_index.core.llms.loading import load_llm\n\n            llm = load_llm(llm)\n            data[\"llm\"] = llm\n\n        return cls(**data)\n\n    @classmethod\n    def class_name(cls) -> str:\n        \"\"\"Get class name.\"\"\"\n        return \"MetadataExtractor\"\n\n    @abstractmethod\n    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        \"\"\"Extracts metadata for a sequence of nodes, returning a list of\n        metadata dictionaries corresponding to each node.\n\n        Args:\n            nodes (Sequence[Document]): nodes to extract metadata from\n\n        \"\"\"\n\n    def extract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        \"\"\"Extracts metadata for a sequence of nodes, returning a list of\n        metadata dictionaries corresponding to each node.\n\n        Args:\n            nodes (Sequence[Document]): nodes to extract metadata from\n\n        \"\"\"\n        return asyncio_run(self.aextract(nodes))\n\n    async def aprocess_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        excluded_embed_metadata_keys: Optional[List[str]] = None,\n        excluded_llm_metadata_keys: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        \"\"\"Post process nodes parsed from documents.\n\n        Allows extractors to be chained.\n\n        Args:\n            nodes (List[BaseNode]): nodes to post-process\n            excluded_embed_metadata_keys (Optional[List[str]]):\n                keys to exclude from embed metadata\n            excluded_llm_metadata_keys (Optional[List[str]]):\n                keys to exclude from llm metadata\n        \"\"\"\n        if self.in_place:\n            new_nodes = nodes\n        else:\n            new_nodes = [deepcopy(node) for node in nodes]\n\n        cur_metadata_list = await self.aextract(new_nodes)\n        for idx, node in enumerate(new_nodes):\n            node.metadata.update(cur_metadata_list[idx])\n\n        for idx, node in enumerate(new_nodes):\n            if excluded_embed_metadata_keys is not None:\n                node.excluded_embed_metadata_keys.extend(excluded_embed_metadata_keys)\n            if excluded_llm_metadata_keys is not None:\n                node.excluded_llm_metadata_keys.extend(excluded_llm_metadata_keys)\n            if not self.disable_template_rewrite:\n                if isinstance(node, TextNode):\n                    cast(TextNode, node).text_template = self.node_text_template\n\n        return new_nodes  # type: ignore\n\n    def process_nodes(\n        self,\n        nodes: Sequence[BaseNode],\n        excluded_embed_metadata_keys: Optional[List[str]] = None,\n        excluded_llm_metadata_keys: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[BaseNode]:\n        return asyncio_run(\n            self.aprocess_nodes(\n                nodes,\n                excluded_embed_metadata_keys=excluded_embed_metadata_keys,\n                excluded_llm_metadata_keys=excluded_llm_metadata_keys,\n                **kwargs,\n            )\n        )\n\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        \"\"\"Post process nodes parsed from documents.\n\n        Allows extractors to be chained.\n\n        Args:\n            nodes (List[BaseNode]): nodes to post-process\n        \"\"\"\n        return self.process_nodes(nodes, **kwargs)\n\n    async def acall(self, nodes: Sequence[BaseNode], **kwargs: Any) -> List[BaseNode]:\n        \"\"\"Post process nodes parsed from documents.\n\n        Allows extractors to be chained.\n\n        Args:\n            nodes (List[BaseNode]): nodes to post-process\n        \"\"\"\n        return await self.aprocess_nodes(nodes, **kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/extractors/document_context.py",
    "filename": "document_context.py",
    "relpath": "extractors/document_context.py",
    "start_line": 1,
    "end_line": 343,
    "length": 343,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "is_text_node",
      "__init__",
      "_count_tokens",
      "_agenerate_node_context",
      "_get_document",
      "aextract"
    ],
    "chunk_class_names": [
      "DocumentContextExtractor"
    ],
    "document_function_names": [
      "is_text_node",
      "__init__",
      "_count_tokens",
      "_agenerate_node_context",
      "_get_document",
      "aextract"
    ],
    "document_class_names": [
      "DocumentContextExtractor"
    ],
    "content": "import asyncio\nimport logging\nimport random\nfrom functools import lru_cache\nfrom typing import (\n    Any,\n    ClassVar,\n    Coroutine,\n    Dict,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Set,\n    Union,\n)\nfrom typing_extensions import TypeGuard\n\nfrom llama_index.core import Settings\nfrom llama_index.core.async_utils import DEFAULT_NUM_WORKERS, run_jobs\nfrom llama_index.core.extractors import BaseExtractor\nfrom llama_index.core.llms import (\n    AudioBlock,\n    ChatMessage,\n    ChatResponse,\n    ImageBlock,\n    LLM,\n    TextBlock,\n)\nfrom llama_index.core.schema import BaseNode, Node, TextNode\nfrom llama_index.core.storage.docstore.simple_docstore import DocumentStore\n\n\ndef is_text_node(node: BaseNode) -> TypeGuard[Union[Node, TextNode]]:\n    return isinstance(node, (Node, TextNode))\n\n\nOversizeStrategy = Literal[\"warn\", \"error\", \"ignore\"]\n\n\n# original context prompt from the Anthropic cookbook/blogpost, works well\nORIGINAL_CONTEXT_PROMPT: str = \"\"\"\nPlease give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\nAnswer only with the succinct context and nothing else.\n\"\"\"\n\n# miniaturized context prompt, generates better results, produces more keyword-laden results for better matches\nSUCCINCT_CONTEXT_PROMPT: str = \"\"\"\nGenerate keywords and brief phrases describing the main topics, entities, and actions in this text. Replace pronouns with their specific referents. Disambiguate pronouns and ambiguous terms in the chunk. Format as comma-separated phrases. Exclude meta-commentary about the text itself.\n\"\"\"\n\n\nclass DocumentContextExtractor(BaseExtractor):\n    \"\"\"\n    An LLM-based context extractor for enhancing RAG accuracy through document analysis.\n\n    ! Nodes that already have the 'key' in node.metadata will NOT be processed - will be skipped !\n\n    This extractor processes documents and their nodes to generate contextual metadata,\n    implementing the approach described in the Anthropic \"Contextual Retrieval\" blog post.\n    It handles rate limits, document size constraints, and parallel processing of nodes.\n\n    Attributes:\n        llm (LLM): Language model instance for generating context\n        docstore (DocumentStore): Storage for parent documents\n        key (str): Metadata key for storing extracted context\n        prompt (str): Prompt template for context generation\n        doc_ids (Set[str]): Set of processed document IDs\n        max_context_length (int): Maximum allowed document context length\n        max_output_tokens (int): Maximum tokens in generated context\n        oversized_document_strategy (OversizeStrategy): Strategy for handling large documents\n\n    Example:\n        ```python\n        extractor = DocumentContextExtractor(\n            docstore=my_docstore,\n            llm=my_llm,\n            max_context_length=64000,\n            max_output_tokens=256\n        )\n        metadata_list = await extractor.aextract(nodes)\n        ```\n    \"\"\"\n\n    # Pydantic fields\n    llm: LLM\n    docstore: DocumentStore\n    key: str\n    prompt: str\n    doc_ids: Set[str]\n    max_context_length: int\n    max_output_tokens: int\n    oversized_document_strategy: OversizeStrategy\n    num_workers: int = DEFAULT_NUM_WORKERS\n\n    ORIGINAL_CONTEXT_PROMPT: ClassVar[str] = ORIGINAL_CONTEXT_PROMPT\n    SUCCINCT_CONTEXT_PROMPT: ClassVar[str] = SUCCINCT_CONTEXT_PROMPT\n\n    DEFAULT_KEY: str = \"context\"\n\n    def __init__(\n        self,\n        docstore: DocumentStore,\n        llm: Optional[LLM] = None,\n        max_context_length: int = 1000,\n        key: str = DEFAULT_KEY,\n        prompt: str = ORIGINAL_CONTEXT_PROMPT,\n        num_workers: int = DEFAULT_NUM_WORKERS,\n        max_output_tokens: int = 512,\n        oversized_document_strategy: OversizeStrategy = \"warn\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        assert hasattr(\n            llm, \"achat\"\n        )  # not all LLMs have this, particularly the huggingfaceapi ones.\n\n        super().__init__(\n            llm=llm or Settings.llm,\n            docstore=docstore,\n            key=key,\n            prompt=prompt,\n            doc_ids=set(),\n            max_context_length=max_context_length,\n            max_output_tokens=max_output_tokens,\n            oversized_document_strategy=oversized_document_strategy,\n            num_workers=num_workers,\n            **kwargs,\n        )\n\n    # this can take a surprisingly long time on longer docs so we cache it. For oversized docs, we end up counting twice, the 2nd time without the cache.\n    # but if you're repeateddly running way oversize docs, the time that takes won't be what matters anyways.\n    @staticmethod\n    @lru_cache(maxsize=1000)\n    def _count_tokens(text: str) -> int:\n        \"\"\"\n        This can take a surprisingly long time on longer docs so we cache it, and we need to call it on every doc, regardless of size.\n        \"\"\"\n        encoder = Settings.tokenizer\n        tokens = encoder(text)\n        return len(tokens)\n\n    async def _agenerate_node_context(\n        self,\n        node: Union[Node, TextNode],\n        metadata: Dict,\n        document: Union[Node, TextNode],\n        prompt: str,\n        key: str,\n    ) -> Dict:\n        \"\"\"\n        Generate context for a node using LLM with retry logic.\n\n        Implements exponential backoff for rate limit handling and uses prompt\n        caching when available. The function retries on rate limits.\n\n        Args:\n            node: Node to generate context for\n            metadata: Metadata dictionary to update\n            document: Parent document containing the node\n            prompt: Prompt template for context generation\n            key: Metadata key for storing generated context\n\n        Returns:\n            Updated metadata dictionary with generated context\n\n        Note:\n            Uses exponential backoff starting at 60 seconds with up to 5 retries\n            for rate limit handling.\n        \"\"\"\n        cached_text = f\"<document>{document.get_content()}</document>\"\n        messages = [\n            ChatMessage(\n                role=\"user\",\n                content=[\n                    TextBlock(\n                        text=cached_text,\n                        type=\"text\",\n                    )\n                ],\n                additional_kwargs={\"cache_control\": {\"type\": \"ephemeral\"}},\n            ),\n            ChatMessage(\n                role=\"user\",\n                content=[\n                    TextBlock(\n                        text=f\"Here is the chunk we want to situate within the whole document:\\n<chunk>{node.get_content()}</chunk>\\n{prompt}\",\n                        type=\"text\",\n                    )\n                ],\n            ),\n        ]\n\n        max_retries = 5\n        base_delay = 60\n\n        for attempt in range(max_retries):\n            try:\n                # Extra headers typically dont cause issues\n                headers = {\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n\n                response: ChatResponse = await self.llm.achat(\n                    messages, max_tokens=self.max_output_tokens, extra_headers=headers\n                )\n\n                first_block: Union[\n                    TextBlock, ImageBlock, AudioBlock\n                ] = response.message.blocks[0]\n                if isinstance(first_block, TextBlock):\n                    metadata[key] = first_block.text\n                else:\n                    logging.warning(\n                        f\"Received non-text block type: {type(first_block)}\"\n                    )\n                return metadata\n\n            except Exception as e:\n                is_rate_limit = any(\n                    message in str(e).lower()\n                    for message in [\"rate limit\", \"too many requests\", \"429\"]\n                )\n\n                if is_rate_limit and attempt < max_retries - 1:\n                    delay = (base_delay * (2**attempt)) + (random.random() * 0.5)\n                    logging.warning(\n                        f\"Rate limit hit, retrying in {delay:.1f} seconds \"\n                        f\"(attempt {attempt + 1}/{max_retries})\"\n                    )\n                    await asyncio.sleep(delay)\n                    continue\n\n                if is_rate_limit:\n                    logging.error(\n                        f\"Failed after {max_retries} retries due to rate limiting\"\n                    )\n                else:\n                    logging.warning(\n                        f\"Error generating context for node {node.node_id}: {e}\",\n                        exc_info=True,\n                    )\n                return metadata\n\n        return metadata\n\n    async def _get_document(self, doc_id: str) -> Optional[Union[Node, TextNode]]:\n        \"\"\"Counting tokens can be slow, as can awaiting the docstore (potentially), so we keep a small lru_cache.\"\"\"\n        # first we need to get the document\n        try:\n            doc = await self.docstore.aget_document(doc_id)\n        except ValueError as e:\n            if \"not found\" in str(e):\n                logging.warning(f\"Document {doc_id} not found in docstore\")\n                return None\n        if not doc:\n            logging.warning(f\"Document {doc_id} not found in docstore\")\n            return None\n        if not is_text_node(doc):\n            logging.warning(f\"Document {doc_id} is not an instance of (TextNode, Node)\")\n            return None\n\n        # then truncate if necessary.\n        if self.max_context_length is not None:\n            strategy = self.oversized_document_strategy\n            token_count = self._count_tokens(doc.get_content())\n            if token_count > self.max_context_length:\n                message = (\n                    f\"Document {doc.node_id} is too large ({token_count} tokens) \"\n                    f\"to be processed. Doc metadata: {doc.metadata}\"\n                )\n\n                if strategy == \"warn\":\n                    logging.warning(message)\n                elif strategy == \"error\":\n                    raise ValueError(message)\n                elif strategy == \"ignore\":\n                    pass\n                else:\n                    raise ValueError(f\"Unknown oversized document strategy: {strategy}\")\n\n        return doc\n\n    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        \"\"\"\n        Extract context for multiple nodes asynchronously, optimized for loosely ordered nodes.\n        Processes each node independently without guaranteeing sequential document handling.\n        Nodes will be *mostly* processed in document-order assuming nodes get passed in document-order.\n\n        Args:\n            nodes: List of nodes to process, ideally grouped by source document\n\n        Returns:\n            List of metadata dictionaries with generated context\n        \"\"\"\n        metadata_list: List[Dict] = []\n        for _ in nodes:\n            metadata_list.append({})\n        metadata_map = {\n            node.node_id: metadata_dict\n            for metadata_dict, node in zip(metadata_list, nodes)\n        }\n\n        # sorting takes a tiny amount of time - 0.4s for 1_000_000 nodes. but 1_000_000 nodes takes potentially hours to process\n        # considering sorting CAN save the users hundreds of dollars in API costs, we just sort and leave no option to do otherwise.\n        # The math always works out in the user's favor and we can't guarantee things are sorted in the first place.\n        sorted_nodes = sorted(\n            nodes, key=lambda n: n.source_node.node_id if n.source_node else \"\"\n        )\n\n        # iterate over all the nodes and generate the jobs\n        node_tasks: List[Coroutine[Any, Any, Any]] = []\n        for node in sorted_nodes:\n            if not (node.source_node and is_text_node(node)):\n                continue\n\n            # Skip already processed nodes\n            if self.key in node.metadata:\n                continue\n\n            doc: Optional[Union[Node, TextNode]] = await self._get_document(\n                node.source_node.node_id\n            )\n            if not doc:\n                continue\n\n            metadata = metadata_map[node.node_id]\n            # this modifies metadata in-place, adding a new key to the dictionary - we needed do anytyhing with the return value\n            task = self._agenerate_node_context(\n                node, metadata, doc, self.prompt, self.key\n            )\n            node_tasks.append(task)\n\n        # then run the jobs - this does return the metadata list, but we already have it\n        await run_jobs(\n            node_tasks,\n            show_progress=self.show_progress,\n            workers=self.num_workers,\n        )\n\n        return metadata_list\n\n\nif __name__ == \"__main__\":\n    print(DocumentContextExtractor.ORIGINAL_CONTEXT_PROMPT)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/extractors/__init__.py",
    "filename": "__init__.py",
    "relpath": "extractors/__init__.py",
    "start_line": 1,
    "end_line": 19,
    "length": 19,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.extractors.interface import BaseExtractor\nfrom llama_index.core.extractors.metadata_extractors import (\n    KeywordExtractor,\n    PydanticProgramExtractor,\n    QuestionsAnsweredExtractor,\n    SummaryExtractor,\n    TitleExtractor,\n)\nfrom llama_index.core.extractors.document_context import DocumentContextExtractor\n\n__all__ = [\n    \"SummaryExtractor\",\n    \"QuestionsAnsweredExtractor\",\n    \"TitleExtractor\",\n    \"KeywordExtractor\",\n    \"BaseExtractor\",\n    \"PydanticProgramExtractor\",\n    \"DocumentContextExtractor\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/extractors/metadata_extractors.py",
    "filename": "metadata_extractors.py",
    "relpath": "extractors/metadata_extractors.py",
    "start_line": 1,
    "end_line": 462,
    "length": 462,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "add_class_name",
      "__init__",
      "class_name",
      "aextract",
      "filter_nodes",
      "separate_nodes_by_ref_id",
      "extract_titles",
      "get_title_candidates",
      "__init__",
      "class_name",
      "_aextract_keywords_from_node",
      "aextract",
      "__init__",
      "class_name",
      "_aextract_questions_from_node",
      "aextract",
      "__init__",
      "class_name",
      "_agenerate_node_summary",
      "aextract"
    ],
    "chunk_class_names": [
      "TitleExtractor",
      "KeywordExtractor",
      "QuestionsAnsweredExtractor",
      "SummaryExtractor"
    ],
    "document_function_names": [
      "add_class_name",
      "__init__",
      "class_name",
      "aextract",
      "filter_nodes",
      "separate_nodes_by_ref_id",
      "extract_titles",
      "get_title_candidates",
      "__init__",
      "class_name",
      "_aextract_keywords_from_node",
      "aextract",
      "__init__",
      "class_name",
      "_aextract_questions_from_node",
      "aextract",
      "__init__",
      "class_name",
      "_agenerate_node_summary",
      "aextract",
      "class_name",
      "_acall_program",
      "aextract"
    ],
    "document_class_names": [
      "TitleExtractor",
      "KeywordExtractor",
      "QuestionsAnsweredExtractor",
      "SummaryExtractor",
      "PydanticProgramExtractor"
    ],
    "content": "\"\"\"\nMetadata extractors for nodes.\nCurrently, only `TextNode` is supported.\n\nSupported metadata:\nNode-level:\n    - `SummaryExtractor`: Summary of each node, and pre and post nodes\n    - `QuestionsAnsweredExtractor`: Questions that the node can answer\n    - `KeywordsExtractor`: Keywords that uniquely identify the node\nDocument-level:\n    - `TitleExtractor`: Document title, possible inferred across multiple nodes\n\nUnimplemented (contributions welcome):\nSubsection:\n    - Position of node in subsection hierarchy (and associated subtitles)\n    - Hierarchically organized summary\n\nThe prompts used to generate the metadata are specifically aimed to help\ndisambiguate the document or subsection from other similar documents or subsections.\n(similar with contrastive learning)\n\"\"\"\n\nfrom typing import Any, Callable, Dict, Generic, List, Optional, Sequence, cast\n\nfrom llama_index.core.async_utils import DEFAULT_NUM_WORKERS, run_jobs\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    PrivateAttr,\n    SerializeAsAny,\n)\nfrom llama_index.core.extractors.interface import BaseExtractor\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.schema import BaseNode, TextNode\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.types import BasePydanticProgram, Model\n\nDEFAULT_TITLE_NODE_TEMPLATE = \"\"\"\\\nContext: {context_str}. Give a title that summarizes all of \\\nthe unique entities, titles or themes found in the context. Title: \"\"\"\n\n\nDEFAULT_TITLE_COMBINE_TEMPLATE = \"\"\"\\\n{context_str}. Based on the above candidate titles and content, \\\nwhat is the comprehensive title for this document? Title: \"\"\"\n\n\ndef add_class_name(value: Any, handler: Callable, info: Any) -> Dict[str, Any]:\n    partial_result = handler(value, info)\n    if hasattr(value, \"class_name\"):\n        partial_result.update({\"class_name\": value.class_name()})\n    return partial_result\n\n\nclass TitleExtractor(BaseExtractor):\n    \"\"\"Title extractor. Useful for long documents. Extracts `document_title`\n    metadata field.\n\n    Args:\n        llm (Optional[LLM]): LLM\n        nodes (int): number of nodes from front to use for title extraction\n        node_template (str): template for node-level title clues extraction\n        combine_template (str): template for combining node-level clues into\n            a document-level title\n    \"\"\"\n\n    is_text_node_only: bool = False  # can work for mixture of text and non-text nodes\n    llm: SerializeAsAny[LLM] = Field(description=\"The LLM to use for generation.\")\n    nodes: int = Field(\n        default=5,\n        description=\"The number of nodes to extract titles from.\",\n        gt=0,\n    )\n    node_template: str = Field(\n        default=DEFAULT_TITLE_NODE_TEMPLATE,\n        description=\"The prompt template to extract titles with.\",\n    )\n    combine_template: str = Field(\n        default=DEFAULT_TITLE_COMBINE_TEMPLATE,\n        description=\"The prompt template to merge titles with.\",\n    )\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        # TODO: llm_predictor arg is deprecated\n        llm_predictor: Optional[LLM] = None,\n        nodes: int = 5,\n        node_template: str = DEFAULT_TITLE_NODE_TEMPLATE,\n        combine_template: str = DEFAULT_TITLE_COMBINE_TEMPLATE,\n        num_workers: int = DEFAULT_NUM_WORKERS,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if nodes < 1:\n            raise ValueError(\"num_nodes must be >= 1\")\n\n        super().__init__(\n            llm=llm or llm_predictor or Settings.llm,\n            nodes=nodes,\n            node_template=node_template,\n            combine_template=combine_template,\n            num_workers=num_workers,\n            **kwargs,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"TitleExtractor\"\n\n    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        nodes_by_doc_id = self.separate_nodes_by_ref_id(nodes)\n        titles_by_doc_id = await self.extract_titles(nodes_by_doc_id)\n        return [{\"document_title\": titles_by_doc_id[node.ref_doc_id]} for node in nodes]\n\n    def filter_nodes(self, nodes: Sequence[BaseNode]) -> List[BaseNode]:\n        filtered_nodes: List[BaseNode] = []\n        for node in nodes:\n            if self.is_text_node_only and not isinstance(node, TextNode):\n                continue\n            filtered_nodes.append(node)\n        return filtered_nodes\n\n    def separate_nodes_by_ref_id(self, nodes: Sequence[BaseNode]) -> Dict:\n        separated_items: Dict[Optional[str], List[BaseNode]] = {}\n\n        for node in nodes:\n            key = node.ref_doc_id\n            if key not in separated_items:\n                separated_items[key] = []\n\n            if len(separated_items[key]) < self.nodes:\n                separated_items[key].append(node)\n\n        return separated_items\n\n    async def extract_titles(self, nodes_by_doc_id: Dict) -> Dict:\n        titles_by_doc_id = {}\n        for key, nodes in nodes_by_doc_id.items():\n            title_candidates = await self.get_title_candidates(nodes)\n            combined_titles = \", \".join(title_candidates)\n            titles_by_doc_id[key] = await self.llm.apredict(\n                PromptTemplate(template=self.combine_template),\n                context_str=combined_titles,\n            )\n        return titles_by_doc_id\n\n    async def get_title_candidates(self, nodes: List[BaseNode]) -> List[str]:\n        title_jobs = [\n            self.llm.apredict(\n                PromptTemplate(template=self.node_template),\n                context_str=cast(TextNode, node).text,\n            )\n            for node in nodes\n        ]\n        return await run_jobs(\n            title_jobs, show_progress=self.show_progress, workers=self.num_workers\n        )\n\n\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE = \"\"\"\\\n{context_str}. Give {keywords} unique keywords for this \\\ndocument. Format as comma separated. Keywords: \"\"\"\n\n\nclass KeywordExtractor(BaseExtractor):\n    \"\"\"Keyword extractor. Node-level extractor. Extracts\n    `excerpt_keywords` metadata field.\n\n    Args:\n        llm (Optional[LLM]): LLM\n        keywords (int): number of keywords to extract\n        prompt_template (str): template for keyword extraction\n    \"\"\"\n\n    llm: SerializeAsAny[LLM] = Field(description=\"The LLM to use for generation.\")\n    keywords: int = Field(\n        default=5, description=\"The number of keywords to extract.\", gt=0\n    )\n\n    prompt_template: str = Field(\n        default=DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n        description=\"Prompt template to use when generating keywords.\",\n    )\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        # TODO: llm_predictor arg is deprecated\n        llm_predictor: Optional[LLM] = None,\n        keywords: int = 5,\n        prompt_template: str = DEFAULT_KEYWORD_EXTRACT_TEMPLATE,\n        num_workers: int = DEFAULT_NUM_WORKERS,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if keywords < 1:\n            raise ValueError(\"num_keywords must be >= 1\")\n\n        super().__init__(\n            llm=llm or llm_predictor or Settings.llm,\n            keywords=keywords,\n            prompt_template=prompt_template,\n            num_workers=num_workers,\n            **kwargs,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"KeywordExtractor\"\n\n    async def _aextract_keywords_from_node(self, node: BaseNode) -> Dict[str, str]:\n        \"\"\"Extract keywords from a node and return it's metadata dict.\"\"\"\n        if self.is_text_node_only and not isinstance(node, TextNode):\n            return {}\n\n        context_str = node.get_content(metadata_mode=self.metadata_mode)\n        keywords = await self.llm.apredict(\n            PromptTemplate(template=self.prompt_template),\n            keywords=self.keywords,\n            context_str=context_str,\n        )\n\n        return {\"excerpt_keywords\": keywords.strip()}\n\n    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        keyword_jobs = []\n        for node in nodes:\n            keyword_jobs.append(self._aextract_keywords_from_node(node))\n\n        metadata_list: List[Dict] = await run_jobs(\n            keyword_jobs, show_progress=self.show_progress, workers=self.num_workers\n        )\n\n        return metadata_list\n\n\nDEFAULT_QUESTION_GEN_TMPL = \"\"\"\\\nHere is the context:\n{context_str}\n\nGiven the contextual information, \\\ngenerate {num_questions} questions this context can provide \\\nspecific answers to which are unlikely to be found elsewhere.\n\nHigher-level summaries of surrounding context may be provided \\\nas well. Try using these summaries to generate better questions \\\nthat this context can answer.\n\n\"\"\"\n\n\nclass QuestionsAnsweredExtractor(BaseExtractor):\n    \"\"\"\n    Questions answered extractor. Node-level extractor.\n    Extracts `questions_this_excerpt_can_answer` metadata field.\n\n    Args:\n        llm (Optional[LLM]): LLM\n        questions (int): number of questions to extract\n        prompt_template (str): template for question extraction,\n        embedding_only (bool): whether to use embedding only\n    \"\"\"\n\n    llm: SerializeAsAny[LLM] = Field(description=\"The LLM to use for generation.\")\n    questions: int = Field(\n        default=5,\n        description=\"The number of questions to generate.\",\n        gt=0,\n    )\n    prompt_template: str = Field(\n        default=DEFAULT_QUESTION_GEN_TMPL,\n        description=\"Prompt template to use when generating questions.\",\n    )\n    embedding_only: bool = Field(\n        default=True, description=\"Whether to use metadata for emebddings only.\"\n    )\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        # TODO: llm_predictor arg is deprecated\n        llm_predictor: Optional[LLM] = None,\n        questions: int = 5,\n        prompt_template: str = DEFAULT_QUESTION_GEN_TMPL,\n        embedding_only: bool = True,\n        num_workers: int = DEFAULT_NUM_WORKERS,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if questions < 1:\n            raise ValueError(\"questions must be >= 1\")\n\n        super().__init__(\n            llm=llm or llm_predictor or Settings.llm,\n            questions=questions,\n            prompt_template=prompt_template,\n            embedding_only=embedding_only,\n            num_workers=num_workers,\n            **kwargs,\n        )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"QuestionsAnsweredExtractor\"\n\n    async def _aextract_questions_from_node(self, node: BaseNode) -> Dict[str, str]:\n        \"\"\"Extract questions from a node and return it's metadata dict.\"\"\"\n        if self.is_text_node_only and not isinstance(node, TextNode):\n            return {}\n\n        context_str = node.get_content(metadata_mode=self.metadata_mode)\n        prompt = PromptTemplate(template=self.prompt_template)\n        questions = await self.llm.apredict(\n            prompt, num_questions=self.questions, context_str=context_str\n        )\n\n        return {\"questions_this_excerpt_can_answer\": questions.strip()}\n\n    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        questions_jobs = []\n        for node in nodes:\n            questions_jobs.append(self._aextract_questions_from_node(node))\n\n        metadata_list: List[Dict] = await run_jobs(\n            questions_jobs, show_progress=self.show_progress, workers=self.num_workers\n        )\n\n        return metadata_list\n\n\nDEFAULT_SUMMARY_EXTRACT_TEMPLATE = \"\"\"\\\nHere is the content of the section:\n{context_str}\n\nSummarize the key topics and entities of the section. \\\n\nSummary: \"\"\"\n\n\nclass SummaryExtractor(BaseExtractor):\n    \"\"\"\n    Summary extractor. Node-level extractor with adjacent sharing.\n    Extracts `section_summary`, `prev_section_summary`, `next_section_summary`\n    metadata fields.\n\n    Args:\n        llm (Optional[LLM]): LLM\n        summaries (List[str]): list of summaries to extract: 'self', 'prev', 'next'\n        prompt_template (str): template for summary extraction\n    \"\"\"\n\n    llm: SerializeAsAny[LLM] = Field(description=\"The LLM to use for generation.\")\n    summaries: List[str] = Field(\n        description=\"List of summaries to extract: 'self', 'prev', 'next'\"\n    )\n    prompt_template: str = Field(\n        default=DEFAULT_SUMMARY_EXTRACT_TEMPLATE,\n        description=\"Template to use when generating summaries.\",\n    )\n\n    _self_summary: bool = PrivateAttr()\n    _prev_summary: bool = PrivateAttr()\n    _next_summary: bool = PrivateAttr()\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        # TODO: llm_predictor arg is deprecated\n        llm_predictor: Optional[LLM] = None,\n        summaries: List[str] = [\"self\"],\n        prompt_template: str = DEFAULT_SUMMARY_EXTRACT_TEMPLATE,\n        num_workers: int = DEFAULT_NUM_WORKERS,\n        **kwargs: Any,\n    ):\n        # validation\n        if not all(s in [\"self\", \"prev\", \"next\"] for s in summaries):\n            raise ValueError(\"summaries must be one of ['self', 'prev', 'next']\")\n\n        super().__init__(\n            llm=llm or llm_predictor or Settings.llm,\n            summaries=summaries,\n            prompt_template=prompt_template,\n            num_workers=num_workers,\n            **kwargs,\n        )\n\n        self._self_summary = \"self\" in summaries\n        self._prev_summary = \"prev\" in summaries\n        self._next_summary = \"next\" in summaries\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SummaryExtractor\"\n\n    async def _agenerate_node_summary(self, node: BaseNode) -> str:\n        \"\"\"Generate a summary for a node.\"\"\"\n        if self.is_text_node_only and not isinstance(node, TextNode):\n            return \"\"\n\n        context_str = node.get_content(metadata_mode=self.metadata_mode)\n        summary = await self.llm.apredict(\n            PromptTemplate(template=self.prompt_template), context_str=context_str\n        )\n\n        return summary.strip()\n\n    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        if not all(isinstance(node, TextNode) for node in nodes):\n            raise ValueError(\"Only `TextNode` is allowed for `Summary` extractor\")\n\n        node_summaries_jobs = []\n        for node in nodes:\n            node_summaries_jobs.append(self._agenerate_node_summary(node))\n\n        node_summaries = await run_jobs(\n            node_summaries_jobs,\n            show_progress=self.show_progress,\n            workers=self.num_workers,\n        )\n\n        # Extract node-level summary metadata\n        metadata_list: List[Dict] = [{} for _ in nodes]\n        for i, metadata in enumerate(metadata_list):\n            if i > 0 and self._prev_summary and node_summaries[i - 1]:\n                metadata[\"prev_section_summary\"] = node_summaries[i - 1]\n            if i < len(nodes) - 1 and self._next_summary and node_summaries[i + 1]:\n                metadata[\"next_section_summary\"] = node_summaries[i + 1]\n            if self._self_summary and node_summaries[i]:\n                metadata[\"section_summary\"] = node_summaries[i]\n\n        return metadata_list\n\n\nDEFAULT_ENTITY_MAP = {\n    \"PER\": \"persons\",\n    \"ORG\": \"organizations\",\n    \"LOC\": \"locations\",\n    \"ANIM\": \"animals\",\n    \"BIO\": \"biological\",\n    \"CEL\": \"celestial\",\n    \"DIS\": \"diseases\",\n    \"EVE\": \"events\",\n    \"FOOD\": \"foods\",\n    \"INST\": \"instruments\",\n    \"MEDIA\": \"media\",\n    \"PLANT\": \"plants\",\n    \"MYTH\": \"mythological\",\n    \"TIME\": \"times\",\n    \"VEHI\": \"vehicles\",\n}\n\nDEFAULT_ENTITY_MODEL = \"tomaarsen/span-marker-mbert-base-multinerd\"\n\n\nDEFAULT_EXTRACT_TEMPLATE_STR = \"\"\"\\\nHere is the content of the section:\n----------------\n{context_str}\n----------------\nGiven the contextual information, extract out a {class_name} object.\\\n\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/extractors/metadata_extractors.py",
    "filename": "metadata_extractors.py",
    "relpath": "extractors/metadata_extractors.py",
    "start_line": 462,
    "end_line": 517,
    "length": 56,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "class_name",
      "_acall_program",
      "aextract"
    ],
    "chunk_class_names": [
      "PydanticProgramExtractor"
    ],
    "document_function_names": [
      "add_class_name",
      "__init__",
      "class_name",
      "aextract",
      "filter_nodes",
      "separate_nodes_by_ref_id",
      "extract_titles",
      "get_title_candidates",
      "__init__",
      "class_name",
      "_aextract_keywords_from_node",
      "aextract",
      "__init__",
      "class_name",
      "_aextract_questions_from_node",
      "aextract",
      "__init__",
      "class_name",
      "_agenerate_node_summary",
      "aextract",
      "class_name",
      "_acall_program",
      "aextract"
    ],
    "document_class_names": [
      "TitleExtractor",
      "KeywordExtractor",
      "QuestionsAnsweredExtractor",
      "SummaryExtractor",
      "PydanticProgramExtractor"
    ],
    "content": "class PydanticProgramExtractor(BaseExtractor, Generic[Model]):\n    \"\"\"Pydantic program extractor.\n\n    Uses an LLM to extract out a Pydantic object. Return attributes of that object\n    in a dictionary.\n\n    \"\"\"\n\n    program: SerializeAsAny[BasePydanticProgram[Model]] = Field(\n        ..., description=\"Pydantic program to extract.\"\n    )\n    input_key: str = Field(\n        default=\"input\",\n        description=(\n            \"Key to use as input to the program (the program \"\n            \"template string must expose this key).\"\n        ),\n    )\n    extract_template_str: str = Field(\n        default=DEFAULT_EXTRACT_TEMPLATE_STR,\n        description=\"Template to use for extraction.\",\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"PydanticModelExtractor\"\n\n    async def _acall_program(self, node: BaseNode) -> Dict[str, Any]:\n        \"\"\"Call the program on a node.\"\"\"\n        if self.is_text_node_only and not isinstance(node, TextNode):\n            return {}\n\n        extract_str = self.extract_template_str.format(\n            context_str=node.get_content(metadata_mode=self.metadata_mode),\n            class_name=self.program.output_cls.__name__,\n        )\n\n        ret_object = await self.program.acall(**{self.input_key: extract_str})\n        assert not isinstance(ret_object, list)\n\n        return ret_object.model_dump()\n\n    async def aextract(self, nodes: Sequence[BaseNode]) -> List[Dict]:\n        \"\"\"Extract pydantic program.\"\"\"\n        program_jobs = []\n        for node in nodes:\n            program_jobs.append(self._acall_program(node))\n\n        metadata_list: List[Dict] = await run_jobs(\n            program_jobs, show_progress=self.show_progress, workers=self.num_workers\n        )\n\n        return metadata_list"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/multi_modal_llms/base.py",
    "filename": "base.py",
    "relpath": "multi_modal_llms/base.py",
    "start_line": 1,
    "end_line": 265,
    "length": 265,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "metadata",
      "complete",
      "stream_complete",
      "chat",
      "stream_chat",
      "acomplete",
      "astream_complete",
      "achat",
      "astream_chat",
      "_as_query_component",
      "__init_subclass__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "MultiModalLLMMetadata",
      "MultiModalLLM",
      "hierarchy",
      "BaseMultiModalComponent",
      "MultiModalCompleteComponent"
    ],
    "document_function_names": [
      "__init__",
      "metadata",
      "complete",
      "stream_complete",
      "chat",
      "stream_chat",
      "acomplete",
      "astream_complete",
      "achat",
      "astream_chat",
      "_as_query_component",
      "__init_subclass__",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "MultiModalLLMMetadata",
      "MultiModalLLM",
      "hierarchy",
      "BaseMultiModalComponent",
      "MultiModalCompleteComponent"
    ],
    "content": "from abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, Sequence, get_args\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseAsyncGen,\n    CompletionResponseGen,\n)\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n)\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.constants import (\n    DEFAULT_CONTEXT_WINDOW,\n    DEFAULT_NUM_INPUT_FILES,\n    DEFAULT_NUM_OUTPUTS,\n)\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.llms.callbacks import llm_chat_callback, llm_completion_callback\nfrom llama_index.core.schema import BaseComponent, ImageDocument, ImageNode\n\n\nclass MultiModalLLMMetadata(BaseModel):\n    model_config = ConfigDict(protected_namespaces=(\"pydantic_model_\",))\n    context_window: Optional[int] = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=(\n            \"Total number of tokens the model can be input when generating a response.\"\n        ),\n    )\n    num_output: Optional[int] = Field(\n        default=DEFAULT_NUM_OUTPUTS,\n        description=\"Number of tokens the model can output when generating a response.\",\n    )\n    num_input_files: Optional[int] = Field(\n        default=DEFAULT_NUM_INPUT_FILES,\n        description=\"Number of input files the model can take when generating a response.\",\n    )\n    is_function_calling_model: Optional[bool] = Field(\n        default=False,\n        # SEE: https://openai.com/blog/function-calling-and-other-api-updates\n        description=(\n            \"Set True if the model supports function calling messages, similar to\"\n            \" OpenAI's function calling API. For example, converting 'Email Anya to\"\n            \" see if she wants to get coffee next Friday' to a function call like\"\n            \" `send_email(to: string, body: string)`.\"\n        ),\n    )\n    model_name: str = Field(\n        default=\"unknown\",\n        description=(\n            \"The model's name used for logging, testing, and sanity checking. For some\"\n            \" models this can be automatically discerned. For other models, like\"\n            \" locally loaded models, this must be manually specified.\"\n        ),\n    )\n\n    is_chat_model: bool = Field(\n        default=False,\n        description=(\n            \"Set True if the model exposes a chat interface (i.e. can be passed a\"\n            \" sequence of messages, rather than text), like OpenAI's\"\n            \" /v1/chat/completions endpoint.\"\n        ),\n    )\n\n\nclass MultiModalLLM(ChainableMixin, BaseComponent, DispatcherSpanMixin):\n    \"\"\"Multi-Modal LLM interface.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=CallbackManager, exclude=True\n    )\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        # Help static checkers understand this class hierarchy\n        super().__init__(*args, **kwargs)\n\n    @property\n    @abstractmethod\n    def metadata(self) -> MultiModalLLMMetadata:\n        \"\"\"Multi-Modal LLM metadata.\"\"\"\n\n    @abstractmethod\n    def complete(\n        self, prompt: str, image_documents: List[ImageNode], **kwargs: Any\n    ) -> CompletionResponse:\n        \"\"\"Completion endpoint for Multi-Modal LLM.\"\"\"\n\n    @abstractmethod\n    def stream_complete(\n        self, prompt: str, image_documents: List[ImageNode], **kwargs: Any\n    ) -> CompletionResponseGen:\n        \"\"\"Streaming completion endpoint for Multi-Modal LLM.\"\"\"\n\n    @abstractmethod\n    def chat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Chat endpoint for Multi-Modal LLM.\"\"\"\n\n    @abstractmethod\n    def stream_chat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponseGen:\n        \"\"\"Stream chat endpoint for Multi-Modal LLM.\"\"\"\n\n    # ===== Async Endpoints =====\n\n    @abstractmethod\n    async def acomplete(\n        self, prompt: str, image_documents: List[ImageNode], **kwargs: Any\n    ) -> CompletionResponse:\n        \"\"\"Async completion endpoint for Multi-Modal LLM.\"\"\"\n\n    @abstractmethod\n    async def astream_complete(\n        self, prompt: str, image_documents: List[ImageNode], **kwargs: Any\n    ) -> CompletionResponseAsyncGen:\n        \"\"\"Async streaming completion endpoint for Multi-Modal LLM.\"\"\"\n\n    @abstractmethod\n    async def achat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponse:\n        \"\"\"Async chat endpoint for Multi-Modal LLM.\"\"\"\n\n    @abstractmethod\n    async def astream_chat(\n        self,\n        messages: Sequence[ChatMessage],\n        **kwargs: Any,\n    ) -> ChatResponseAsyncGen:\n        \"\"\"Async streaming chat endpoint for Multi-Modal LLM.\"\"\"\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"Return query component.\"\"\"\n        if self.metadata.is_chat_model:\n            # TODO: we don't have a separate chat component\n            return MultiModalCompleteComponent(multi_modal_llm=self, **kwargs)\n        else:\n            return MultiModalCompleteComponent(multi_modal_llm=self, **kwargs)\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        \"\"\"\n        The callback decorators installs events, so they must be applied before\n        the span decorators, otherwise the spans wouldn't contain the events.\n        \"\"\"\n        for attr in (\n            \"complete\",\n            \"acomplete\",\n            \"stream_complete\",\n            \"astream_complete\",\n            \"chat\",\n            \"achat\",\n            \"stream_chat\",\n            \"astream_chat\",\n        ):\n            if callable(method := cls.__dict__.get(attr)):\n                if attr.endswith(\"chat\"):\n                    setattr(cls, attr, llm_chat_callback()(method))\n                else:\n                    setattr(cls, attr, llm_completion_callback()(method))\n        super().__init_subclass__(**kwargs)\n\n\nclass BaseMultiModalComponent(QueryComponent):\n    \"\"\"Base LLM component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    multi_modal_llm: MultiModalLLM = Field(..., description=\"LLM\")\n    streaming: bool = Field(default=False, description=\"Streaming mode\")\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: make callbacks work with multi-modal\n\n\nclass MultiModalCompleteComponent(BaseMultiModalComponent):\n    \"\"\"Multi-modal completion component.\"\"\"\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        if \"prompt\" not in input:\n            raise ValueError(\"Prompt must be in input dict.\")\n\n        # do special check to see if prompt is a list of chat messages\n        if isinstance(input[\"prompt\"], get_args(List[ChatMessage])):\n            raise NotImplementedError(\n                \"Chat messages not yet supported as input to multi-modal model.\"\n            )\n        else:\n            input[\"prompt\"] = validate_and_convert_stringable(input[\"prompt\"])\n\n        # make sure image documents are valid\n        if \"image_documents\" in input:\n            if not isinstance(input[\"image_documents\"], list):\n                raise ValueError(\"image_documents must be a list.\")\n            for doc in input[\"image_documents\"]:\n                if not isinstance(doc, (ImageDocument, ImageNode)):\n                    raise ValueError(\n                        \"image_documents must be a list of ImageNode objects.\"\n                    )\n\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # TODO: support only complete for now\n        prompt = kwargs[\"prompt\"]\n        image_documents = kwargs.get(\"image_documents\", [])\n\n        response: Any\n        if self.streaming:\n            response = self.multi_modal_llm.stream_complete(prompt, image_documents)\n        else:\n            response = self.multi_modal_llm.complete(prompt, image_documents)\n        return {\"output\": response}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # TODO: support only complete for now\n        # non-trivial to figure how to support chat/complete/etc.\n        prompt = kwargs[\"prompt\"]\n        image_documents = kwargs.get(\"image_documents\", [])\n\n        response: Any\n        if self.streaming:\n            response = await self.multi_modal_llm.astream_complete(\n                prompt, image_documents\n            )\n        else:\n            response = await self.multi_modal_llm.acomplete(prompt, image_documents)\n        return {\"output\": response}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # TODO: support only complete for now\n        return InputKeys.from_keys({\"prompt\", \"image_documents\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/multi_modal_llms/generic_utils.py",
    "filename": "generic_utils.py",
    "relpath": "multi_modal_llms/generic_utils.py",
    "start_line": 1,
    "end_line": 155,
    "length": 155,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "load_image_urls",
      "encode_image",
      "image_documents_to_base64",
      "infer_image_mimetype_from_file_path",
      "infer_image_mimetype_from_base64",
      "set_base64_and_mimetype_for_image_docs"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "load_image_urls",
      "encode_image",
      "image_documents_to_base64",
      "infer_image_mimetype_from_file_path",
      "infer_image_mimetype_from_base64",
      "set_base64_and_mimetype_for_image_docs"
    ],
    "document_class_names": [],
    "content": "import base64\nimport filetype\nimport logging\nfrom typing import List, Optional, Sequence\n\nimport requests\n\nfrom llama_index.core.schema import ImageDocument\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_image_urls(image_urls: List[str]) -> List[ImageDocument]:\n    \"\"\"Convert a list of image URLs into ImageDocument objects.\n\n    Args:\n        image_urls (List[str]): List of strings containing valid image URLs.\n\n    Returns:\n        List[ImageDocument]: List of ImageDocument objects.\n    \"\"\"\n    return [ImageDocument(image_url=url) for url in image_urls]\n\n\ndef encode_image(image_path: str) -> str:\n    \"\"\"Create base64 representation of an image.\n\n    Args:\n        image_path (str): Path to the image file\n\n    Returns:\n        str: Base64 encoded string of the image\n\n    Raises:\n        FileNotFoundError: If the `image_path` doesn't exist.\n        IOError: If there's an error reading the file.\n    \"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n\ndef image_documents_to_base64(\n    image_documents: Sequence[ImageDocument],\n) -> List[str]:\n    \"\"\"Convert ImageDocument objects to base64-encoded strings.\n\n    Args:\n        image_documents (Sequence[ImageDocument]: Sequence of\n            ImageDocument objects\n\n    Returns:\n        List[str]: List of base64-encoded image strings\n    \"\"\"\n    image_encodings = []\n\n    # Encode image documents to base64\n    for image_document in image_documents:\n        if image_document.image:  # This field is already base64-encoded\n            image_encodings.append(image_document.image)\n        elif (\n            image_document.image_path\n        ):  # This field is a path to the image, which is then encoded.\n            image_encodings.append(encode_image(image_document.image_path))\n        elif (\n            \"file_path\" in image_document.metadata\n            and image_document.metadata[\"file_path\"] != \"\"\n        ):  # Alternative path to the image, which is then encoded.\n            image_encodings.append(encode_image(image_document.metadata[\"file_path\"]))\n        elif image_document.image_url:  # Image can also be pulled from the URL.\n            response = requests.get(image_document.image_url)\n            try:\n                image_encodings.append(\n                    base64.b64encode(response.content).decode(\"utf-8\")\n                )\n            except Exception as e:\n                logger.warning(f\"Cannot encode the image pulled from URL -> {e}\")\n    return image_encodings\n\n\ndef infer_image_mimetype_from_file_path(image_file_path: str) -> str:\n    \"\"\"Infer the MIME of an image file based on its file extension.\n\n    Currently only supports the following types of images:\n        * image/jpeg\n        * image/png\n        * image/gif\n        * image/webp\n\n    Args:\n        image_file_path (str): Path to the image file.\n\n    Returns:\n        str: MIME type of the image: image/jpeg, image/png, image/gif, or image/webp.\n            Defaults to `image/jpeg`.\n    \"\"\"\n    # Get the file extension\n    file_extension = image_file_path.split(\".\")[-1].lower()\n\n    # Map file extensions to mimetypes\n    if file_extension == \"jpg\" or file_extension == \"jpeg\":\n        return \"image/jpeg\"\n    elif file_extension == \"png\":\n        return \"image/png\"\n    elif file_extension == \"gif\":\n        return \"image/gif\"\n    elif file_extension == \"webp\":\n        return \"image/webp\"\n\n    # If the file extension is not recognized\n    return \"image/jpeg\"\n\n\ndef infer_image_mimetype_from_base64(base64_string: str) -> Optional[str]:\n    \"\"\"Infer the MIME of an image from the base64 encoding.\n\n    Args:\n        base64_string (str): Base64-encoded string of the image.\n\n    Returns:\n        Optional[str]: MIME type of the image: image/jpeg, image/png, image/gif, or image/webp.\n          `None` if the MIME type cannot be inferred.\n    \"\"\"\n    # Decode the base64 string\n    decoded_data = base64.b64decode(base64_string)\n\n    # Use filetype to guess the MIME type\n    kind = filetype.guess(decoded_data)\n\n    # Return the MIME type if detected, otherwise return None\n    return kind.mime if kind is not None else None\n\n\ndef set_base64_and_mimetype_for_image_docs(\n    image_documents: Sequence[ImageDocument],\n) -> Sequence[ImageDocument]:\n    \"\"\"Set the base64 and mimetype fields for the image documents.\n\n    Args:\n        image_documents (Sequence[ImageDocument]): Sequence of ImageDocument objects.\n\n    Returns:\n        Sequence[ImageDocument]: ImageDocuments with base64 and detected mimetypes set.\n    \"\"\"\n    base64_strings = image_documents_to_base64(image_documents)\n    for image_doc, base64_str in zip(image_documents, base64_strings):\n        image_doc.image = base64_str\n        image_doc.image_mimetype = infer_image_mimetype_from_base64(image_doc.image)\n        if not image_doc.image_mimetype and image_doc.image_path:\n            image_doc.image_mimetype = infer_image_mimetype_from_file_path(\n                image_doc.image_path\n            )\n        else:\n            # Defaults to `image/jpeg` if the mimetype cannot be inferred\n            image_doc.image_mimetype = \"image/jpeg\"\n    return image_documents"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/multi_modal_llms/__init__.py",
    "filename": "__init__.py",
    "relpath": "multi_modal_llms/__init__.py",
    "start_line": 1,
    "end_line": 9,
    "length": 9,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.multi_modal_llms.base import (\n    MultiModalLLM,\n    MultiModalLLMMetadata,\n)\n\n__all__ = [\n    \"MultiModalLLMMetadata\",\n    \"MultiModalLLM\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/display_utils.py",
    "filename": "display_utils.py",
    "relpath": "prompts/display_utils.py",
    "start_line": 1,
    "end_line": 20,
    "length": 20,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "display_prompt_dict"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "display_prompt_dict"
    ],
    "document_class_names": [],
    "content": "\"\"\"Prompt display utils.\"\"\"\n\nfrom llama_index.core.prompts.mixin import PromptDictType\n\n\n# define prompt viewing function\ndef display_prompt_dict(prompts_dict: PromptDictType) -> None:\n    \"\"\"Display prompt dict.\n\n    Args:\n        prompts_dict: prompt dict\n\n    \"\"\"\n    from IPython.display import Markdown, display\n\n    for k, p in prompts_dict.items():\n        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n        display(Markdown(text_md))\n        print(p.get_template())\n        display(Markdown(\"<br><br>\"))"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/base.py",
    "filename": "base.py",
    "relpath": "prompts/base.py",
    "start_line": 1,
    "end_line": 418,
    "length": 418,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_map_template_vars",
      "_map_function_vars",
      "_map_all_vars",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "_as_query_component",
      "__init__",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "__init__",
      "from_messages",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "_as_query_component",
      "__init__",
      "select",
      "partial_format",
      "format",
      "format_messages",
      "get_template"
    ],
    "chunk_class_names": [
      "BasePromptTemplate",
      "PromptTemplate",
      "ChatPromptTemplate",
      "SelectorPromptTemplate"
    ],
    "document_function_names": [
      "_map_template_vars",
      "_map_function_vars",
      "_map_all_vars",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "_as_query_component",
      "__init__",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "__init__",
      "from_messages",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "_as_query_component",
      "__init__",
      "select",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "__init__",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "BasePromptTemplate",
      "PromptTemplate",
      "ChatPromptTemplate",
      "SelectorPromptTemplate",
      "LangchainPromptTemplate",
      "PromptComponent"
    ],
    "content": "\"\"\"Prompts.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom copy import deepcopy\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\nfrom typing_extensions import Annotated\n\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    WithJsonSchema,\n    PlainSerializer,\n    SerializeAsAny,\n)\n\nif TYPE_CHECKING:\n    from llama_index.core.bridge.langchain import (\n        BasePromptTemplate as LangchainTemplate,\n    )\n\n    # pants: no-infer-dep\n    from llama_index.core.bridge.langchain import (\n        ConditionalPromptSelector as LangchainSelector,\n    )\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel, ConfigDict\nfrom llama_index.core.base.llms.base import BaseLLM\nfrom llama_index.core.base.llms.generic_utils import (\n    messages_to_prompt as default_messages_to_prompt,\n)\nfrom llama_index.core.base.llms.generic_utils import (\n    prompt_to_messages,\n)\nfrom llama_index.core.base.llms.types import ContentBlock, TextBlock\nfrom llama_index.core.prompts.prompt_type import PromptType\nfrom llama_index.core.prompts.utils import get_template_vars, format_string\nfrom llama_index.core.types import BaseOutputParser\n\n\nAnnotatedCallable = Annotated[\n    Callable,\n    WithJsonSchema({\"type\": \"string\"}),\n    WithJsonSchema({\"type\": \"string\"}),\n    PlainSerializer(lambda x: f\"{x.__module__}.{x.__name__}\", return_type=str),\n]\n\n\nclass BasePromptTemplate(ChainableMixin, BaseModel, ABC):  # type: ignore[no-redef]\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    metadata: Dict[str, Any]\n    template_vars: List[str]\n    kwargs: Dict[str, str]\n    output_parser: Optional[BaseOutputParser]\n    template_var_mappings: Optional[Dict[str, Any]] = Field(\n        default_factory=dict,  # type: ignore\n        description=\"Template variable mappings (Optional).\",\n    )\n    function_mappings: Optional[Dict[str, AnnotatedCallable]] = Field(\n        default_factory=dict,  # type: ignore\n        description=(\n            \"Function mappings (Optional). This is a mapping from template \"\n            \"variable names to functions that take in the current kwargs and \"\n            \"return a string.\"\n        ),\n    )\n\n    def _map_template_vars(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"For keys in template_var_mappings, swap in the right keys.\"\"\"\n        template_var_mappings = self.template_var_mappings or {}\n        return {template_var_mappings.get(k, k): v for k, v in kwargs.items()}\n\n    def _map_function_vars(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"For keys in function_mappings, compute values and combine w/ kwargs.\n\n        Users can pass in functions instead of fixed values as format variables.\n        For each function, we call the function with the current kwargs,\n        get back the value, and then use that value in the template\n        for the corresponding format variable.\n\n        \"\"\"\n        function_mappings = self.function_mappings or {}\n        # first generate the values for the functions\n        new_kwargs = {}\n        for k, v in function_mappings.items():\n            # TODO: figure out what variables to pass into each function\n            # is it the kwargs specified during query time? just the fixed kwargs?\n            # all kwargs?\n            new_kwargs[k] = v(**kwargs)\n\n        # then, add the fixed variables only if not in new_kwargs already\n        # (implying that function mapping will override fixed variables)\n        for k, v in kwargs.items():\n            if k not in new_kwargs:\n                new_kwargs[k] = v\n\n        return new_kwargs\n\n    def _map_all_vars(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Map both template and function variables.\n\n        We (1) first call function mappings to compute functions,\n        and then (2) call the template_var_mappings.\n\n        \"\"\"\n        # map function\n        new_kwargs = self._map_function_vars(kwargs)\n        # map template vars (to point to existing format vars in string template)\n        return self._map_template_vars(new_kwargs)\n\n    @abstractmethod\n    def partial_format(self, **kwargs: Any) -> \"BasePromptTemplate\":\n        ...\n\n    @abstractmethod\n    def format(self, llm: Optional[BaseLLM] = None, **kwargs: Any) -> str:\n        ...\n\n    @abstractmethod\n    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        ...\n\n    @abstractmethod\n    def get_template(self, llm: Optional[BaseLLM] = None) -> str:\n        ...\n\n    def _as_query_component(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> QueryComponent:\n        \"\"\"As query component.\"\"\"\n        return PromptComponent(prompt=self, format_messages=False, llm=llm)\n\n\nclass PromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    template: str\n\n    def __init__(\n        self,\n        template: str,\n        prompt_type: str = PromptType.CUSTOM,\n        output_parser: Optional[BaseOutputParser] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        template_var_mappings: Optional[Dict[str, Any]] = None,\n        function_mappings: Optional[Dict[str, Callable]] = None,\n        **kwargs: Any,\n    ) -> None:\n        if metadata is None:\n            metadata = {}\n        metadata[\"prompt_type\"] = prompt_type\n\n        template_vars = get_template_vars(template)\n\n        super().__init__(\n            template=template,\n            template_vars=template_vars,\n            kwargs=kwargs,\n            metadata=metadata,\n            output_parser=output_parser,\n            template_var_mappings=template_var_mappings,\n            function_mappings=function_mappings,\n        )\n\n    def partial_format(self, **kwargs: Any) -> \"PromptTemplate\":\n        \"\"\"Partially format the prompt.\"\"\"\n        # NOTE: this is a hack to get around deepcopy failing on output parser\n        output_parser = self.output_parser\n        self.output_parser = None\n\n        # get function and fixed kwargs, and add that to a copy\n        # of the current prompt object\n        prompt = deepcopy(self)\n        prompt.kwargs.update(kwargs)\n\n        # NOTE: put the output parser back\n        prompt.output_parser = output_parser\n        self.output_parser = output_parser\n        return prompt\n\n    def format(\n        self,\n        llm: Optional[BaseLLM] = None,\n        completion_to_prompt: Optional[Callable[[str], str]] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Format the prompt into a string.\"\"\"\n        del llm  # unused\n        all_kwargs = {\n            **self.kwargs,\n            **kwargs,\n        }\n\n        mapped_all_kwargs = self._map_all_vars(all_kwargs)\n        prompt = format_string(self.template, **mapped_all_kwargs)\n\n        if self.output_parser is not None:\n            prompt = self.output_parser.format(prompt)\n\n        if completion_to_prompt is not None:\n            prompt = completion_to_prompt(prompt)\n\n        return prompt\n\n    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Format the prompt into a list of chat messages.\"\"\"\n        del llm  # unused\n        prompt = self.format(**kwargs)\n        return prompt_to_messages(prompt)\n\n    def get_template(self, llm: Optional[BaseLLM] = None) -> str:\n        return self.template\n\n\nclass ChatPromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    message_templates: List[ChatMessage]\n\n    def __init__(\n        self,\n        message_templates: Sequence[ChatMessage],\n        prompt_type: str = PromptType.CUSTOM,\n        output_parser: Optional[BaseOutputParser] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        template_var_mappings: Optional[Dict[str, Any]] = None,\n        function_mappings: Optional[Dict[str, Callable]] = None,\n        **kwargs: Any,\n    ):\n        if metadata is None:\n            metadata = {}\n        metadata[\"prompt_type\"] = prompt_type\n\n        template_vars = []\n        for message_template in message_templates:\n            template_vars.extend(get_template_vars(message_template.content or \"\"))\n\n        super().__init__(\n            message_templates=message_templates,\n            kwargs=kwargs,\n            metadata=metadata,\n            output_parser=output_parser,\n            template_vars=template_vars,\n            template_var_mappings=template_var_mappings,\n            function_mappings=function_mappings,\n        )\n\n    @classmethod\n    def from_messages(\n        cls,\n        message_templates: Union[List[Tuple[str, str]], List[ChatMessage]],\n        **kwargs: Any,\n    ) -> \"ChatPromptTemplate\":\n        \"\"\"From messages.\"\"\"\n        if isinstance(message_templates[0], tuple):\n            message_templates = [\n                ChatMessage.from_str(role=role, content=content)  # type: ignore[arg-type]\n                for role, content in message_templates\n            ]\n        return cls(message_templates=message_templates, **kwargs)  # type: ignore[arg-type]\n\n    def partial_format(self, **kwargs: Any) -> \"ChatPromptTemplate\":\n        prompt = deepcopy(self)\n        prompt.kwargs.update(kwargs)\n        return prompt\n\n    def format(\n        self,\n        llm: Optional[BaseLLM] = None,\n        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,\n        **kwargs: Any,\n    ) -> str:\n        del llm  # unused\n        messages = self.format_messages(**kwargs)\n\n        if messages_to_prompt is not None:\n            return messages_to_prompt(messages)\n\n        return default_messages_to_prompt(messages)\n\n    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        del llm  # unused\n        \"\"\"Format the prompt into a list of chat messages.\"\"\"\n        all_kwargs = {\n            **self.kwargs,\n            **kwargs,\n        }\n        mapped_all_kwargs = self._map_all_vars(all_kwargs)\n\n        messages: List[ChatMessage] = []\n        for message_template in self.message_templates:\n            # Handle messages with multiple blocks\n            if message_template.blocks:\n                formatted_blocks: List[ContentBlock] = []\n                for block in message_template.blocks:\n                    if isinstance(block, TextBlock):\n                        template_vars = get_template_vars(block.text)\n                        relevant_kwargs = {\n                            k: v\n                            for k, v in mapped_all_kwargs.items()\n                            if k in template_vars\n                        }\n                        formatted_text = format_string(block.text, **relevant_kwargs)\n                        formatted_blocks.append(TextBlock(text=formatted_text))\n                    else:\n                        # For non-text blocks (like images), keep them as is\n                        # TODO: can images be formatted as variables?\n                        formatted_blocks.append(block)\n\n                message = message_template.model_copy()\n                message.blocks = formatted_blocks\n                messages.append(message)\n            else:\n                # Handle empty messages (if any)\n                messages.append(message_template.model_copy())\n\n        if self.output_parser is not None:\n            messages = self.output_parser.format_messages(messages)\n\n        return messages\n\n    def get_template(self, llm: Optional[BaseLLM] = None) -> str:\n        return default_messages_to_prompt(self.message_templates)\n\n    def _as_query_component(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> QueryComponent:\n        \"\"\"As query component.\"\"\"\n        return PromptComponent(prompt=self, format_messages=True, llm=llm)\n\n\nclass SelectorPromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    default_template: SerializeAsAny[BasePromptTemplate]\n    conditionals: Optional[\n        Sequence[Tuple[Callable[[BaseLLM], bool], BasePromptTemplate]]\n    ] = None\n\n    def __init__(\n        self,\n        default_template: BasePromptTemplate,\n        conditionals: Optional[\n            Sequence[Tuple[Callable[[BaseLLM], bool], BasePromptTemplate]]\n        ] = None,\n    ):\n        metadata = default_template.metadata\n        kwargs = default_template.kwargs\n        template_vars = default_template.template_vars\n        output_parser = default_template.output_parser\n        super().__init__(\n            default_template=default_template,\n            conditionals=conditionals,\n            metadata=metadata,\n            kwargs=kwargs,\n            template_vars=template_vars,\n            output_parser=output_parser,\n        )\n\n    def select(self, llm: Optional[BaseLLM] = None) -> BasePromptTemplate:\n        # ensure output parser is up to date\n        self.default_template.output_parser = self.output_parser\n\n        if llm is None:\n            return self.default_template\n\n        if self.conditionals is not None:\n            for condition, prompt in self.conditionals:\n                if condition(llm):\n                    # ensure output parser is up to date\n                    prompt.output_parser = self.output_parser\n                    return prompt\n\n        return self.default_template\n\n    def partial_format(self, **kwargs: Any) -> \"SelectorPromptTemplate\":\n        default_template = self.default_template.partial_format(**kwargs)\n        if self.conditionals is None:\n            conditionals = None\n        else:\n            conditionals = [\n                (condition, prompt.partial_format(**kwargs))\n                for condition, prompt in self.conditionals\n            ]\n        return SelectorPromptTemplate(\n            default_template=default_template, conditionals=conditionals\n        )\n\n    def format(self, llm: Optional[BaseLLM] = None, **kwargs: Any) -> str:\n        \"\"\"Format the prompt into a string.\"\"\"\n        prompt = self.select(llm=llm)\n        return prompt.format(**kwargs)\n\n    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Format the prompt into a list of chat messages.\"\"\"\n        prompt = self.select(llm=llm)\n        return prompt.format_messages(**kwargs)\n\n    def get_template(self, llm: Optional[BaseLLM] = None) -> str:\n        prompt = self.select(llm=llm)\n        return prompt.get_template(llm=llm)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/base.py",
    "filename": "base.py",
    "relpath": "prompts/base.py",
    "start_line": 418,
    "end_line": 615,
    "length": 198,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "LangchainPromptTemplate",
      "PromptComponent"
    ],
    "document_function_names": [
      "_map_template_vars",
      "_map_function_vars",
      "_map_all_vars",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "_as_query_component",
      "__init__",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "__init__",
      "from_messages",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "_as_query_component",
      "__init__",
      "select",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "__init__",
      "partial_format",
      "format",
      "format_messages",
      "get_template",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "BasePromptTemplate",
      "PromptTemplate",
      "ChatPromptTemplate",
      "SelectorPromptTemplate",
      "LangchainPromptTemplate",
      "PromptComponent"
    ],
    "content": "class LangchainPromptTemplate(BasePromptTemplate):  # type: ignore[no-redef]\n    selector: Any\n    requires_langchain_llm: bool = False\n\n    def __init__(\n        self,\n        template: Optional[\"LangchainTemplate\"] = None,\n        selector: Optional[\"LangchainSelector\"] = None,\n        output_parser: Optional[BaseOutputParser] = None,\n        prompt_type: str = PromptType.CUSTOM,\n        metadata: Optional[Dict[str, Any]] = None,\n        template_var_mappings: Optional[Dict[str, Any]] = None,\n        function_mappings: Optional[Dict[str, Callable]] = None,\n        requires_langchain_llm: bool = False,\n    ) -> None:\n        try:\n            from llama_index.core.bridge.langchain import (\n                ConditionalPromptSelector as LangchainSelector,\n            )\n        except ImportError:\n            raise ImportError(\n                \"Must install `llama_index[langchain]` to use LangchainPromptTemplate.\"\n            )\n        if selector is None:\n            if template is None:\n                raise ValueError(\"Must provide either template or selector.\")\n            selector = LangchainSelector(default_prompt=template)\n        else:\n            if template is not None:\n                raise ValueError(\"Must provide either template or selector.\")\n            selector = selector\n\n        kwargs = selector.default_prompt.partial_variables\n        template_vars = selector.default_prompt.input_variables\n\n        if metadata is None:\n            metadata = {}\n        metadata[\"prompt_type\"] = prompt_type\n\n        super().__init__(\n            selector=selector,\n            metadata=metadata,\n            kwargs=kwargs,\n            template_vars=template_vars,\n            output_parser=output_parser,\n            template_var_mappings=template_var_mappings,\n            function_mappings=function_mappings,\n            requires_langchain_llm=requires_langchain_llm,\n        )\n\n    def partial_format(self, **kwargs: Any) -> \"BasePromptTemplate\":\n        \"\"\"Partially format the prompt.\"\"\"\n        from llama_index.core.bridge.langchain import (\n            ConditionalPromptSelector as LangchainSelector,\n        )\n\n        mapped_kwargs = self._map_all_vars(kwargs)\n        default_prompt = self.selector.default_prompt.partial(**mapped_kwargs)\n        conditionals = [\n            (condition, prompt.partial(**mapped_kwargs))\n            for condition, prompt in self.selector.conditionals\n        ]\n        lc_selector = LangchainSelector(\n            default_prompt=default_prompt, conditionals=conditionals\n        )\n\n        # copy full prompt object, replace selector\n        lc_prompt = deepcopy(self)\n        lc_prompt.selector = lc_selector\n        return lc_prompt\n\n    def format(self, llm: Optional[BaseLLM] = None, **kwargs: Any) -> str:\n        \"\"\"Format the prompt into a string.\"\"\"\n        from llama_index.llms.langchain import LangChainLLM  # pants: no-infer-dep\n\n        if llm is not None:\n            # if llamaindex LLM is provided, and we require a langchain LLM,\n            # then error. but otherwise if `requires_langchain_llm` is False,\n            # then we can just use the default prompt\n            if not isinstance(llm, LangChainLLM) and self.requires_langchain_llm:\n                raise ValueError(\"Must provide a LangChainLLM.\")\n            elif not isinstance(llm, LangChainLLM):\n                lc_template = self.selector.default_prompt\n            else:\n                lc_template = self.selector.get_prompt(llm=llm.llm)\n        else:\n            lc_template = self.selector.default_prompt\n\n        # if there's mappings specified, make sure those are used\n        mapped_kwargs = self._map_all_vars(kwargs)\n        return lc_template.format(**mapped_kwargs)\n\n    def format_messages(\n        self, llm: Optional[BaseLLM] = None, **kwargs: Any\n    ) -> List[ChatMessage]:\n        \"\"\"Format the prompt into a list of chat messages.\"\"\"\n        from llama_index.llms.langchain import LangChainLLM  # pants: no-infer-dep\n        from llama_index.llms.langchain.utils import (\n            from_lc_messages,\n        )  # pants: no-infer-dep\n\n        if llm is not None:\n            # if llamaindex LLM is provided, and we require a langchain LLM,\n            # then error. but otherwise if `requires_langchain_llm` is False,\n            # then we can just use the default prompt\n            if not isinstance(llm, LangChainLLM) and self.requires_langchain_llm:\n                raise ValueError(\"Must provide a LangChainLLM.\")\n            elif not isinstance(llm, LangChainLLM):\n                lc_template = self.selector.default_prompt\n            else:\n                lc_template = self.selector.get_prompt(llm=llm.llm)\n        else:\n            lc_template = self.selector.default_prompt\n\n        # if there's mappings specified, make sure those are used\n        mapped_kwargs = self._map_all_vars(kwargs)\n        lc_prompt_value = lc_template.format_prompt(**mapped_kwargs)\n        lc_messages = lc_prompt_value.to_messages()\n        return from_lc_messages(lc_messages)\n\n    def get_template(self, llm: Optional[BaseLLM] = None) -> str:\n        from llama_index.llms.langchain import LangChainLLM  # pants: no-infer-dep\n\n        if llm is not None:\n            # if llamaindex LLM is provided, and we require a langchain LLM,\n            # then error. but otherwise if `requires_langchain_llm` is False,\n            # then we can just use the default prompt\n            if not isinstance(llm, LangChainLLM) and self.requires_langchain_llm:\n                raise ValueError(\"Must provide a LangChainLLM.\")\n            elif not isinstance(llm, LangChainLLM):\n                lc_template = self.selector.default_prompt\n            else:\n                lc_template = self.selector.get_prompt(llm=llm.llm)\n        else:\n            lc_template = self.selector.default_prompt\n\n        try:\n            return str(lc_template.template)  # type: ignore\n        except AttributeError:\n            return str(lc_template)\n\n\n# NOTE: only for backwards compatibility\nPrompt = PromptTemplate\n\n\nclass PromptComponent(QueryComponent):\n    \"\"\"Prompt component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    prompt: SerializeAsAny[BasePromptTemplate] = Field(..., description=\"Prompt\")\n    llm: Optional[SerializeAsAny[BaseLLM]] = Field(\n        default=None, description=\"LLM to use for formatting prompt.\"\n    )\n    format_messages: bool = Field(\n        default=False,\n        description=\"Whether to format the prompt into a list of chat messages.\",\n    )\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        keys = list(input.keys())\n        for k in keys:\n            input[k] = validate_and_convert_stringable(input[k])\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        if self.format_messages:\n            output: Union[str, List[ChatMessage]] = self.prompt.format_messages(\n                llm=self.llm, **kwargs\n            )\n        else:\n            output = self.prompt.format(llm=self.llm, **kwargs)\n        return {\"prompt\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        # NOTE: no native async for prompt\n        return self._run_component(**kwargs)\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys(\n            set(self.prompt.template_vars) - set(self.prompt.kwargs)\n        )\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"prompt\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/chat_prompts.py",
    "filename": "chat_prompts.py",
    "relpath": "prompts/chat_prompts.py",
    "start_line": 1,
    "end_line": 109,
    "length": 109,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Prompts for ChatGPT.\"\"\"\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.prompts.base import ChatPromptTemplate\n\n# text qa prompt\nTEXT_QA_SYSTEM_PROMPT = ChatMessage(\n    content=(\n        \"You are an expert Q&A system that is trusted around the world.\\n\"\n        \"Always answer the query using the provided context information, \"\n        \"and not prior knowledge.\\n\"\n        \"Some rules to follow:\\n\"\n        \"1. Never directly reference the given context in your answer.\\n\"\n        \"2. Avoid statements like 'Based on the context, ...' or \"\n        \"'The context information ...' or anything along \"\n        \"those lines.\"\n    ),\n    role=MessageRole.SYSTEM,\n)\n\nTEXT_QA_PROMPT_TMPL_MSGS = [\n    TEXT_QA_SYSTEM_PROMPT,\n    ChatMessage(\n        content=(\n            \"Context information is below.\\n\"\n            \"---------------------\\n\"\n            \"{context_str}\\n\"\n            \"---------------------\\n\"\n            \"Given the context information and not prior knowledge, \"\n            \"answer the query.\\n\"\n            \"Query: {query_str}\\n\"\n            \"Answer: \"\n        ),\n        role=MessageRole.USER,\n    ),\n]\n\nCHAT_TEXT_QA_PROMPT = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)\n\n# Tree Summarize\nTREE_SUMMARIZE_PROMPT_TMPL_MSGS = [\n    TEXT_QA_SYSTEM_PROMPT,\n    ChatMessage(\n        content=(\n            \"Context information from multiple sources is below.\\n\"\n            \"---------------------\\n\"\n            \"{context_str}\\n\"\n            \"---------------------\\n\"\n            \"Given the information from multiple sources and not prior knowledge, \"\n            \"answer the query.\\n\"\n            \"Query: {query_str}\\n\"\n            \"Answer: \"\n        ),\n        role=MessageRole.USER,\n    ),\n]\n\nCHAT_TREE_SUMMARIZE_PROMPT = ChatPromptTemplate(\n    message_templates=TREE_SUMMARIZE_PROMPT_TMPL_MSGS\n)\n\n\n# Refine Prompt\nCHAT_REFINE_PROMPT_TMPL_MSGS = [\n    ChatMessage(\n        content=(\n            \"You are an expert Q&A system that strictly operates in two modes \"\n            \"when refining existing answers:\\n\"\n            \"1. **Rewrite** an original answer using the new context.\\n\"\n            \"2. **Repeat** the original answer if the new context isn't useful.\\n\"\n            \"Never reference the original answer or context directly in your answer.\\n\"\n            \"When in doubt, just repeat the original answer.\\n\"\n            \"New Context: {context_msg}\\n\"\n            \"Query: {query_str}\\n\"\n            \"Original Answer: {existing_answer}\\n\"\n            \"New Answer: \"\n        ),\n        role=MessageRole.USER,\n    )\n]\n\n\nCHAT_REFINE_PROMPT = ChatPromptTemplate(message_templates=CHAT_REFINE_PROMPT_TMPL_MSGS)\n\n\n# Table Context Refine Prompt\nCHAT_REFINE_TABLE_CONTEXT_TMPL_MSGS = [\n    ChatMessage(content=\"{query_str}\", role=MessageRole.USER),\n    ChatMessage(content=\"{existing_answer}\", role=MessageRole.ASSISTANT),\n    ChatMessage(\n        content=(\n            \"We have provided a table schema below. \"\n            \"---------------------\\n\"\n            \"{schema}\\n\"\n            \"---------------------\\n\"\n            \"We have also provided some context information below. \"\n            \"{context_msg}\\n\"\n            \"---------------------\\n\"\n            \"Given the context information and the table schema, \"\n            \"refine the original answer to better \"\n            \"answer the question. \"\n            \"If the context isn't useful, return the original answer.\"\n        ),\n        role=MessageRole.USER,\n    ),\n]\nCHAT_REFINE_TABLE_CONTEXT_PROMPT = ChatPromptTemplate(\n    message_templates=CHAT_REFINE_TABLE_CONTEXT_TMPL_MSGS\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/prompt_type.py",
    "filename": "prompt_type.py",
    "relpath": "prompts/prompt_type.py",
    "start_line": 1,
    "end_line": 80,
    "length": 80,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "PromptType"
    ],
    "document_function_names": [],
    "document_class_names": [
      "PromptType"
    ],
    "content": "\"\"\"Prompt types enum.\"\"\"\n\nfrom enum import Enum\n\n\nclass PromptType(str, Enum):\n    \"\"\"Prompt type.\"\"\"\n\n    # summarization\n    SUMMARY = \"summary\"\n    # tree insert node\n    TREE_INSERT = \"insert\"\n    # tree select query prompt\n    TREE_SELECT = \"tree_select\"\n    # tree select query prompt (multiple)\n    TREE_SELECT_MULTIPLE = \"tree_select_multiple\"\n    # question-answer\n    QUESTION_ANSWER = \"text_qa\"\n    # refine\n    REFINE = \"refine\"\n    # keyword extract\n    KEYWORD_EXTRACT = \"keyword_extract\"\n    # query keyword extract\n    QUERY_KEYWORD_EXTRACT = \"query_keyword_extract\"\n\n    # schema extract\n    SCHEMA_EXTRACT = \"schema_extract\"\n\n    # text to sql\n    TEXT_TO_SQL = \"text_to_sql\"\n\n    # text to graph query\n    TEXT_TO_GRAPH_QUERY = \"text_to_graph_query\"\n\n    # table context\n    TABLE_CONTEXT = \"table_context\"\n\n    # KG extraction prompt\n    KNOWLEDGE_TRIPLET_EXTRACT = \"knowledge_triplet_extract\"\n\n    # Simple Input prompt\n    SIMPLE_INPUT = \"simple_input\"\n\n    # Pandas prompt\n    PANDAS = \"pandas\"\n\n    # JSON path prompt\n    JSON_PATH = \"json_path\"\n\n    # Single select prompt\n    SINGLE_SELECT = \"single_select\"\n\n    # Multiple select prompt\n    MULTI_SELECT = \"multi_select\"\n\n    VECTOR_STORE_QUERY = \"vector_store_query\"\n\n    # Sub question prompt\n    SUB_QUESTION = \"sub_question\"\n\n    # SQL response synthesis prompt\n    SQL_RESPONSE_SYNTHESIS = \"sql_response_synthesis\"\n\n    # SQL response synthesis prompt (v2)\n    SQL_RESPONSE_SYNTHESIS_V2 = \"sql_response_synthesis_v2\"\n\n    # Conversation\n    CONVERSATION = \"conversation\"\n\n    # Decompose query transform\n    DECOMPOSE = \"decompose\"\n\n    # Choice select\n    CHOICE_SELECT = \"choice_select\"\n\n    # custom (by default)\n    CUSTOM = \"custom\"\n\n    # RankGPT rerank\n    RANKGPT_RERANK = \"rankgpt_rerank\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/prompt_utils.py",
    "filename": "prompt_utils.py",
    "relpath": "prompts/prompt_utils.py",
    "start_line": 1,
    "end_line": 30,
    "length": 30,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_empty_prompt_txt",
      "get_biggest_prompt"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_empty_prompt_txt",
      "get_biggest_prompt"
    ],
    "document_class_names": [],
    "content": "from typing import List\n\nfrom llama_index.core.prompts.base import BasePromptTemplate\n\n\ndef get_empty_prompt_txt(prompt: BasePromptTemplate) -> str:\n    \"\"\"Get empty prompt text.\n\n    Substitute empty strings in parts of the prompt that have\n    not yet been filled out. Skip variables that have already\n    been partially formatted. This is used to compute the initial tokens.\n\n    \"\"\"\n    partial_kargs = prompt.kwargs\n    empty_kwargs = {v: \"\" for v in prompt.template_vars if v not in partial_kargs}\n    all_kwargs = {**partial_kargs, **empty_kwargs}\n    return prompt.format(llm=None, **all_kwargs)\n\n\ndef get_biggest_prompt(prompts: List[BasePromptTemplate]) -> BasePromptTemplate:\n    \"\"\"Get biggest prompt.\n\n    Oftentimes we need to fetch the biggest prompt, in order to\n    be the most conservative about chunking text. This\n    is a helper utility for that.\n\n    \"\"\"\n    empty_prompt_txts = [get_empty_prompt_txt(prompt) for prompt in prompts]\n    empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]\n    return prompts[empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/__init__.py",
    "filename": "__init__.py",
    "relpath": "prompts/__init__.py",
    "start_line": 1,
    "end_line": 26,
    "length": 26,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Prompt class.\"\"\"\n\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.prompts.base import (\n    BasePromptTemplate,\n    ChatPromptTemplate,\n    LangchainPromptTemplate,\n    Prompt,\n    PromptTemplate,\n    PromptType,\n    SelectorPromptTemplate,\n)\nfrom llama_index.core.prompts.display_utils import display_prompt_dict\n\n__all__ = [\n    \"Prompt\",\n    \"PromptTemplate\",\n    \"SelectorPromptTemplate\",\n    \"ChatPromptTemplate\",\n    \"LangchainPromptTemplate\",\n    \"BasePromptTemplate\",\n    \"PromptType\",\n    \"ChatMessage\",\n    \"MessageRole\",\n    \"display_prompt_dict\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/utils.py",
    "filename": "utils.py",
    "relpath": "prompts/utils.py",
    "start_line": 1,
    "end_line": 61,
    "length": 61,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "format",
      "parse",
      "_replace_match",
      "format_string",
      "format_content_blocks",
      "get_template_vars",
      "is_chat_model"
    ],
    "chunk_class_names": [
      "SafeFormatter"
    ],
    "document_function_names": [
      "__init__",
      "format",
      "parse",
      "_replace_match",
      "format_string",
      "format_content_blocks",
      "get_template_vars",
      "is_chat_model"
    ],
    "document_class_names": [
      "SafeFormatter"
    ],
    "content": "from typing import Dict, List, Optional\nimport re\n\nfrom llama_index.core.base.llms.base import BaseLLM\nfrom llama_index.core.base.llms.types import ContentBlock, TextBlock\n\n\nclass SafeFormatter:\n    \"\"\"Safe string formatter that does not raise KeyError if key is missing.\"\"\"\n\n    def __init__(self, format_dict: Optional[Dict[str, str]] = None):\n        self.format_dict = format_dict or {}\n\n    def format(self, format_string: str) -> str:\n        return re.sub(r\"\\{([^{}]+)\\}\", self._replace_match, format_string)\n\n    def parse(self, format_string: str) -> List[str]:\n        return re.findall(\n            r\"\\{([a-zA-Z_][a-zA-Z0-9_]*(?:\\.[a-zA-Z_][a-zA-Z0-9_]*)*)\\}\", format_string\n        )\n\n    def _replace_match(self, match: re.Match) -> str:\n        key = match.group(1)\n        return str(self.format_dict.get(key, match.group(0)))\n\n\ndef format_string(string_to_format: str, **kwargs: str) -> str:\n    \"\"\"Format a string with kwargs.\"\"\"\n    formatter = SafeFormatter(format_dict=kwargs)\n    return formatter.format(string_to_format)\n\n\ndef format_content_blocks(\n    content_blocks: List[ContentBlock], **kwargs: str\n) -> List[ContentBlock]:\n    \"\"\"Format content blocks with kwargs.\"\"\"\n    formatter = SafeFormatter(format_dict=kwargs)\n    formatted_blocks: List[ContentBlock] = []\n    for block in content_blocks:\n        if isinstance(block, TextBlock):\n            formatted_blocks.append(TextBlock(text=formatter.format(block.text)))\n        else:\n            formatted_blocks.append(block)\n\n    return formatted_blocks\n\n\ndef get_template_vars(template_str: str) -> List[str]:\n    \"\"\"Get template variables from a template string.\"\"\"\n    variables = []\n    formatter = SafeFormatter()\n\n    for variable_name in formatter.parse(template_str):\n        if variable_name:\n            variables.append(variable_name)\n\n    return variables\n\n\ndef is_chat_model(llm: BaseLLM) -> bool:\n    return llm.metadata.is_chat_model"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/default_prompt_selectors.py",
    "filename": "default_prompt_selectors.py",
    "relpath": "prompts/default_prompt_selectors.py",
    "start_line": 1,
    "end_line": 86,
    "length": 86,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Default prompt selectors.\"\"\"\nfrom llama_index.core.prompts import SelectorPromptTemplate\nfrom llama_index.core.prompts.chat_prompts import (\n    CHAT_REFINE_PROMPT,\n    CHAT_REFINE_TABLE_CONTEXT_PROMPT,\n    CHAT_TEXT_QA_PROMPT,\n    CHAT_TREE_SUMMARIZE_PROMPT,\n)\nfrom llama_index.core.prompts.default_prompts import (\n    DEFAULT_REFINE_PROMPT,\n    DEFAULT_REFINE_TABLE_CONTEXT_PROMPT,\n    DEFAULT_TEXT_QA_PROMPT,\n    DEFAULT_TREE_SUMMARIZE_PROMPT,\n)\nfrom llama_index.core.prompts.utils import is_chat_model\n\ntry:\n    from llama_index.llms.cohere import (\n        is_cohere_model,\n        COHERE_QA_TEMPLATE,\n        COHERE_REFINE_TEMPLATE,\n        COHERE_TREE_SUMMARIZE_TEMPLATE,\n        COHERE_REFINE_TABLE_CONTEXT_PROMPT,\n    )  # pants: no-infer-dep\nexcept ImportError:\n    COHERE_QA_TEMPLATE = None\n    COHERE_REFINE_TEMPLATE = None\n    COHERE_TREE_SUMMARIZE_TEMPLATE = None\n    COHERE_REFINE_TABLE_CONTEXT_PROMPT = None\n\n# Define prompt selectors for Text QA, Tree Summarize, Refine, and Refine Table.\n# Note: Cohere models accept a special argument `documents` for RAG calls. To pass on retrieved documents to the `documents` argument,\n# specialised templates have been defined. The conditionals below ensure that these templates are called by default when a retriever\n# is called with a Cohere model for generator.\n\n# Text QA\ndefault_text_qa_conditionals = [(is_chat_model, CHAT_TEXT_QA_PROMPT)]\nif COHERE_QA_TEMPLATE is not None:\n    default_text_qa_conditionals = [\n        (is_cohere_model, COHERE_QA_TEMPLATE),\n        (is_chat_model, CHAT_TEXT_QA_PROMPT),\n    ]\n\nDEFAULT_TEXT_QA_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_TEXT_QA_PROMPT,\n    conditionals=default_text_qa_conditionals,\n)\n\n# Tree Summarize\ndefault_tree_summarize_conditionals = [(is_chat_model, CHAT_TREE_SUMMARIZE_PROMPT)]\nif COHERE_TREE_SUMMARIZE_TEMPLATE is not None:\n    default_tree_summarize_conditionals = [\n        (is_cohere_model, COHERE_TREE_SUMMARIZE_TEMPLATE),\n        (is_chat_model, CHAT_TREE_SUMMARIZE_PROMPT),\n    ]\n\nDEFAULT_TREE_SUMMARIZE_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_TREE_SUMMARIZE_PROMPT,\n    conditionals=default_tree_summarize_conditionals,\n)\n\n# Refine\ndefault_refine_conditionals = [(is_chat_model, CHAT_REFINE_PROMPT)]\nif COHERE_REFINE_TEMPLATE is not None:\n    default_refine_conditionals = [\n        (is_cohere_model, COHERE_REFINE_TEMPLATE),\n        (is_chat_model, CHAT_REFINE_PROMPT),\n    ]\n\nDEFAULT_REFINE_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_REFINE_PROMPT,\n    conditionals=default_refine_conditionals,\n)\n\n# Refine Table Context\ndefault_refine_table_conditionals = [(is_chat_model, CHAT_REFINE_TABLE_CONTEXT_PROMPT)]\nif COHERE_REFINE_TABLE_CONTEXT_PROMPT is not None:\n    default_refine_table_conditionals = [\n        (is_cohere_model, COHERE_REFINE_TABLE_CONTEXT_PROMPT),\n        (is_chat_model, CHAT_REFINE_TABLE_CONTEXT_PROMPT),\n    ]\n\nDEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL = SelectorPromptTemplate(\n    default_template=DEFAULT_REFINE_TABLE_CONTEXT_PROMPT,\n    conditionals=default_refine_table_conditionals,\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/prompts.py",
    "filename": "prompts.py",
    "relpath": "prompts/prompts.py",
    "start_line": 1,
    "end_line": 140,
    "length": 140,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Subclasses from base prompt.\"\"\"\n\nfrom llama_index.core.prompts.base import PromptTemplate\n\n# deprecated, kept for backward compatibility\n\n\"\"\"Summary prompt.\n\nPromptTemplate to summarize the provided `context_str`.\n\nRequired template variables: `context_str`\n\"\"\"\nSummaryPrompt = PromptTemplate\n\n\"\"\"Tree Insert prompt.\n\nPromptTemplate to insert a new chunk of text `new_chunk_text` into the tree index.\nMore specifically, this prompt has the LLM select the relevant candidate\nchild node to continue tree traversal.\n\nRequired template variables: `num_chunks`, `context_list`, `new_chunk_text`\n\"\"\"\nTreeInsertPrompt = PromptTemplate\n\n\"\"\"Tree select prompt.\n\nPromptTemplate to select a candidate child node out of all child nodes\nprovided in `context_list`, given a query `query_str`. `num_chunks` is\nthe number of child nodes in `context_list`.\n\nRequired template variables: `num_chunks`, `context_list`, `query_str`\n\n\"\"\"\nTreeSelectPrompt = PromptTemplate\n\n\"\"\"Tree select multiple prompt.\n\nPromptTemplate to select multiple candidate child nodes out of all\nchild nodes provided in `context_list`, given a query `query_str`.\n`branching_factor` refers to the number of child nodes to select, and\n`num_chunks` is the number of child nodes in `context_list`.\n\nRequired template variables: `num_chunks`, `context_list`, `query_str`,\n    `branching_factor`\n\"\"\"\nTreeSelectMultiplePrompt = PromptTemplate\n\n\"\"\"Refine prompt.\n\nPromptTemplate to refine an existing answer `existing_answer`\ngiven a context `context_msg`, and a query `query_str`.\n\nRequired template variables: `query_str`, `existing_answer`, `context_msg`\n\"\"\"\nRefinePrompt = PromptTemplate\n\n\"\"\"Question Answer prompt.\n\nPromptTemplate to answer a question `query_str` given a context `context_str`.\n\nRequired template variables: `context_str`, `query_str`\n\"\"\"\nQuestionAnswerPrompt = PromptTemplate\n\n\"\"\"Keyword extract prompt.\n\nPromptTemplate to extract keywords from a text `text` with a maximum of\n`max_keywords` keywords.\n\nRequired template variables: `text`, `max_keywords`\n\"\"\"\nKeywordExtractPrompt = PromptTemplate\n\n\"\"\"Query keyword extract prompt.\n\nPromptTemplate to extract keywords from a query `query_str` with a maximum\nof `max_keywords` keywords.\n\nRequired template variables: `query_str`, `max_keywords`\n\"\"\"\nQueryKeywordExtractPrompt = PromptTemplate\n\n\"\"\"Schema extract prompt.\n\nPromptTemplate to extract schema from unstructured text `text`.\n\nRequired template variables: `text`, `schema`\n\"\"\"\nSchemaExtractPrompt = PromptTemplate\n\n\"\"\"Text to SQL prompt.\n\nPromptTemplate to translate a natural language query into SQL in the dialect\n`dialect` given a schema `schema`.\n\nRequired template variables: `query_str`, `schema`, `dialect`\n\"\"\"\nTextToSQLPrompt = PromptTemplate\n\"\"\"Table context prompt.\n\nPromptTemplate to generate a table context given a table schema `schema`,\nas well as unstructured text context `context_str`, and\na task `query_str`.\nThis includes both a high-level description of the table\nas well as a description of each column in the table.\n\"\"\"\nTableContextPrompt = PromptTemplate\n\n\"\"\"Refine Table context prompt.\n\nPromptTemplate to refine a table context given a table schema `schema`,\nas well as unstructured text context `context_msg`, and\na task `query_str`.\nThis includes both a high-level description of the table\nas well as a description of each column in the table.\n\n\"\"\"\nRefineTableContextPrompt = PromptTemplate\n\n\"\"\"Define the knowledge graph triplet extraction prompt.\"\"\"\nKnowledgeGraphPrompt = PromptTemplate\n\n\"\"\"Simple Input prompt.\n\nRequired template variables: `query_str`.\n\"\"\"\nSimpleInputPrompt = PromptTemplate\n\n\"\"\"Pandas prompt. Convert query to python code.\n\nRequired template variables: `query_str`, `df_str`, `instruction_str`.\n\"\"\"\nPandasPrompt = PromptTemplate\n\n\n\"\"\"Choice select prompt. Select from a list of choices.\n\nRequired template variables: `context_str`, `query_str`.\n\"\"\"\nChoiceSelectPrompt = PromptTemplate"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/system.py",
    "filename": "system.py",
    "relpath": "prompts/system.py",
    "start_line": 1,
    "end_line": 91,
    "length": 91,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# List of system prompts from Azure AI Studio\n\nSHAKESPEARE_WRITING_ASSISTANT = \"\"\"\\\nYou are a Shakespearean writing assistant who speaks in a Shakespearean style. \\\nYou help people come up with creative ideas and content like stories, poems, \\\nand songs that use Shakespearean style of writing style, including words like \\\n\"thou\" and \"hath\u201d.\nHere are some example of Shakespeare's style:\n - Romeo, Romeo! Wherefore art thou Romeo?\n - Love looks not with the eyes, but with the mind; and therefore is winged Cupid \\\npainted blind.\n - Shall I compare thee to a summer's day? Thou art more lovely and more temperate.\n\"\"\"\n\nIRS_TAX_CHATBOT = \"\"\"\\\n\u2022\tYou are an IRS chatbot whose primary goal is to help users with filing their tax \\\nreturns for the 2022 year.\n\u2022\tProvide concise replies that are polite and professional.\n\u2022\tAnswer questions truthfully based on official government information, with \\\nconsideration to context provided below on changes for 2022 that can affect \\\ntax refund.\n\u2022\tDo not answer questions that are not related to United States tax procedures and \\\nrespond with \"I can only help with any tax-related questions you may have.\".\n\u2022\tIf you do not know the answer to a question, respond by saying \u201cI do not know the \\\nanswer to your question. You may be able to find your answer at www.irs.gov/faqs\u201d\n\nChanges for 2022 that can affect tax refund:\n\u2022\tChanges in the number of dependents, employment or self-employment income and \\\ndivorce, among other factors, may affect your tax-filing status and refund. \\\nNo additional stimulus payments. Unlike 2020 and 2021, there were no new \\\nstimulus payments for 2022 so taxpayers should not expect to get an \\\nadditional payment.\n\u2022\tSome tax credits return to 2019 levels.  This means that taxpayers will likely \\\nreceive a significantly smaller refund compared with the previous tax year. \\\nChanges include amounts for the Child Tax Credit (CTC), the Earned Income \\\nTax Credit (EITC) and the Child and Dependent Care Credit will revert \\\nto pre-COVID levels.\n\u2022\tFor 2022, the CTC is worth $2,000 for each qualifying child. A child must be \\\nunder age 17 at the end of 2022 to be a qualifying child.For the EITC, eligible \\\ntaxpayers with no children will get $560 for the 2022 tax year.The Child and \\\nDependent Care Credit returns to a maximum of $2,100 in 2022.\n\u2022\tNo above-the-line charitable deductions. During COVID, taxpayers were able to take \\\nup to a $600 charitable donation tax deduction on their tax returns. However, for \\\ntax year 2022, taxpayers who don\u2019t itemize and who take the standard deduction, \\\nwon\u2019t be able to deduct their charitable contributions.\n\u2022\tMore people may be eligible for the Premium Tax Credit. For tax year 2022, \\\ntaxpayers may qualify for temporarily expanded eligibility for the premium \\\ntax credit.\n\u2022\tEligibility rules changed to claim a tax credit for clean vehicles. Review the \\\nchanges under the Inflation Reduction Act of 2022 to qualify for a \\\nClean Vehicle Credit.\n\"\"\"\n\nMARKETING_WRITING_ASSISTANT = \"\"\"\\\nYou are a marketing writing assistant. You help come up with creative content ideas \\\nand content like marketing emails, blog posts, tweets, ad copy and product \\\ndescriptions. You write in a friendly yet professional tone but can tailor \\\nyour writing style that best works for a user-specified audience.\u00a0\\\nIf you do not know the answer to a question, respond by saying \\\n\"I do not know the answer to your question.\"\n\"\"\"\n\nXBOX_CUSTOMER_SUPPORT_AGENT = \"\"\"\\\nYou are an Xbox customer support agent whose primary goal is to help users with issues \\\nthey are experiencing with their Xbox devices. You are friendly and concise. \\\nYou only provide factual answers to queries, and do not provide answers \\\nthat are not related to Xbox.\n\"\"\"\n\nHIKING_RECOMMENDATION_CHATBOT = \"\"\"\\\nI am a hiking enthusiast named Forest who helps people discover fun hikes in their \\\narea. I am upbeat and friendly. I introduce myself when first saying hello. \\\nWhen helping people out, I always ask them for this information to inform the \\\nhiking recommendation I provide:\n1.\tWhere they are located\n2.\tWhat hiking intensity they are looking for\nI will then provide three suggestions for nearby hikes that vary in length after I get \\\nthis information. I will also share an interesting fact about the local nature on \\\nthe hikes when making a recommendation.\n\"\"\"\n\nJSON_FORMATTER_ASSISTANT = \"\"\"\\\nAssistant is an AI chatbot that helps users turn a natural language list into JSON \\\nformat. After users input a list they want in JSON format, it will provide \\\nsuggested list of attribute labels if the user has not provided any, \\\nthen ask the user to confirm them before creating the list.\n\"\"\"\n\nDEFAULT = \"\"\"\\\nYou are an AI assistant that helps people find information.\n\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/guidance_utils.py",
    "filename": "guidance_utils.py",
    "relpath": "prompts/guidance_utils.py",
    "start_line": 1,
    "end_line": 156,
    "length": 156,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "convert_to_handlebars",
      "wrap_json_markdown",
      "pydantic_to_guidance_output_template",
      "pydantic_to_guidance_output_template_markdown",
      "json_schema_to_guidance_output_template",
      "parse_pydantic_from_guidance_program"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "convert_to_handlebars",
      "wrap_json_markdown",
      "pydantic_to_guidance_output_template",
      "pydantic_to_guidance_output_template_markdown",
      "json_schema_to_guidance_output_template",
      "parse_pydantic_from_guidance_program"
    ],
    "document_class_names": [],
    "content": "from typing import Optional, Type, TypeVar\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.output_parsers.base import OutputParserException\nfrom llama_index.core.output_parsers.utils import parse_json_markdown\n\n\ndef convert_to_handlebars(text: str) -> str:\n    \"\"\"Convert a python format string to handlebars-style template.\n\n    In python format string, single braces {} are used for variable substitution,\n        and double braces {{}} are used for escaping actual braces (e.g. for JSON dict)\n    In handlebars template, double braces {{}} are used for variable substitution,\n        and single braces are actual braces (e.g. for JSON dict)\n\n    This is currently only used to convert a python format string based prompt template\n    to a guidance program template.\n    \"\"\"\n    # Replace double braces with a temporary placeholder\n    var_left = \"TEMP_BRACE_LEFT\"\n    var_right = \"TEMP_BRACE_RIGHT\"\n    text = text.replace(\"{{\", var_left)\n    text = text.replace(\"}}\", var_right)\n\n    # Replace single braces with double braces\n    text = text.replace(\"{\", \"{{\")\n    text = text.replace(\"}\", \"}}\")\n\n    # Replace the temporary placeholder with single braces\n    text = text.replace(var_left, \"{\")\n    return text.replace(var_right, \"}\")\n\n\ndef wrap_json_markdown(text: str) -> str:\n    \"\"\"Wrap text in json markdown formatting block.\"\"\"\n    return \"```json\\n\" + text + \"\\n```\"\n\n\ndef pydantic_to_guidance_output_template(cls: Type[BaseModel]) -> str:\n    \"\"\"Convert a pydantic model to guidance output template.\"\"\"\n    return json_schema_to_guidance_output_template(\n        cls.model_json_schema(), root=cls.model_json_schema()\n    )\n\n\ndef pydantic_to_guidance_output_template_markdown(cls: Type[BaseModel]) -> str:\n    \"\"\"Convert a pydantic model to guidance output template wrapped in json markdown.\"\"\"\n    output = json_schema_to_guidance_output_template(\n        cls.model_json_schema(), root=cls.model_json_schema()\n    )\n    return wrap_json_markdown(output)\n\n\ndef json_schema_to_guidance_output_template(\n    schema: dict,\n    key: Optional[str] = None,\n    indent: int = 0,\n    root: Optional[dict] = None,\n    use_pattern_control: bool = False,\n) -> str:\n    \"\"\"Convert a json schema to guidance output template.\n\n    Implementation based on https://github.com/microsoft/guidance/\\\n        blob/main/notebooks/applications/jsonformer.ipynb\n    Modified to support nested pydantic models.\n    \"\"\"\n    out = \"\"\n    if \"type\" not in schema and \"$ref\" in schema:\n        if root is None:\n            raise ValueError(\"Must specify root schema for nested object\")\n\n        ref = schema[\"$ref\"]\n        model = ref.split(\"/\")[-1]\n        return json_schema_to_guidance_output_template(\n            root[\"$defs\"][model], key, indent, root\n        )\n\n    if schema[\"type\"] == \"object\":\n        out += \"  \" * indent + \"{\\n\"\n        for k, v in schema[\"properties\"].items():\n            out += (\n                \"  \" * (indent + 1)\n                + f'\"{k}\"'\n                + \": \"\n                + json_schema_to_guidance_output_template(v, k, indent + 1, root)\n                + \",\\n\"\n            )\n        out += \"  \" * indent + \"}\"\n        return out\n    elif schema[\"type\"] == \"array\":\n        if key is None:\n            raise ValueError(\"Key should not be None\")\n        if \"max_items\" in schema:\n            extra_args = f\" max_iterations={schema['max_items']}\"\n        else:\n            extra_args = \"\"\n        return (\n            \"[{{#geneach '\"\n            + key\n            + \"' stop=']'\"\n            + extra_args\n            + \"}}{{#unless @first}}, {{/unless}}\"\n            + json_schema_to_guidance_output_template(schema[\"items\"], \"this\", 0, root)\n            + \"{{/geneach}}]\"\n        )\n    elif schema[\"type\"] == \"string\":\n        if key is None:\n            raise ValueError(\"key should not be None\")\n        return \"\\\"{{gen '\" + key + \"' stop='\\\"'}}\\\"\"\n    elif schema[\"type\"] in [\"integer\", \"number\"]:\n        if key is None:\n            raise ValueError(\"key should not be None\")\n        if use_pattern_control:\n            return \"{{gen '\" + key + \"' pattern='[0-9\\\\.]' stop=','}}\"\n        else:\n            return \"\\\"{{gen '\" + key + \"' stop='\\\"'}}\\\"\"\n    elif schema[\"type\"] == \"boolean\":\n        if key is None:\n            raise ValueError(\"key should not be None\")\n        return \"{{#select '\" + key + \"'}}True{{or}}False{{/select}}\"\n    else:\n        schema_type = schema[\"type\"]\n        raise ValueError(f\"Unknown schema type {schema_type}\")\n\n\nModel = TypeVar(\"Model\", bound=BaseModel)\n\n\ndef parse_pydantic_from_guidance_program(\n    response: str, cls: Type[Model], verbose: bool = False\n) -> Model:\n    \"\"\"Parse output from guidance program.\n\n    This is a temporary solution for parsing a pydantic object out of an executed\n    guidance program.\n\n    NOTE: right now we assume the output is the last markdown formatted json block\n\n    NOTE: a better way is to extract via Program.variables, but guidance does not\n          support extracting nested objects right now.\n          So we call back to manually parsing the final text after program execution\n    \"\"\"\n    try:\n        output = response.split(\"```json\")[-1]\n        output = \"```json\" + output\n        if verbose:\n            print(\"Raw output:\")\n            print(output)\n        json_dict = parse_json_markdown(output)\n        sub_questions = cls.model_validate(json_dict)\n    except Exception as e:\n        raise OutputParserException(\n            \"Failed to parse pydantic object from guidance program\"\n            \". Probably the LLM failed to produce data with right json schema\"\n        ) from e\n    return sub_questions"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/mixin.py",
    "filename": "mixin.py",
    "relpath": "prompts/mixin.py",
    "start_line": 1,
    "end_line": 96,
    "length": 96,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_validate_prompts",
      "get_prompts",
      "update_prompts",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts"
    ],
    "chunk_class_names": [
      "PromptMixin"
    ],
    "document_function_names": [
      "_validate_prompts",
      "get_prompts",
      "update_prompts",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts"
    ],
    "document_class_names": [
      "PromptMixin"
    ],
    "content": "\"\"\"Prompt Mixin.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom typing import Dict, Union\n\nfrom llama_index.core.prompts.base import BasePromptTemplate\n\nHasPromptType = Union[\"PromptMixin\", BasePromptTemplate]\nPromptDictType = Dict[str, BasePromptTemplate]\nPromptMixinType = Dict[str, \"PromptMixin\"]\n\n\nclass PromptMixin(ABC):\n    \"\"\"Prompt mixin.\n\n    This mixin is used in other modules, like query engines, response synthesizers.\n    This shows that the module supports getting, setting prompts,\n    both within the immediate module as well as child modules.\n\n    \"\"\"\n\n    def _validate_prompts(\n        self,\n        prompts_dict: PromptDictType,\n        module_dict: PromptMixinType,\n    ) -> None:\n        \"\"\"Validate prompts.\"\"\"\n        # check if prompts_dict, module_dict has restricted \":\" token\n        for key in prompts_dict:\n            if \":\" in key:\n                raise ValueError(f\"Prompt key {key} cannot contain ':'.\")\n\n        for key in module_dict:\n            if \":\" in key:\n                raise ValueError(f\"Prompt key {key} cannot contain ':'.\")\n\n    def get_prompts(self) -> Dict[str, BasePromptTemplate]:\n        \"\"\"Get a prompt.\"\"\"\n        prompts_dict = self._get_prompts()\n        module_dict = self._get_prompt_modules()\n        self._validate_prompts(prompts_dict, module_dict)\n\n        # avoid modifying the original dict\n        all_prompts = deepcopy(prompts_dict)\n        for module_name, prompt_module in module_dict.items():\n            # append module name to each key in sub-modules by \":\"\n            for key, prompt in prompt_module.get_prompts().items():\n                all_prompts[f\"{module_name}:{key}\"] = prompt\n        return all_prompts\n\n    def update_prompts(self, prompts_dict: Dict[str, BasePromptTemplate]) -> None:\n        \"\"\"Update prompts.\n\n        Other prompts will remain in place.\n\n        \"\"\"\n        prompt_modules = self._get_prompt_modules()\n\n        # update prompts for current module\n        self._update_prompts(prompts_dict)\n\n        # get sub-module keys\n        # mapping from module name to sub-module prompt keys\n        sub_prompt_dicts: Dict[str, PromptDictType] = defaultdict(dict)\n        for key in prompts_dict:\n            if \":\" in key:\n                module_name, sub_key = key.split(\":\")\n                sub_prompt_dicts[module_name][sub_key] = prompts_dict[key]\n\n        # now update prompts for submodules\n        for module_name, sub_prompt_dict in sub_prompt_dicts.items():\n            if module_name not in prompt_modules:\n                raise ValueError(f\"Module {module_name} not found.\")\n            module = prompt_modules[module_name]\n            module.update_prompts(sub_prompt_dict)\n\n    @abstractmethod\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n\n    @abstractmethod\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\n\n        Return a dictionary of sub-modules within the current module\n        that also implement PromptMixin (so that their prompts can also be get/set).\n\n        Can be blank if no sub-modules.\n\n        \"\"\"\n\n    @abstractmethod\n    def _update_prompts(self, prompts_dict: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/default_prompts.py",
    "filename": "default_prompts.py",
    "relpath": "prompts/default_prompts.py",
    "start_line": 1,
    "end_line": 368,
    "length": 368,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Set of default prompts.\"\"\"\n\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.prompts.prompt_type import PromptType\n\n############################################\n# Tree\n############################################\n\nDEFAULT_SUMMARY_PROMPT_TMPL = (\n    \"Write a summary of the following. Try to use only the \"\n    \"information provided. \"\n    \"Try to include as many key details as possible.\\n\"\n    \"\\n\"\n    \"\\n\"\n    \"{context_str}\\n\"\n    \"\\n\"\n    \"\\n\"\n    'SUMMARY:\"\"\"\\n'\n)\n\nDEFAULT_SUMMARY_PROMPT = PromptTemplate(\n    DEFAULT_SUMMARY_PROMPT_TMPL, prompt_type=PromptType.SUMMARY\n)\n\n# insert prompts\nDEFAULT_INSERT_PROMPT_TMPL = (\n    \"Context information is below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"---------------------\\n\"\n    \"Given the context information, here is a new piece of \"\n    \"information: {new_chunk_text}\\n\"\n    \"Answer with the number corresponding to the summary that should be updated. \"\n    \"The answer should be the number corresponding to the \"\n    \"summary that is most relevant to the question.\\n\"\n)\nDEFAULT_INSERT_PROMPT = PromptTemplate(\n    DEFAULT_INSERT_PROMPT_TMPL, prompt_type=PromptType.TREE_INSERT\n)\n\n\n# # single choice\nDEFAULT_QUERY_PROMPT_TMPL = (\n    \"Some choices are given below. It is provided in a numbered list \"\n    \"(1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return \"\n    \"the choice that is most relevant to the question: '{query_str}'\\n\"\n    \"Provide choice in the following format: 'ANSWER: <number>' and explain why \"\n    \"this summary was selected in relation to the question.\\n\"\n)\nDEFAULT_QUERY_PROMPT = PromptTemplate(\n    DEFAULT_QUERY_PROMPT_TMPL, prompt_type=PromptType.TREE_SELECT\n)\n\n# multiple choice\nDEFAULT_QUERY_PROMPT_MULTIPLE_TMPL = (\n    \"Some choices are given below. It is provided in a numbered \"\n    \"list (1 to {num_chunks}), \"\n    \"where each item in the list corresponds to a summary.\\n\"\n    \"---------------------\\n\"\n    \"{context_list}\"\n    \"\\n---------------------\\n\"\n    \"Using only the choices above and not prior knowledge, return the top choices \"\n    \"(no more than {branching_factor}, ranked by most relevant to least) that \"\n    \"are most relevant to the question: '{query_str}'\\n\"\n    \"Provide choices in the following format: 'ANSWER: <numbers>' and explain why \"\n    \"these summaries were selected in relation to the question.\\n\"\n)\nDEFAULT_QUERY_PROMPT_MULTIPLE = PromptTemplate(\n    DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL, prompt_type=PromptType.TREE_SELECT_MULTIPLE\n)\n\n\nDEFAULT_REFINE_PROMPT_TMPL = (\n    \"The original query is as follows: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the query. \"\n    \"If the context isn't useful, return the original answer.\\n\"\n    \"Refined Answer: \"\n)\nDEFAULT_REFINE_PROMPT = PromptTemplate(\n    DEFAULT_REFINE_PROMPT_TMPL, prompt_type=PromptType.REFINE\n)\n\n\nDEFAULT_TEXT_QA_PROMPT_TMPL = (\n    \"Context information is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nDEFAULT_TEXT_QA_PROMPT = PromptTemplate(\n    DEFAULT_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n)\n\nDEFAULT_TREE_SUMMARIZE_TMPL = (\n    \"Context information from multiple sources is below.\\n\"\n    \"---------------------\\n\"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the information from multiple sources and not prior knowledge, \"\n    \"answer the query.\\n\"\n    \"Query: {query_str}\\n\"\n    \"Answer: \"\n)\nDEFAULT_TREE_SUMMARIZE_PROMPT = PromptTemplate(\n    DEFAULT_TREE_SUMMARIZE_TMPL, prompt_type=PromptType.SUMMARY\n)\n\n\n############################################\n# Keyword Table\n############################################\n\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"Some text is provided below. Given the text, extract up to {max_keywords} \"\n    \"keywords from the text. Avoid stopwords.\"\n    \"---------------------\\n\"\n    \"{text}\\n\"\n    \"---------------------\\n\"\n    \"Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\\n\"\n)\nDEFAULT_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT\n)\n\n\n# NOTE: the keyword extraction for queries can be the same as\n# the one used to build the index, but here we tune it to see if performance is better.\nDEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n    \"A question is provided below. Given the question, extract up to {max_keywords} \"\n    \"keywords from the text. Focus on extracting the keywords that we can use \"\n    \"to best lookup answers to the question. Avoid stopwords.\\n\"\n    \"---------------------\\n\"\n    \"{question}\\n\"\n    \"---------------------\\n\"\n    \"Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\\n\"\n)\nDEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL,\n    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n)\n\n\n############################################\n# Structured Store\n############################################\n\nDEFAULT_SCHEMA_EXTRACT_TMPL = (\n    \"We wish to extract relevant fields from an unstructured text chunk into \"\n    \"a structured schema. We first provide the unstructured text, and then \"\n    \"we provide the schema that we wish to extract. \"\n    \"-----------text-----------\\n\"\n    \"{text}\\n\"\n    \"-----------schema-----------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"Given the text and schema, extract the relevant fields from the text in \"\n    \"the following format: \"\n    \"field1: <value>\\nfield2: <value>\\n...\\n\\n\"\n    \"If a field is not present in the text, don't include it in the output.\"\n    \"If no fields are present in the text, return a blank string.\\n\"\n    \"Fields: \"\n)\nDEFAULT_SCHEMA_EXTRACT_PROMPT = PromptTemplate(\n    DEFAULT_SCHEMA_EXTRACT_TMPL, prompt_type=PromptType.SCHEMA_EXTRACT\n)\n\n# NOTE: taken from langchain and adapted\n# https://github.com/langchain-ai/langchain/blob/v0.0.303/libs/langchain/langchain/chains/sql_database/prompt.py\nDEFAULT_TEXT_TO_SQL_TMPL = (\n    \"Given an input question, first create a syntactically correct {dialect} \"\n    \"query to run, then look at the results of the query and return the answer. \"\n    \"You can order the results by a relevant column to return the most \"\n    \"interesting examples in the database.\\n\\n\"\n    \"Never query for all the columns from a specific table, only ask for a \"\n    \"few relevant columns given the question.\\n\\n\"\n    \"Pay attention to use only the column names that you can see in the schema \"\n    \"description. \"\n    \"Be careful to not query for columns that do not exist. \"\n    \"Pay attention to which column is in which table. \"\n    \"Also, qualify column names with the table name when needed. \"\n    \"You are required to use the following format, each taking one line:\\n\\n\"\n    \"Question: Question here\\n\"\n    \"SQLQuery: SQL Query to run\\n\"\n    \"SQLResult: Result of the SQLQuery\\n\"\n    \"Answer: Final answer here\\n\\n\"\n    \"Only use tables listed below.\\n\"\n    \"{schema}\\n\\n\"\n    \"Question: {query_str}\\n\"\n    \"SQLQuery: \"\n)\n\nDEFAULT_TEXT_TO_SQL_PROMPT = PromptTemplate(\n    DEFAULT_TEXT_TO_SQL_TMPL,\n    prompt_type=PromptType.TEXT_TO_SQL,\n)\n\nDEFAULT_TEXT_TO_SQL_PGVECTOR_TMPL = \"\"\"\\\nGiven an input question, first create a syntactically correct {dialect} \\\nquery to run, then look at the results of the query and return the answer. \\\nYou can order the results by a relevant column to return the most \\\ninteresting examples in the database.\n\nPay attention to use only the column names that you can see in the schema \\\ndescription. Be careful to not query for columns that do not exist. \\\nPay attention to which column is in which table. Also, qualify column names \\\nwith the table name when needed.\n\nIMPORTANT NOTE: you can use specialized pgvector syntax (`<->`) to do nearest \\\nneighbors/semantic search to a given vector from an embeddings column in the table. \\\nThe embeddings value for a given row typically represents the semantic meaning of that row. \\\nThe vector represents an embedding representation \\\nof the question, given below. Do NOT fill in the vector values directly, but rather specify a \\\n`[query_vector]` placeholder. For instance, some select statement examples below \\\n(the name of the embeddings column is `embedding`):\nSELECT * FROM items ORDER BY embedding <-> '[query_vector]' LIMIT 5;\nSELECT * FROM items WHERE id != 1 ORDER BY embedding <-> (SELECT embedding FROM items WHERE id = 1) LIMIT 5;\nSELECT * FROM items WHERE embedding <-> '[query_vector]' < 5;\n\nYou are required to use the following format, \\\neach taking one line:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\nOnly use tables listed below.\n{schema}\n\n\nQuestion: {query_str}\nSQLQuery: \\\n\"\"\"\n\nDEFAULT_TEXT_TO_SQL_PGVECTOR_PROMPT = PromptTemplate(\n    DEFAULT_TEXT_TO_SQL_PGVECTOR_TMPL,\n    prompt_type=PromptType.TEXT_TO_SQL,\n)\n\n\n# NOTE: by partially filling schema, we can reduce to a QuestionAnswer prompt\n# that we can feed to ur table\nDEFAULT_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"We have also provided context information below. \"\n    \"{context_str}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and the table schema, \"\n    \"give a response to the following task: {query_str}\"\n)\n\nDEFAULT_TABLE_CONTEXT_QUERY = (\n    \"Provide a high-level description of the table, \"\n    \"as well as a description of each column in the table. \"\n    \"Provide answers in the following format:\\n\"\n    \"TableDescription: <description>\\n\"\n    \"Column1Description: <description>\\n\"\n    \"Column2Description: <description>\\n\"\n    \"...\\n\\n\"\n)\n\nDEFAULT_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    DEFAULT_TABLE_CONTEXT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\n\n# NOTE: by partially filling schema, we can reduce to a refine prompt\n# that we can feed to ur table\nDEFAULT_REFINE_TABLE_CONTEXT_TMPL = (\n    \"We have provided a table schema below. \"\n    \"---------------------\\n\"\n    \"{schema}\\n\"\n    \"---------------------\\n\"\n    \"We have also provided some context information below. \"\n    \"{context_msg}\\n\"\n    \"---------------------\\n\"\n    \"Given the context information and the table schema, \"\n    \"give a response to the following task: {query_str}\\n\"\n    \"We have provided an existing answer: {existing_answer}\\n\"\n    \"Given the new context, refine the original answer to better \"\n    \"answer the question. \"\n    \"If the context isn't useful, return the original answer.\"\n)\nDEFAULT_REFINE_TABLE_CONTEXT_PROMPT = PromptTemplate(\n    DEFAULT_REFINE_TABLE_CONTEXT_TMPL, prompt_type=PromptType.TABLE_CONTEXT\n)\n\n\n############################################\n# Knowledge-Graph Table\n############################################\n\nDEFAULT_KG_TRIPLET_EXTRACT_TMPL = (\n    \"Some text is provided below. Given the text, extract up to \"\n    \"{max_knowledge_triplets} \"\n    \"knowledge triplets in the form of (subject, predicate, object). Avoid stopwords.\\n\"\n    \"---------------------\\n\"\n    \"Example:\"\n    \"Text: Alice is Bob's mother.\"\n    \"Triplets:\\n(Alice, is mother of, Bob)\\n\"\n    \"Text: Philz is a coffee shop founded in Berkeley in 1982.\\n\"\n    \"Triplets:\\n\"\n    \"(Philz, is, coffee shop)\\n\"\n    \"(Philz, founded in, Berkeley)\\n\"\n    \"(Philz, founded in, 1982)\\n\"\n    \"---------------------\\n\"\n    \"Text: {text}\\n\"\n    \"Triplets:\\n\"\n)\nDEFAULT_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(\n    DEFAULT_KG_TRIPLET_EXTRACT_TMPL,\n    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n)\n\nDEFAULT_DYNAMIC_EXTRACT_TMPL = (\n    \"Extract up to {max_knowledge_triplets} knowledge triplets from the given text. \"\n    \"Each triplet should be in the form of (head, relation, tail) with their respective types.\\n\"\n    \"---------------------\\n\"\n    \"INITIAL ONTOLOGY:\\n\"\n    \"Entity Types: {allowed_entity_types}\\n\"\n    \"Relation Types: {allowed_relation_types}\\n\"\n    \"\\n\"\n    \"Use these types as a starting point, but introduce new types if necessary based on the context.\\n\"\n    \"\\n\"\n    \"GUIDELINES:\\n\"\n    \"- Output in JSON format: [{{'head': '', 'head_type': '', 'relation': '', 'tail': '', 'tail_type': ''}}]\\n\"\n    \"- Use the most complete form for entities (e.g., 'United States of America' instead of 'USA')\\n\"\n    \"- Keep entities concise (3-5 words max)\\n\"\n    \"- Break down complex phrases into multiple triplets\\n\"\n    \"- Ensure the knowledge graph is coherent and easily understandable\\n\"\n    \"---------------------\\n\"\n    \"EXAMPLE:\\n\"\n    \"Text: Tim Cook, CEO of Apple Inc., announced the new Apple Watch that monitors heart health. \"\n    \"UC Berkeley researchers studied the benefits of apples.\\n\"\n    \"Output:\\n\"\n    \"[{{'head': 'Tim Cook', 'head_type': 'PERSON', 'relation': 'CEO_OF', 'tail': 'Apple Inc.', 'tail_type': 'COMPANY'}},\\n\"\n    \" {{'head': 'Apple Inc.', 'head_type': 'COMPANY', 'relation': 'PRODUCES', 'tail': 'Apple Watch', 'tail_type': 'PRODUCT'}},\\n\"\n    \" {{'head': 'Apple Watch', 'head_type': 'PRODUCT', 'relation': 'MONITORS', 'tail': 'heart health', 'tail_type': 'HEALTH_METRIC'}},\\n\"\n    \" {{'head': 'UC Berkeley', 'head_type': 'UNIVERSITY', 'relation': 'STUDIES', 'tail': 'benefits of apples', 'tail_type': 'RESEARCH_TOPIC'}}]\\n\"\n    \"---------------------\\n\"\n    \"Text: {text}\\n\"\n    \"Output:\\n\"\n)\n\nDEFAULT_DYNAMIC_EXTRACT_PROMPT = PromptTemplate(\n    DEFAULT_DYNAMIC_EXTRACT_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/prompts/default_prompts.py",
    "filename": "default_prompts.py",
    "relpath": "prompts/default_prompts.py",
    "start_line": 368,
    "end_line": 523,
    "length": 156,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "DEFAULT_DYNAMIC_EXTRACT_PROPS_TMPL = (\n    \"Extract up to {max_knowledge_triplets} knowledge triplets from the given text. \"\n    \"Each triplet should be in the form of (head, relation, tail) with their respective types and properties.\\n\"\n    \"---------------------\\n\"\n    \"INITIAL ONTOLOGY:\\n\"\n    \"Entity Types: {allowed_entity_types}\\n\"\n    \"Entity Properties: {allowed_entity_properties}\\n\"\n    \"Relation Types: {allowed_relation_types}\\n\"\n    \"Relation Properties: {allowed_relation_properties}\\n\"\n    \"\\n\"\n    \"Use these types as a starting point, but introduce new types if necessary based on the context.\\n\"\n    \"\\n\"\n    \"GUIDELINES:\\n\"\n    \"- Output in JSON format: [{{'head': '', 'head_type': '', 'head_props': {{...}}, 'relation': '', 'relation_props': {{...}}, 'tail': '', 'tail_type': '', 'tail_props': {{...}}}}]\\n\"\n    \"- Use the most complete form for entities (e.g., 'United States of America' instead of 'USA')\\n\"\n    \"- Keep entities concise (3-5 words max)\\n\"\n    \"- Break down complex phrases into multiple triplets\\n\"\n    \"- Ensure the knowledge graph is coherent and easily understandable\\n\"\n    \"---------------------\\n\"\n    \"EXAMPLE:\\n\"\n    \"Text: Tim Cook, CEO of Apple Inc., announced the new Apple Watch that monitors heart health. \"\n    \"UC Berkeley researchers studied the benefits of apples.\\n\"\n    \"Output:\\n\"\n    \"[{{'head': 'Tim Cook', 'head_type': 'PERSON', 'head_props': {{'prop1': 'val', ...}}, 'relation': 'CEO_OF', 'relation_props': {{'prop1': 'val', ...}}, 'tail': 'Apple Inc.', 'tail_type': 'COMPANY', 'tail_props': {{'prop1': 'val', ...}}}},\\n\"\n    \" {{'head': 'Apple Inc.', 'head_type': 'COMPANY', 'head_props': {{'prop1': 'val', ...}}, 'relation': 'PRODUCES', 'relation_props': {{'prop1': 'val', ...}}, 'tail': 'Apple Watch', 'tail_type': 'PRODUCT', 'tail_props': {{'prop1': 'val', ...}}}},\\n\"\n    \" {{'head': 'Apple Watch', 'head_type': 'PRODUCT', 'head_props': {{'prop1': 'val', ...}}, 'relation': 'MONITORS', 'relation_props': {{'prop1': 'val', ...}}, 'tail': 'heart health', 'tail_type': 'HEALTH_METRIC', 'tail_props': {{'prop1': 'val', ...}}}},\\n\"\n    \" {{'head': 'UC Berkeley', 'head_type': 'UNIVERSITY', 'head_props': {{'prop1': 'val', ...}}, 'relation': 'STUDIES', 'relation_props': {{'prop1': 'val', ...}}, 'tail': 'benefits of apples', 'tail_type': 'RESEARCH_TOPIC', 'tail_props': {{'prop1': 'val', ...}}}}]\\n\"\n    \"---------------------\\n\"\n    \"Text: {text}\\n\"\n    \"Output:\\n\"\n)\n\nDEFAULT_DYNAMIC_EXTRACT_PROPS_PROMPT = PromptTemplate(\n    DEFAULT_DYNAMIC_EXTRACT_PROPS_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT\n)\n\n############################################\n# HYDE\n##############################################\n\nHYDE_TMPL = (\n    \"Please write a passage to answer the question\\n\"\n    \"Try to include as many key details as possible.\\n\"\n    \"\\n\"\n    \"\\n\"\n    \"{context_str}\\n\"\n    \"\\n\"\n    \"\\n\"\n    'Passage:\"\"\"\\n'\n)\n\nDEFAULT_HYDE_PROMPT = PromptTemplate(HYDE_TMPL, prompt_type=PromptType.SUMMARY)\n\n\n############################################\n# Simple Input\n############################################\n\nDEFAULT_SIMPLE_INPUT_TMPL = \"{query_str}\"\nDEFAULT_SIMPLE_INPUT_PROMPT = PromptTemplate(\n    DEFAULT_SIMPLE_INPUT_TMPL, prompt_type=PromptType.SIMPLE_INPUT\n)\n\n\n############################################\n# JSON Path\n############################################\n\nDEFAULT_JSON_PATH_TMPL = (\n    \"We have provided a JSON schema below:\\n\"\n    \"{schema}\\n\"\n    \"Given a task, respond with a JSON Path query that \"\n    \"can retrieve data from a JSON value that matches the schema.\\n\"\n    \"Provide the JSON Path query in the following format: 'JSONPath: <JSONPath>'\\n\"\n    \"You must include the value 'JSONPath:' before the provided JSON Path query.\"\n    \"Example Format:\\n\"\n    \"Task: What is John's age?\\n\"\n    \"Response: JSONPath: $.John.age\\n\"\n    \"Let's try this now: \\n\\n\"\n    \"Task: {query_str}\\n\"\n    \"Response: \"\n)\n\nDEFAULT_JSON_PATH_PROMPT = PromptTemplate(\n    DEFAULT_JSON_PATH_TMPL, prompt_type=PromptType.JSON_PATH\n)\n\n\n############################################\n# Choice Select\n############################################\n\nDEFAULT_CHOICE_SELECT_PROMPT_TMPL = (\n    \"A list of documents is shown below. Each document has a number next to it along \"\n    \"with a summary of the document. A question is also provided. \\n\"\n    \"Respond with the numbers of the documents \"\n    \"you should consult to answer the question, in order of relevance, as well \\n\"\n    \"as the relevance score. The relevance score is a number from 1-10 based on \"\n    \"how relevant you think the document is to the question.\\n\"\n    \"Do not include any documents that are not relevant to the question. \\n\"\n    \"Example format: \\n\"\n    \"Document 1:\\n<summary of document 1>\\n\\n\"\n    \"Document 2:\\n<summary of document 2>\\n\\n\"\n    \"...\\n\\n\"\n    \"Document 10:\\n<summary of document 10>\\n\\n\"\n    \"Question: <question>\\n\"\n    \"Answer:\\n\"\n    \"Doc: 9, Relevance: 7\\n\"\n    \"Doc: 3, Relevance: 4\\n\"\n    \"Doc: 7, Relevance: 3\\n\\n\"\n    \"Let's try this now: \\n\\n\"\n    \"{context_str}\\n\"\n    \"Question: {query_str}\\n\"\n    \"Answer:\\n\"\n)\nDEFAULT_CHOICE_SELECT_PROMPT = PromptTemplate(\n    DEFAULT_CHOICE_SELECT_PROMPT_TMPL, prompt_type=PromptType.CHOICE_SELECT\n)\n\n\n############################################\n# RankGPT Rerank template\n############################################\n\nRANKGPT_RERANK_PROMPT_TMPL = (\n    \"Search Query: {query}. \\nRank the {num} passages above \"\n    \"based on their relevance to the search query. The passages \"\n    \"should be listed in descending order using identifiers. \"\n    \"The most relevant passages should be listed first. \"\n    \"The output format should be [] > [], e.g., [1] > [2]. \"\n    \"Only response the ranking results, \"\n    \"do not say any word or explain.\"\n)\nRANKGPT_RERANK_PROMPT = PromptTemplate(\n    RANKGPT_RERANK_PROMPT_TMPL, prompt_type=PromptType.RANKGPT_RERANK\n)\n\n\n############################################\n# JSONalyze Query Template\n############################################\n\nDEFAULT_JSONALYZE_PROMPT_TMPL = (\n    \"You are given a table named: '{table_name}' with schema, \"\n    \"generate SQLite SQL query to answer the given question.\\n\"\n    \"Table schema:\\n\"\n    \"{table_schema}\\n\"\n    \"Question: {question}\\n\\n\"\n    \"SQLQuery: \"\n)\n\nDEFAULT_JSONALYZE_PROMPT = PromptTemplate(\n    DEFAULT_JSONALYZE_PROMPT_TMPL, prompt_type=PromptType.TEXT_TO_SQL\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/composability/base.py",
    "filename": "base.py",
    "relpath": "composability/base.py",
    "start_line": 1,
    "end_line": 4,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Composable graph.\"\"\"\n\n# TODO: remove this file, only keep for backwards compatibility\nfrom llama_index.core.indices.composability.graph import ComposableGraph  # noqa"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/composability/joint_qa_summary.py",
    "filename": "joint_qa_summary.py",
    "relpath": "composability/joint_qa_summary.py",
    "start_line": 1,
    "end_line": 97,
    "length": 97,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "build_from_documents"
    ],
    "chunk_class_names": [
      "QASummaryQueryEngineBuilder"
    ],
    "document_function_names": [
      "__init__",
      "build_from_documents"
    ],
    "document_class_names": [
      "QASummaryQueryEngineBuilder"
    ],
    "content": "\"\"\"Joint QA Summary graph.\"\"\"\n\nfrom typing import List, Optional, Sequence\n\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.list.base import SummaryIndex\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom llama_index.core.ingestion import run_transformations\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.query_engine.router_query_engine import RouterQueryEngine\nfrom llama_index.core.schema import Document, TransformComponent\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.storage.storage_context import StorageContext\nfrom llama_index.core.tools.query_engine import QueryEngineTool\n\nDEFAULT_SUMMARY_TEXT = \"Use this index for summarization queries\"\nDEFAULT_QA_TEXT = (\n    \"Use this index for queries that require retrieval of specific \"\n    \"context from documents.\"\n)\n\n\nclass QASummaryQueryEngineBuilder:\n    \"\"\"Joint QA Summary graph builder.\n\n    Can build a graph that provides a unified query interface\n    for both QA and summarization tasks.\n\n    NOTE: this is a beta feature. The API may change in the future.\n\n    Args:\n        docstore (BaseDocumentStore): A BaseDocumentStore to use for storing nodes.\n        summary_text (str): Text to use for the summary index.\n        qa_text (str): Text to use for the QA index.\n        node_parser (NodeParser): A NodeParser to use for parsing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        embed_model: Optional[BaseEmbedding] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        storage_context: Optional[StorageContext] = None,\n        summary_text: str = DEFAULT_SUMMARY_TEXT,\n        qa_text: str = DEFAULT_QA_TEXT,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n        self._callback_manager = callback_manager or Settings.callback_manager\n        self._embed_model = embed_model or Settings.embed_model\n        self._transformations = transformations or Settings.transformations\n\n        self._storage_context = storage_context or StorageContext.from_defaults()\n        self._summary_text = summary_text\n        self._qa_text = qa_text\n\n    def build_from_documents(\n        self,\n        documents: Sequence[Document],\n    ) -> RouterQueryEngine:\n        \"\"\"Build query engine.\"\"\"\n        # parse nodes\n        nodes = run_transformations(documents, self._transformations)  # type: ignore\n\n        # ingest nodes\n        self._storage_context.docstore.add_documents(nodes, allow_update=True)\n\n        # build indices\n        vector_index = VectorStoreIndex(\n            nodes=nodes,\n            transformations=self._transformations,\n            embed_model=self._embed_model,\n            storage_context=self._storage_context,\n        )\n        summary_index = SummaryIndex(nodes, storage_context=self._storage_context)\n\n        vector_query_engine = vector_index.as_query_engine(llm=self._llm)\n        list_query_engine = summary_index.as_query_engine(\n            llm=self._llm, response_mode=\"tree_summarize\"\n        )\n\n        # build query engine\n        return RouterQueryEngine.from_defaults(\n            llm=self._llm,\n            query_engine_tools=[\n                QueryEngineTool.from_defaults(\n                    vector_query_engine, description=self._qa_text\n                ),\n                QueryEngineTool.from_defaults(\n                    list_query_engine, description=self._summary_text\n                ),\n            ],\n            select_multi=False,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/composability/__init__.py",
    "filename": "__init__.py",
    "relpath": "composability/__init__.py",
    "start_line": 1,
    "end_line": 9,
    "length": 9,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init composability.\"\"\"\n\n\nfrom llama_index.core.composability.base import ComposableGraph\nfrom llama_index.core.composability.joint_qa_summary import (\n    QASummaryQueryEngineBuilder,\n)\n\n__all__ = [\"ComposableGraph\", \"QASummaryQueryEngineBuilder\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/program/llm_prompt_program.py",
    "filename": "llm_prompt_program.py",
    "relpath": "program/llm_prompt_program.py",
    "start_line": 1,
    "end_line": 33,
    "length": 33,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_defaults"
    ],
    "chunk_class_names": [
      "BaseLLMFunctionProgram",
      "for"
    ],
    "document_function_names": [
      "from_defaults"
    ],
    "document_class_names": [
      "BaseLLMFunctionProgram",
      "for"
    ],
    "content": "\"\"\"LLM Prompt Program.\"\"\"\nfrom abc import abstractmethod\nfrom typing import Any, Generic, Optional, Type, TypeVar\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.types import BasePydanticProgram, Model\n\nLM = TypeVar(\"LM\")\n\n\nclass BaseLLMFunctionProgram(BasePydanticProgram[BaseModel], Generic[LM]):\n    \"\"\"Base LLM Prompt Program.\n\n    This is a base class for LLM endpoints that can return\n    a structured output given the prompt.\n\n    NOTE: this only works for structured endpoints atm\n    (does not work for text completion endpoints.)\n\n    \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def from_defaults(\n        cls,\n        output_cls: Type[Model],\n        prompt_template_str: Optional[str] = None,\n        prompt: Optional[PromptTemplate] = None,\n        llm: Optional[LM] = None,\n        **kwargs: Any,\n    ) -> \"BaseLLMFunctionProgram\":\n        \"\"\"Initialize program from defaults.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/program/llm_program.py",
    "filename": "llm_program.py",
    "relpath": "program/llm_program.py",
    "start_line": 1,
    "end_line": 134,
    "length": 134,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "output_cls",
      "prompt",
      "prompt",
      "__call__",
      "acall"
    ],
    "chunk_class_names": [
      "LLMTextCompletionProgram",
      "if"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "output_cls",
      "prompt",
      "prompt",
      "__call__",
      "acall"
    ],
    "document_class_names": [
      "LLMTextCompletionProgram",
      "if"
    ],
    "content": "from typing import Any, Dict, Optional, Type, cast\n\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.output_parsers.pydantic import PydanticOutputParser\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.types import BaseOutputParser, BasePydanticProgram, Model\n\n\nclass LLMTextCompletionProgram(BasePydanticProgram[Model]):\n    \"\"\"\n    LLM Text Completion Program.\n\n    Uses generic LLM text completion + an output parser to generate a structured output.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        output_parser: BaseOutputParser,\n        output_cls: Type[Model],\n        prompt: BasePromptTemplate,\n        llm: LLM,\n        verbose: bool = False,\n    ) -> None:\n        self._output_parser = output_parser\n        self._output_cls = output_cls\n        self._llm = llm\n        self._prompt = prompt\n        self._verbose = verbose\n\n        self._prompt.output_parser = output_parser\n\n    @classmethod\n    def from_defaults(\n        cls,\n        output_parser: Optional[BaseOutputParser] = None,\n        output_cls: Optional[Type[Model]] = None,\n        prompt_template_str: Optional[str] = None,\n        prompt: Optional[BasePromptTemplate] = None,\n        llm: Optional[LLM] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"LLMTextCompletionProgram[Model]\":\n        llm = llm or Settings.llm\n        if prompt is None and prompt_template_str is None:\n            raise ValueError(\"Must provide either prompt or prompt_template_str.\")\n        if prompt is not None and prompt_template_str is not None:\n            raise ValueError(\"Must provide either prompt or prompt_template_str.\")\n        if prompt_template_str is not None:\n            prompt = PromptTemplate(prompt_template_str)\n\n        # decide default output class if not set\n        if output_cls is None:\n            if not isinstance(output_parser, PydanticOutputParser):\n                raise ValueError(\"Output parser must be PydanticOutputParser.\")\n            output_cls = output_parser.output_cls\n        else:\n            if output_parser is None:\n                output_parser = PydanticOutputParser(output_cls=output_cls)\n\n        return cls(\n            output_parser,\n            output_cls,\n            prompt=cast(PromptTemplate, prompt),\n            llm=llm,\n            verbose=verbose,\n        )\n\n    @property\n    def output_cls(self) -> Type[Model]:\n        return self._output_cls\n\n    @property\n    def prompt(self) -> BasePromptTemplate:\n        return self._prompt\n\n    @prompt.setter\n    def prompt(self, prompt: BasePromptTemplate) -> None:\n        self._prompt = prompt\n\n    def __call__(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Model:\n        llm_kwargs = llm_kwargs or {}\n        if self._llm.metadata.is_chat_model:\n            messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n            messages = self._llm._extend_messages(messages)\n            chat_response = self._llm.chat(messages, **llm_kwargs)\n\n            raw_output = chat_response.message.content or \"\"\n        else:\n            formatted_prompt = self._prompt.format(llm=self._llm, **kwargs)\n\n            response = self._llm.complete(formatted_prompt, **llm_kwargs)\n\n            raw_output = response.text\n\n        output = self._output_parser.parse(raw_output)\n        if not isinstance(output, self._output_cls):\n            raise ValueError(\n                f\"Output parser returned {type(output)} but expected {self._output_cls}\"\n            )\n        return output\n\n    async def acall(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> Model:\n        llm_kwargs = llm_kwargs or {}\n        if self._llm.metadata.is_chat_model:\n            messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n            messages = self._llm._extend_messages(messages)\n            chat_response = await self._llm.achat(messages, **llm_kwargs)\n\n            raw_output = chat_response.message.content or \"\"\n        else:\n            formatted_prompt = self._prompt.format(llm=self._llm, **kwargs)\n\n            response = await self._llm.acomplete(formatted_prompt, **llm_kwargs)\n\n            raw_output = response.text\n\n        output = self._output_parser.parse(raw_output)\n        if not isinstance(output, self._output_cls):\n            raise ValueError(\n                f\"Output parser returned {type(output)} but expected {self._output_cls}\"\n            )\n        return output"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/program/__init__.py",
    "filename": "__init__.py",
    "relpath": "program/__init__.py",
    "start_line": 1,
    "end_line": 13,
    "length": 13,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.program.llm_program import LLMTextCompletionProgram\nfrom llama_index.core.program.function_program import FunctionCallingProgram\nfrom llama_index.core.program.multi_modal_llm_program import (\n    MultiModalLLMCompletionProgram,\n)\nfrom llama_index.core.types import BasePydanticProgram\n\n__all__ = [\n    \"BasePydanticProgram\",\n    \"LLMTextCompletionProgram\",\n    \"MultiModalLLMCompletionProgram\",\n    \"FunctionCallingProgram\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/program/function_program.py",
    "filename": "function_program.py",
    "relpath": "program/function_program.py",
    "start_line": 1,
    "end_line": 329,
    "length": 329,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_function_tool",
      "model_fn",
      "__init__",
      "from_defaults",
      "output_cls",
      "prompt",
      "prompt",
      "__call__",
      "acall",
      "_parse_tool_outputs",
      "_process_objects",
      "stream_call",
      "astream_call",
      "gen"
    ],
    "chunk_class_names": [
      "FunctionCallingProgram",
      "return"
    ],
    "document_function_names": [
      "get_function_tool",
      "model_fn",
      "__init__",
      "from_defaults",
      "output_cls",
      "prompt",
      "prompt",
      "__call__",
      "acall",
      "_parse_tool_outputs",
      "_process_objects",
      "stream_call",
      "astream_call",
      "gen"
    ],
    "document_class_names": [
      "FunctionCallingProgram",
      "return"
    ],
    "content": "\"\"\"Pydantic program through function calling.\"\"\"\n\nimport logging\nfrom typing import (\n    Any,\n    Dict,\n    Optional,\n    Type,\n    cast,\n    Union,\n    List,\n    Generator,\n    AsyncGenerator,\n)\n\nfrom llama_index.core.bridge.pydantic import (\n    ValidationError,\n)\nfrom llama_index.core.base.llms.types import ChatResponse\nfrom llama_index.core.llms.function_calling import FunctionCallingLLM\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.types import BasePydanticProgram, Model\nfrom llama_index.core.tools.function_tool import FunctionTool\nfrom llama_index.core.chat_engine.types import AgentChatResponse\nfrom llama_index.core.program.utils import (\n    FlexibleModel,\n    process_streaming_objects,\n    num_valid_fields,\n)\n\n_logger = logging.getLogger(__name__)\n\n\ndef get_function_tool(output_cls: Type[Model]) -> FunctionTool:\n    \"\"\"Get function tool.\"\"\"\n    schema = output_cls.model_json_schema()\n    schema_description = schema.get(\"description\", None)\n\n    # NOTE: this does not specify the schema in the function signature,\n    # so instead we'll directly provide it in the fn_schema in the ToolMetadata\n    def model_fn(**kwargs: Any) -> Model:\n        \"\"\"Model function.\"\"\"\n        return output_cls(**kwargs)\n\n    return FunctionTool.from_defaults(\n        fn=model_fn,\n        name=schema[\"title\"],\n        description=schema_description,\n        fn_schema=output_cls,\n    )\n\n\nclass FunctionCallingProgram(BasePydanticProgram[Model]):\n    \"\"\"Function Calling Program.\n\n    Uses function calling LLMs to obtain a structured output.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_cls: Type[Model],\n        llm: FunctionCallingLLM,\n        prompt: BasePromptTemplate,\n        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n        allow_parallel_tool_calls: bool = False,\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._output_cls = output_cls\n        self._llm = llm\n        self._prompt = prompt\n        self._verbose = verbose\n        self._allow_parallel_tool_calls = allow_parallel_tool_calls\n        self._tool_choice = tool_choice\n\n    @classmethod\n    def from_defaults(\n        cls,\n        output_cls: Type[Model],\n        prompt_template_str: Optional[str] = None,\n        prompt: Optional[BasePromptTemplate] = None,\n        llm: Optional[LLM] = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n        **kwargs: Any,\n    ) -> \"FunctionCallingProgram\":\n        llm = llm or Settings.llm  # type: ignore\n        assert llm is not None\n\n        if not llm.metadata.is_function_calling_model:\n            raise ValueError(\n                f\"Model name {llm.metadata.model_name} does not support \"\n                \"function calling API. \"\n            )\n\n        if prompt is None and prompt_template_str is None:\n            raise ValueError(\"Must provide either prompt or prompt_template_str.\")\n        if prompt is not None and prompt_template_str is not None:\n            raise ValueError(\"Must provide either prompt or prompt_template_str.\")\n        if prompt_template_str is not None:\n            prompt = PromptTemplate(prompt_template_str)\n\n        return cls(\n            output_cls=output_cls,\n            llm=llm,  # type: ignore\n            prompt=cast(PromptTemplate, prompt),\n            tool_choice=tool_choice,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            verbose=verbose,\n        )\n\n    @property\n    def output_cls(self) -> Type[Model]:\n        return self._output_cls\n\n    @property\n    def prompt(self) -> BasePromptTemplate:\n        return self._prompt\n\n    @prompt.setter\n    def prompt(self, prompt: BasePromptTemplate) -> None:\n        self._prompt = prompt\n\n    def __call__(\n        self,\n        *args: Any,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Union[Model, List[Model]]:\n        llm_kwargs = llm_kwargs or {}\n        tool = get_function_tool(self._output_cls)\n\n        messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n        messages = self._llm._extend_messages(messages)\n\n        agent_response = self._llm.predict_and_call(\n            [tool],\n            chat_history=messages,\n            verbose=self._verbose,\n            allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n            **llm_kwargs,\n        )\n        return self._parse_tool_outputs(\n            agent_response,\n            allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n        )\n\n    async def acall(\n        self,\n        *args: Any,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Union[Model, List[Model]]:\n        llm_kwargs = llm_kwargs or {}\n        tool = get_function_tool(self._output_cls)\n\n        agent_response = await self._llm.apredict_and_call(\n            [tool],\n            chat_history=self._prompt.format_messages(llm=self._llm, **kwargs),\n            verbose=self._verbose,\n            allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n            **llm_kwargs,\n        )\n        return self._parse_tool_outputs(\n            agent_response,\n            allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n        )\n\n    def _parse_tool_outputs(\n        self,\n        agent_response: AgentChatResponse,\n        allow_parallel_tool_calls: bool = False,\n    ) -> Union[Model, List[Model]]:\n        \"\"\"Parse tool outputs.\"\"\"\n        outputs = [cast(Model, s.raw_output) for s in agent_response.sources]\n        if allow_parallel_tool_calls:\n            return outputs\n        else:\n            if len(outputs) > 1:\n                _logger.warning(\n                    \"Multiple outputs found, returning first one. \"\n                    \"If you want to return all outputs, set output_multiple=True.\"\n                )\n\n            return outputs[0]\n\n    def _process_objects(\n        self,\n        chat_response: ChatResponse,\n        output_cls: Type[Model],\n        cur_objects: Optional[List[Model]] = None,\n    ) -> Union[Model, List[Model]]:\n        \"\"\"Process stream.\"\"\"\n        tool_calls = self._llm.get_tool_calls_from_response(\n            chat_response,\n            # error_on_no_tool_call=True\n            error_on_no_tool_call=False,\n        )\n        # TODO: change\n        if len(tool_calls) == 0:\n            # if no tool calls, return single blank output_class\n            return output_cls()\n\n        tool_fn_args = [call.tool_kwargs for call in tool_calls]\n        objects = [\n            output_cls.model_validate(tool_fn_arg) for tool_fn_arg in tool_fn_args\n        ]\n\n        if cur_objects is None or num_valid_fields(objects) > num_valid_fields(\n            cur_objects\n        ):\n            cur_objects = objects\n\n        # right now the objects are typed according to a flexible schema\n        # try to do a pass to convert the objects to the output_cls\n        new_cur_objects = []\n        for obj in cur_objects:\n            try:\n                new_obj = self._output_cls.model_validate(obj.model_dump())\n            except ValidationError as e:\n                _logger.warning(f\"Failed to parse object: {e}\")\n                new_obj = obj\n            new_cur_objects.append(new_obj)\n\n        if self._allow_parallel_tool_calls:\n            return new_cur_objects\n        else:\n            if len(new_cur_objects) > 1:\n                _logger.warning(\n                    \"Multiple outputs found, returning first one. \"\n                    \"If you want to return all outputs, set output_multiple=True.\"\n                )\n            return new_cur_objects[0]\n\n    def stream_call(\n        self, *args: Any, llm_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n    ) -> Generator[\n        Union[Model, List[Model], FlexibleModel, List[FlexibleModel]], None, None\n    ]:\n        \"\"\"Stream object.\n\n        Returns a generator returning partials of the same object\n        or a list of objects until it returns.\n        \"\"\"\n        # TODO: we can extend this to non-function calling LLMs as well, coming soon\n        if not isinstance(self._llm, FunctionCallingLLM):\n            raise ValueError(\"stream_call is only supported for LLMs.\")\n\n        llm_kwargs = llm_kwargs or {}\n        tool = get_function_tool(self._output_cls)\n\n        messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n        messages = self._llm._extend_messages(messages)\n\n        chat_response_gen = self._llm.stream_chat_with_tools(\n            [tool],\n            chat_history=messages,\n            verbose=self._verbose,\n            allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n            **llm_kwargs,\n        )\n\n        cur_objects = None\n        for partial_resp in chat_response_gen:\n            try:\n                objects = process_streaming_objects(\n                    partial_resp,\n                    self._output_cls,\n                    cur_objects=cur_objects,\n                    allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n                    flexible_mode=True,\n                    llm=self._llm,\n                )\n                cur_objects = objects if isinstance(objects, list) else [objects]\n                yield objects\n            except Exception as e:\n                _logger.warning(f\"Failed to parse streaming response: {e}\")\n                continue\n\n    async def astream_call(\n        self, *args: Any, llm_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n    ) -> AsyncGenerator[\n        Union[Model, List[Model], FlexibleModel, List[FlexibleModel]], None\n    ]:\n        \"\"\"Stream objects.\n\n        Returns a generator returning partials of the same object\n        or a list of objects until it returns.\n        \"\"\"\n        if not isinstance(self._llm, FunctionCallingLLM):\n            raise ValueError(\"stream_call is only supported for LLMs.\")\n\n        tool = get_function_tool(self._output_cls)\n\n        messages = self._prompt.format_messages(llm=self._llm, **kwargs)\n        messages = self._llm._extend_messages(messages)\n\n        chat_response_gen = await self._llm.astream_chat_with_tools(\n            [tool],\n            chat_history=messages,\n            verbose=self._verbose,\n            allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n            **(llm_kwargs or {}),\n        )\n\n        async def gen() -> AsyncGenerator[\n            Union[Model, List[Model], FlexibleModel, List[FlexibleModel]], None\n        ]:\n            cur_objects = None\n            async for partial_resp in chat_response_gen:\n                try:\n                    objects = process_streaming_objects(\n                        partial_resp,\n                        self._output_cls,\n                        cur_objects=cur_objects,\n                        allow_parallel_tool_calls=self._allow_parallel_tool_calls,\n                        flexible_mode=True,\n                        llm=self._llm,\n                    )\n                    cur_objects = objects if isinstance(objects, list) else [objects]\n                    yield objects\n                except Exception as e:\n                    _logger.warning(f\"Failed to parse streaming response: {e}\")\n                    continue\n\n        return gen()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/program/utils.py",
    "filename": "utils.py",
    "relpath": "program/utils.py",
    "start_line": 1,
    "end_line": 289,
    "length": 289,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "create_flexible_model",
      "create_list_model",
      "get_program_for_llm",
      "_repair_incomplete_json",
      "process_streaming_objects",
      "num_valid_fields"
    ],
    "chunk_class_names": [
      "FlexibleModel",
      "cur_objects",
      "return"
    ],
    "document_function_names": [
      "create_flexible_model",
      "create_list_model",
      "get_program_for_llm",
      "_repair_incomplete_json",
      "process_streaming_objects",
      "num_valid_fields"
    ],
    "document_class_names": [
      "FlexibleModel",
      "cur_objects",
      "return"
    ],
    "content": "\"\"\"Program utils.\"\"\"\nimport logging\nfrom typing import Any, List, Type, Sequence, Union, Optional, Dict\n\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    ValidationError,\n    create_model,\n)\nfrom llama_index.core.llms.llm import LLM, ToolSelection\nfrom llama_index.core.llms.function_calling import FunctionCallingLLM\nfrom llama_index.core.output_parsers.pydantic import PydanticOutputParser\nfrom llama_index.core.prompts.base import BasePromptTemplate\nfrom llama_index.core.types import BasePydanticProgram, Model, PydanticProgramMode\nfrom llama_index.core.base.llms.types import ChatResponse\n\n_logger = logging.getLogger(__name__)\n\n\nclass FlexibleModel(BaseModel):\n    model_config = ConfigDict(extra=\"allow\")\n\n\ndef create_flexible_model(model: Type[BaseModel]) -> Type[FlexibleModel]:\n    \"\"\"Create a flexible version of the model that allows any fields.\"\"\"\n    return create_model(\n        f\"Flexible{model.__name__}\",\n        __base__=FlexibleModel,\n        **{field: (Optional[Any], None) for field in model.model_fields},\n    )  # type: ignore\n\n\ndef create_list_model(base_cls: Type[BaseModel]) -> Type[BaseModel]:\n    \"\"\"Create a list version of an existing Pydantic object.\"\"\"\n    # NOTE: this is directly taken from\n    # https://github.com/jxnl/openai_function_call/blob/main/examples/streaming_multitask/streaming_multitask.py\n    # all credits go to the openai_function_call repo\n\n    name = f\"{base_cls.__name__}List\"\n    list_items = (\n        List[base_cls],  # type: ignore\n        Field(\n            default_factory=list,  # type: ignore\n            repr=False,\n            description=f\"List of {base_cls.__name__} items\",\n        ),\n    )\n\n    new_cls = create_model(name, items=list_items)\n    new_cls.__doc__ = f\"A list of {base_cls.__name__} objects. \"\n\n    return new_cls\n\n\ndef get_program_for_llm(\n    output_cls: Type[Model],\n    prompt: BasePromptTemplate,\n    llm: LLM,\n    pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,\n    **kwargs: Any,\n) -> BasePydanticProgram[Model]:\n    \"\"\"Get a program based on the compatible LLM.\"\"\"\n    if pydantic_program_mode == PydanticProgramMode.DEFAULT:\n        if llm.metadata.is_function_calling_model:\n            from llama_index.core.program.function_program import FunctionCallingProgram\n\n            return FunctionCallingProgram.from_defaults(\n                output_cls=output_cls,\n                llm=llm,\n                prompt=prompt,\n                **kwargs,\n            )\n        else:\n            from llama_index.core.program.llm_program import (\n                LLMTextCompletionProgram,\n            )\n\n            return LLMTextCompletionProgram.from_defaults(\n                output_parser=PydanticOutputParser(output_cls=output_cls),\n                llm=llm,\n                prompt=prompt,\n                **kwargs,\n            )\n    elif pydantic_program_mode == PydanticProgramMode.OPENAI:\n        from llama_index.program.openai import (\n            OpenAIPydanticProgram,\n        )  # pants: no-infer-dep\n\n        return OpenAIPydanticProgram.from_defaults(\n            output_cls=output_cls,\n            llm=llm,\n            prompt=prompt,  # type: ignore\n            **kwargs,\n        )\n    elif pydantic_program_mode == PydanticProgramMode.FUNCTION:\n        from llama_index.core.program.function_program import FunctionCallingProgram\n\n        return FunctionCallingProgram.from_defaults(\n            output_cls=output_cls,\n            llm=llm,\n            prompt=prompt,\n            **kwargs,\n        )\n\n    elif pydantic_program_mode == PydanticProgramMode.LLM:\n        from llama_index.core.program.llm_program import LLMTextCompletionProgram\n\n        return LLMTextCompletionProgram.from_defaults(\n            output_parser=PydanticOutputParser(output_cls=output_cls),\n            llm=llm,\n            prompt=prompt,\n            **kwargs,\n        )\n    elif pydantic_program_mode == PydanticProgramMode.LM_FORMAT_ENFORCER:\n        try:\n            from llama_index.program.lmformatenforcer import (\n                LMFormatEnforcerPydanticProgram,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"This mode requires the `llama-index-program-lmformatenforcer package. Please\"\n                \" install it by running `pip install llama-index-program-lmformatenforcer`.\"\n            )\n\n        return LMFormatEnforcerPydanticProgram.from_defaults(\n            output_cls=output_cls,\n            llm=llm,\n            prompt=prompt,\n            **kwargs,\n        )\n    else:\n        raise ValueError(f\"Unsupported pydantic program mode: {pydantic_program_mode}\")\n\n\ndef _repair_incomplete_json(json_str: str) -> str:\n    \"\"\"Attempt to repair incomplete JSON strings.\n\n    Args:\n        json_str (str): Potentially incomplete JSON string\n\n    Returns:\n        str: Repaired JSON string\n    \"\"\"\n    if not json_str.strip():\n        return \"{}\"\n\n    # Add missing quotes\n    quote_count = json_str.count('\"')\n    if quote_count % 2 == 1:\n        json_str += '\"'\n\n    # Add missing braces\n    brace_count = json_str.count(\"{\") - json_str.count(\"}\")\n    if brace_count > 0:\n        json_str += \"}\" * brace_count\n\n    return json_str\n\n\ndef process_streaming_objects(\n    chat_response: ChatResponse,\n    output_cls: Type[Model],\n    cur_objects: Optional[Sequence[Model]] = None,\n    allow_parallel_tool_calls: bool = False,\n    flexible_mode: bool = True,\n    llm: Optional[FunctionCallingLLM] = None,\n) -> Union[Model, List[Model], FlexibleModel, List[FlexibleModel]]:\n    \"\"\"Process streaming response into structured objects.\n\n    Args:\n        chat_response (ChatResponse): The chat response to process\n        output_cls (Type[BaseModel]): The target output class\n        cur_objects (Optional[List[BaseModel]]): Current accumulated objects\n        allow_parallel_tool_calls (bool): Whether to allow multiple tool calls\n        flexible_mode (bool): Whether to use flexible schema during parsing\n\n    Returns:\n        Union[BaseModel, List[BaseModel]]: Processed object(s)\n    \"\"\"\n    if flexible_mode:\n        # Create flexible version of model that allows partial responses\n        partial_output_cls = create_flexible_model(output_cls)\n    else:\n        partial_output_cls = output_cls  # type: ignore\n\n    # Get tool calls from response, if there are any\n    if not chat_response.message.additional_kwargs.get(\"tool_calls\"):\n        output_cls_args = [chat_response.message.content]\n    else:\n        tool_calls: List[ToolSelection] = []\n        if not llm:\n            raise ValueError(\"LLM is required to get tool calls\")\n\n        if isinstance(chat_response.message.additional_kwargs.get(\"tool_calls\"), list):\n            tool_calls = llm.get_tool_calls_from_response(\n                chat_response, error_on_no_tool_call=False\n            )\n\n        if len(tool_calls) == 0:\n            # If no tool calls, return single blank output class\n            return partial_output_cls()\n\n        # Extract arguments from tool calls\n        output_cls_args = [call.tool_kwargs for call in tool_calls]  # type: ignore\n\n    # Try to parse objects, handling potential incomplete JSON\n    objects = []\n    for output_cls_arg in output_cls_args:\n        try:\n            # First try direct validation\n            obj = partial_output_cls.model_validate(output_cls_arg)\n            objects.append(obj)\n        except (ValidationError, ValueError):\n            try:\n                # Try repairing the JSON if it's a string\n                if isinstance(output_cls_arg, str):\n                    repaired_json = _repair_incomplete_json(output_cls_arg)\n                    obj = partial_output_cls.model_validate_json(repaired_json)\n                    objects.append(obj)\n                else:\n                    raise\n            except (ValidationError, ValueError) as e2:\n                _logger.debug(f\"Validation error during streaming: {e2}\")\n                # If we have previous objects, keep using those\n                if cur_objects:\n                    objects = cur_objects  # type: ignore\n                else:\n                    # Return a blank object if we can't parse anything\n                    return partial_output_cls()\n\n    # Update if we have more valid fields than before\n    if cur_objects is None or num_valid_fields(objects) >= num_valid_fields(\n        cur_objects\n    ):\n        cur_objects = objects  # type: ignore\n\n    # Try to convert flexible objects to target schema\n    new_cur_objects = []\n    cur_objects = cur_objects or []\n    for o in cur_objects:\n        try:\n            new_obj = output_cls.model_validate(o.model_dump(exclude_unset=True))\n        except ValidationError:\n            new_obj = o\n        new_cur_objects.append(new_obj)\n\n    if allow_parallel_tool_calls:\n        return new_cur_objects\n    else:\n        if len(new_cur_objects) > 1:\n            _logger.warning(\n                \"Multiple outputs found, returning first one. \"\n                \"If you want to return all outputs, set allow_parallel_tool_calls=True.\"\n            )\n        return new_cur_objects[0]\n\n\ndef num_valid_fields(\n    obj: Union[BaseModel, Sequence[BaseModel], Dict[str, BaseModel]]\n) -> int:\n    \"\"\"\n    Recursively count the number of fields in a Pydantic object (including nested objects) that aren't None.\n\n    Args:\n        obj (Any): A Pydantic model instance or any other object.\n\n    Returns:\n        int: The number of fields that have non-None values.\n    \"\"\"\n    if isinstance(obj, BaseModel):\n        count = 0\n        for value in obj.__dict__.values():\n            if isinstance(value, (list, tuple)):\n                count += sum(num_valid_fields(item) for item in value)\n            elif isinstance(value, dict):\n                count += sum(num_valid_fields(item) for item in value.values())\n            elif isinstance(value, BaseModel):\n                count += num_valid_fields(value)\n            elif value is not None:\n                count += 1\n        return count\n    elif isinstance(obj, (list, tuple)):\n        return sum(num_valid_fields(item) for item in obj)\n    elif isinstance(obj, dict):\n        return sum(num_valid_fields(item) for item in obj.values())\n    else:\n        return 1 if obj is not None else 0"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/program/multi_modal_llm_program.py",
    "filename": "multi_modal_llm_program.py",
    "relpath": "program/multi_modal_llm_program.py",
    "start_line": 1,
    "end_line": 136,
    "length": 136,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_defaults",
      "output_cls",
      "prompt",
      "prompt",
      "__call__",
      "acall"
    ],
    "chunk_class_names": [
      "MultiModalLLMCompletionProgram"
    ],
    "document_function_names": [
      "__init__",
      "from_defaults",
      "output_cls",
      "prompt",
      "prompt",
      "__call__",
      "acall"
    ],
    "document_class_names": [
      "MultiModalLLMCompletionProgram"
    ],
    "content": "from typing import Any, Dict, List, Optional, Type, cast\n\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.multi_modal_llms import MultiModalLLM\nfrom llama_index.core.output_parsers.pydantic import PydanticOutputParser\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.schema import ImageNode\nfrom llama_index.core.types import BasePydanticProgram\nfrom llama_index.core.utils import print_text\n\n\nclass MultiModalLLMCompletionProgram(BasePydanticProgram[BaseModel]):\n    \"\"\"\n    Multi Modal LLM Completion Program.\n\n    Uses generic Multi Modal LLM completion + an output parser to generate a structured output.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        output_parser: PydanticOutputParser,\n        prompt: BasePromptTemplate,\n        multi_modal_llm: MultiModalLLM,\n        image_documents: List[ImageNode],\n        verbose: bool = False,\n    ) -> None:\n        self._output_parser = output_parser\n        self._multi_modal_llm = multi_modal_llm\n        self._prompt = prompt\n        self._image_documents = image_documents\n        self._verbose = verbose\n\n        self._prompt.output_parser = output_parser\n\n    @classmethod\n    def from_defaults(\n        cls,\n        output_parser: Optional[PydanticOutputParser] = None,\n        output_cls: Optional[Type[BaseModel]] = None,\n        prompt_template_str: Optional[str] = None,\n        prompt: Optional[PromptTemplate] = None,\n        multi_modal_llm: Optional[MultiModalLLM] = None,\n        image_documents: Optional[List[ImageNode]] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"MultiModalLLMCompletionProgram\":\n        if multi_modal_llm is None:\n            try:\n                from llama_index.multi_modal_llms.openai import (\n                    OpenAIMultiModal,\n                )  # pants: no-infer-dep\n\n                multi_modal_llm = OpenAIMultiModal(\n                    model=\"gpt-4-vision-preview\", temperature=0\n                )\n            except ImportError as e:\n                raise ImportError(\n                    \"`llama-index-multi-modal-llms-openai` package cannot be found. \"\n                    \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"\n                )\n        if prompt is None and prompt_template_str is None:\n            raise ValueError(\"Must provide either prompt or prompt_template_str.\")\n        if prompt is not None and prompt_template_str is not None:\n            raise ValueError(\"Must provide either prompt or prompt_template_str.\")\n        if prompt_template_str is not None:\n            prompt = PromptTemplate(prompt_template_str)\n\n        if output_parser is None:\n            if output_cls is None:\n                raise ValueError(\"Must provide either output_cls or output_parser.\")\n            output_parser = PydanticOutputParser(output_cls=output_cls)\n\n        return cls(\n            output_parser,\n            prompt=cast(PromptTemplate, prompt),\n            multi_modal_llm=multi_modal_llm,\n            image_documents=image_documents or [],\n            verbose=verbose,\n        )\n\n    @property\n    def output_cls(self) -> Type[BaseModel]:\n        return self._output_parser.output_cls\n\n    @property\n    def prompt(self) -> BasePromptTemplate:\n        return self._prompt\n\n    @prompt.setter\n    def prompt(self, prompt: BasePromptTemplate) -> None:\n        self._prompt = prompt\n\n    def __call__(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        image_documents: Optional[List[ImageNode]] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> BaseModel:\n        llm_kwargs = llm_kwargs or {}\n        formatted_prompt = self._prompt.format(llm=self._multi_modal_llm, **kwargs)  # type: ignore\n\n        response = self._multi_modal_llm.complete(\n            formatted_prompt,\n            image_documents=image_documents or self._image_documents,\n            **llm_kwargs,\n        )\n\n        raw_output = response.text\n        if self._verbose:\n            print_text(f\"> Raw output: {raw_output}\\n\", color=\"llama_blue\")\n\n        return self._output_parser.parse(raw_output)\n\n    async def acall(\n        self,\n        llm_kwargs: Optional[Dict[str, Any]] = None,\n        image_documents: Optional[List[ImageNode]] = None,\n        *args: Any,\n        **kwargs: Any,\n    ) -> BaseModel:\n        llm_kwargs = llm_kwargs or {}\n        formatted_prompt = self._prompt.format(llm=self._multi_modal_llm, **kwargs)  # type: ignore\n\n        response = await self._multi_modal_llm.acomplete(\n            formatted_prompt,\n            image_documents=image_documents or self._image_documents,\n            **llm_kwargs,\n        )\n\n        raw_output = response.text\n        if self._verbose:\n            print_text(f\"> Raw output: {raw_output}\\n\", color=\"llama_blue\")\n\n        return self._output_parser.parse(raw_output)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/base.py",
    "filename": "base.py",
    "relpath": "evaluation/base.py",
    "start_line": 1,
    "end_line": 133,
    "length": 133,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompt_modules",
      "evaluate",
      "aevaluate",
      "evaluate_response",
      "aevaluate_response"
    ],
    "chunk_class_names": [
      "EvaluationResult",
      "BaseEvaluator"
    ],
    "document_function_names": [
      "_get_prompt_modules",
      "evaluate",
      "aevaluate",
      "evaluate_response",
      "aevaluate_response"
    ],
    "document_class_names": [
      "EvaluationResult",
      "BaseEvaluator"
    ],
    "content": "\"\"\"Evaluator.\"\"\"\nfrom abc import abstractmethod\nfrom typing import Any, Optional, Sequence\n\nfrom llama_index.core.async_utils import asyncio_run\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.prompts.mixin import PromptMixin, PromptMixinType\n\n\nclass EvaluationResult(BaseModel):\n    \"\"\"Evaluation result.\n\n    Output of an BaseEvaluator.\n    \"\"\"\n\n    query: Optional[str] = Field(default=None, description=\"Query string\")\n    contexts: Optional[Sequence[str]] = Field(\n        default=None, description=\"Context strings\"\n    )\n    response: Optional[str] = Field(default=None, description=\"Response string\")\n    passing: Optional[bool] = Field(\n        default=None, description=\"Binary evaluation result (passing or not)\"\n    )\n    feedback: Optional[str] = Field(\n        default=None, description=\"Feedback or reasoning for the response\"\n    )\n    score: Optional[float] = Field(default=None, description=\"Score for the response\")\n    pairwise_source: Optional[str] = Field(\n        default=None,\n        description=(\n            \"Used only for pairwise and specifies whether it is from original order of\"\n            \" presented answers or flipped order\"\n        ),\n    )\n    invalid_result: bool = Field(\n        default=False, description=\"Whether the evaluation result is an invalid one.\"\n    )\n    invalid_reason: Optional[str] = Field(\n        default=None, description=\"Reason for invalid evaluation.\"\n    )\n\n\nclass BaseEvaluator(PromptMixin):\n    \"\"\"Base Evaluator class.\"\"\"\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def evaluate(\n        self,\n        query: Optional[str] = None,\n        response: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Run evaluation with query string, retrieved contexts,\n        and generated response string.\n\n        Subclasses can override this method to provide custom evaluation logic and\n        take in additional arguments.\n        \"\"\"\n        return asyncio_run(\n            self.aevaluate(\n                query=query,\n                response=response,\n                contexts=contexts,\n                **kwargs,\n            )\n        )\n\n    @abstractmethod\n    async def aevaluate(\n        self,\n        query: Optional[str] = None,\n        response: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Run evaluation with query string, retrieved contexts,\n        and generated response string.\n\n        Subclasses can override this method to provide custom evaluation logic and\n        take in additional arguments.\n        \"\"\"\n        raise NotImplementedError\n\n    def evaluate_response(\n        self,\n        query: Optional[str] = None,\n        response: Optional[Response] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Run evaluation with query string and generated Response object.\n\n        Subclasses can override this method to provide custom evaluation logic and\n        take in additional arguments.\n        \"\"\"\n        response_str: Optional[str] = None\n        contexts: Optional[Sequence[str]] = None\n        if response is not None:\n            response_str = response.response\n            contexts = [node.get_content() for node in response.source_nodes]\n\n        return self.evaluate(\n            query=query, response=response_str, contexts=contexts, **kwargs\n        )\n\n    async def aevaluate_response(\n        self,\n        query: Optional[str] = None,\n        response: Optional[Response] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Run evaluation with query string and generated Response object.\n\n        Subclasses can override this method to provide custom evaluation logic and\n        take in additional arguments.\n        \"\"\"\n        response_str: Optional[str] = None\n        contexts: Optional[Sequence[str]] = None\n        if response is not None:\n            response_str = response.response\n            contexts = [node.get_content() for node in response.source_nodes]\n\n        return await self.aevaluate(\n            query=query, response=response_str, contexts=contexts, **kwargs\n        )\n\n\n# legacy: backward compatibility\nEvaluation = EvaluationResult"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/answer_relevancy.py",
    "filename": "answer_relevancy.py",
    "relpath": "evaluation/answer_relevancy.py",
    "start_line": 1,
    "end_line": 144,
    "length": 144,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_default_parser_function",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "chunk_class_names": [
      "AnswerRelevancyEvaluator"
    ],
    "document_function_names": [
      "_default_parser_function",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "document_class_names": [
      "AnswerRelevancyEvaluator"
    ],
    "content": "\"\"\"Relevancy evaluation.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport re\nfrom typing import Any, Callable, Optional, Sequence, Tuple\n\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.settings import Settings\n\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the response is relevant to the query.\\n\"\n    \"The evaluation should be performed in a step-by-step manner by answering the following questions:\\n\"\n    \"1. Does the provided response match the subject matter of the user's query?\\n\"\n    \"2. Does the provided response attempt to address the focus or perspective \"\n    \"on the subject matter taken on by the user's query?\\n\"\n    \"Each question above is worth 1 point. Provide detailed feedback on response according to the criteria questions above  \"\n    \"After your feedback provide a final result by strictly following this format: '[RESULT] followed by the integer number representing the total score assigned to the response'\\n\\n\"\n    \"Query: \\n {query}\\n\"\n    \"Response: \\n {response}\\n\"\n    \"Feedback:\"\n)\n\n_DEFAULT_SCORE_THRESHOLD = 2.0\n\n\ndef _default_parser_function(output_str: str) -> Tuple[Optional[float], Optional[str]]:\n    # Pattern to match the feedback and response\n    # This pattern looks for any text ending with '[RESULT]' followed by a number\n    pattern = r\"([\\s\\S]+)(?:\\[RESULT\\]\\s*)(\\d)\"\n\n    # Using regex to find all matches\n    result = re.search(pattern, output_str)\n\n    # Check if any match is found\n    if result:\n        # Assuming there's only one match in the text, extract feedback and response\n        feedback, score = result.groups()\n        score = float(score) if score is not None else score\n        return score, feedback.strip()\n    else:\n        return None, None\n\n\nclass AnswerRelevancyEvaluator(BaseEvaluator):\n    \"\"\"Answer relevancy evaluator.\n\n    Evaluates the relevancy of response to a query.\n    This evaluator considers the query string and response string.\n\n    Args:\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        refine_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for refinement.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        raise_error: bool = False,\n        eval_template: str | BasePromptTemplate | None = None,\n        score_threshold: float = _DEFAULT_SCORE_THRESHOLD,\n        parser_function: Callable[\n            [str], Tuple[Optional[float], Optional[str]]\n        ] = _default_parser_function,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n        self._raise_error = raise_error\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self.parser_function = parser_function\n        self.score_threshold = score_threshold\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n            \"refine_template\": self._refine_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n        if \"refine_template\" in prompts:\n            self._refine_template = prompts[\"refine_template\"]\n\n    async def aevaluate(\n        self,\n        query: str | None = None,\n        response: str | None = None,\n        contexts: Sequence[str] | None = None,\n        sleep_time_in_seconds: int = 0,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the response is relevant to the query.\"\"\"\n        del kwargs  # Unused\n        del contexts  # Unused\n\n        if query is None or response is None:\n            raise ValueError(\"query and response must be provided\")\n\n        await asyncio.sleep(sleep_time_in_seconds)\n\n        eval_response = await self._llm.apredict(\n            prompt=self._eval_template,\n            query=query,\n            response=response,\n        )\n\n        score, reasoning = self.parser_function(eval_response)\n\n        invalid_result, invalid_reason = False, None\n        if score is None and reasoning is None:\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n            invalid_result = True\n            invalid_reason = \"Unable to parse the output string.\"\n\n        if score:\n            score /= self.score_threshold\n\n        return EvaluationResult(\n            query=query,\n            response=response,\n            score=score,\n            feedback=eval_response,\n            invalid_result=invalid_result,\n            invalid_reason=invalid_reason,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/eval_utils.py",
    "filename": "eval_utils.py",
    "relpath": "evaluation/eval_utils.py",
    "start_line": 1,
    "end_line": 239,
    "length": 239,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "aget_responses",
      "get_responses",
      "get_results_df",
      "_download_llama_dataset_from_hub",
      "upload_eval_dataset",
      "upload_eval_results",
      "default_parser"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "aget_responses",
      "get_responses",
      "get_results_df",
      "_download_llama_dataset_from_hub",
      "upload_eval_dataset",
      "upload_eval_results",
      "default_parser"
    ],
    "document_class_names": [],
    "content": "\"\"\"Get evaluation utils.\n\nNOTE: These are beta functions, might change.\n\n\"\"\"\n\nimport subprocess\nimport tempfile\nfrom collections import defaultdict\nfrom typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING\n\nimport numpy as np\n\nfrom llama_index.core.async_utils import asyncio_module, asyncio_run\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.constants import DEFAULT_PROJECT_NAME\nfrom llama_index.core.evaluation.base import EvaluationResult\nfrom llama_index.core.ingestion.api_utils import get_client\n\nif TYPE_CHECKING:\n    from llama_index.core.llama_dataset import LabelledRagDataset\n\n\nasync def aget_responses(\n    questions: List[str], query_engine: BaseQueryEngine, show_progress: bool = False\n) -> List[str]:\n    \"\"\"Get responses.\"\"\"\n    tasks = []\n    for question in questions:\n        tasks.append(query_engine.aquery(question))\n    asyncio_mod = asyncio_module(show_progress=show_progress)\n    return await asyncio_mod.gather(*tasks)\n\n\ndef get_responses(\n    *args: Any,\n    **kwargs: Any,\n) -> List[str]:\n    \"\"\"Get responses.\n\n    Sync version of aget_responses.\n\n    \"\"\"\n    return asyncio_run(aget_responses(*args, **kwargs))\n\n\ndef get_results_df(\n    eval_results_list: List[Dict[str, List[EvaluationResult]]],\n    names: List[str],\n    metric_keys: List[str],\n) -> Any:\n    \"\"\"Get results df.\n\n    Args:\n        eval_results_list (List[Dict[str, List[EvaluationResult]]]):\n            List of evaluation results.\n        names (List[str]):\n            Names of the evaluation results.\n        metric_keys (List[str]):\n            List of metric keys to get.\n\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"Pandas is required to get results dataframes. Please install it with `pip install pandas`.\"\n        )\n\n    metric_dict = defaultdict(list)\n    metric_dict[\"names\"] = names\n    for metric_key in metric_keys:\n        for eval_results in eval_results_list:\n            mean_score = np.array(\n                [r.score or 0.0 for r in eval_results[metric_key]]\n            ).mean()\n            metric_dict[metric_key].append(mean_score)\n    return pd.DataFrame(metric_dict)\n\n\ndef _download_llama_dataset_from_hub(llama_dataset_id: str) -> \"LabelledRagDataset\":\n    \"\"\"Uses a subprocess and llamaindex-cli to download a dataset from llama-hub.\"\"\"\n    from llama_index.core.llama_dataset import LabelledRagDataset\n\n    with tempfile.TemporaryDirectory() as tmp:\n        try:\n            subprocess.run(\n                [\n                    \"llamaindex-cli\",\n                    \"download-llamadataset\",\n                    f\"{llama_dataset_id}\",\n                    \"--download-dir\",\n                    f\"{tmp}\",\n                ]\n            )\n            return LabelledRagDataset.from_json(f\"{tmp}/rag_dataset.json\")  # type: ignore\n        except FileNotFoundError as err:\n            raise ValueError(\n                \"No dataset associated with the supplied `llama_dataset_id`\"\n            ) from err\n\n\ndef upload_eval_dataset(\n    dataset_name: str,\n    questions: Optional[List[str]] = None,\n    llama_dataset_id: Optional[str] = None,\n    project_name: str = DEFAULT_PROJECT_NAME,\n    base_url: Optional[str] = None,\n    api_key: Optional[str] = None,\n    overwrite: bool = False,\n    append: bool = False,\n) -> str:\n    \"\"\"Upload questions to platform dataset.\"\"\"\n    from llama_cloud import ProjectCreate\n    from llama_cloud.types.eval_question_create import EvalQuestionCreate\n\n    if questions is None and llama_dataset_id is None:\n        raise ValueError(\n            \"Must supply either a list of `questions`, or a `llama_dataset_id` to import from llama-hub.\"\n        )\n\n    client = get_client(base_url=base_url, api_key=api_key)\n\n    project = client.projects.upsert_project(request=ProjectCreate(name=project_name))\n    assert project.id is not None\n\n    existing_datasets = client.projects.get_datasets_for_project(project_id=project.id)\n\n    # check if dataset already exists\n    cur_dataset = None\n    for dataset in existing_datasets:\n        if dataset.name == dataset_name:\n            if overwrite:\n                assert dataset.id is not None\n                client.evals.delete_dataset(dataset_id=dataset.id)\n                break\n            elif not append:\n                raise ValueError(\n                    f\"Dataset {dataset_name} already exists in project {project_name}.\"\n                    \" Set overwrite=True to overwrite or append=True to append.\"\n                )\n            else:\n                cur_dataset = dataset\n                break\n\n    # either create new dataset or use existing one\n    if cur_dataset is None:\n        eval_dataset = client.projects.create_eval_dataset_for_project(\n            project_id=project.id, name=dataset_name\n        )\n    else:\n        eval_dataset = cur_dataset\n\n    assert eval_dataset.id is not None\n\n    # create questions\n    if questions:\n        questions = questions\n    else:\n        # download `LabelledRagDataset` from llama-hub\n        assert llama_dataset_id is not None\n        rag_dataset = _download_llama_dataset_from_hub(llama_dataset_id)\n        questions = [example.query for example in rag_dataset[:]]  # type: ignore\n\n    eval_questions = client.evals.create_questions(\n        dataset_id=eval_dataset.id,\n        request=[EvalQuestionCreate(content=q) for q in questions],\n    )\n\n    assert len(eval_questions) == len(questions)\n    print(f\"Uploaded {len(questions)} questions to dataset {dataset_name}\")\n    return eval_dataset.id\n\n\ndef upload_eval_results(\n    project_name: str, app_name: str, results: Dict[str, List[EvaluationResult]]\n) -> None:\n    \"\"\"Upload the evaluation results to LlamaCloud.\n\n    Args:\n        project_name (str): The name of the project.\n        app_name (str): The name of the app.\n        results (Dict[str, List[EvaluationResult]]):\n            The evaluation results, a mapping of metric name to a list of EvaluationResult objects.\n\n    Examples:\n        ```python\n        from llama_index.core.evaluation.eval_utils import upload_eval_results\n\n        result = evaluator.evaluate(...)\n        upload_eval_results(\n            project_name=\"my_project\",\n            app_name=\"my_app\",\n            results={\"evaluator_name\": [result]}\n        )\n        ```\n    \"\"\"\n    from llama_cloud import ProjectCreate\n\n    client = get_client()\n\n    project = client.projects.upsert_project(request=ProjectCreate(name=project_name))\n    assert project.id is not None\n\n    client.projects.create_local_eval_set_for_project(\n        project_id=project.id,\n        app_name=app_name,\n        results=results,\n    )\n\n    for key, val in results.items():\n        print(\n            f\"Uploaded {len(val)} results for metric {key} under project {project_name}/{app_name}.\"\n        )\n\n\ndef default_parser(eval_response: str) -> Tuple[Optional[float], Optional[str]]:\n    \"\"\"\n    Default parser function for evaluation response.\n\n    Args:\n        eval_response (str): The response string from the evaluation.\n\n    Returns:\n        Tuple[float, str]: A tuple containing the score as a float and the reasoning as a string.\n    \"\"\"\n    if not eval_response.strip():\n        # Return None or default values if the response is empty\n        return None, \"No response\"\n\n    score_str, reasoning_str = eval_response.split(\"\\n\", 1)\n\n    try:\n        score = float(score_str)\n    except ValueError:\n        score = None\n\n    reasoning = reasoning_str.lstrip(\"\\n\")\n    return score, reasoning"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/guideline.py",
    "filename": "guideline.py",
    "relpath": "evaluation/guideline.py",
    "start_line": 1,
    "end_line": 123,
    "length": 123,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "chunk_class_names": [
      "EvaluationData",
      "GuidelineEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "document_class_names": [
      "EvaluationData",
      "GuidelineEvaluator"
    ],
    "content": "\"\"\"Guideline evaluation.\"\"\"\n\nimport asyncio\nimport logging\nfrom typing import Any, Optional, Sequence, Union, cast\n\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.output_parsers import PydanticOutputParser\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_GUIDELINES = (\n    \"The response should fully answer the query.\\n\"\n    \"The response should avoid being vague or ambiguous.\\n\"\n    \"The response should be specific and use statistics or numbers when possible.\\n\"\n)\n\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Here is the original query:\\n\"\n    \"Query: {query}\\n\"\n    \"Critique the following response based on the guidelines below:\\n\"\n    \"Response: {response}\\n\"\n    \"Guidelines: {guidelines}\\n\"\n    \"Now please provide constructive criticism.\\n\"\n)\n\n\nclass EvaluationData(BaseModel):\n    passing: bool = Field(description=\"Whether the response passes the guidelines.\")\n    feedback: str = Field(\n        description=\"The feedback for the response based on the guidelines.\"\n    )\n\n\nclass GuidelineEvaluator(BaseEvaluator):\n    \"\"\"Guideline evaluator.\n\n    Evaluates whether a query and response pair passes the given guidelines.\n\n    This evaluator only considers the query string and the response string.\n\n    Args:\n        guidelines(Optional[str]): User-added guidelines to use for evaluation.\n            Defaults to None, which uses the default guidelines.\n        eval_template(Optional[Union[str, BasePromptTemplate]] ):\n            The template to use for evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        guidelines: Optional[str] = None,\n        eval_template: Optional[Union[str, BasePromptTemplate]] = None,\n        output_parser: Optional[PydanticOutputParser] = None,\n    ) -> None:\n        self._llm = llm or Settings.llm\n        self._guidelines = guidelines or DEFAULT_GUIDELINES\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self._output_parser = output_parser or PydanticOutputParser(\n            output_cls=EvaluationData\n        )\n        self._eval_template.output_parser = self._output_parser\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n\n    async def aevaluate(\n        self,\n        query: Optional[str] = None,\n        response: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,\n        sleep_time_in_seconds: int = 0,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the query and response pair passes the guidelines.\"\"\"\n        del contexts  # Unused\n        del kwargs  # Unused\n        if query is None or response is None:\n            raise ValueError(\"query and response must be provided\")\n\n        logger.debug(\"prompt: %s\", self._eval_template)\n        logger.debug(\"query: %s\", query)\n        logger.debug(\"response: %s\", response)\n        logger.debug(\"guidelines: %s\", self._guidelines)\n\n        await asyncio.sleep(sleep_time_in_seconds)\n\n        eval_response = await self._llm.apredict(\n            self._eval_template,\n            query=query,\n            response=response,\n            guidelines=self._guidelines,\n        )\n        eval_data = self._output_parser.parse(eval_response)\n        eval_data = cast(EvaluationData, eval_data)\n\n        return EvaluationResult(\n            query=query,\n            response=response,\n            passing=eval_data.passing,\n            score=1.0 if eval_data.passing else 0.0,\n            feedback=eval_data.feedback,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/correctness.py",
    "filename": "correctness.py",
    "relpath": "evaluation/correctness.py",
    "start_line": 1,
    "end_line": 151,
    "length": 151,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "chunk_class_names": [
      "CorrectnessEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "document_class_names": [
      "CorrectnessEvaluator"
    ],
    "content": "\"\"\"Correctness evaluation.\"\"\"\n\nimport asyncio\nfrom typing import Any, Callable, Optional, Sequence, Tuple, Union\n\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.evaluation.eval_utils import default_parser\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import (\n    BasePromptTemplate,\n    ChatMessage,\n    ChatPromptTemplate,\n    MessageRole,\n    PromptTemplate,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.settings import Settings\n\nDEFAULT_SYSTEM_TEMPLATE = \"\"\"\nYou are an expert evaluation system for a question answering chatbot.\n\nYou are given the following information:\n- a user query, and\n- a generated answer\n\nYou may also be given a reference answer to use for reference in your evaluation.\n\nYour job is to judge the relevance and correctness of the generated answer.\nOutput a single score that represents a holistic evaluation.\nYou must return your response in a line with only the score.\nDo not return answers in any other format.\nOn a separate line provide your reasoning for the score as well.\n\nFollow these guidelines for scoring:\n- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n- If the generated answer is not relevant to the user query, \\\nyou should give a score of 1.\n- If the generated answer is relevant but contains mistakes, \\\nyou should give a score between 2 and 3.\n- If the generated answer is relevant and fully correct, \\\nyou should give a score between 4 and 5.\n\nExample Response:\n4.0\nThe generated answer has the exact same metrics as the reference answer, \\\n    but it is not as concise.\n\n\"\"\"\n\nDEFAULT_USER_TEMPLATE = \"\"\"\n## User Query\n{query}\n\n## Reference Answer\n{reference_answer}\n\n## Generated Answer\n{generated_answer}\n\"\"\"\n\nDEFAULT_EVAL_TEMPLATE = ChatPromptTemplate(\n    message_templates=[\n        ChatMessage(role=MessageRole.SYSTEM, content=DEFAULT_SYSTEM_TEMPLATE),\n        ChatMessage(role=MessageRole.USER, content=DEFAULT_USER_TEMPLATE),\n    ]\n)\n\n\nclass CorrectnessEvaluator(BaseEvaluator):\n    \"\"\"Correctness evaluator.\n\n    Evaluates the correctness of a question answering system.\n    This evaluator depends on `reference` answer to be provided, in addition to the\n    query string and response string.\n\n    It outputs a score between 1 and 5, where 1 is the worst and 5 is the best,\n    along with a reasoning for the score.\n    Passing is defined as a score greater than or equal to the given threshold.\n\n    Args:\n        eval_template (Optional[Union[BasePromptTemplate, str]]):\n            Template for the evaluation prompt.\n        score_threshold (float): Numerical threshold for passing the evaluation,\n            defaults to 4.0.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        eval_template: Optional[Union[BasePromptTemplate, str]] = None,\n        score_threshold: float = 4.0,\n        parser_function: Callable[\n            [str], Tuple[Optional[float], Optional[str]]\n        ] = default_parser,\n    ) -> None:\n        self._llm = llm or Settings.llm\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self._score_threshold = score_threshold\n        self.parser_function = parser_function\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n\n    async def aevaluate(\n        self,\n        query: Optional[str] = None,\n        response: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,\n        reference: Optional[str] = None,\n        sleep_time_in_seconds: int = 0,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        del kwargs  # Unused\n        del contexts  # Unused\n\n        await asyncio.sleep(sleep_time_in_seconds)\n\n        if query is None or response is None:\n            raise ValueError(\"query, and response must be provided\")\n\n        eval_response = await self._llm.apredict(\n            prompt=self._eval_template,\n            query=query,\n            generated_answer=response,\n            reference_answer=reference or \"(NO REFERENCE ANSWER SUPPLIED)\",\n        )\n\n        # Use the parser function\n        score, reasoning = self.parser_function(eval_response)\n\n        return EvaluationResult(\n            query=query,\n            response=response,\n            passing=score >= self._score_threshold if score is not None else None,\n            score=score,\n            feedback=reasoning,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/faithfulness.py",
    "filename": "faithfulness.py",
    "relpath": "evaluation/faithfulness.py",
    "start_line": 1,
    "end_line": 204,
    "length": 204,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "chunk_class_names": [
      "FaithfulnessEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "document_class_names": [
      "FaithfulnessEvaluator"
    ],
    "content": "\"\"\"Faithfulness evaluation.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom typing import Any, Optional, Sequence, Union\n\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.indices import SummaryIndex\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import Document\nfrom llama_index.core.settings import Settings\n\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Please tell if a given piece of information \"\n    \"is supported by the context.\\n\"\n    \"You need to answer with either YES or NO.\\n\"\n    \"Answer YES if any of the context supports the information, even \"\n    \"if most of the context is unrelated. \"\n    \"Some examples are provided below. \\n\\n\"\n    \"Information: Apple pie is generally double-crusted.\\n\"\n    \"Context: An apple pie is a fruit pie in which the principal filling \"\n    \"ingredient is apples. \\n\"\n    \"Apple pie is often served with whipped cream, ice cream \"\n    \"('apple pie \u00e0 la mode'), custard or cheddar cheese.\\n\"\n    \"It is generally double-crusted, with pastry both above \"\n    \"and below the filling; the upper crust may be solid or \"\n    \"latticed (woven of crosswise strips).\\n\"\n    \"Answer: YES\\n\"\n    \"Information: Apple pies tastes bad.\\n\"\n    \"Context: An apple pie is a fruit pie in which the principal filling \"\n    \"ingredient is apples. \\n\"\n    \"Apple pie is often served with whipped cream, ice cream \"\n    \"('apple pie \u00e0 la mode'), custard or cheddar cheese.\\n\"\n    \"It is generally double-crusted, with pastry both above \"\n    \"and below the filling; the upper crust may be solid or \"\n    \"latticed (woven of crosswise strips).\\n\"\n    \"Answer: NO\\n\"\n    \"Information: {query_str}\\n\"\n    \"Context: {context_str}\\n\"\n    \"Answer: \"\n)\n\nDEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following information is present \"\n    \"in the context information: {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"\n    \"If the information is present in the new context, answer YES. \"\n    \"Otherwise answer NO.\\n\"\n)\n\nLLAMA3_8B_EVAL_TEMPLATE = PromptTemplate(\n    \"\"\"Please tell if a given piece of information is supported by the context.\nYou need to answer with either YES or NO.\nAnswer YES if **any part** of the context supports the information, even if most of the context is unrelated.\nAnswer NO if the context does not support the information at all.\nBe sure to read all provided context segments carefully before making your decision.\n\nSome examples are provided below:\n\nExample 1:\nInformation: The Eiffel Tower is located in Paris.\nContext: The Eiffel Tower, a symbol of French culture, stands prominently in the city of Paris.\nAnswer: YES\n\nExample 2:\nInformation: Bananas are a type of berry.\nContext: Bananas are a popular fruit enjoyed worldwide and are rich in potassium.\nAnswer: NO\n\nExample 3:\nInformation: Cats are reptiles.\nContext: Cats are domesticated felines known for their agility and companionship.\nAnswer: NO\n\nExample 4:\nInformation: Amazon started as an online bookstore.\nContext: Amazon initially launched as an online store for books but has since expanded into a global e-commerce giant\noffering various products and services.\nAnswer: YES\n\nInformation: {query}\nContext: {reference_contexts}\nAnswer:\"\"\"\n)\n\nTEMPLATES_CATALOG = {\"llama3:8b\": LLAMA3_8B_EVAL_TEMPLATE}\n\n\nclass FaithfulnessEvaluator(BaseEvaluator):\n    \"\"\"\n    Faithfulness evaluator.\n\n    Evaluates whether a response is faithful to the contexts\n    (i.e. whether the response is supported by the contexts or hallucinated.)\n\n    This evaluator only considers the response string and the list of context strings.\n\n    Args:\n        raise_error(bool): Whether to raise an error when the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        refine_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for refining the evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        raise_error: bool = False,\n        eval_template: Optional[Union[str, BasePromptTemplate]] = None,\n        refine_template: Optional[Union[str, BasePromptTemplate]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n        self._raise_error = raise_error\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        if isinstance(eval_template, BasePromptTemplate):\n            self._eval_template = eval_template\n        else:\n            model_name = self._llm.metadata.model_name\n            self._eval_template = TEMPLATES_CATALOG.get(\n                model_name, DEFAULT_EVAL_TEMPLATE\n            )\n\n        self._refine_template: BasePromptTemplate\n        if isinstance(refine_template, str):\n            self._refine_template = PromptTemplate(refine_template)\n        else:\n            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n            \"refine_template\": self._refine_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n        if \"refine_template\" in prompts:\n            self._refine_template = prompts[\"refine_template\"]\n\n    async def aevaluate(\n        self,\n        query: str | None = None,\n        response: str | None = None,\n        contexts: Sequence[str] | None = None,\n        sleep_time_in_seconds: int = 0,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the response is faithful to the contexts.\"\"\"\n        del kwargs  # Unused\n\n        await asyncio.sleep(sleep_time_in_seconds)\n\n        if contexts is None or response is None:\n            raise ValueError(\"contexts and response must be provided\")\n\n        docs = [Document(text=context) for context in contexts]\n        index = SummaryIndex.from_documents(docs)\n\n        query_engine = index.as_query_engine(\n            llm=self._llm,\n            text_qa_template=self._eval_template,\n            refine_template=self._refine_template,\n        )\n        response_obj = await query_engine.aquery(response)\n\n        raw_response_txt = str(response_obj)\n\n        if \"yes\" in raw_response_txt.lower():\n            passing = True\n        else:\n            passing = False\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n\n        return EvaluationResult(\n            query=query,\n            response=response,\n            contexts=contexts,\n            passing=passing,\n            score=1.0 if passing else 0.0,\n            feedback=raw_response_txt,\n        )\n\n\n# legacy: backward compatibility\nResponseEvaluator = FaithfulnessEvaluator"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/__init__.py",
    "filename": "__init__.py",
    "relpath": "evaluation/__init__.py",
    "start_line": 1,
    "end_line": 86,
    "length": 86,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Evaluation modules.\"\"\"\n\nfrom llama_index.core.evaluation.answer_relevancy import AnswerRelevancyEvaluator\nfrom llama_index.core.evaluation.base import (\n    BaseEvaluator,\n    EvaluationResult,\n)\nfrom llama_index.core.evaluation.batch_runner import BatchEvalRunner\nfrom llama_index.core.evaluation.context_relevancy import ContextRelevancyEvaluator\nfrom llama_index.core.evaluation.correctness import CorrectnessEvaluator\nfrom llama_index.core.evaluation.dataset_generation import (\n    DatasetGenerator,\n    QueryResponseDataset,\n)\nfrom llama_index.core.evaluation.faithfulness import (\n    FaithfulnessEvaluator,\n    ResponseEvaluator,\n)\nfrom llama_index.core.evaluation.guideline import GuidelineEvaluator\nfrom llama_index.core.evaluation.notebook_utils import get_retrieval_results_df\nfrom llama_index.core.evaluation.pairwise import PairwiseComparisonEvaluator\nfrom llama_index.core.evaluation.relevancy import (\n    QueryResponseEvaluator,\n    RelevancyEvaluator,\n)\nfrom llama_index.core.evaluation.retrieval.base import (\n    BaseRetrievalEvaluator,\n    RetrievalEvalResult,\n)\nfrom llama_index.core.evaluation.retrieval.evaluator import (\n    MultiModalRetrieverEvaluator,\n    RetrieverEvaluator,\n)\nfrom llama_index.core.evaluation.retrieval.metrics import (\n    MRR,\n    HitRate,\n    RetrievalMetricResult,\n    resolve_metrics,\n)\nfrom llama_index.core.evaluation.semantic_similarity import (\n    SemanticSimilarityEvaluator,\n)\n\n# import dataset generation too\nfrom llama_index.core.llama_dataset.legacy.embedding import (\n    EmbeddingQAFinetuneDataset,\n    generate_qa_embedding_pairs,\n)\n\n# aliases for generate_qa_embedding_pairs\ngenerate_question_context_pairs = generate_qa_embedding_pairs\nLabelledQADataset = EmbeddingQAFinetuneDataset\n\n__all__ = [\n    \"BaseEvaluator\",\n    \"AnswerRelevancyEvaluator\",\n    \"ContextRelevancyEvaluator\",\n    \"EvaluationResult\",\n    \"FaithfulnessEvaluator\",\n    \"RelevancyEvaluator\",\n    \"RelevanceEvaluator\",\n    \"DatasetGenerator\",\n    \"QueryResponseDataset\",\n    \"GuidelineEvaluator\",\n    \"CorrectnessEvaluator\",\n    \"SemanticSimilarityEvaluator\",\n    \"PairwiseComparisonEvaluator\",\n    \"BatchEvalRunner\",\n    # legacy: kept for backward compatibility\n    \"QueryResponseEvaluator\",\n    \"ResponseEvaluator\",\n    # retrieval\n    \"generate_qa_embedding_pairs\",\n    \"generate_question_context_pairs\",\n    \"EmbeddingQAFinetuneDataset\",\n    \"BaseRetrievalEvaluator\",\n    \"RetrievalEvalResult\",\n    \"RetrieverEvaluator\",\n    \"MultiModalRetrieverEvaluator\",\n    \"RetrievalMetricResult\",\n    \"resolve_metrics\",\n    \"HitRate\",\n    \"MRR\",\n    \"get_retrieval_results_df\",\n    \"LabelledQADataset\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/dataset_generation.py",
    "filename": "dataset_generation.py",
    "relpath": "evaluation/dataset_generation.py",
    "start_line": 1,
    "end_line": 339,
    "length": 339,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_qr_pairs",
      "qr_pairs",
      "questions",
      "save_json",
      "from_json",
      "__init__",
      "from_documents",
      "_agenerate_dataset",
      "agenerate_questions_from_nodes",
      "agenerate_dataset_from_nodes",
      "generate_questions_from_nodes",
      "generate_dataset_from_nodes",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts"
    ],
    "chunk_class_names": [
      "QueryResponseDataset",
      "DatasetGenerator"
    ],
    "document_function_names": [
      "from_qr_pairs",
      "qr_pairs",
      "questions",
      "save_json",
      "from_json",
      "__init__",
      "from_documents",
      "_agenerate_dataset",
      "agenerate_questions_from_nodes",
      "agenerate_dataset_from_nodes",
      "generate_questions_from_nodes",
      "generate_dataset_from_nodes",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts"
    ],
    "document_class_names": [
      "QueryResponseDataset",
      "DatasetGenerator"
    ],
    "content": "\"\"\"Dataset generation from documents.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport re\nimport uuid\nfrom typing import Coroutine, Dict, List, Optional, Tuple\n\nfrom deprecated import deprecated\nfrom llama_index.core.async_utils import asyncio_run\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.indices.list import SummaryIndex\nfrom llama_index.core.ingestion import run_transformations\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.postprocessor.node import KeywordNodePostprocessor\nfrom llama_index.core.prompts.base import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT\nfrom llama_index.core.prompts.mixin import (\n    PromptDictType,\n    PromptMixin,\n    PromptMixinType,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    Document,\n    MetadataMode,\n    NodeWithScore,\n    TransformComponent,\n)\nfrom llama_index.core.settings import (\n    Settings,\n)\n\nDEFAULT_QUESTION_GENERATION_PROMPT = \"\"\"\\\nContext information is below.\n---------------------\n{context_str}\n---------------------\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n{query_str}\n\"\"\"\n\n\n@deprecated(\n    \"Deprecated in favor of `LabelledRagDataset` which should be used instead.\",\n    action=\"always\",\n)\nclass QueryResponseDataset(BaseModel):\n    \"\"\"Query Response Dataset.\n\n    The response can be empty if the dataset is generated from documents.\n\n    Args:\n        queries (Dict[str, str]): Query id -> query.\n        responses (Dict[str, str]): Query id -> response.\n\n    \"\"\"\n\n    queries: Dict[str, str] = Field(\n        default_factory=dict, description=\"Query id -> query\"\n    )\n    responses: Dict[str, str] = Field(\n        default_factory=dict, description=\"Query id -> response\"\n    )\n\n    @classmethod\n    def from_qr_pairs(\n        cls,\n        qr_pairs: List[Tuple[str, str]],\n    ) -> QueryResponseDataset:\n        \"\"\"Create from qr pairs.\"\"\"\n        # define ids as simple integers\n        queries = {str(idx): query for idx, (query, _) in enumerate(qr_pairs)}\n        responses = {str(idx): response for idx, (_, response) in enumerate(qr_pairs)}\n        return cls(queries=queries, responses=responses)\n\n    @property\n    def qr_pairs(self) -> List[Tuple[str, str]]:\n        \"\"\"Get pairs.\"\"\"\n        # if query_id not in response, throw error\n        for query_id in self.queries:\n            if query_id not in self.responses:\n                raise ValueError(f\"Query id {query_id} not in responses\")\n\n        return [\n            (self.queries[query_id], self.responses[query_id])\n            for query_id in self.queries\n        ]\n\n    @property\n    def questions(self) -> List[str]:\n        \"\"\"Get questions.\"\"\"\n        return list(self.queries.values())\n\n    def save_json(self, path: str) -> None:\n        \"\"\"Save json.\"\"\"\n        with open(path, \"w\") as f:\n            json.dump(self.model_dump(), f, indent=4)\n\n    @classmethod\n    def from_json(cls, path: str) -> QueryResponseDataset:\n        \"\"\"Load json.\"\"\"\n        with open(path) as f:\n            data = json.load(f)\n        return cls(**data)\n\n\n@deprecated(\n    \"Deprecated in favor of `RagDatasetGenerator` which should be used instead.\",\n    action=\"always\",\n)\nclass DatasetGenerator(PromptMixin):\n    \"\"\"Generate dataset (question/ question-answer pairs) \\\n    based on the given documents.\n\n    NOTE: this is a beta feature, subject to change!\n\n    Args:\n        nodes (List[Node]): List of nodes. (Optional)\n        llm (LLM): Language model.\n        callback_manager (CallbackManager): Callback manager.\n        num_questions_per_chunk: number of question to be \\\n        generated per chunk. Each document is chunked of size 512 words.\n        text_question_template: Question generation template.\n        question_gen_query: Question generation query.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: List[BaseNode],\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        num_questions_per_chunk: int = 10,\n        text_question_template: BasePromptTemplate | None = None,\n        text_qa_template: BasePromptTemplate | None = None,\n        question_gen_query: str | None = None,\n        metadata_mode: MetadataMode = MetadataMode.NONE,\n        show_progress: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self.llm = llm or Settings.llm\n        self.callback_manager = callback_manager or Settings.callback_manager\n        self.text_question_template = text_question_template or PromptTemplate(\n            DEFAULT_QUESTION_GENERATION_PROMPT\n        )\n        self.text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT\n        self.question_gen_query = (\n            question_gen_query\n            or f\"You are a Teacher/Professor. Your task is to setup \\\n                        {num_questions_per_chunk} questions for an upcoming \\\n                        quiz/examination. The questions should be diverse in nature \\\n                            across the document. Restrict the questions to the \\\n                                context information provided.\"\n        )\n        self.nodes = nodes\n        self._metadata_mode = metadata_mode\n        self._show_progress = show_progress\n\n    @classmethod\n    def from_documents(\n        cls,\n        documents: List[Document],\n        llm: Optional[LLM] = None,\n        transformations: Optional[List[TransformComponent]] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        num_questions_per_chunk: int = 10,\n        text_question_template: BasePromptTemplate | None = None,\n        text_qa_template: BasePromptTemplate | None = None,\n        question_gen_query: str | None = None,\n        required_keywords: List[str] | None = None,\n        exclude_keywords: List[str] | None = None,\n        show_progress: bool = False,\n    ) -> DatasetGenerator:\n        \"\"\"Generate dataset from documents.\"\"\"\n        llm = llm or Settings.llm\n        transformations = transformations or Settings.transformations\n        callback_manager = callback_manager or Settings.callback_manager\n\n        nodes = run_transformations(\n            documents, transformations, show_progress=show_progress\n        )\n\n        # use node postprocessor to filter nodes\n        required_keywords = required_keywords or []\n        exclude_keywords = exclude_keywords or []\n        node_postprocessor = KeywordNodePostprocessor(\n            callback_manager=callback_manager,\n            required_keywords=required_keywords,\n            exclude_keywords=exclude_keywords,\n        )\n        node_with_scores = [NodeWithScore(node=node) for node in nodes]\n        node_with_scores = node_postprocessor.postprocess_nodes(node_with_scores)\n        nodes = [node_with_score.node for node_with_score in node_with_scores]\n\n        return cls(\n            nodes=nodes,\n            llm=llm,\n            callback_manager=callback_manager,\n            num_questions_per_chunk=num_questions_per_chunk,\n            text_question_template=text_question_template,\n            text_qa_template=text_qa_template,\n            question_gen_query=question_gen_query,\n            show_progress=show_progress,\n        )\n\n    async def _agenerate_dataset(\n        self,\n        nodes: List[BaseNode],\n        num: int | None = None,\n        generate_response: bool = False,\n    ) -> QueryResponseDataset:\n        \"\"\"Node question generator.\"\"\"\n        query_tasks: List[Coroutine] = []\n        queries: Dict[str, str] = {}\n        responses_dict: Dict[str, str] = {}\n\n        if self._show_progress:\n            from tqdm.asyncio import tqdm_asyncio\n\n            async_module = tqdm_asyncio\n        else:\n            async_module = asyncio\n\n        summary_indices: List[SummaryIndex] = []\n        for node in nodes:\n            if num is not None and len(query_tasks) >= num:\n                break\n            index = SummaryIndex.from_documents(\n                [\n                    Document(\n                        text=node.get_content(metadata_mode=self._metadata_mode),\n                        metadata=node.metadata,  # type: ignore\n                    )\n                ],\n                callback_manager=self.callback_manager,\n            )\n\n            query_engine = index.as_query_engine(\n                llm=self.llm,\n                text_qa_template=self.text_question_template,\n                use_async=True,\n            )\n            task = query_engine.aquery(\n                self.question_gen_query,\n            )\n            query_tasks.append(task)\n            summary_indices.append(index)\n\n        responses = await async_module.gather(*query_tasks)\n        for idx, response in enumerate(responses):\n            result = str(response).strip().split(\"\\n\")\n            cleaned_questions = [\n                re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n            ]\n            cleaned_questions = [\n                question for question in cleaned_questions if len(question) > 0\n            ]\n            cur_queries = {\n                str(uuid.uuid4()): question for question in cleaned_questions\n            }\n            queries.update(cur_queries)\n\n            if generate_response:\n                index = summary_indices[idx]\n                qr_tasks = []\n                cur_query_items = list(cur_queries.items())\n                cur_query_keys = [query_id for query_id, _ in cur_query_items]\n                for query_id, query in cur_query_items:\n                    qa_query_engine = index.as_query_engine(\n                        llm=self.llm,\n                        text_qa_template=self.text_qa_template,\n                    )\n                    qr_task = qa_query_engine.aquery(query)\n                    qr_tasks.append(qr_task)\n                qr_responses = await async_module.gather(*qr_tasks)\n                for query_id, qa_response in zip(cur_query_keys, qr_responses):\n                    responses_dict[query_id] = str(qa_response)\n            else:\n                pass\n\n        query_ids = list(queries.keys())\n        if num is not None:\n            query_ids = query_ids[:num]\n            # truncate queries, responses to the subset of query ids\n            queries = {query_id: queries[query_id] for query_id in query_ids}\n            if generate_response:\n                responses_dict = {\n                    query_id: responses_dict[query_id] for query_id in query_ids\n                }\n\n        return QueryResponseDataset(queries=queries, responses=responses_dict)\n\n    async def agenerate_questions_from_nodes(self, num: int | None = None) -> List[str]:\n        \"\"\"Generates questions for each document.\"\"\"\n        dataset = await self._agenerate_dataset(\n            self.nodes, num=num, generate_response=False\n        )\n        return dataset.questions\n\n    async def agenerate_dataset_from_nodes(\n        self, num: int | None = None\n    ) -> QueryResponseDataset:\n        \"\"\"Generates questions for each document.\"\"\"\n        return await self._agenerate_dataset(\n            self.nodes, num=num, generate_response=True\n        )\n\n    def generate_questions_from_nodes(self, num: int | None = None) -> List[str]:\n        \"\"\"Generates questions for each document.\"\"\"\n        return asyncio_run(self.agenerate_questions_from_nodes(num=num))\n\n    def generate_dataset_from_nodes(\n        self, num: int | None = None\n    ) -> QueryResponseDataset:\n        \"\"\"Generates questions for each document.\"\"\"\n        return asyncio_run(self.agenerate_dataset_from_nodes(num=num))\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"text_question_template\": self.text_question_template,\n            \"text_qa_template\": self.text_qa_template,\n        }\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"text_question_template\" in prompts:\n            self.text_question_template = prompts[\"text_question_template\"]\n        if \"text_qa_template\" in prompts:\n            self.text_qa_template = prompts[\"text_qa_template\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/context_relevancy.py",
    "filename": "context_relevancy.py",
    "relpath": "evaluation/context_relevancy.py",
    "start_line": 1,
    "end_line": 175,
    "length": 175,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_default_parser_function",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "chunk_class_names": [
      "ContextRelevancyEvaluator"
    ],
    "document_function_names": [
      "_default_parser_function",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "document_class_names": [
      "ContextRelevancyEvaluator"
    ],
    "content": "\"\"\"Relevancy evaluation.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport re\nfrom typing import Any, Callable, Optional, Sequence, Tuple\n\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.indices import SummaryIndex\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import Document\n\n\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the retrieved context from the document sources are relevant to the query.\\n\"\n    \"The evaluation should be performed in a step-by-step manner by answering the following questions:\\n\"\n    \"1. Does the retrieved context match the subject matter of the user's query?\\n\"\n    \"2. Can the retrieved context be used exclusively to provide a full answer to the user's query?\\n\"\n    \"Each question above is worth 2 points, where partial marks are allowed and encouraged. Provide detailed feedback on the response \"\n    \"according to the criteria questions previously mentioned. \"\n    \"After your feedback provide a final result by strictly following this format: \"\n    \"'[RESULT] followed by the float number representing the total score assigned to the response'\\n\\n\"\n    \"Query: \\n {query_str}\\n\"\n    \"Context: \\n {context_str}\\n\"\n    \"Feedback:\"\n)\n\n_DEFAULT_SCORE_THRESHOLD = 4.0\n\nDEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following query and response is\"\n    \"in line with the context information: \\n {query_str}\\n\"\n    \"We have provided an existing evaluation score: \\n {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing evaluation \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    f\"If the existing evaluation was already {_DEFAULT_SCORE_THRESHOLD}, still answer {_DEFAULT_SCORE_THRESHOLD}. \"\n    f\"If the information is present in the new context, answer {_DEFAULT_SCORE_THRESHOLD}. \"\n    \"Otherwise answer {existing_answer}.\\n\"\n)\n\n\ndef _default_parser_function(output_str: str) -> Tuple[Optional[float], Optional[str]]:\n    # Pattern to match the feedback and response\n    # This pattern looks for any text ending with '[RESULT]' followed by a number\n    pattern = r\"([\\s\\S]+)(?:\\[RESULT\\]\\s*)([\\d.]+)\"\n\n    # Using regex to find all matches\n    result = re.search(pattern, output_str)\n\n    # Check if any match is found\n    if result:\n        # Assuming there's only one match in the text, extract feedback and response\n        feedback, score = result.groups()\n        score = float(score) if score is not None else score\n        return score, feedback.strip()\n    else:\n        return None, None\n\n\nclass ContextRelevancyEvaluator(BaseEvaluator):\n    \"\"\"Context relevancy evaluator.\n\n    Evaluates the relevancy of retrieved contexts to a query.\n    This evaluator considers the query string and retrieved contexts.\n\n    Args:\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        refine_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for refinement.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        raise_error: bool = False,\n        eval_template: str | BasePromptTemplate | None = None,\n        refine_template: str | BasePromptTemplate | None = None,\n        score_threshold: float = _DEFAULT_SCORE_THRESHOLD,\n        parser_function: Callable[\n            [str], Tuple[Optional[float], Optional[str]]\n        ] = _default_parser_function,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        from llama_index.core import Settings\n\n        self._llm = llm or Settings.llm\n        self._raise_error = raise_error\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self._refine_template: BasePromptTemplate\n        if isinstance(refine_template, str):\n            self._refine_template = PromptTemplate(refine_template)\n        else:\n            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE\n\n        self.parser_function = parser_function\n        self.score_threshold = score_threshold\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n            \"refine_template\": self._refine_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n        if \"refine_template\" in prompts:\n            self._refine_template = prompts[\"refine_template\"]\n\n    async def aevaluate(\n        self,\n        query: str | None = None,\n        response: str | None = None,\n        contexts: Sequence[str] | None = None,\n        sleep_time_in_seconds: int = 0,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the contexts is relevant to the query.\"\"\"\n        del kwargs  # Unused\n        del response  # Unused\n\n        if query is None or contexts is None:\n            raise ValueError(\"Both query and contexts must be provided\")\n\n        docs = [Document(text=context) for context in contexts]\n        index = SummaryIndex.from_documents(docs)\n\n        await asyncio.sleep(sleep_time_in_seconds)\n\n        query_engine = index.as_query_engine(\n            llm=self._llm,\n            text_qa_template=self._eval_template,\n            refine_template=self._refine_template,\n        )\n        response_obj = await query_engine.aquery(query)\n        raw_response_txt = str(response_obj)\n\n        score, reasoning = self.parser_function(raw_response_txt)\n\n        invalid_result, invalid_reason = False, None\n        if score is None and reasoning is None:\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n            invalid_result = True\n            invalid_reason = \"Unable to parse the output string.\"\n\n        if score:\n            score /= self.score_threshold\n\n        return EvaluationResult(\n            query=query,\n            contexts=contexts,\n            score=score,\n            feedback=raw_response_txt,\n            invalid_result=invalid_result,\n            invalid_reason=invalid_reason,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/pairwise.py",
    "filename": "pairwise.py",
    "relpath": "evaluation/pairwise.py",
    "start_line": 1,
    "end_line": 278,
    "length": 278,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_default_parser_function",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_eval_result",
      "_resolve_results",
      "aevaluate"
    ],
    "chunk_class_names": [
      "EvaluationSource",
      "PairwiseComparisonEvaluator"
    ],
    "document_function_names": [
      "_default_parser_function",
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "_get_eval_result",
      "_resolve_results",
      "aevaluate"
    ],
    "document_class_names": [
      "EvaluationSource",
      "PairwiseComparisonEvaluator"
    ],
    "content": "\"\"\"Pairwise evaluation.\"\"\"\n\nimport asyncio\nfrom enum import Enum\nfrom typing import Any, Callable, Optional, Sequence, Tuple, Union\n\nfrom llama_index.core.evaluation.base import (\n    BaseEvaluator,\n    EvaluationResult,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import (\n    BasePromptTemplate,\n    ChatMessage,\n    ChatPromptTemplate,\n    MessageRole,\n    PromptTemplate,\n)\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.settings import Settings\n\nDEFAULT_SYSTEM_TEMPLATE = (\n    \"Please act as an impartial judge and evaluate the quality of the responses provided by two \"\n    \"AI question-answering assistants to the user question perhaps with added reference which \"\n    \"are displayed below. You should choose the assistant that \"\n    \"follows the user\u2019s instructions and answers the user\u2019s question better using the provided \"\n    \"context. Your evaluation \"\n    \"should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, \"\n    \"and level of detail of their responses. Begin your evaluation by comparing the two \"\n    \"responses and provide a short explanation. Avoid any position biases and ensure that the \"\n    \"order in which the responses were presented does not influence your decision. Do not allow \"\n    \"the length of the responses to influence your evaluation. Do not favor certain names of \"\n    \"the assistants. Be as objective as possible. After providing your explanation, output your \"\n    \"final verdict by strictly following this format: '[[A]]' if assistant A is better, '[[B]]' \"\n    \"if assistant B is better, and '[[C]]' for a tie.\\n\"\n)\n\nDEFAULT_USER_TEMPLATE = (\n    \"[User Question]\\n\"\n    \"{query}\"\n    \"\\n\\n\"\n    \"[The Start of Reference]\\n\"\n    \"{reference}\\n\"\n    \"[The End of Reference]\"\n    \"\\n\\n\"\n    \"[The Start of Assistant A\u2019s Answer]\\n\"\n    \"{answer_1}\\n\"\n    \"[The End of Assistant A\u2019s Answer]\"\n    \"\\n\\n\"\n    \"[The Start of Assistant B\u2019s Answer]\\n\"\n    \"{answer_2}\\n\"\n    \"[The End of Assistant B\u2019s Answer]\"\n)\n\nDEFAULT_EVAL_TEMPLATE = ChatPromptTemplate(\n    message_templates=[\n        ChatMessage(role=MessageRole.SYSTEM, content=DEFAULT_SYSTEM_TEMPLATE),\n        ChatMessage(role=MessageRole.USER, content=DEFAULT_USER_TEMPLATE),\n    ]\n)\n\n\ndef _default_parser_function(\n    eval_response: str,\n) -> Tuple[Optional[bool], Optional[float], Optional[str]]:\n    # Extract from response\n    feedback: Optional[str] = \"\"\n    if \"[[A]]\" in eval_response:\n        passing: Optional[bool] = True\n        score = 1.0\n    elif \"[[B]]\" in eval_response:\n        passing = False\n        score = 0.0\n    elif \"[[C]]\" in eval_response:\n        passing = None\n        score = 0.5\n    else:\n        passing = None\n        score = None\n        feedback = None\n    return passing, score, feedback\n\n\nclass EvaluationSource(str, Enum):\n    \"\"\"To distinguish between flipped or original.\"\"\"\n\n    ORIGINAL = \"original\"\n    FLIPPED = \"flipped\"\n    NEITHER = \"neither\"\n\n\nclass PairwiseComparisonEvaluator(BaseEvaluator):\n    \"\"\"Pairwise comparison evaluator.\n\n    Evaluates the quality of a response vs. a \"reference\" response given a question by\n    having an LLM judge which response is better.\n\n    Outputs whether the `response` given is better than the `reference` response.\n\n    Args:\n        eval_template (Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        enforce_consensus (bool): Whether to enforce consensus (consistency if we\n            flip the order of the answers). Defaults to True.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        eval_template: Optional[Union[BasePromptTemplate, str]] = None,\n        parser_function: Callable[\n            [str], Tuple[Optional[bool], Optional[float], Optional[str]]\n        ] = _default_parser_function,\n        enforce_consensus: bool = True,\n    ) -> None:\n        self._llm = llm or Settings.llm\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self._enforce_consensus = enforce_consensus\n        self._parser_function = parser_function\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n\n    async def _get_eval_result(\n        self,\n        query: str,\n        response: str,\n        second_response: str,\n        reference: Optional[str],\n    ) -> EvaluationResult:\n        \"\"\"Get evaluation result.\"\"\"\n        eval_response = await self._llm.apredict(\n            prompt=self._eval_template,\n            query=query,\n            answer_1=response,\n            answer_2=second_response,\n            reference=reference or \"\",\n        )\n\n        # Extract from response\n        passing, score, feedback = self._parser_function(eval_response)\n\n        if passing is None and score is None and feedback is None:\n            return EvaluationResult(\n                query=query,\n                invalid_result=True,\n                invalid_reason=\"Output cannot be parsed\",\n                feedback=eval_response,\n            )\n        else:\n            return EvaluationResult(\n                query=query,\n                response=eval_response,\n                passing=passing,\n                score=score,\n                feedback=eval_response,\n                pairwise_source=EvaluationSource.ORIGINAL,\n            )\n\n    async def _resolve_results(\n        self,\n        eval_result: EvaluationResult,\n        flipped_eval_result: EvaluationResult,\n    ) -> EvaluationResult:\n        \"\"\"Resolve eval results from evaluation + flipped evaluation.\n\n        Args:\n            eval_result (EvaluationResult): Result when answer_1 is shown first\n            flipped_eval_result (EvaluationResult): Result when answer_2 is shown first\n\n        Returns:\n            EvaluationResult: The final evaluation result\n        \"\"\"\n        # add pairwise_source to eval_result and flipped_eval_result\n        eval_result.pairwise_source = EvaluationSource.ORIGINAL\n        flipped_eval_result.pairwise_source = EvaluationSource.FLIPPED\n\n        # count the votes for each of the 2 answers\n        votes_1 = 0.0\n        votes_2 = 0.0\n        if eval_result.score is not None and flipped_eval_result.score is not None:\n            votes_1 = eval_result.score + (1 - flipped_eval_result.score)\n            votes_2 = (1 - eval_result.score) + flipped_eval_result.score\n\n        if votes_1 + votes_2 != 2:  # each round, the judge can give a total of 1 vote\n            raise ValueError(\"Impossible score results. Total amount of votes is 2.\")\n\n        # get the judges (original and flipped) who voted for answer_1\n        voters_1 = [eval_result] * (eval_result.score == 1.0) + [\n            flipped_eval_result\n        ] * (flipped_eval_result.score == 0.0)\n\n        # get the judges (original and flipped) who voted for answer_2\n        voters_2 = [eval_result] * (eval_result.score == 0.0) + [\n            flipped_eval_result\n        ] * (flipped_eval_result.score == 1.0)\n\n        if votes_1 > votes_2:\n            return voters_1[0]  # return any voter for answer_1\n        elif votes_2 > votes_1:\n            return voters_2[0]  # return any vote for answer_2\n        else:\n            if (\n                eval_result.score == 0.5\n            ):  # votes_1 == votes_2 can only happen if both are 1.0 (so actual tie)\n                # doesn't matter which one we return here\n                return eval_result\n            else:  # Inconclusive case!\n                return EvaluationResult(\n                    query=eval_result.query,\n                    response=\"\",\n                    passing=None,\n                    score=0.5,\n                    feedback=\"\",\n                    pairwise_source=EvaluationSource.NEITHER,\n                )\n\n    async def aevaluate(\n        self,\n        query: Optional[str] = None,\n        response: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,\n        second_response: Optional[str] = None,\n        reference: Optional[str] = None,\n        sleep_time_in_seconds: int = 0,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        del kwargs  # Unused\n        del contexts  # Unused\n\n        if query is None or response is None or second_response is None:\n            raise ValueError(\n                \"query, response, second_response, and reference must be provided\"\n            )\n\n        await asyncio.sleep(sleep_time_in_seconds)\n\n        eval_result = await self._get_eval_result(\n            query, response, second_response, reference\n        )\n        if self._enforce_consensus and not eval_result.invalid_result:\n            # Flip the order of the answers and see if the answer is consistent\n            # (which means that the score should flip from 0 to 1 and vice-versa)\n            # if not, then we return a tie\n            flipped_eval_result = await self._get_eval_result(\n                query, second_response, response, reference\n            )\n            if not flipped_eval_result.invalid_result:\n                resolved_eval_result = await self._resolve_results(\n                    eval_result, flipped_eval_result\n                )\n            else:\n                resolved_eval_result = EvaluationResult(\n                    query=eval_result.query,\n                    response=eval_result.response,\n                    feedback=flipped_eval_result.response,\n                    invalid_result=True,\n                    invalid_reason=\"Output cannot be parsed.\",\n                )\n        else:\n            resolved_eval_result = eval_result\n\n        return resolved_eval_result"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/batch_runner.py",
    "filename": "batch_runner.py",
    "relpath": "evaluation/batch_runner.py",
    "start_line": 1,
    "end_line": 442,
    "length": 442,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "eval_response_worker",
      "eval_worker",
      "response_worker",
      "__init__",
      "_format_results",
      "_validate_and_clean_inputs",
      "_validate_nested_eval_kwargs_types",
      "_get_eval_kwargs",
      "aevaluate_response_strs",
      "aevaluate_responses",
      "aevaluate_queries",
      "evaluate_response_strs",
      "evaluate_responses",
      "evaluate_queries",
      "upload_eval_results"
    ],
    "chunk_class_names": [
      "BatchEvalRunner"
    ],
    "document_function_names": [
      "eval_response_worker",
      "eval_worker",
      "response_worker",
      "__init__",
      "_format_results",
      "_validate_and_clean_inputs",
      "_validate_nested_eval_kwargs_types",
      "_get_eval_kwargs",
      "aevaluate_response_strs",
      "aevaluate_responses",
      "aevaluate_queries",
      "evaluate_response_strs",
      "evaluate_responses",
      "evaluate_queries",
      "upload_eval_results"
    ],
    "document_class_names": [
      "BatchEvalRunner"
    ],
    "content": "import asyncio\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple, cast\n\nfrom llama_index.core.async_utils import asyncio_module, asyncio_run\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE, Response\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\n\n\n@retry(\n    reraise=True,\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\nasync def eval_response_worker(\n    semaphore: asyncio.Semaphore,\n    evaluator: BaseEvaluator,\n    evaluator_name: str,\n    query: Optional[str] = None,\n    response: Optional[Response] = None,\n    eval_kwargs: Optional[Dict[str, Any]] = None,\n) -> Tuple[str, EvaluationResult]:\n    \"\"\"Get aevaluate_response tasks with semaphore.\"\"\"\n    eval_kwargs = eval_kwargs or {}\n    async with semaphore:\n        return (\n            evaluator_name,\n            await evaluator.aevaluate_response(\n                query=query, response=response, **eval_kwargs\n            ),\n        )\n\n\n@retry(\n    reraise=True,\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\nasync def eval_worker(\n    semaphore: asyncio.Semaphore,\n    evaluator: BaseEvaluator,\n    evaluator_name: str,\n    query: Optional[str] = None,\n    response_str: Optional[str] = None,\n    contexts: Optional[Sequence[str]] = None,\n    eval_kwargs: Optional[Dict[str, Any]] = None,\n) -> Tuple[str, EvaluationResult]:\n    \"\"\"Get aevaluate tasks with semaphore.\"\"\"\n    eval_kwargs = eval_kwargs or {}\n    async with semaphore:\n        return (\n            evaluator_name,\n            await evaluator.aevaluate(\n                query=query, response=response_str, contexts=contexts, **eval_kwargs\n            ),\n        )\n\n\n@retry(\n    reraise=True,\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n)\nasync def response_worker(\n    semaphore: asyncio.Semaphore,\n    query_engine: BaseQueryEngine,\n    query: str,\n) -> RESPONSE_TYPE:\n    \"\"\"Get aquery tasks with semaphore.\"\"\"\n    async with semaphore:\n        return await query_engine.aquery(query)\n\n\nclass BatchEvalRunner:\n    \"\"\"\n    Batch evaluation runner.\n\n    Args:\n        evaluators (Dict[str, BaseEvaluator]): Dictionary of evaluators.\n        workers (int): Number of workers to use for parallelization.\n            Defaults to 2.\n        show_progress (bool): Whether to show progress bars. Defaults to False.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        evaluators: Dict[str, BaseEvaluator],\n        workers: int = 2,\n        show_progress: bool = False,\n    ):\n        self.evaluators = evaluators\n        self.workers = workers\n        self.semaphore = asyncio.Semaphore(self.workers)\n        self.show_progress = show_progress\n        self.asyncio_mod = asyncio_module(show_progress=self.show_progress)\n\n    def _format_results(\n        self, results: List[Tuple[str, EvaluationResult]]\n    ) -> Dict[str, List[EvaluationResult]]:\n        \"\"\"Format results.\"\"\"\n        # Format results\n        results_dict: Dict[str, List[EvaluationResult]] = {\n            name: [] for name in self.evaluators\n        }\n        for name, result in results:\n            results_dict[name].append(result)\n\n        return results_dict\n\n    def _validate_and_clean_inputs(\n        self,\n        *inputs_list: Any,\n    ) -> List[Any]:\n        \"\"\"\n        Validate and clean input lists.\n\n        Enforce that at least one of the inputs is not None.\n        Make sure that all inputs have the same length.\n        Make sure that None inputs are replaced with [None] * len(inputs).\n\n        \"\"\"\n        assert len(inputs_list) > 0\n        # first, make sure at least one of queries or response_strs is not None\n        input_len: Optional[int] = None\n        for inputs in inputs_list:\n            if inputs is not None:\n                input_len = len(inputs)\n                break\n        if input_len is None:\n            raise ValueError(\"At least one item in inputs_list must be provided.\")\n\n        new_inputs_list = []\n        for inputs in inputs_list:\n            if inputs is None:\n                new_inputs_list.append([None] * input_len)\n            else:\n                if len(inputs) != input_len:\n                    raise ValueError(\"All inputs must have the same length.\")\n                new_inputs_list.append(inputs)\n        return new_inputs_list\n\n    def _validate_nested_eval_kwargs_types(\n        self, eval_kwargs_lists: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Ensure eval kwargs are acceptable format.\n            either a Dict[str, List] or a Dict[str, Dict[str, List]].\n\n        Allows use of different kwargs (e.g. references) with different evaluators\n            while keeping backwards compatibility for single evaluators\n\n        \"\"\"\n        if not isinstance(eval_kwargs_lists, dict):\n            raise ValueError(\n                f\"eval_kwargs_lists must be a dict. Got {eval_kwargs_lists}\"\n            )\n\n        for evaluator, eval_kwargs in eval_kwargs_lists.items():\n            if isinstance(eval_kwargs, list):\n                # maintain backwards compatibility - for use with single evaluator\n                eval_kwargs_lists[evaluator] = self._validate_and_clean_inputs(\n                    eval_kwargs\n                )[0]\n            elif isinstance(eval_kwargs, dict):\n                # for use with multiple evaluators\n                for k in eval_kwargs:\n                    v = eval_kwargs[k]\n                    if not isinstance(v, list):\n                        raise ValueError(\n                            f\"nested inner values in eval_kwargs must be a list. Got {evaluator}: {k}: {v}\"\n                        )\n                    eval_kwargs_lists[evaluator][k] = self._validate_and_clean_inputs(\n                        v\n                    )[0]\n            else:\n                raise ValueError(\n                    f\"eval_kwargs must be a list or a dict. Got {evaluator}: {eval_kwargs}\"\n                )\n        return eval_kwargs_lists\n\n    def _get_eval_kwargs(\n        self, eval_kwargs_lists: Dict[str, Any], idx: int\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Get eval kwargs from eval_kwargs_lists at a given idx.\n\n        Since eval_kwargs_lists is a dict of lists, we need to get the\n        value at idx for each key.\n\n        \"\"\"\n        return {k: v[idx] for k, v in eval_kwargs_lists.items()}\n\n    async def aevaluate_response_strs(\n        self,\n        queries: Optional[List[str]] = None,\n        response_strs: Optional[List[str]] = None,\n        contexts_list: Optional[List[List[str]]] = None,\n        **eval_kwargs_lists: Dict[str, Any],\n    ) -> Dict[str, List[EvaluationResult]]:\n        \"\"\"\n        Evaluate query, response pairs.\n\n        This evaluates queries, responses, contexts as string inputs.\n        Can supply additional kwargs to the evaluator in eval_kwargs_lists.\n\n        Args:\n            queries (Optional[List[str]]): List of query strings. Defaults to None.\n            response_strs (Optional[List[str]]): List of response strings.\n                Defaults to None.\n            contexts_list (Optional[List[List[str]]]): List of context lists.\n                Defaults to None.\n            **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists\n                of kwargs to pass to evaluator. Defaults to None.\n                    multiple evaluators: {evaluator: {kwarg: [list of values]},...}\n                    single evaluator:    {kwarg: [list of values]}\n\n        \"\"\"\n        queries, response_strs, contexts_list = self._validate_and_clean_inputs(\n            queries, response_strs, contexts_list\n        )\n        eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)\n\n        # boolean to check if using multi kwarg evaluator\n        multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(\n            next(iter(eval_kwargs_lists.values())), dict\n        )\n\n        # run evaluations\n        eval_jobs = []\n        for idx, query in enumerate(cast(List[str], queries)):\n            response_str = cast(List, response_strs)[idx]\n            contexts = cast(List, contexts_list)[idx]\n            for name, evaluator in self.evaluators.items():\n                if multi_kwargs:\n                    # multi-evaluator - get appropriate runtime kwargs if present\n                    kwargs = (\n                        eval_kwargs_lists[name] if name in eval_kwargs_lists else {}\n                    )\n                else:\n                    # single evaluator (maintain backwards compatibility)\n                    kwargs = eval_kwargs_lists\n                eval_kwargs = self._get_eval_kwargs(kwargs, idx)\n                eval_jobs.append(\n                    eval_worker(\n                        self.semaphore,\n                        evaluator,\n                        name,\n                        query=query,\n                        response_str=response_str,\n                        contexts=contexts,\n                        eval_kwargs=eval_kwargs,\n                    )\n                )\n        results = await self.asyncio_mod.gather(*eval_jobs)\n\n        # Format results\n        return self._format_results(results)\n\n    async def aevaluate_responses(\n        self,\n        queries: Optional[List[str]] = None,\n        responses: Optional[List[Response]] = None,\n        **eval_kwargs_lists: Dict[str, Any],\n    ) -> Dict[str, List[EvaluationResult]]:\n        \"\"\"\n        Evaluate query, response pairs.\n\n        This evaluates queries and response objects.\n\n        Args:\n            queries (Optional[List[str]]): List of query strings. Defaults to None.\n            responses (Optional[List[Response]]): List of response objects.\n                Defaults to None.\n            **eval_kwargs_lists (Dict[str, Any]): Dict of either dicts or lists\n                of kwargs to pass to evaluator. Defaults to None.\n                    multiple evaluators: {evaluator: {kwarg: [list of values]},...}\n                    single evaluator:    {kwarg: [list of values]}\n\n        \"\"\"\n        queries, responses = self._validate_and_clean_inputs(queries, responses)\n        eval_kwargs_lists = self._validate_nested_eval_kwargs_types(eval_kwargs_lists)\n\n        # boolean to check if using multi kwarg evaluator\n        multi_kwargs = len(eval_kwargs_lists) > 0 and isinstance(\n            next(iter(eval_kwargs_lists.values())), dict\n        )\n\n        # run evaluations\n        eval_jobs = []\n        for idx, query in enumerate(cast(List[str], queries)):\n            response = cast(List, responses)[idx]\n            for name, evaluator in self.evaluators.items():\n                if multi_kwargs:\n                    # multi-evaluator - get appropriate runtime kwargs if present\n                    kwargs = (\n                        eval_kwargs_lists[name] if name in eval_kwargs_lists else {}\n                    )\n                else:\n                    # single evaluator (maintain backwards compatibility)\n                    kwargs = eval_kwargs_lists\n                eval_kwargs = self._get_eval_kwargs(kwargs, idx)\n                eval_jobs.append(\n                    eval_response_worker(\n                        self.semaphore,\n                        evaluator,\n                        name,\n                        query=query,\n                        response=response,\n                        eval_kwargs=eval_kwargs,\n                    )\n                )\n        results = await self.asyncio_mod.gather(*eval_jobs)\n\n        # Format results\n        return self._format_results(results)\n\n    async def aevaluate_queries(\n        self,\n        query_engine: BaseQueryEngine,\n        queries: Optional[List[str]] = None,\n        **eval_kwargs_lists: Dict[str, Any],\n    ) -> Dict[str, List[EvaluationResult]]:\n        \"\"\"\n        Evaluate queries.\n\n        Args:\n            query_engine (BaseQueryEngine): Query engine.\n            queries (Optional[List[str]]): List of query strings. Defaults to None.\n            **eval_kwargs_lists (Dict[str, Any]): Dict of lists of kwargs to\n                pass to evaluator. Defaults to None.\n\n        \"\"\"\n        if queries is None:\n            raise ValueError(\"`queries` must be provided\")\n\n        # gather responses\n        response_jobs = []\n        for query in queries:\n            response_jobs.append(response_worker(self.semaphore, query_engine, query))\n        responses = await self.asyncio_mod.gather(*response_jobs)\n\n        return await self.aevaluate_responses(\n            queries=queries,\n            responses=responses,\n            **eval_kwargs_lists,\n        )\n\n    def evaluate_response_strs(\n        self,\n        queries: Optional[List[str]] = None,\n        response_strs: Optional[List[str]] = None,\n        contexts_list: Optional[List[List[str]]] = None,\n        **eval_kwargs_lists: Dict[str, Any],\n    ) -> Dict[str, List[EvaluationResult]]:\n        \"\"\"\n        Evaluate query, response pairs.\n\n        Sync version of aevaluate_response_strs.\n\n        \"\"\"\n        return asyncio_run(\n            self.aevaluate_response_strs(\n                queries=queries,\n                response_strs=response_strs,\n                contexts_list=contexts_list,\n                **eval_kwargs_lists,\n            )\n        )\n\n    def evaluate_responses(\n        self,\n        queries: Optional[List[str]] = None,\n        responses: Optional[List[Response]] = None,\n        **eval_kwargs_lists: Dict[str, Any],\n    ) -> Dict[str, List[EvaluationResult]]:\n        \"\"\"\n        Evaluate query, response objs.\n\n        Sync version of aevaluate_responses.\n\n        \"\"\"\n        return asyncio_run(\n            self.aevaluate_responses(\n                queries=queries,\n                responses=responses,\n                **eval_kwargs_lists,\n            )\n        )\n\n    def evaluate_queries(\n        self,\n        query_engine: BaseQueryEngine,\n        queries: Optional[List[str]] = None,\n        **eval_kwargs_lists: Dict[str, Any],\n    ) -> Dict[str, List[EvaluationResult]]:\n        \"\"\"\n        Evaluate queries.\n\n        Sync version of aevaluate_queries.\n\n        \"\"\"\n        return asyncio_run(\n            self.aevaluate_queries(\n                query_engine=query_engine,\n                queries=queries,\n                **eval_kwargs_lists,\n            )\n        )\n\n    def upload_eval_results(\n        self,\n        project_name: str,\n        app_name: str,\n        results: Dict[str, List[EvaluationResult]],\n    ) -> None:\n        \"\"\"\n        Upload the evaluation results to LlamaCloud.\n\n        Args:\n            project_name (str): The name of the project.\n            app_name (str): The name of the app.\n            results (Dict[str, List[EvaluationResult]]):\n                The evaluation results, a mapping of metric name to a list of EvaluationResult objects.\n\n        Examples:\n            ```python\n            results = batch_runner.evaluate_responses(...)\n\n            batch_runner.upload_eval_results(\n                project_name=\"my_project\",\n                app_name=\"my_app\",\n                results=results\n            )\n            ```\n        \"\"\"\n        from llama_index.core.evaluation.eval_utils import upload_eval_results\n\n        upload_eval_results(\n            project_name=project_name, app_name=app_name, results=results\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/relevancy.py",
    "filename": "relevancy.py",
    "relpath": "evaluation/relevancy.py",
    "start_line": 1,
    "end_line": 142,
    "length": 142,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "chunk_class_names": [
      "RelevancyEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "document_class_names": [
      "RelevancyEvaluator"
    ],
    "content": "\"\"\"Relevancy evaluation.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom typing import Any, Optional, Sequence, Union\n\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.indices import SummaryIndex\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import Document\nfrom llama_index.core.settings import Settings\n\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the response for the query \\\n    is in line with the context information provided.\\n\"\n    \"You have two options to answer. Either YES/ NO.\\n\"\n    \"Answer - YES, if the response for the query \\\n    is in line with context information otherwise NO.\\n\"\n    \"Query and Response: \\n {query_str}\\n\"\n    \"Context: \\n {context_str}\\n\"\n    \"Answer: \"\n)\n\nDEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following query and response is\"\n    \"in line with the context information: \\n {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"\n    \"If the information is present in the new context, answer YES. \"\n    \"Otherwise answer NO.\\n\"\n)\n\n\nclass RelevancyEvaluator(BaseEvaluator):\n    \"\"\"Relenvancy evaluator.\n\n    Evaluates the relevancy of retrieved contexts and response to a query.\n    This evaluator considers the query string, retrieved contexts, and response string.\n\n    Args:\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        refine_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for refinement.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: Optional[LLM] = None,\n        raise_error: bool = False,\n        eval_template: Optional[Union[str, BasePromptTemplate]] = None,\n        refine_template: Optional[Union[str, BasePromptTemplate]] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._llm = llm or Settings.llm\n        self._raise_error = raise_error\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self._refine_template: BasePromptTemplate\n        if isinstance(refine_template, str):\n            self._refine_template = PromptTemplate(refine_template)\n        else:\n            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n            \"refine_template\": self._refine_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n        if \"refine_template\" in prompts:\n            self._refine_template = prompts[\"refine_template\"]\n\n    async def aevaluate(\n        self,\n        query: str | None = None,\n        response: str | None = None,\n        contexts: Sequence[str] | None = None,\n        sleep_time_in_seconds: int = 0,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the contexts and response are relevant to the query.\"\"\"\n        del kwargs  # Unused\n\n        if query is None or contexts is None or response is None:\n            raise ValueError(\"query, contexts, and response must be provided\")\n\n        docs = [Document(text=context) for context in contexts]\n        index = SummaryIndex.from_documents(docs)\n\n        query_response = f\"Question: {query}\\nResponse: {response}\"\n\n        await asyncio.sleep(sleep_time_in_seconds)\n\n        query_engine = index.as_query_engine(\n            llm=self._llm,\n            text_qa_template=self._eval_template,\n            refine_template=self._refine_template,\n        )\n        response_obj = await query_engine.aquery(query_response)\n\n        raw_response_txt = str(response_obj)\n\n        if \"yes\" in raw_response_txt.lower():\n            passing = True\n        else:\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n            passing = False\n\n        return EvaluationResult(\n            query=query,\n            response=response,\n            passing=passing,\n            score=1.0 if passing else 0.0,\n            feedback=raw_response_txt,\n            contexts=contexts,\n        )\n\n\nQueryResponseEvaluator = RelevancyEvaluator"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/semantic_similarity.py",
    "filename": "semantic_similarity.py",
    "relpath": "evaluation/semantic_similarity.py",
    "start_line": 1,
    "end_line": 79,
    "length": 79,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "chunk_class_names": [
      "SemanticSimilarityEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "aevaluate"
    ],
    "document_class_names": [
      "SemanticSimilarityEvaluator"
    ],
    "content": "from typing import Any, Callable, Optional, Sequence\n\nfrom llama_index.core.base.embeddings.base import (\n    BaseEmbedding,\n    SimilarityMode,\n    similarity,\n)\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.settings import Settings\n\n\nclass SemanticSimilarityEvaluator(BaseEvaluator):\n    \"\"\"Embedding similarity evaluator.\n\n    Evaluate the quality of a question answering system by\n    comparing the similarity between embeddings of the generated answer\n    and the reference answer.\n\n    Inspired by this paper:\n    - Semantic Answer Similarity for Evaluating Question Answering Models\n        https://arxiv.org/pdf/2108.06130.pdf\n\n    Args:\n        similarity_threshold (float): Embedding similarity threshold for \"passing\".\n            Defaults to 0.8.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_model: Optional[BaseEmbedding] = None,\n        similarity_fn: Optional[Callable[..., float]] = None,\n        similarity_mode: Optional[SimilarityMode] = None,\n        similarity_threshold: float = 0.8,\n    ) -> None:\n        self._embed_model = embed_model or Settings.embed_model\n\n        if similarity_fn is None:\n            similarity_mode = similarity_mode or SimilarityMode.DEFAULT\n            self._similarity_fn = lambda x, y: similarity(x, y, mode=similarity_mode)\n        else:\n            if similarity_mode is not None:\n                raise ValueError(\n                    \"Cannot specify both similarity_fn and similarity_mode\"\n                )\n            self._similarity_fn = similarity_fn\n\n        self._similarity_threshold = similarity_threshold\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    async def aevaluate(\n        self,\n        query: Optional[str] = None,\n        response: Optional[str] = None,\n        contexts: Optional[Sequence[str]] = None,\n        reference: Optional[str] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        del query, contexts, kwargs  # Unused\n\n        if response is None or reference is None:\n            raise ValueError(\"Must specify both response and reference\")\n\n        response_embedding = await self._embed_model.aget_text_embedding(response)\n        reference_embedding = await self._embed_model.aget_text_embedding(reference)\n\n        similarity_score = self._similarity_fn(response_embedding, reference_embedding)\n        passing = similarity_score >= self._similarity_threshold\n        return EvaluationResult(\n            score=similarity_score,\n            passing=passing,\n            feedback=f\"Similarity score: {similarity_score}\",\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/notebook_utils.py",
    "filename": "notebook_utils.py",
    "relpath": "evaluation/notebook_utils.py",
    "start_line": 1,
    "end_line": 89,
    "length": 89,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_retrieval_results_df",
      "get_eval_results_df"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_retrieval_results_df",
      "get_eval_results_df"
    ],
    "document_class_names": [],
    "content": "\"\"\"Notebook utils.\"\"\"\n\nfrom collections import defaultdict\nfrom typing import Any, List, Optional, Tuple\n\nfrom llama_index.core.evaluation import EvaluationResult\nfrom llama_index.core.evaluation.retrieval.base import RetrievalEvalResult\n\nDEFAULT_METRIC_KEYS = [\"hit_rate\", \"mrr\"]\n\n\ndef get_retrieval_results_df(\n    names: List[str],\n    results_arr: List[List[RetrievalEvalResult]],\n    metric_keys: Optional[List[str]] = None,\n) -> Any:\n    \"\"\"Display retrieval results.\"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"pandas is required for this function. Please install it with `pip install pandas`.\"\n        )\n\n    metric_keys = metric_keys or DEFAULT_METRIC_KEYS\n\n    avg_metrics_dict = defaultdict(list)\n    for name, eval_results in zip(names, results_arr):\n        metric_dicts = []\n        for eval_result in eval_results:\n            metric_dict = eval_result.metric_vals_dict\n            metric_dicts.append(metric_dict)\n        results_df = pd.DataFrame(metric_dicts)\n\n        for metric_key in metric_keys:\n            if metric_key not in results_df.columns:\n                raise ValueError(f\"Metric key {metric_key} not in results_df\")\n            avg_metrics_dict[metric_key].append(results_df[metric_key].mean())\n\n    return pd.DataFrame({\"retrievers\": names, **avg_metrics_dict})\n\n\ndef get_eval_results_df(\n    names: List[str], results_arr: List[EvaluationResult], metric: Optional[str] = None\n) -> Tuple[Any, Any]:\n    \"\"\"Organizes EvaluationResults into a deep dataframe and computes the mean\n    score.\n\n    result:\n        result_df: pd.DataFrame representing all the evaluation results\n        mean_df: pd.DataFrame of average scores groupby names\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"pandas is required for this function. Please install it with `pip install pandas`.\"\n        )\n\n    if len(names) != len(results_arr):\n        raise ValueError(\"names and results_arr must have same length.\")\n\n    qs = []\n    ss = []\n    fs = []\n    rs = []\n    cs = []\n    for res in results_arr:\n        qs.append(res.query)\n        ss.append(res.score)\n        fs.append(res.feedback)\n        rs.append(res.response)\n        cs.append(res.contexts)\n\n    deep_df = pd.DataFrame(\n        {\n            \"rag\": names,\n            \"query\": qs,\n            \"answer\": rs,\n            \"contexts\": cs,\n            \"scores\": ss,\n            \"feedbacks\": fs,\n        }\n    )\n    mean_df = pd.DataFrame(deep_df.groupby([\"rag\"])[\"scores\"].mean()).T\n    if metric:\n        mean_df.index = [f\"mean_{metric}_score\"]\n\n    return deep_df, mean_df"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/faithfulness.py",
    "filename": "faithfulness.py",
    "relpath": "evaluation/multi_modal/faithfulness.py",
    "start_line": 1,
    "end_line": 227,
    "length": 227,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "evaluate",
      "aevaluate"
    ],
    "chunk_class_names": [
      "MultiModalFaithfulnessEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "evaluate",
      "aevaluate"
    ],
    "document_class_names": [
      "MultiModalFaithfulnessEvaluator"
    ],
    "content": "\"\"\"Faithfulness evaluation.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, List, Optional, Sequence, Union\n\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.multi_modal_llms.base import MultiModalLLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import ImageNode\n\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Please tell if a given piece of information \"\n    \"is supported by the visual as well as textual context information.\\n\"\n    \"You need to answer with either YES or NO.\\n\"\n    \"Answer YES if any of the image(s) and textual context supports the information, even \"\n    \"if most of the context is unrelated. \"\n    \"Some examples are provided below with only text context, but please do use\\n\"\n    \"any images for context if they are provided.\\n\\n\"\n    \"Information: Apple pie is generally double-crusted.\\n\"\n    \"Context: An apple pie is a fruit pie in which the principal filling \"\n    \"ingredient is apples. \\n\"\n    \"Apple pie is often served with whipped cream, ice cream \"\n    \"('apple pie \u00e0 la mode'), custard or cheddar cheese.\\n\"\n    \"It is generally double-crusted, with pastry both above \"\n    \"and below the filling; the upper crust may be solid or \"\n    \"latticed (woven of crosswise strips).\\n\"\n    \"Answer: YES\\n\"\n    \"Information: Apple pies tastes bad.\\n\"\n    \"Context: An apple pie is a fruit pie in which the principal filling \"\n    \"ingredient is apples. \\n\"\n    \"Apple pie is often served with whipped cream, ice cream \"\n    \"('apple pie \u00e0 la mode'), custard or cheddar cheese.\\n\"\n    \"It is generally double-crusted, with pastry both above \"\n    \"and below the filling; the upper crust may be solid or \"\n    \"latticed (woven of crosswise strips).\\n\"\n    \"Answer: NO\\n\"\n    \"Information: {query_str}\\n\"\n    \"Context: {context_str}\\n\"\n    \"Answer: \"\n)\n\nDEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following information is present \"\n    \"in the context information: {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"\n    \"If the information is present in the new context, answer YES. \"\n    \"Otherwise answer NO.\\n\"\n)\n\n\nclass MultiModalFaithfulnessEvaluator(BaseEvaluator):\n    \"\"\"Multi-Modal Faithfulness evaluator.\n\n    Evaluates whether a response is faithful to the contexts\n    (i.e. whether the response is supported by the contexts or hallucinated.)\n\n    This evaluator only considers the response string and the list of context strings.\n\n    Args:\n        multi_modal_llm(Optional[MultiModalLLM]):\n            The Multi-Modal LLM Judge to use for evaluations.\n        raise_error(bool): Whether to raise an error when the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        refine_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for refining the evaluation.\n    \"\"\"\n\n    def __init__(\n        self,\n        multi_modal_llm: Optional[MultiModalLLM] = None,\n        raise_error: bool = False,\n        eval_template: Union[str, BasePromptTemplate, None] = None,\n        refine_template: Union[str, BasePromptTemplate, None] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if multi_modal_llm is None:\n            try:\n                from llama_index.multi_modal_llms.openai import (\n                    OpenAIMultiModal,\n                )  # pants: no-infer-dep\n            except ImportError:\n                raise ImportError(\n                    \"OpenAIMultiModal is not installed. \"\n                    \"Please install it using `pip install llama-index-multi-modal-llms-openai`\"\n                )\n\n            self._multi_modal_llm: MultiModalLLM = OpenAIMultiModal(\n                model=\"gpt-4-vision-preview\", max_new_tokens=1000\n            )\n        else:\n            self._multi_modal_llm = multi_modal_llm\n\n        self._raise_error = raise_error\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self._refine_template: BasePromptTemplate\n        if isinstance(refine_template, str):\n            self._refine_template = PromptTemplate(refine_template)\n        else:\n            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n            \"refine_template\": self._refine_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n        if \"refine_template\" in prompts:\n            self._refine_template = prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,\n        contexts: Union[Sequence[str], None] = None,\n        image_paths: Union[List[str], None] = None,\n        image_urls: Union[List[str], None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the response is faithful to the multi-modal contexts.\"\"\"\n        del query  # Unused\n        del kwargs  # Unused\n        if contexts is None or response is None:\n            raise ValueError(\"contexts and response must be provided\")\n\n        context_str = \"\\n\\n\".join(contexts)\n        fmt_prompt = self._eval_template.format(\n            context_str=context_str, query_str=response\n        )\n\n        if image_paths:\n            image_nodes = [\n                ImageNode(image_path=image_path) for image_path in image_paths\n            ]\n        if image_urls:\n            image_nodes = [ImageNode(image_url=image_url) for image_url in image_urls]\n\n        response_obj = self._multi_modal_llm.complete(\n            prompt=fmt_prompt,\n            image_documents=image_nodes,\n        )\n\n        raw_response_txt = str(response_obj)\n\n        if \"yes\" in raw_response_txt.lower():\n            passing = True\n        else:\n            passing = False\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n\n        return EvaluationResult(\n            response=response,\n            contexts=contexts,\n            passing=passing,\n            score=1.0 if passing else 0.0,\n            feedback=raw_response_txt,\n        )\n\n    async def aevaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,\n        contexts: Union[Sequence[str], None] = None,\n        image_paths: Union[List[str], None] = None,\n        image_urls: Union[List[str], None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Async evaluate whether the response is faithful to the multi-modal contexts.\"\"\"\n        del query  # Unused\n        del kwargs  # Unused\n        if contexts is None or response is None:\n            raise ValueError(\"contexts and response must be provided\")\n\n        context_str = \"\\n\\n\".join(contexts)\n        fmt_prompt = self._eval_template.format(\n            context_str=context_str, query_str=response\n        )\n\n        if image_paths:\n            image_nodes = [\n                ImageNode(image_path=image_path) for image_path in image_paths\n            ]\n        if image_urls:\n            image_nodes = [ImageNode(image_url=image_url) for image_url in image_urls]\n\n        response_obj = await self._multi_modal_llm.acomplete(\n            prompt=fmt_prompt,\n            image_documents=image_nodes,\n        )\n\n        raw_response_txt = str(response_obj)\n\n        if \"yes\" in raw_response_txt.lower():\n            passing = True\n        else:\n            passing = False\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n\n        return EvaluationResult(\n            response=response,\n            contexts=contexts,\n            passing=passing,\n            score=1.0 if passing else 0.0,\n            feedback=raw_response_txt,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/__init__.py",
    "filename": "__init__.py",
    "relpath": "evaluation/multi_modal/__init__.py",
    "start_line": 1,
    "end_line": 10,
    "length": 10,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Multi-Modal Evaluation Modules.\"\"\"\n\nfrom llama_index.core.evaluation.multi_modal.faithfulness import (\n    MultiModalFaithfulnessEvaluator,\n)\nfrom llama_index.core.evaluation.multi_modal.relevancy import (\n    MultiModalRelevancyEvaluator,\n)\n\n__all__ = [\"MultiModalRelevancyEvaluator\", \"MultiModalFaithfulnessEvaluator\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/multi_modal/relevancy.py",
    "filename": "relevancy.py",
    "relpath": "evaluation/multi_modal/relevancy.py",
    "start_line": 1,
    "end_line": 208,
    "length": 208,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "evaluate",
      "aevaluate"
    ],
    "chunk_class_names": [
      "MultiModalRelevancyEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "evaluate",
      "aevaluate"
    ],
    "document_class_names": [
      "MultiModalRelevancyEvaluator"
    ],
    "content": "\"\"\"Relevancy evaluation.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, List, Sequence, Union\n\nfrom llama_index.core.evaluation.base import BaseEvaluator, EvaluationResult\nfrom llama_index.core.multi_modal_llms.base import MultiModalLLM\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.schema import ImageNode\n\nDEFAULT_EVAL_TEMPLATE = PromptTemplate(\n    \"Your task is to evaluate if the response for the query \\\n    is in line with the images and textual context information provided.\\n\"\n    \"You have two options to answer. Either YES/ NO.\\n\"\n    \"Answer - YES, if the response for the query \\\n    is in line with context information otherwise NO.\\n\"\n    \"Query and Response: \\n {query_str}\\n\"\n    \"Context: \\n {context_str}\\n\"\n    \"Answer: \"\n)\n\nDEFAULT_REFINE_TEMPLATE = PromptTemplate(\n    \"We want to understand if the following query and response is\"\n    \"in line with the textual and visual context information: \\n {query_str}\\n\"\n    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n    \"We have the opportunity to refine the existing answer \"\n    \"(only if needed) with some more context below.\\n\"\n    \"------------\\n\"\n    \"{context_msg}\\n\"\n    \"------------\\n\"\n    \"If the existing answer was already YES, still answer YES. \"\n    \"If the information is present in the new context, answer YES. \"\n    \"Otherwise answer NO.\\n\"\n)\n\n\nclass MultiModalRelevancyEvaluator(BaseEvaluator):\n    \"\"\"Relevancy evaluator.\n\n    Evaluates the relevancy of retrieved image and textual contexts and response to a query.\n    This evaluator considers the query string, retrieved contexts, and response string.\n\n    Args:\n        multi_modal_llm(Optional[MultiModalLLM]):\n            The Multi-Modal LLM Judge to use for evaluations.\n        raise_error(Optional[bool]):\n            Whether to raise an error if the response is invalid.\n            Defaults to False.\n        eval_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for evaluation.\n        refine_template(Optional[Union[str, BasePromptTemplate]]):\n            The template to use for refinement.\n    \"\"\"\n\n    def __init__(\n        self,\n        multi_modal_llm: Union[MultiModalLLM, None] = None,\n        raise_error: bool = False,\n        eval_template: Union[str, BasePromptTemplate, None] = None,\n        refine_template: Union[str, BasePromptTemplate, None] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if multi_modal_llm is None:\n            try:\n                from llama_index.multi_modal_llms.openai import (\n                    OpenAIMultiModal,\n                )  # pants: no-infer-dep\n            except ImportError:\n                raise ImportError(\n                    \"OpenAIMultiModal is not installed. \"\n                    \"Please install it using `pip install llama-index-multi-modal-llms-openai`\"\n                )\n\n            self._multi_modal_llm: MultiModalLLM = OpenAIMultiModal(\n                model=\"gpt-4-vision-preview\", max_new_tokens=1000\n            )\n        else:\n            self._multi_modal_llm = multi_modal_llm\n\n        self._raise_error = raise_error\n\n        self._eval_template: BasePromptTemplate\n        if isinstance(eval_template, str):\n            self._eval_template = PromptTemplate(eval_template)\n        else:\n            self._eval_template = eval_template or DEFAULT_EVAL_TEMPLATE\n\n        self._refine_template: BasePromptTemplate\n        if isinstance(refine_template, str):\n            self._refine_template = PromptTemplate(refine_template)\n        else:\n            self._refine_template = refine_template or DEFAULT_REFINE_TEMPLATE\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"eval_template\": self._eval_template,\n            \"refine_template\": self._refine_template,\n        }\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"eval_template\" in prompts:\n            self._eval_template = prompts[\"eval_template\"]\n        if \"refine_template\" in prompts:\n            self._refine_template = prompts[\"refine_template\"]\n\n    def evaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,\n        contexts: Union[Sequence[str], None] = None,\n        image_paths: Union[List[str], None] = None,\n        image_urls: Union[List[str], None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Evaluate whether the multi-modal contexts and response are relevant to the query.\"\"\"\n        del kwargs  # Unused\n\n        if query is None or contexts is None or response is None:\n            raise ValueError(\"query, contexts, and response must be provided\")\n\n        context_str = \"\\n\\n\".join(contexts)\n        evaluation_query_str = f\"Question: {query}\\nResponse: {response}\"\n        fmt_prompt = self._eval_template.format(\n            context_str=context_str, query_str=evaluation_query_str\n        )\n\n        if image_paths:\n            image_nodes = [\n                ImageNode(image_path=image_path) for image_path in image_paths\n            ]\n        if image_urls:\n            image_nodes = [ImageNode(image_url=image_url) for image_url in image_urls]\n\n        response_obj = self._multi_modal_llm.complete(\n            prompt=fmt_prompt,\n            image_documents=image_nodes,\n        )\n\n        raw_response_txt = str(response_obj)\n\n        if \"yes\" in raw_response_txt.lower():\n            passing = True\n        else:\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n            passing = False\n\n        return EvaluationResult(\n            query=query,\n            response=response,\n            passing=passing,\n            score=1.0 if passing else 0.0,\n            feedback=raw_response_txt,\n        )\n\n    async def aevaluate(\n        self,\n        query: Union[str, None] = None,\n        response: Union[str, None] = None,\n        contexts: Union[Sequence[str], None] = None,\n        image_paths: Union[List[str], None] = None,\n        image_urls: Union[List[str], None] = None,\n        **kwargs: Any,\n    ) -> EvaluationResult:\n        \"\"\"Async evaluate whether the multi-modal contexts and response are relevant to the query.\"\"\"\n        del kwargs  # Unused\n\n        if query is None or contexts is None or response is None:\n            raise ValueError(\"query, contexts, and response must be provided\")\n\n        context_str = \"\\n\\n\".join(contexts)\n        evaluation_query_str = f\"Question: {query}\\nResponse: {response}\"\n        fmt_prompt = self._eval_template.format(\n            context_str=context_str, query_str=evaluation_query_str\n        )\n\n        if image_paths:\n            image_nodes = [\n                ImageNode(image_path=image_path) for image_path in image_paths\n            ]\n        if image_urls:\n            image_nodes = [ImageNode(image_url=image_url) for image_url in image_urls]\n\n        response_obj = await self._multi_modal_llm.acomplete(\n            prompt=fmt_prompt,\n            image_documents=image_nodes,\n        )\n\n        raw_response_txt = str(response_obj)\n\n        if \"yes\" in raw_response_txt.lower():\n            passing = True\n        else:\n            if self._raise_error:\n                raise ValueError(\"The response is invalid\")\n            passing = False\n\n        return EvaluationResult(\n            query=query,\n            response=response,\n            passing=passing,\n            score=1.0 if passing else 0.0,\n            feedback=raw_response_txt,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/retrieval/evaluator.py",
    "filename": "evaluator.py",
    "relpath": "evaluation/retrieval/evaluator.py",
    "start_line": 1,
    "end_line": 101,
    "length": 101,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_aget_retrieved_ids_and_texts",
      "_aget_retrieved_ids_and_texts"
    ],
    "chunk_class_names": [
      "RetrieverEvaluator",
      "MultiModalRetrieverEvaluator"
    ],
    "document_function_names": [
      "_aget_retrieved_ids_and_texts",
      "_aget_retrieved_ids_and_texts"
    ],
    "document_class_names": [
      "RetrieverEvaluator",
      "MultiModalRetrieverEvaluator"
    ],
    "content": "\"\"\"Retrieval evaluators.\"\"\"\n\nfrom typing import List, Optional, Tuple\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.bridge.pydantic import Field, SerializeAsAny\nfrom llama_index.core.evaluation.retrieval.base import (\n    BaseRetrievalEvaluator,\n    RetrievalEvalMode,\n)\nfrom llama_index.core.indices.base_retriever import BaseRetriever\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import ImageNode, TextNode\n\n\nclass RetrieverEvaluator(BaseRetrievalEvaluator):\n    \"\"\"Retriever evaluator.\n\n    This module will evaluate a retriever using a set of metrics.\n\n    Args:\n        metrics (List[BaseRetrievalMetric]): Sequence of metrics to evaluate\n        retriever: Retriever to evaluate.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Post-processor to apply after retrieval.\n\n\n    \"\"\"\n\n    retriever: BaseRetriever = Field(..., description=\"Retriever to evaluate\")\n    node_postprocessors: Optional[List[SerializeAsAny[BaseNodePostprocessor]]] = Field(\n        default=None, description=\"Optional post-processor\"\n    )\n\n    async def _aget_retrieved_ids_and_texts(\n        self, query: str, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT\n    ) -> Tuple[List[str], List[str]]:\n        \"\"\"Get retrieved ids and texts, potentially applying a post-processor.\"\"\"\n        retrieved_nodes = await self.retriever.aretrieve(query)\n\n        if self.node_postprocessors:\n            for node_postprocessor in self.node_postprocessors:\n                retrieved_nodes = node_postprocessor.postprocess_nodes(\n                    retrieved_nodes, query_str=query\n                )\n\n        return (\n            [node.node.node_id for node in retrieved_nodes],\n            [node.text for node in retrieved_nodes],\n        )\n\n\nclass MultiModalRetrieverEvaluator(BaseRetrievalEvaluator):\n    \"\"\"Retriever evaluator.\n\n    This module will evaluate a retriever using a set of metrics.\n\n    Args:\n        metrics (List[BaseRetrievalMetric]): Sequence of metrics to evaluate\n        retriever: Retriever to evaluate.\n        node_postprocessors (Optional[List[BaseNodePostprocessor]]): Post-processor to apply after retrieval.\n\n    \"\"\"\n\n    retriever: BaseRetriever = Field(..., description=\"Retriever to evaluate\")\n    node_postprocessors: Optional[List[SerializeAsAny[BaseNodePostprocessor]]] = Field(\n        default=None, description=\"Optional post-processor\"\n    )\n\n    async def _aget_retrieved_ids_and_texts(\n        self, query: str, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT\n    ) -> Tuple[List[str], List[str]]:\n        \"\"\"Get retrieved ids.\"\"\"\n        retrieved_nodes = await self.retriever.aretrieve(query)\n        image_nodes: List[ImageNode] = []\n        text_nodes: List[TextNode] = []\n\n        if self.node_postprocessors:\n            for node_postprocessor in self.node_postprocessors:\n                retrieved_nodes = node_postprocessor.postprocess_nodes(\n                    retrieved_nodes, query_str=query\n                )\n\n        for scored_node in retrieved_nodes:\n            node = scored_node.node\n            if isinstance(node, ImageNode):\n                image_nodes.append(node)\n            if isinstance(node, TextNode):\n                text_nodes.append(node)\n\n        if mode == \"text\":\n            return (\n                [node.node_id for node in text_nodes],\n                [node.text for node in text_nodes],\n            )\n        elif mode == \"image\":\n            return (\n                [node.node_id for node in image_nodes],\n                [node.text for node in image_nodes],\n            )\n        else:\n            raise ValueError(\"Unsupported mode.\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/retrieval/base.py",
    "filename": "base.py",
    "relpath": "evaluation/retrieval/base.py",
    "start_line": 1,
    "end_line": 196,
    "length": 196,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_str",
      "metric_vals_dict",
      "__str__",
      "from_metric_names",
      "_aget_retrieved_ids_and_texts",
      "evaluate",
      "aevaluate",
      "aevaluate_dataset",
      "eval_worker"
    ],
    "chunk_class_names": [
      "RetrievalEvalMode",
      "RetrievalEvalResult",
      "BaseRetrievalEvaluator"
    ],
    "document_function_names": [
      "from_str",
      "metric_vals_dict",
      "__str__",
      "from_metric_names",
      "_aget_retrieved_ids_and_texts",
      "evaluate",
      "aevaluate",
      "aevaluate_dataset",
      "eval_worker"
    ],
    "document_class_names": [
      "RetrievalEvalMode",
      "RetrievalEvalResult",
      "BaseRetrievalEvaluator"
    ],
    "content": "\"\"\"Base retrieval abstractions.\"\"\"\n\nimport asyncio\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom llama_index.core.async_utils import asyncio_run\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.evaluation.retrieval.metrics import resolve_metrics\nfrom llama_index.core.evaluation.retrieval.metrics_base import (\n    BaseRetrievalMetric,\n    RetrievalMetricResult,\n)\nfrom llama_index.core.llama_dataset.legacy.embedding import (\n    EmbeddingQAFinetuneDataset,\n)\n\n\nclass RetrievalEvalMode(str, Enum):\n    \"\"\"Evaluation of retrieval modality.\"\"\"\n\n    TEXT = \"text\"\n    IMAGE = \"image\"\n\n    @classmethod\n    def from_str(cls, label: str) -> \"RetrievalEvalMode\":\n        if label == \"text\":\n            return RetrievalEvalMode.TEXT\n        elif label == \"image\":\n            return RetrievalEvalMode.IMAGE\n        else:\n            raise NotImplementedError\n\n\nclass RetrievalEvalResult(BaseModel):\n    \"\"\"Retrieval eval result.\n\n    NOTE: this abstraction might change in the future.\n\n    Attributes:\n        query (str): Query string\n        expected_ids (List[str]): Expected ids\n        retrieved_ids (List[str]): Retrieved ids\n        metric_dict (Dict[str, BaseRetrievalMetric]): \\\n            Metric dictionary for the evaluation\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    query: str = Field(..., description=\"Query string\")\n    expected_ids: List[str] = Field(..., description=\"Expected ids\")\n    expected_texts: Optional[List[str]] = Field(\n        default=None,\n        description=\"Expected texts associated with nodes provided in `expected_ids`\",\n    )\n    retrieved_ids: List[str] = Field(..., description=\"Retrieved ids\")\n    retrieved_texts: List[str] = Field(..., description=\"Retrieved texts\")\n    mode: \"RetrievalEvalMode\" = Field(\n        default=RetrievalEvalMode.TEXT, description=\"text or image\"\n    )\n    metric_dict: Dict[str, RetrievalMetricResult] = Field(\n        ..., description=\"Metric dictionary for the evaluation\"\n    )\n\n    @property\n    def metric_vals_dict(self) -> Dict[str, float]:\n        \"\"\"Dictionary of metric values.\"\"\"\n        return {k: v.score for k, v in self.metric_dict.items()}\n\n    def __str__(self) -> str:\n        \"\"\"String representation.\"\"\"\n        return f\"Query: {self.query}\\n\" f\"Metrics: {self.metric_vals_dict!s}\\n\"\n\n\nclass BaseRetrievalEvaluator(BaseModel):\n    \"\"\"Base Retrieval Evaluator class.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    metrics: List[BaseRetrievalMetric] = Field(\n        ..., description=\"List of metrics to evaluate\"\n    )\n\n    @classmethod\n    def from_metric_names(\n        cls, metric_names: List[str], **kwargs: Any\n    ) -> \"BaseRetrievalEvaluator\":\n        \"\"\"Create evaluator from metric names.\n\n        Args:\n            metric_names (List[str]): List of metric names\n            **kwargs: Additional arguments for the evaluator\n\n        \"\"\"\n        metric_types = resolve_metrics(metric_names)\n        return cls(metrics=[metric() for metric in metric_types], **kwargs)\n\n    @abstractmethod\n    async def _aget_retrieved_ids_and_texts(\n        self, query: str, mode: RetrievalEvalMode = RetrievalEvalMode.TEXT\n    ) -> Tuple[List[str], List[str]]:\n        \"\"\"Get retrieved ids and texts.\"\"\"\n        raise NotImplementedError\n\n    def evaluate(\n        self,\n        query: str,\n        expected_ids: List[str],\n        expected_texts: Optional[List[str]] = None,\n        mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,\n        **kwargs: Any,\n    ) -> RetrievalEvalResult:\n        \"\"\"Run evaluation results with query string and expected ids.\n\n        Args:\n            query (str): Query string\n            expected_ids (List[str]): Expected ids\n\n        Returns:\n            RetrievalEvalResult: Evaluation result\n\n        \"\"\"\n        return asyncio_run(\n            self.aevaluate(\n                query=query,\n                expected_ids=expected_ids,\n                expected_texts=expected_texts,\n                mode=mode,\n                **kwargs,\n            )\n        )\n\n    # @abstractmethod\n    async def aevaluate(\n        self,\n        query: str,\n        expected_ids: List[str],\n        expected_texts: Optional[List[str]] = None,\n        mode: RetrievalEvalMode = RetrievalEvalMode.TEXT,\n        **kwargs: Any,\n    ) -> RetrievalEvalResult:\n        \"\"\"Run evaluation with query string, retrieved contexts,\n        and generated response string.\n\n        Subclasses can override this method to provide custom evaluation logic and\n        take in additional arguments.\n        \"\"\"\n        retrieved_ids, retrieved_texts = await self._aget_retrieved_ids_and_texts(\n            query, mode\n        )\n        metric_dict = {}\n        for metric in self.metrics:\n            eval_result = metric.compute(\n                query, expected_ids, retrieved_ids, expected_texts, retrieved_texts\n            )\n            metric_dict[metric.metric_name] = eval_result\n\n        return RetrievalEvalResult(\n            query=query,\n            expected_ids=expected_ids,\n            expected_texts=expected_texts,\n            retrieved_ids=retrieved_ids,\n            retrieved_texts=retrieved_texts,\n            mode=mode,\n            metric_dict=metric_dict,\n        )\n\n    async def aevaluate_dataset(\n        self,\n        dataset: EmbeddingQAFinetuneDataset,\n        workers: int = 2,\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[RetrievalEvalResult]:\n        \"\"\"Run evaluation with dataset.\"\"\"\n        semaphore = asyncio.Semaphore(workers)\n\n        async def eval_worker(\n            query: str, expected_ids: List[str], mode: RetrievalEvalMode\n        ) -> RetrievalEvalResult:\n            async with semaphore:\n                return await self.aevaluate(query, expected_ids=expected_ids, mode=mode)\n\n        response_jobs = []\n        mode = RetrievalEvalMode.from_str(dataset.mode)\n        for query_id, query in dataset.queries.items():\n            expected_ids = dataset.relevant_docs[query_id]\n            response_jobs.append(eval_worker(query, expected_ids, mode))\n        if show_progress:\n            from tqdm.asyncio import tqdm_asyncio\n\n            eval_results = await tqdm_asyncio.gather(*response_jobs)\n        else:\n            eval_results = await asyncio.gather(*response_jobs)\n\n        return eval_results"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/retrieval/metrics.py",
    "filename": "metrics.py",
    "relpath": "evaluation/retrieval/metrics.py",
    "start_line": 1,
    "end_line": 385,
    "length": 385,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "compute",
      "compute",
      "compute",
      "compute",
      "compute",
      "discounted_gain",
      "compute"
    ],
    "chunk_class_names": [
      "HitRate",
      "MRR",
      "Precision",
      "Recall",
      "AveragePrecision",
      "NDCG"
    ],
    "document_function_names": [
      "compute",
      "compute",
      "compute",
      "compute",
      "compute",
      "discounted_gain",
      "compute",
      "__init__",
      "_get_agg_func",
      "compute",
      "resolve_metrics"
    ],
    "document_class_names": [
      "HitRate",
      "MRR",
      "Precision",
      "Recall",
      "AveragePrecision",
      "NDCG",
      "CohereRerankRelevancyMetric"
    ],
    "content": "import math\nimport os\nfrom typing import Any, Callable, ClassVar, Dict, List, Literal, Optional, Type\n\nimport numpy as np\nfrom llama_index.core.bridge.pydantic import Field, PrivateAttr\nfrom llama_index.core.evaluation.retrieval.metrics_base import (\n    BaseRetrievalMetric,\n    RetrievalMetricResult,\n)\nfrom typing_extensions import assert_never\n\n_AGG_FUNC: Dict[str, Callable] = {\"mean\": np.mean, \"median\": np.median, \"max\": np.max}\n\n\nclass HitRate(BaseRetrievalMetric):\n    \"\"\"Hit rate metric: Compute hit rate with two calculation options.\n\n    - The default method checks for a single match between any of the retrieved docs and expected docs.\n    - The more granular method checks for all potential matches between retrieved docs and expected docs.\n\n    Attributes:\n        metric_name (str): The name of the metric.\n        use_granular_hit_rate (bool): Determines whether to use the granular method for calculation.\n    \"\"\"\n\n    metric_name: ClassVar[str] = \"hit_rate\"\n    use_granular_hit_rate: bool = False\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute metric based on the provided inputs.\n\n        Parameters:\n            query (Optional[str]): The query string (not used in the current implementation).\n            expected_ids (Optional[List[str]]): Expected document IDs.\n            retrieved_ids (Optional[List[str]]): Retrieved document IDs.\n            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).\n            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).\n\n        Raises:\n            ValueError: If the necessary IDs are not provided.\n\n        Returns:\n            RetrievalMetricResult: The result with the computed hit rate score.\n        \"\"\"\n        # Checking for the required arguments\n        if (\n            retrieved_ids is None\n            or expected_ids is None\n            or not retrieved_ids\n            or not expected_ids\n        ):\n            raise ValueError(\"Retrieved ids and expected ids must be provided\")\n\n        if self.use_granular_hit_rate:\n            # Granular HitRate calculation: Calculate all hits and divide by the number of expected docs\n            expected_set = set(expected_ids)\n            hits = sum(1 for doc_id in retrieved_ids if doc_id in expected_set)\n            score = hits / len(expected_ids) if expected_ids else 0.0\n        else:\n            # Default HitRate calculation: Check if there is a single hit\n            is_hit = any(id in expected_ids for id in retrieved_ids)\n            score = 1.0 if is_hit else 0.0\n\n        return RetrievalMetricResult(score=score)\n\n\nclass MRR(BaseRetrievalMetric):\n    \"\"\"MRR (Mean Reciprocal Rank) metric with two calculation options.\n\n    - The default method calculates the reciprocal rank of the first relevant retrieved document.\n    - The more granular method sums the reciprocal ranks of all relevant retrieved documents and divides by the count of relevant documents.\n\n    Attributes:\n        metric_name (str): The name of the metric.\n        use_granular_mrr (bool): Determines whether to use the granular method for calculation.\n    \"\"\"\n\n    metric_name: ClassVar[str] = \"mrr\"\n    use_granular_mrr: bool = False\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute MRR based on the provided inputs and selected method.\n\n        Parameters:\n            query (Optional[str]): The query string (not used in the current implementation).\n            expected_ids (Optional[List[str]]): Expected document IDs.\n            retrieved_ids (Optional[List[str]]): Retrieved document IDs.\n            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).\n            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).\n\n        Raises:\n            ValueError: If the necessary IDs are not provided.\n\n        Returns:\n            RetrievalMetricResult: The result with the computed MRR score.\n        \"\"\"\n        # Checking for the required arguments\n        if (\n            retrieved_ids is None\n            or expected_ids is None\n            or not retrieved_ids\n            or not expected_ids\n        ):\n            raise ValueError(\"Retrieved ids and expected ids must be provided\")\n\n        if self.use_granular_mrr:\n            # Granular MRR calculation: All relevant retrieved docs have their reciprocal ranks summed and averaged\n            expected_set = set(expected_ids)\n            reciprocal_rank_sum = 0.0\n            relevant_docs_count = 0\n            for index, doc_id in enumerate(retrieved_ids):\n                if doc_id in expected_set:\n                    relevant_docs_count += 1\n                    reciprocal_rank_sum += 1.0 / (index + 1)\n            mrr_score = (\n                reciprocal_rank_sum / relevant_docs_count\n                if relevant_docs_count > 0\n                else 0.0\n            )\n        else:\n            # Default MRR calculation: Reciprocal rank of the first relevant document retrieved\n            for i, id in enumerate(retrieved_ids):\n                if id in expected_ids:\n                    return RetrievalMetricResult(score=1.0 / (i + 1))\n            mrr_score = 0.0\n\n        return RetrievalMetricResult(score=mrr_score)\n\n\nclass Precision(BaseRetrievalMetric):\n    \"\"\"Precision metric.\n\n    The `K`-value in `Precision@K` usually corresponds to `top_k` of the retriever.\n\n    Attributes:\n        metric_name (str): The name of the metric.\n    \"\"\"\n\n    metric_name: ClassVar[str] = \"precision\"\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute precision based on the provided inputs and selected method.\n\n        Parameters:\n            query (Optional[str]): The query string (not used in the current implementation).\n            expected_ids (Optional[List[str]]): Expected document IDs.\n            retrieved_ids (Optional[List[str]]): Retrieved document IDs.\n            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).\n            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).\n\n        Raises:\n            ValueError: If the necessary IDs are not provided.\n\n        Returns:\n            RetrievalMetricResult: The result with the computed precision score.\n        \"\"\"\n        # Checking for the required arguments\n        if (\n            retrieved_ids is None\n            or expected_ids is None\n            or not retrieved_ids\n            or not expected_ids\n        ):\n            raise ValueError(\"Retrieved ids and expected ids must be provided\")\n\n        retrieved_set = set(retrieved_ids)\n        expected_set = set(expected_ids)\n        precision = len(retrieved_set & expected_set) / len(retrieved_set)\n\n        return RetrievalMetricResult(score=precision)\n\n\nclass Recall(BaseRetrievalMetric):\n    \"\"\"Recall metric.\n\n    Attributes:\n        metric_name (str): The name of the metric.\n    \"\"\"\n\n    metric_name: ClassVar[str] = \"recall\"\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute recall based on the provided inputs and selected method.\n\n        Parameters:\n            query (Optional[str]): The query string (not used in the current implementation).\n            expected_ids (Optional[List[str]]): Expected document IDs.\n            retrieved_ids (Optional[List[str]]): Retrieved document IDs.\n            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).\n            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).\n\n        Raises:\n            ValueError: If the necessary IDs are not provided.\n\n        Returns:\n            RetrievalMetricResult: The result with the computed recall score.\n        \"\"\"\n        # Checking for the required arguments\n        if (\n            retrieved_ids is None\n            or expected_ids is None\n            or not retrieved_ids\n            or not expected_ids\n        ):\n            raise ValueError(\"Retrieved ids and expected ids must be provided\")\n\n        retrieved_set = set(retrieved_ids)\n        expected_set = set(expected_ids)\n        recall = len(retrieved_set & expected_set) / len(expected_set)\n\n        return RetrievalMetricResult(score=recall)\n\n\nclass AveragePrecision(BaseRetrievalMetric):\n    \"\"\"Average Precision (AP) metric.\n\n    Attributes:\n        metric_name (str): The name of the metric.\n    \"\"\"\n\n    metric_name: ClassVar[str] = \"ap\"\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute average precision based on the provided inputs and selected method.\n\n        Parameters:\n            query (Optional[str]): The query string (not used in the current implementation).\n            expected_ids (Optional[List[str]]): Expected document IDs.\n            retrieved_ids (Optional[List[str]]): Retrieved document IDs, ordered by relevance from highest to lowest.\n            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).\n            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).\n\n        Raises:\n            ValueError: If the necessary IDs are not provided.\n\n        Returns:\n            RetrievalMetricResult: The result with the computed average precision score.\n        \"\"\"\n        # Checking for the required arguments\n        if (\n            retrieved_ids is None\n            or expected_ids is None\n            or not retrieved_ids\n            or not expected_ids\n        ):\n            raise ValueError(\"Retrieved ids and expected ids must be provided\")\n\n        expected_set = set(expected_ids)\n\n        relevant_count, total_precision = 0, 0.0\n        for i, retrieved_id in enumerate(retrieved_ids, start=1):\n            if retrieved_id in expected_set:\n                relevant_count += 1\n                total_precision += relevant_count / i\n\n        average_precision = total_precision / len(expected_set)\n\n        return RetrievalMetricResult(score=average_precision)\n\n\nDiscountedGainMode = Literal[\"linear\", \"exponential\"]\n\n\ndef discounted_gain(*, rel: float, i: int, mode: DiscountedGainMode) -> float:\n    # Avoid unnecessary calculations. Note that `False == 0` and `True == 1`.\n    if rel == 0:\n        return 0\n    if rel == 1:\n        return 1 / math.log2(i + 1)\n\n    if mode == \"linear\":\n        return rel / math.log2(i + 1)\n    elif mode == \"exponential\":\n        return (2**rel - 1) / math.log2(i + 1)\n    else:\n        assert_never(mode)\n\n\nclass NDCG(BaseRetrievalMetric):\n    \"\"\"NDCG (Normalized Discounted Cumulative Gain) metric.\n\n    The position `p` is taken as the size of the query results (which is usually\n    `top_k` of the retriever).\n\n    Currently only supports binary relevance\n    (``rel=1`` if document is in ``expected_ids``, otherwise ``rel=0``)\n    since we assume that ``expected_ids`` is unordered.\n\n    Attributes:\n        metric_name (str): The name of the metric.\n        mode (DiscountedGainMode): Determines the formula for each item in the summation.\n    \"\"\"\n\n    metric_name: ClassVar[str] = \"ndcg\"\n    mode: DiscountedGainMode = \"linear\"\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute NDCG based on the provided inputs and selected method.\n\n        Parameters:\n            query (Optional[str]): The query string (not used in the current implementation).\n            expected_ids (Optional[List[str]]): Expected document IDs, unordered by relevance.\n            retrieved_ids (Optional[List[str]]): Retrieved document IDs, ordered by relevance from highest to lowest.\n            expected_texts (Optional[List[str]]): Expected texts (not used in the current implementation).\n            retrieved_texts (Optional[List[str]]): Retrieved texts (not used in the current implementation).\n\n        Raises:\n            ValueError: If the necessary IDs are not provided.\n\n        Returns:\n            RetrievalMetricResult: The result with the computed NDCG score.\n        \"\"\"\n        # Checking for the required arguments\n        if (\n            retrieved_ids is None\n            or expected_ids is None\n            or not retrieved_ids\n            or not expected_ids\n        ):\n            raise ValueError(\"Retrieved ids and expected ids must be provided\")\n\n        mode = self.mode\n        expected_set = set(expected_ids)\n\n        dcg = sum(\n            discounted_gain(rel=docid in expected_set, i=i, mode=mode)\n            for i, docid in enumerate(retrieved_ids, start=1)\n        )\n\n        idcg = sum(\n            discounted_gain(rel=True, i=i, mode=mode)\n            for i in range(1, len(expected_ids) + 1)\n        )\n\n        ndcg_score = dcg / idcg\n        return RetrievalMetricResult(score=ndcg_score)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/retrieval/metrics.py",
    "filename": "metrics.py",
    "relpath": "evaluation/retrieval/metrics.py",
    "start_line": 385,
    "end_line": 471,
    "length": 87,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_agg_func",
      "compute",
      "resolve_metrics"
    ],
    "chunk_class_names": [
      "CohereRerankRelevancyMetric"
    ],
    "document_function_names": [
      "compute",
      "compute",
      "compute",
      "compute",
      "compute",
      "discounted_gain",
      "compute",
      "__init__",
      "_get_agg_func",
      "compute",
      "resolve_metrics"
    ],
    "document_class_names": [
      "HitRate",
      "MRR",
      "Precision",
      "Recall",
      "AveragePrecision",
      "NDCG",
      "CohereRerankRelevancyMetric"
    ],
    "content": "class CohereRerankRelevancyMetric(BaseRetrievalMetric):\n    \"\"\"Cohere rerank relevancy metric.\"\"\"\n\n    metric_name: ClassVar[str] = \"cohere_rerank_relevancy\"\n    model: str = Field(description=\"Cohere model name.\")\n\n    _client: Any = PrivateAttr()\n\n    def __init__(\n        self,\n        model: str = \"rerank-english-v2.0\",\n        api_key: Optional[str] = None,\n    ):\n        try:\n            api_key = api_key or os.environ[\"COHERE_API_KEY\"]\n        except IndexError:\n            raise ValueError(\n                \"Must pass in cohere api key or \"\n                \"specify via COHERE_API_KEY environment variable \"\n            )\n        try:\n            from cohere import Client  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"Cannot import cohere package, please `pip install cohere`.\"\n            )\n\n        super().__init__(model=model)\n        self._client = Client(api_key=api_key)\n\n    def _get_agg_func(self, agg: Literal[\"max\", \"median\", \"mean\"]) -> Callable:\n        \"\"\"Get agg func.\"\"\"\n        return _AGG_FUNC[agg]\n\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        agg: Literal[\"max\", \"median\", \"mean\"] = \"max\",\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute metric.\"\"\"\n        del expected_texts  # unused\n\n        if retrieved_texts is None:\n            raise ValueError(\"Retrieved texts must be provided\")\n\n        results = self._client.rerank(\n            model=self.model,\n            top_n=len(\n                retrieved_texts\n            ),  # i.e. get a rank score for each retrieved chunk\n            query=query,\n            documents=retrieved_texts,\n        )\n        relevance_scores = [r.relevance_score for r in results.results]\n        agg_func = self._get_agg_func(agg)\n\n        return RetrievalMetricResult(\n            score=agg_func(relevance_scores), metadata={\"agg\": agg}\n        )\n\n\nMETRIC_REGISTRY: Dict[str, Type[BaseRetrievalMetric]] = {\n    \"hit_rate\": HitRate,\n    \"mrr\": MRR,\n    \"precision\": Precision,\n    \"recall\": Recall,\n    \"ap\": AveragePrecision,\n    \"ndcg\": NDCG,\n    \"cohere_rerank_relevancy\": CohereRerankRelevancyMetric,\n}\n\n\ndef resolve_metrics(metrics: List[str]) -> List[Type[BaseRetrievalMetric]]:\n    \"\"\"Resolve metrics from list of metric names.\"\"\"\n    for metric in metrics:\n        if metric not in METRIC_REGISTRY:\n            raise ValueError(f\"Invalid metric name: {metric}\")\n\n    return [METRIC_REGISTRY[metric] for metric in metrics]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/retrieval/metrics_base.py",
    "filename": "metrics_base.py",
    "relpath": "evaluation/retrieval/metrics_base.py",
    "start_line": 1,
    "end_line": 54,
    "length": 54,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__str__",
      "__float__",
      "compute"
    ],
    "chunk_class_names": [
      "RetrievalMetricResult",
      "BaseRetrievalMetric",
      "for"
    ],
    "document_function_names": [
      "__str__",
      "__float__",
      "compute"
    ],
    "document_class_names": [
      "RetrievalMetricResult",
      "BaseRetrievalMetric",
      "for"
    ],
    "content": "from abc import ABC, abstractmethod\nfrom typing import Any, ClassVar, Dict, List, Optional\n\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\n\n\nclass RetrievalMetricResult(BaseModel):\n    \"\"\"Metric result.\n\n    Attributes:\n        score (float): Score for the metric\n        metadata (Dict[str, Any]): Metadata for the metric result\n\n    \"\"\"\n\n    score: float = Field(..., description=\"Score for the metric\")\n    metadata: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Metadata for the metric result\"\n    )\n\n    def __str__(self) -> str:\n        \"\"\"String representation.\"\"\"\n        return f\"Score: {self.score}\\nMetadata: {self.metadata}\"\n\n    def __float__(self) -> float:\n        \"\"\"Float representation.\"\"\"\n        return self.score\n\n\nclass BaseRetrievalMetric(BaseModel, ABC):\n    \"\"\"Base class for retrieval metrics.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    metric_name: ClassVar[str]\n\n    @abstractmethod\n    def compute(\n        self,\n        query: Optional[str] = None,\n        expected_ids: Optional[List[str]] = None,\n        retrieved_ids: Optional[List[str]] = None,\n        expected_texts: Optional[List[str]] = None,\n        retrieved_texts: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> RetrievalMetricResult:\n        \"\"\"Compute metric.\n\n        Args:\n            query (Optional[str]): Query string\n            expected_ids (Optional[List[str]]): Expected ids\n            retrieved_ids (Optional[List[str]]): Retrieved ids\n            **kwargs: Additional keyword arguments\n\n        \"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/__init__.py",
    "filename": "__init__.py",
    "relpath": "evaluation/benchmarks/__init__.py",
    "start_line": 1,
    "end_line": 4,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.evaluation.benchmarks.beir import BeirEvaluator\nfrom llama_index.core.evaluation.benchmarks.hotpotqa import HotpotQAEvaluator\n\n__all__ = [\"BeirEvaluator\", \"HotpotQAEvaluator\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/beir.py",
    "filename": "beir.py",
    "relpath": "evaluation/benchmarks/beir.py",
    "start_line": 1,
    "end_line": 109,
    "length": 109,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_download_datasets",
      "run"
    ],
    "chunk_class_names": [
      "BeirEvaluator"
    ],
    "document_function_names": [
      "__init__",
      "_download_datasets",
      "run"
    ],
    "document_class_names": [
      "BeirEvaluator"
    ],
    "content": "import os\nfrom shutil import rmtree\nfrom typing import Callable, Dict, List, Optional\n\nimport tqdm\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.postprocessor.types import BaseNodePostprocessor\nfrom llama_index.core.schema import Document, QueryBundle\nfrom llama_index.core.utils import get_cache_dir\n\n\nclass BeirEvaluator:\n    \"\"\"\n    Refer to: https://github.com/beir-cellar/beir for a full list of supported datasets\n    and a full description of BEIR.\n    \"\"\"\n\n    def __init__(self) -> None:\n        try:\n            pass\n        except ImportError:\n            raise ImportError(\n                \"Please install beir to use this feature: \" \"`pip install beir`\",\n            )\n\n    def _download_datasets(self, datasets: List[str] = [\"nfcorpus\"]) -> Dict[str, str]:\n        from beir import util\n\n        cache_dir = get_cache_dir()\n\n        dataset_paths = {}\n        for dataset in datasets:\n            dataset_full_path = os.path.join(cache_dir, \"datasets\", \"BeIR__\" + dataset)\n            if not os.path.exists(dataset_full_path):\n                url = f\"\"\"https://public.ukp.informatik.tu-darmstadt.de/thakur\\\n/BEIR/datasets/{dataset}.zip\"\"\"\n                try:\n                    util.download_and_unzip(url, dataset_full_path)\n                except Exception as e:\n                    print(\n                        \"Dataset:\", dataset, \"not found at:\", url, \"Removing cached dir\"\n                    )\n                    rmtree(dataset_full_path)\n                    raise ValueError(f\"invalid BEIR dataset: {dataset}\") from e\n\n            print(\"Dataset:\", dataset, \"downloaded at:\", dataset_full_path)\n            dataset_paths[dataset] = os.path.join(dataset_full_path, dataset)\n        return dataset_paths\n\n    def run(\n        self,\n        create_retriever: Callable[[List[Document]], BaseRetriever],\n        datasets: List[str] = [\"nfcorpus\"],\n        metrics_k_values: List[int] = [3, 10],\n        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,\n    ) -> None:\n        from beir.datasets.data_loader import GenericDataLoader\n        from beir.retrieval.evaluation import EvaluateRetrieval\n\n        dataset_paths = self._download_datasets(datasets)\n        for dataset in datasets:\n            dataset_path = dataset_paths[dataset]\n            print(\"Evaluating on dataset:\", dataset)\n            print(\"-------------------------------------\")\n\n            corpus, queries, qrels = GenericDataLoader(data_folder=dataset_path).load(\n                split=\"test\"\n            )\n\n            documents = []\n            for id, val in corpus.items():\n                doc = Document(\n                    text=val[\"text\"], metadata={\"title\": val[\"title\"], \"doc_id\": id}\n                )\n                documents.append(doc)\n\n            retriever = create_retriever(documents)\n\n            print(\"Retriever created for: \", dataset)\n\n            print(\"Evaluating retriever on questions against qrels\")\n\n            results = {}\n            for key, query in tqdm.tqdm(queries.items()):\n                nodes_with_score = retriever.retrieve(query)\n                node_postprocessors = node_postprocessors or []\n                for node_postprocessor in node_postprocessors:\n                    nodes_with_score = node_postprocessor.postprocess_nodes(\n                        nodes_with_score, query_bundle=QueryBundle(query_str=query)\n                    )\n                results[key] = {\n                    node.node.metadata[\"doc_id\"]: node.score\n                    for node in nodes_with_score\n                }\n\n            ndcg, map_, recall, precision = EvaluateRetrieval.evaluate(\n                qrels, results, metrics_k_values\n            )\n            print(\"Results for:\", dataset)\n            for k in metrics_k_values:\n                print(\n                    {\n                        f\"NDCG@{k}\": ndcg[f\"NDCG@{k}\"],\n                        f\"MAP@{k}\": map_[f\"MAP@{k}\"],\n                        f\"Recall@{k}\": recall[f\"Recall@{k}\"],\n                        f\"precision@{k}\": precision[f\"P@{k}\"],\n                    }\n                )\n            print(\"-------------------------------------\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/evaluation/benchmarks/hotpotqa.py",
    "filename": "hotpotqa.py",
    "relpath": "evaluation/benchmarks/hotpotqa.py",
    "start_line": 1,
    "end_line": 213,
    "length": 213,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_download_datasets",
      "run",
      "__init__",
      "_retrieve",
      "__str__",
      "normalize_answer",
      "remove_articles",
      "white_space_fix",
      "remove_punc",
      "lower",
      "f1_score",
      "exact_match_score"
    ],
    "chunk_class_names": [
      "HotpotQAEvaluator",
      "HotpotQARetriever"
    ],
    "document_function_names": [
      "_download_datasets",
      "run",
      "__init__",
      "_retrieve",
      "__str__",
      "normalize_answer",
      "remove_articles",
      "white_space_fix",
      "remove_punc",
      "lower",
      "f1_score",
      "exact_match_score"
    ],
    "document_class_names": [
      "HotpotQAEvaluator",
      "HotpotQARetriever"
    ],
    "content": "import json\nimport os\nimport re\nimport string\nfrom collections import Counter\nfrom shutil import rmtree\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport requests\nimport tqdm\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.query_engine.retriever_query_engine import (\n    RetrieverQueryEngine,\n)\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\nfrom llama_index.core.utils import get_cache_dir\n\nDEV_DISTRACTOR_URL = \"\"\"http://curtis.ml.cmu.edu/datasets/\\\nhotpot/hotpot_dev_distractor_v1.json\"\"\"\n\n\nclass HotpotQAEvaluator:\n    \"\"\"\n    Refer to https://hotpotqa.github.io/ for more details on the dataset.\n    \"\"\"\n\n    def _download_datasets(self) -> Dict[str, str]:\n        cache_dir = get_cache_dir()\n\n        dataset_paths = {}\n        dataset = \"hotpot_dev_distractor\"\n        dataset_full_path = os.path.join(cache_dir, \"datasets\", \"HotpotQA\")\n        if not os.path.exists(dataset_full_path):\n            url = DEV_DISTRACTOR_URL\n            try:\n                os.makedirs(dataset_full_path, exist_ok=True)\n                save_file = open(\n                    os.path.join(dataset_full_path, \"dev_distractor.json\"), \"wb\"\n                )\n                response = requests.get(url, stream=True)\n\n                # Define the size of each chunk\n                chunk_size = 1024\n\n                # Loop over the chunks and parse the JSON data\n                for chunk in tqdm.tqdm(response.iter_content(chunk_size=chunk_size)):\n                    if chunk:\n                        save_file.write(chunk)\n            except Exception as e:\n                if os.path.exists(dataset_full_path):\n                    print(\n                        \"Dataset:\", dataset, \"not found at:\", url, \"Removing cached dir\"\n                    )\n                    rmtree(dataset_full_path)\n                raise ValueError(f\"could not download {dataset} dataset\") from e\n        dataset_paths[dataset] = os.path.join(dataset_full_path, \"dev_distractor.json\")\n        print(\"Dataset:\", dataset, \"downloaded at:\", dataset_full_path)\n        return dataset_paths\n\n    def run(\n        self,\n        query_engine: BaseQueryEngine,\n        queries: int = 10,\n        queries_fraction: Optional[float] = None,\n        show_result: bool = False,\n    ) -> None:\n        dataset_paths = self._download_datasets()\n        dataset = \"hotpot_dev_distractor\"\n        dataset_path = dataset_paths[dataset]\n        print(\"Evaluating on dataset:\", dataset)\n        print(\"-------------------------------------\")\n\n        f = open(dataset_path)\n        query_objects = json.loads(f.read())\n        if queries_fraction:\n            queries_to_load = int(len(query_objects) * queries_fraction)\n        else:\n            queries_to_load = queries\n            queries_fraction = round(queries / len(query_objects), 5)\n\n        print(\n            f\"Loading {queries_to_load} queries out of \\\n{len(query_objects)} (fraction: {queries_fraction})\"\n        )\n        query_objects = query_objects[:queries_to_load]\n\n        assert isinstance(\n            query_engine, RetrieverQueryEngine\n        ), \"query_engine must be a RetrieverQueryEngine for this evaluation\"\n        retriever = HotpotQARetriever(query_objects)\n        # Mock the query engine with a retriever\n        query_engine = query_engine.with_retriever(retriever=retriever)\n\n        scores = {\"exact_match\": 0.0, \"f1\": 0.0}\n\n        for query in query_objects:\n            query_bundle = QueryBundle(\n                query_str=query[\"question\"]\n                + \" Give a short factoid answer (as few words as possible).\",\n                custom_embedding_strs=[query[\"question\"]],\n            )\n            response = query_engine.query(query_bundle)\n            em = int(\n                exact_match_score(\n                    prediction=str(response), ground_truth=query[\"answer\"]\n                )\n            )\n            f1, _, _ = f1_score(prediction=str(response), ground_truth=query[\"answer\"])\n            scores[\"exact_match\"] += em\n            scores[\"f1\"] += f1\n            if show_result:\n                print(\"Question: \", query[\"question\"])\n                print(\"Response:\", response)\n                print(\"Correct answer: \", query[\"answer\"])\n                print(\"EM:\", em, \"F1:\", f1)\n                print(\"-------------------------------------\")\n\n        for score in scores:\n            scores[score] /= len(query_objects)\n\n        print(\"Scores: \", scores)\n\n\nclass HotpotQARetriever(BaseRetriever):\n    \"\"\"\n    This is a mocked retriever for HotpotQA dataset. It is only meant to be used\n    with the hotpotqa dev dataset in the distractor setting. This is the setting that\n    does not require retrieval but requires identifying the supporting facts from\n    a list of 10 sources.\n    \"\"\"\n\n    def __init__(self, query_objects: Any) -> None:\n        assert isinstance(\n            query_objects,\n            list,\n        ), f\"query_objects must be a list, got: {type(query_objects)}\"\n        self._queries = {}\n        for object in query_objects:\n            self._queries[object[\"question\"]] = object\n\n    def _retrieve(self, query: QueryBundle) -> List[NodeWithScore]:\n        if query.custom_embedding_strs:\n            query_str = query.custom_embedding_strs[0]\n        else:\n            query_str = query.query_str\n        contexts = self._queries[query_str][\"context\"]\n        node_with_scores = []\n        for ctx in contexts:\n            text_list = ctx[1]\n            text = \"\\n\".join(text_list)\n            node = TextNode(text=text, metadata={\"title\": ctx[0]})\n            node_with_scores.append(NodeWithScore(node=node, score=1.0))\n\n        return node_with_scores\n\n    def __str__(self) -> str:\n        return \"HotpotQARetriever\"\n\n\n\"\"\"\nUtils from https://github.com/hotpotqa/hotpot/blob/master/hotpot_evaluate_v1.py\n\"\"\"\n\n\ndef normalize_answer(s: str) -> str:\n    def remove_articles(text: str) -> str:\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\n    def white_space_fix(text: str) -> str:\n        return \" \".join(text.split())\n\n    def remove_punc(text: str) -> str:\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text: str) -> str:\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction: str, ground_truth: str) -> Tuple[float, float, float]:\n    normalized_prediction = normalize_answer(prediction)\n    normalized_ground_truth = normalize_answer(ground_truth)\n\n    ZERO_METRIC = (0, 0, 0)\n\n    if (\n        normalized_prediction in [\"yes\", \"no\", \"noanswer\"]\n        and normalized_prediction != normalized_ground_truth\n    ):\n        return ZERO_METRIC\n    if (\n        normalized_ground_truth in [\"yes\", \"no\", \"noanswer\"]\n        and normalized_prediction != normalized_ground_truth\n    ):\n        return ZERO_METRIC\n\n    prediction_tokens = normalized_prediction.split()\n    ground_truth_tokens = normalized_ground_truth.split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return ZERO_METRIC\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1, precision, recall\n\n\ndef exact_match_score(prediction: str, ground_truth: str) -> bool:\n    return normalize_answer(prediction) == normalize_answer(ground_truth)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/types.py",
    "filename": "types.py",
    "relpath": "agent/types.py",
    "start_line": 1,
    "end_line": 17,
    "length": 17,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Base agent types.\"\"\"\n\nfrom llama_index.core.base.agent.types import (\n    TaskStep,\n    TaskStepOutput,\n    Task,\n    BaseAgent,\n    BaseAgentWorker,\n)\n\n__all__ = [\n    \"TaskStep\",\n    \"TaskStepOutput\",\n    \"Task\",\n    \"BaseAgent\",\n    \"BaseAgentWorker\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/__init__.py",
    "filename": "__init__.py",
    "relpath": "agent/__init__.py",
    "start_line": 1,
    "end_line": 38,
    "length": 38,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "# agent runner + agent worker\nfrom llama_index.core.agent.custom.pipeline_worker import QueryPipelineAgentWorker\nfrom llama_index.core.agent.custom.simple import CustomSimpleAgentWorker\nfrom llama_index.core.agent.custom.simple_function import FnAgentWorker\nfrom llama_index.core.agent.react.base import ReActAgent\nfrom llama_index.core.agent.react.formatter import ReActChatFormatter\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.agent.react.step import ReActAgentWorker\nfrom llama_index.core.agent.react_multimodal.step import MultimodalReActAgentWorker\nfrom llama_index.core.agent.runner.base import AgentRunner\nfrom llama_index.core.agent.runner.planner import StructuredPlannerAgent\nfrom llama_index.core.agent.runner.parallel import ParallelAgentRunner\nfrom llama_index.core.agent.types import Task\nfrom llama_index.core.chat_engine.types import AgentChatResponse\nfrom llama_index.core.agent.function_calling.base import FunctionCallingAgent\nfrom llama_index.core.agent.function_calling.step import FunctionCallingAgentWorker\n\n__all__ = [\n    \"AgentRunner\",\n    \"StructuredPlannerAgent\",\n    \"ParallelAgentRunner\",\n    \"ReActAgentWorker\",\n    \"ReActAgent\",\n    \"ReActOutputParser\",\n    \"CustomSimpleAgentWorker\",\n    \"QueryPipelineAgentWorker\",\n    \"ReActChatFormatter\",\n    \"FunctionCallingAgentWorker\",\n    \"FnAgentWorker\",\n    \"FunctionCallingAgent\",\n    # beta\n    \"MultimodalReActAgentWorker\",\n    # schema-related\n    \"AgentChatResponse\",\n    \"Task\",\n    \"TaskStep\",\n    \"TaskStepOutput\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/utils.py",
    "filename": "utils.py",
    "relpath": "agent/utils.py",
    "start_line": 1,
    "end_line": 15,
    "length": 15,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "add_user_step_to_memory"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "add_user_step_to_memory"
    ],
    "document_class_names": [],
    "content": "\"\"\"Agent utils.\"\"\"\n\nfrom llama_index.core.agent.types import TaskStep\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.memory import BaseMemory\n\n\ndef add_user_step_to_memory(\n    step: TaskStep, memory: BaseMemory, verbose: bool = False\n) -> None:\n    \"\"\"Add user step to memory.\"\"\"\n    user_message = ChatMessage(content=step.input, role=MessageRole.USER)\n    memory.put(user_message)\n    if verbose:\n        print(f\"Added user message to memory: {step.input}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/function_calling/base.py",
    "filename": "base.py",
    "relpath": "agent/function_calling/base.py",
    "start_line": 1,
    "end_line": 82,
    "length": 82,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_tools"
    ],
    "chunk_class_names": [
      "FunctionCallingAgent"
    ],
    "document_function_names": [
      "from_tools"
    ],
    "document_class_names": [
      "FunctionCallingAgent"
    ],
    "content": "\"\"\"Function calling agent.\"\"\"\n\nfrom typing import Any, List, Optional\n\nfrom llama_index.core.agent.runner.base import AgentRunner, AgentState\nfrom llama_index.core.agent.function_calling.step import (\n    FunctionCallingAgentWorker,\n    DEFAULT_MAX_FUNCTION_CALLS,\n)\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.llms.function_calling import FunctionCallingLLM\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.types import BaseTool\n\n\nclass FunctionCallingAgent(AgentRunner):\n    \"\"\"Function calling agent.\n\n    Light wrapper around AgentRunner.\n    \"\"\"\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: Optional[List[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        llm: Optional[FunctionCallingLLM] = None,\n        verbose: bool = False,\n        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,\n        callback_manager: Optional[CallbackManager] = None,\n        system_prompt: Optional[str] = None,\n        prefix_messages: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        state: Optional[AgentState] = None,\n        allow_parallel_tool_calls: bool = True,\n        **kwargs: Any,\n    ) -> \"FunctionCallingAgent\":\n        \"\"\"Create a FunctionCallingAgent from a list of tools.\"\"\"\n        tools = tools or []\n\n        llm = llm or Settings.llm  # type: ignore\n        assert isinstance(\n            llm, FunctionCallingLLM\n        ), \"llm must be an instance of FunctionCallingLLM\"\n\n        if callback_manager is not None:\n            llm.callback_manager = callback_manager\n\n        if system_prompt is not None:\n            if prefix_messages is not None:\n                raise ValueError(\n                    \"Cannot specify both system_prompt and prefix_messages\"\n                )\n            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]\n\n        prefix_messages = prefix_messages or []\n\n        agent_worker = FunctionCallingAgentWorker.from_tools(\n            tools,\n            tool_retriever=tool_retriever,\n            llm=llm,\n            verbose=verbose,\n            max_function_calls=max_function_calls,\n            callback_manager=callback_manager,\n            prefix_messages=prefix_messages,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n        )\n\n        return cls(\n            agent_worker=agent_worker,\n            memory=memory,\n            chat_history=chat_history,\n            state=state,\n            llm=llm,\n            callback_manager=callback_manager,\n            verbose=verbose,\n            **kwargs,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/function_calling/step.py",
    "filename": "step.py",
    "relpath": "agent/function_calling/step.py",
    "start_line": 1,
    "end_line": 83,
    "length": 83,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_function_by_name",
      "build_missing_tool_message",
      "build_error_tool_output",
      "build_missing_tool_output"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "get_function_by_name",
      "build_missing_tool_message",
      "build_error_tool_output",
      "build_missing_tool_output",
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "get_all_messages",
      "_call_function",
      "_acall_function",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task"
    ],
    "document_class_names": [
      "FunctionCallingAgentWorker"
    ],
    "content": "\"\"\"Function calling agent worker.\"\"\"\n\nimport json\nimport logging\nimport uuid\nfrom typing import Any, List, Optional, Sequence, cast\nimport asyncio\nimport llama_index.core.instrumentation as instrument\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.agent.utils import add_user_step_to_memory\nfrom llama_index.core.base.llms.types import MessageRole\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    CBEventType,\n    EventPayload,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AgentChatResponse,\n)\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.instrumentation.events.agent import AgentToolCallEvent\nfrom llama_index.core.llms.function_calling import FunctionCallingLLM, ToolSelection\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools import BaseTool, ToolOutput, adapt_to_async_tool\nfrom llama_index.core.tools.calling import (\n    call_tool_with_selection,\n    acall_tool_with_selection,\n)\nfrom llama_index.core.tools import BaseTool, ToolOutput, adapt_to_async_tool\nfrom llama_index.core.tools.types import AsyncBaseTool, ToolMetadata\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.WARNING)\n\ndispatcher = instrument.get_dispatcher(__name__)\n\nDEFAULT_MAX_FUNCTION_CALLS = 5\n\n\ndef get_function_by_name(tools: Sequence[BaseTool], name: str) -> Optional[BaseTool]:\n    \"\"\"Get function by name. If the function is not found, None is returned.\"\"\"\n    name_to_tool = {tool.metadata.name: tool for tool in tools}\n    return name_to_tool.get(name, None)\n\n\ndef build_missing_tool_message(missing_tool_name: str) -> str:\n    \"\"\"\n    Build an error message for the case where a tool is not found. This message\n    instructs the LLM to double check the tool name, since it was hallucinated.\n    \"\"\"\n    return f\"Tool with name {missing_tool_name} not found, please double check.\"\n\n\ndef build_error_tool_output(tool_name: str, tool_args: Any, err_msg: str) -> ToolOutput:\n    \"\"\"Build a ToolOutput for an error that has occurred.\"\"\"\n    return ToolOutput(\n        content=err_msg,\n        tool_name=tool_name,\n        raw_input={\"args\": str(tool_args)},\n        raw_output=err_msg,\n        is_error=True,\n    )\n\n\ndef build_missing_tool_output(bad_tool_call: ToolSelection) -> ToolOutput:\n    \"\"\"\n    Build a ToolOutput for the case where a tool is not found. This output contains\n    instructions that ask the LLM to double check the tool name, along with the\n    hallucinated tool name itself.\n    \"\"\"\n    return build_error_tool_output(\n        bad_tool_call.tool_name,\n        bad_tool_call.tool_kwargs,\n        build_missing_tool_message(bad_tool_call.tool_name),\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/function_calling/step.py",
    "filename": "step.py",
    "relpath": "agent/function_calling/step.py",
    "start_line": 83,
    "end_line": 499,
    "length": 417,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "get_all_messages",
      "_call_function",
      "_acall_function",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task"
    ],
    "chunk_class_names": [
      "FunctionCallingAgentWorker"
    ],
    "document_function_names": [
      "get_function_by_name",
      "build_missing_tool_message",
      "build_error_tool_output",
      "build_missing_tool_output",
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "get_all_messages",
      "_call_function",
      "_acall_function",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task"
    ],
    "document_class_names": [
      "FunctionCallingAgentWorker"
    ],
    "content": "class FunctionCallingAgentWorker(BaseAgentWorker):\n    \"\"\"Function calling agent worker.\"\"\"\n\n    def __init__(\n        self,\n        tools: List[BaseTool],\n        llm: FunctionCallingLLM,\n        prefix_messages: List[ChatMessage],\n        verbose: bool = False,\n        max_function_calls: int = 5,\n        callback_manager: Optional[CallbackManager] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        allow_parallel_tool_calls: bool = True,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        if not llm.metadata.is_function_calling_model:\n            raise ValueError(\n                f\"Model name {llm.metadata.model_name} does not support function calling API. \"\n            )\n        self._llm = llm\n        self._verbose = verbose\n        self._max_function_calls = max_function_calls\n        self.prefix_messages = prefix_messages\n        self.callback_manager = callback_manager or self._llm.callback_manager\n        self.allow_parallel_tool_calls = allow_parallel_tool_calls\n\n        if len(tools) > 0 and tool_retriever is not None:\n            raise ValueError(\"Cannot specify both tools and tool_retriever\")\n        elif len(tools) > 0:\n            self._get_tools = lambda _: tools\n        elif tool_retriever is not None:\n            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)\n            self._get_tools = lambda message: tool_retriever_c.retrieve(message)\n        else:\n            # no tools\n            self._get_tools = lambda _: []\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: Optional[List[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        llm: Optional[FunctionCallingLLM] = None,\n        verbose: bool = False,\n        max_function_calls: int = DEFAULT_MAX_FUNCTION_CALLS,\n        allow_parallel_tool_calls: bool = True,\n        callback_manager: Optional[CallbackManager] = None,\n        system_prompt: Optional[str] = None,\n        prefix_messages: Optional[List[ChatMessage]] = None,\n        **kwargs: Any,\n    ) -> \"FunctionCallingAgentWorker\":\n        \"\"\"Create an FunctionCallingAgentWorker from a list of tools.\n\n        Similar to `from_defaults` in other classes, this method will\n        infer defaults for a variety of parameters, including the LLM,\n        if they are not specified.\n\n        \"\"\"\n        tools = tools or []\n\n        llm = llm or Settings.llm  # type: ignore\n        assert isinstance(\n            llm, FunctionCallingLLM\n        ), \"llm must be an instance of FunctionCallingLLM\"\n\n        if callback_manager is not None:\n            llm.callback_manager = callback_manager\n\n        if system_prompt is not None:\n            if prefix_messages is not None:\n                raise ValueError(\n                    \"Cannot specify both system_prompt and prefix_messages\"\n                )\n            prefix_messages = [ChatMessage(content=system_prompt, role=\"system\")]\n\n        prefix_messages = prefix_messages or []\n\n        return cls(\n            tools=tools,\n            tool_retriever=tool_retriever,\n            llm=llm,\n            prefix_messages=prefix_messages,\n            verbose=verbose,\n            max_function_calls=max_function_calls,\n            callback_manager=callback_manager,\n            allow_parallel_tool_calls=allow_parallel_tool_calls,\n            **kwargs,\n        )\n\n    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:\n        \"\"\"Initialize step from task.\"\"\"\n        sources: List[ToolOutput] = []\n        # temporary memory for new messages\n        new_memory = ChatMemoryBuffer.from_defaults()\n        # initialize task state\n        task_state = {\n            \"sources\": sources,\n            \"n_function_calls\": 0,\n            \"new_memory\": new_memory,\n        }\n        task.extra_state.update(task_state)\n\n        return TaskStep(\n            task_id=task.task_id,\n            step_id=str(uuid.uuid4()),\n            input=task.input,\n        )\n\n    def get_tools(self, input: str) -> List[AsyncBaseTool]:\n        \"\"\"Get tools.\"\"\"\n        return [adapt_to_async_tool(t) for t in self._get_tools(input)]\n\n    def get_all_messages(self, task: Task) -> List[ChatMessage]:\n        return (\n            self.prefix_messages\n            + task.memory.get(input=task.input)\n            + task.extra_state[\"new_memory\"].get_all()\n        )\n\n    def _call_function(\n        self,\n        tools: Sequence[BaseTool],\n        tool_call: ToolSelection,\n        memory: BaseMemory,\n        sources: List[ToolOutput],\n        verbose: bool = False,\n    ) -> bool:\n        tool = get_function_by_name(tools, tool_call.tool_name)\n        tool_args_str = json.dumps(tool_call.tool_kwargs)\n        tool_metadata = (\n            tool.metadata\n            if tool is not None\n            else ToolMetadata(description=\"\", name=tool_call.tool_name)\n        )\n\n        dispatcher.event(\n            AgentToolCallEvent(arguments=tool_args_str, tool=tool_metadata)\n        )\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: tool_args_str,\n                EventPayload.TOOL: tool_metadata,\n            },\n        ) as event:\n            tool_output = (\n                call_tool_with_selection(tool_call, tools, verbose=verbose)\n                if tool is not None\n                else build_missing_tool_output(tool_call)\n            )\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n\n        function_message = ChatMessage(\n            content=str(tool_output),\n            role=MessageRole.TOOL,\n            additional_kwargs={\n                \"name\": tool_call.tool_name,\n                \"tool_call_id\": tool_call.tool_id,\n            },\n        )\n        sources.append(tool_output)\n        memory.put(function_message)\n\n        return tool.metadata.return_direct if tool is not None else False\n\n    async def _acall_function(\n        self,\n        tools: Sequence[BaseTool],\n        tool_call: ToolSelection,\n        memory: BaseMemory,\n        sources: List[ToolOutput],\n        verbose: bool = False,\n    ) -> bool:\n        tool = get_function_by_name(tools, tool_call.tool_name)\n        tool_args_str = json.dumps(tool_call.tool_kwargs)\n        tool_metadata = (\n            tool.metadata\n            if tool is not None\n            else ToolMetadata(description=\"\", name=tool_call.tool_name)\n        )\n\n        dispatcher.event(\n            AgentToolCallEvent(arguments=tool_args_str, tool=tool_metadata)\n        )\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: tool_args_str,\n                EventPayload.TOOL: tool_metadata,\n            },\n        ) as event:\n            tool_output = (\n                await acall_tool_with_selection(tool_call, tools, verbose=verbose)\n                if tool is not None\n                else build_missing_tool_output(tool_call)\n            )\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n\n        function_message = ChatMessage(\n            content=str(tool_output),\n            role=MessageRole.TOOL,\n            additional_kwargs={\n                \"name\": tool_call.tool_name,\n                \"tool_call_id\": tool_call.tool_id,\n            },\n        )\n        sources.append(tool_output)\n        memory.put(function_message)\n\n        return tool.metadata.return_direct if tool is not None else False\n\n    @trace_method(\"run_step\")\n    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        if step.input is not None:\n            add_user_step_to_memory(\n                step, task.extra_state[\"new_memory\"], verbose=self._verbose\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n\n        # get response and tool call (if exists)\n        response = self._llm.chat_with_tools(\n            tools=tools,\n            user_msg=None,\n            chat_history=self.get_all_messages(task),\n            verbose=self._verbose,\n            allow_parallel_tool_calls=self.allow_parallel_tool_calls,\n        )\n        tool_calls = self._llm.get_tool_calls_from_response(\n            response, error_on_no_tool_call=False\n        )\n        tool_outputs: List[ToolOutput] = []\n\n        if self._verbose and response.message.content:\n            print(\"=== LLM Response ===\")\n            print(str(response.message.content))\n\n        if not self.allow_parallel_tool_calls and len(tool_calls) > 1:\n            raise ValueError(\n                \"Parallel tool calls not supported for synchronous function calling agent\"\n            )\n\n        # call all tools, gather responses\n        task.extra_state[\"new_memory\"].put(response.message)\n        if (\n            len(tool_calls) == 0\n            or task.extra_state[\"n_function_calls\"] >= self._max_function_calls\n        ):\n            # we are done\n            is_done = True\n            new_steps = []\n        else:\n            is_done = False\n            for i, tool_call in enumerate(tool_calls):\n                # TODO: maybe execute this with multi-threading\n                return_direct = self._call_function(\n                    tools,\n                    tool_call,\n                    task.extra_state[\"new_memory\"],\n                    tool_outputs,\n                    verbose=self._verbose,\n                )\n                task.extra_state[\"sources\"].append(tool_outputs[-1])\n                task.extra_state[\"n_function_calls\"] += 1\n\n                # check if any of the tools return directly -- only works if there is one tool call\n                if i == 0 and return_direct:\n                    is_done = True\n                    response = task.extra_state[\"sources\"][-1].content\n                    break\n\n            # put tool output in sources and memory\n            new_steps = (\n                [\n                    step.get_next_step(\n                        step_id=str(uuid.uuid4()),\n                        # NOTE: input is unused\n                        input=None,\n                    )\n                ]\n                if not is_done\n                else []\n            )\n\n        # get response string\n        # return_direct can change the response type\n        try:\n            response_str = str(response.message.content)\n        except AttributeError:\n            response_str = str(response)\n\n        agent_response = AgentChatResponse(response=response_str, sources=tool_outputs)\n\n        return TaskStepOutput(\n            output=agent_response,\n            task_step=step,\n            is_last=is_done,\n            next_steps=new_steps,\n        )\n\n    @trace_method(\"run_step\")\n    async def arun_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        if step.input is not None:\n            add_user_step_to_memory(\n                step, task.extra_state[\"new_memory\"], verbose=self._verbose\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n\n        # get response and tool call (if exists)\n        response = await self._llm.achat_with_tools(\n            tools=tools,\n            user_msg=None,\n            chat_history=self.get_all_messages(task),\n            verbose=self._verbose,\n            allow_parallel_tool_calls=self.allow_parallel_tool_calls,\n        )\n        tool_calls = self._llm.get_tool_calls_from_response(\n            response, error_on_no_tool_call=False\n        )\n        tool_outputs: List[ToolOutput] = []\n\n        if self._verbose and response.message.content:\n            print(\"=== LLM Response ===\")\n            print(str(response.message.content))\n\n        if not self.allow_parallel_tool_calls and len(tool_calls) > 1:\n            raise ValueError(\n                \"Parallel tool calls not supported for synchronous function calling agent\"\n            )\n\n        # call all tools, gather responses\n        task.extra_state[\"new_memory\"].put(response.message)\n        if (\n            len(tool_calls) == 0\n            or task.extra_state[\"n_function_calls\"] >= self._max_function_calls\n        ):\n            # we are done\n            is_done = True\n            new_steps = []\n        else:\n            is_done = False\n            tasks = [\n                self._acall_function(\n                    tools,\n                    tool_call,\n                    task.extra_state[\"new_memory\"],\n                    tool_outputs,\n                    verbose=self._verbose,\n                )\n                for tool_call in tool_calls\n            ]\n            return_directs = await asyncio.gather(*tasks)\n            task.extra_state[\"sources\"].extend(tool_outputs)\n\n            # check if any of the tools return directly -- only works if there is one tool call\n            if len(return_directs) == 1 and return_directs[0]:\n                is_done = True\n                response = tool_outputs[-1].content  # type: ignore\n\n            task.extra_state[\"n_function_calls\"] += len(tool_calls)\n            # put tool output in sources and memory\n            new_steps = (\n                [\n                    step.get_next_step(\n                        step_id=str(uuid.uuid4()),\n                        # NOTE: input is unused\n                        input=None,\n                    )\n                ]\n                if not is_done\n                else []\n            )\n\n        # get response string\n        # return_direct can change the response type\n        try:\n            response_str = str(response.message.content)\n        except AttributeError:\n            response_str = str(response)\n\n        agent_response = AgentChatResponse(response=response_str, sources=tool_outputs)\n\n        return TaskStepOutput(\n            output=agent_response,\n            task_step=step,\n            is_last=is_done,\n            next_steps=new_steps,\n        )\n\n    @trace_method(\"run_step\")\n    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        raise NotImplementedError(\"Stream not supported for function calling agent\")\n\n    @trace_method(\"run_step\")\n    async def astream_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        raise NotImplementedError(\"Stream not supported for function calling agent\")\n\n    def finalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n        # add new messages to memory\n        task.memory.set(\n            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()\n        )\n        # reset new memory\n        task.extra_state[\"new_memory\"].reset()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/base.py",
    "filename": "base.py",
    "relpath": "agent/runner/base.py",
    "start_line": 1,
    "end_line": 205,
    "length": 205,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "create_task",
      "delete_task",
      "list_tasks",
      "get_completed_tasks",
      "get_task_output",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_completed_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "undo_step",
      "validate_step_from_args",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "reset"
    ],
    "chunk_class_names": [
      "BaseAgentRunner",
      "TaskState",
      "AgentState"
    ],
    "document_function_names": [
      "create_task",
      "delete_task",
      "list_tasks",
      "get_completed_tasks",
      "get_task_output",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_completed_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "undo_step",
      "validate_step_from_args",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "reset",
      "__init__",
      "from_llm",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_task_output",
      "get_completed_tasks",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step",
      "create_plan",
      "get_next_tasks",
      "mark_task_complete",
      "refine_plan",
      "run_task",
      "acreate_plan",
      "arefine_plan",
      "arun_task",
      "_chat",
      "_achat"
    ],
    "document_class_names": [
      "BaseAgentRunner",
      "TaskState",
      "AgentState",
      "AgentRunner",
      "BasePlanningAgentRunner"
    ],
    "content": "import os\nfrom abc import abstractmethod\nfrom collections import deque\nfrom typing import Any, Deque, Dict, List, Optional, Union, cast\n\nfrom llama_index.core.agent.types import (\n    BaseAgent,\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.async_utils import asyncio_run, run_jobs\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    CBEventType,\n    EventPayload,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n    AgentChatResponse,\n    ChatResponseMode,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.tools.types import BaseTool\nfrom llama_index.core.instrumentation.events.agent import (\n    AgentRunStepEndEvent,\n    AgentRunStepStartEvent,\n    AgentChatWithStepStartEvent,\n    AgentChatWithStepEndEvent,\n)\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass BaseAgentRunner(BaseAgent):\n    \"\"\"Base agent runner.\"\"\"\n\n    @abstractmethod\n    def create_task(self, input: str, **kwargs: Any) -> Task:\n        \"\"\"Create task.\"\"\"\n\n    @abstractmethod\n    def delete_task(\n        self,\n        task_id: str,\n    ) -> None:\n        \"\"\"Delete task.\n\n        NOTE: this will not delete any previous executions from memory.\n\n        \"\"\"\n\n    @abstractmethod\n    def list_tasks(self, **kwargs: Any) -> List[Task]:\n        \"\"\"List tasks.\"\"\"\n\n    @abstractmethod\n    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:\n        \"\"\"Get completed tasks.\"\"\"\n\n    @abstractmethod\n    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Get task output.\"\"\"\n\n    @abstractmethod\n    def get_task(self, task_id: str, **kwargs: Any) -> Task:\n        \"\"\"Get task.\"\"\"\n\n    @abstractmethod\n    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:\n        \"\"\"Get upcoming steps.\"\"\"\n\n    @abstractmethod\n    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:\n        \"\"\"Get completed steps.\"\"\"\n\n    def get_completed_step(\n        self, task_id: str, step_id: str, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Get completed step.\"\"\"\n        # call get_completed_steps, and then find the right task\n        completed_steps = self.get_completed_steps(task_id, **kwargs)\n        for step_output in completed_steps:\n            if step_output.task_step.step_id == step_id:\n                return step_output\n        raise ValueError(f\"Could not find step_id: {step_id}\")\n\n    @abstractmethod\n    def run_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n\n    @abstractmethod\n    async def arun_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n\n    @abstractmethod\n    def stream_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n\n    @abstractmethod\n    async def astream_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n\n    @abstractmethod\n    def finalize_response(\n        self,\n        task_id: str,\n        step_output: Optional[TaskStepOutput] = None,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Finalize response.\"\"\"\n\n    async def afinalize_response(\n        self,\n        task_id: str,\n        step_output: Optional[TaskStepOutput] = None,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Finalize response.\"\"\"\n        return self.finalize_response(task_id, step_output)\n\n    @abstractmethod\n    def undo_step(self, task_id: str) -> None:\n        \"\"\"Undo previous step.\"\"\"\n        raise NotImplementedError(\"undo_step not implemented\")\n\n\ndef validate_step_from_args(\n    task_id: str, input: Optional[str] = None, step: Optional[Any] = None, **kwargs: Any\n) -> Optional[TaskStep]:\n    \"\"\"Validate step from args.\"\"\"\n    if step is not None:\n        if input is not None:\n            raise ValueError(\"Cannot specify both `step` and `input`\")\n        if not isinstance(step, TaskStep):\n            raise ValueError(f\"step must be TaskStep: {step}\")\n        return step\n    else:\n        return None\n\n\nclass TaskState(BaseModel):\n    \"\"\"Task state.\"\"\"\n\n    task: Task = Field(..., description=\"Task.\")\n    step_queue: Deque[TaskStep] = Field(\n        default_factory=deque, description=\"Task step queue.\"\n    )\n    completed_steps: List[TaskStepOutput] = Field(\n        default_factory=list, description=\"Completed step outputs.\"\n    )\n\n\nclass AgentState(BaseModel):\n    \"\"\"Agent state.\"\"\"\n\n    task_dict: Dict[str, TaskState] = Field(\n        default_factory=dict, description=\"Task dictionary.\"\n    )\n\n    def get_task(self, task_id: str) -> Task:\n        \"\"\"Get task state.\"\"\"\n        return self.task_dict[task_id].task\n\n    def get_completed_steps(self, task_id: str) -> List[TaskStepOutput]:\n        \"\"\"Get completed steps.\"\"\"\n        return self.task_dict[task_id].completed_steps\n\n    def get_step_queue(self, task_id: str) -> Deque[TaskStep]:\n        \"\"\"Get step queue.\"\"\"\n        return self.task_dict[task_id].step_queue\n\n    def reset(self) -> None:\n        \"\"\"Reset.\"\"\"\n        self.task_dict = {}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/base.py",
    "filename": "base.py",
    "relpath": "agent/runner/base.py",
    "start_line": 205,
    "end_line": 208,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "AgentRunner"
    ],
    "document_function_names": [
      "create_task",
      "delete_task",
      "list_tasks",
      "get_completed_tasks",
      "get_task_output",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_completed_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "undo_step",
      "validate_step_from_args",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "reset",
      "__init__",
      "from_llm",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_task_output",
      "get_completed_tasks",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step",
      "create_plan",
      "get_next_tasks",
      "mark_task_complete",
      "refine_plan",
      "run_task",
      "acreate_plan",
      "arefine_plan",
      "arun_task",
      "_chat",
      "_achat"
    ],
    "document_class_names": [
      "BaseAgentRunner",
      "TaskState",
      "AgentState",
      "AgentRunner",
      "BasePlanningAgentRunner"
    ],
    "content": "class AgentRunner(BaseAgentRunner):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/base.py",
    "filename": "base.py",
    "relpath": "agent/runner/base.py",
    "start_line": 208,
    "end_line": 640,
    "length": 433,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_llm",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_task_output",
      "get_completed_tasks",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "_chat"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "create_task",
      "delete_task",
      "list_tasks",
      "get_completed_tasks",
      "get_task_output",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_completed_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "undo_step",
      "validate_step_from_args",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "reset",
      "__init__",
      "from_llm",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_task_output",
      "get_completed_tasks",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step",
      "create_plan",
      "get_next_tasks",
      "mark_task_complete",
      "refine_plan",
      "run_task",
      "acreate_plan",
      "arefine_plan",
      "arun_task",
      "_chat",
      "_achat"
    ],
    "document_class_names": [
      "BaseAgentRunner",
      "TaskState",
      "AgentState",
      "AgentRunner",
      "BasePlanningAgentRunner"
    ],
    "content": "\"\"\"Agent runner.\n\n    Top-level agent orchestrator that can create tasks, run each step in a task,\n    or run a task e2e. Stores state and keeps track of tasks.\n\n    Args:\n        agent_worker (BaseAgentWorker): step executor\n        chat_history (Optional[List[ChatMessage]], optional): chat history. Defaults to None.\n        state (Optional[AgentState], optional): agent state. Defaults to None.\n        memory (Optional[BaseMemory], optional): memory. Defaults to None.\n        llm (Optional[LLM], optional): LLM. Defaults to None.\n        callback_manager (Optional[CallbackManager], optional): callback manager. Defaults to None.\n        init_task_state_kwargs (Optional[dict], optional): init task state kwargs. Defaults to None.\n\n    \"\"\"\n\n    # # TODO: implement this in Pydantic\n\n    def __init__(\n        self,\n        agent_worker: BaseAgentWorker,\n        chat_history: Optional[List[ChatMessage]] = None,\n        state: Optional[AgentState] = None,\n        memory: Optional[BaseMemory] = None,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        init_task_state_kwargs: Optional[dict] = None,\n        delete_task_on_finish: bool = False,\n        default_tool_choice: str = \"auto\",\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.agent_worker = agent_worker\n        self.state = state or AgentState()\n        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)\n\n        # get and set callback manager\n        if callback_manager is not None:\n            self.agent_worker.set_callback_manager(callback_manager)\n            self.callback_manager = callback_manager\n        else:\n            # TODO: This is *temporary*\n            # Stopgap before having a callback on the BaseAgentWorker interface.\n            # Doing that requires a bit more refactoring to make sure existing code\n            # doesn't break.\n            if hasattr(self.agent_worker, \"callback_manager\"):\n                self.callback_manager = (\n                    self.agent_worker.callback_manager or CallbackManager()\n                )\n            else:\n                self.callback_manager = CallbackManager()\n        self.init_task_state_kwargs = init_task_state_kwargs or {}\n        self.delete_task_on_finish = delete_task_on_finish\n        self.default_tool_choice = default_tool_choice\n        self.verbose = verbose\n\n    @staticmethod\n    def from_llm(\n        tools: Optional[List[BaseTool]] = None,\n        llm: Optional[LLM] = None,\n        **kwargs: Any,\n    ) -> \"AgentRunner\":\n        from llama_index.core.agent import ReActAgent\n\n        if os.getenv(\"IS_TESTING\"):\n            return ReActAgent.from_tools(\n                tools=tools,\n                llm=llm,\n                **kwargs,\n            )\n\n        try:\n            from llama_index.llms.openai import OpenAI  # pants: no-infer-dep\n            from llama_index.llms.openai.utils import (\n                is_function_calling_model,\n            )  # pants: no-infer-dep\n        except ImportError:\n            raise ImportError(\n                \"`llama-index-llms-openai` package not found. Please \"\n                \"install by running `pip install llama-index-llms-openai`.\"\n            )\n\n        if isinstance(llm, OpenAI) and is_function_calling_model(llm.model):\n            from llama_index.agent.openai import OpenAIAgent  # pants: no-infer-dep\n\n            return OpenAIAgent.from_tools(\n                tools=tools,\n                llm=llm,\n                **kwargs,\n            )\n        else:\n            return ReActAgent.from_tools(\n                tools=tools,\n                llm=llm,\n                **kwargs,\n            )\n\n    @property\n    def chat_history(self) -> List[ChatMessage]:\n        return self.memory.get_all()\n\n    def reset(self) -> None:\n        self.memory.reset()\n        self.state.reset()\n\n    def create_task(self, input: str, **kwargs: Any) -> Task:\n        \"\"\"Create task.\"\"\"\n        if not self.init_task_state_kwargs:\n            extra_state = kwargs.pop(\"extra_state\", {})\n        else:\n            if \"extra_state\" in kwargs:\n                raise ValueError(\n                    \"Cannot specify both `extra_state` and `init_task_state_kwargs`\"\n                )\n            else:\n                extra_state = self.init_task_state_kwargs\n\n        callback_manager = kwargs.pop(\"callback_manager\", self.callback_manager)\n        task = Task(\n            input=input,\n            memory=self.memory,\n            extra_state=extra_state,\n            callback_manager=callback_manager,\n            **kwargs,\n        )\n        # # put input into memory\n        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))\n\n        # get initial step from task, and put it in the step queue\n        initial_step = self.agent_worker.initialize_step(task)\n        task_state = TaskState(\n            task=task,\n            step_queue=deque([initial_step]),\n        )\n        # add it to state\n        self.state.task_dict[task.task_id] = task_state\n\n        return task\n\n    def delete_task(\n        self,\n        task_id: str,\n    ) -> None:\n        \"\"\"Delete task.\n\n        NOTE: this will not delete any previous executions from memory.\n\n        \"\"\"\n        self.state.task_dict.pop(task_id)\n\n    def list_tasks(self, **kwargs: Any) -> List[Task]:\n        \"\"\"List tasks.\"\"\"\n        return [task_state.task for task_state in self.state.task_dict.values()]\n\n    def get_task(self, task_id: str, **kwargs: Any) -> Task:\n        \"\"\"Get task.\"\"\"\n        return self.state.get_task(task_id)\n\n    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:\n        \"\"\"Get upcoming steps.\"\"\"\n        return list(self.state.get_step_queue(task_id))\n\n    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:\n        \"\"\"Get completed steps.\"\"\"\n        return self.state.get_completed_steps(task_id)\n\n    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Get task output.\"\"\"\n        completed_steps = self.get_completed_steps(task_id)\n        if len(completed_steps) == 0:\n            raise ValueError(f\"No completed steps for task_id: {task_id}\")\n        return completed_steps[-1]\n\n    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:\n        \"\"\"Get completed tasks.\"\"\"\n        task_states = list(self.state.task_dict.values())\n        completed_tasks = []\n        for task_state in task_states:\n            completed_steps = self.get_completed_steps(task_state.task.task_id)\n            if len(completed_steps) > 0 and completed_steps[-1].is_last:\n                completed_tasks.append(task_state.task)\n\n        return completed_tasks\n\n    @dispatcher.span\n    def _run_step(\n        self,\n        task_id: str,\n        step: Optional[TaskStep] = None,\n        input: Optional[str] = None,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Execute step.\"\"\"\n        task = self.state.get_task(task_id)\n        step_queue = self.state.get_step_queue(task_id)\n        step = step or step_queue.popleft()\n        if input is not None:\n            step.input = input\n\n        dispatcher.event(\n            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)\n        )\n\n        if self.verbose:\n            print(f\"> Running step {step.step_id}. Step input: {step.input}\")\n\n        # TODO: figure out if you can dynamically swap in different step executors\n        # not clear when you would do that by theoretically possible\n\n        if mode == ChatResponseMode.WAIT:\n            cur_step_output = self.agent_worker.run_step(step, task, **kwargs)\n        elif mode == ChatResponseMode.STREAM:\n            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n        # append cur_step_output next steps to queue\n        next_steps = cur_step_output.next_steps\n        step_queue.extend(next_steps)\n\n        # add cur_step_output to completed steps\n        completed_steps = self.state.get_completed_steps(task_id)\n        completed_steps.append(cur_step_output)\n\n        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))\n        return cur_step_output\n\n    @dispatcher.span\n    async def _arun_step(\n        self,\n        task_id: str,\n        step: Optional[TaskStep] = None,\n        input: Optional[str] = None,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Execute step.\"\"\"\n        dispatcher.event(\n            AgentRunStepStartEvent(task_id=task_id, step=step, input=input)\n        )\n        task = self.state.get_task(task_id)\n        step_queue = self.state.get_step_queue(task_id)\n        step = step or step_queue.popleft()\n        if input is not None:\n            step.input = input\n\n        if self.verbose:\n            print(f\"> Running step {step.step_id}. Step input: {step.input}\")\n\n        # TODO: figure out if you can dynamically swap in different step executors\n        # not clear when you would do that by theoretically possible\n        if mode == ChatResponseMode.WAIT:\n            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)\n        elif mode == ChatResponseMode.STREAM:\n            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n        # append cur_step_output next steps to queue\n        next_steps = cur_step_output.next_steps\n        step_queue.extend(next_steps)\n\n        # add cur_step_output to completed steps\n        completed_steps = self.state.get_completed_steps(task_id)\n        completed_steps.append(cur_step_output)\n\n        dispatcher.event(AgentRunStepEndEvent(step_output=cur_step_output))\n        return cur_step_output\n\n    @dispatcher.span\n    def run_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        step = validate_step_from_args(task_id, input, step, **kwargs)\n        return self._run_step(\n            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs\n        )\n\n    @dispatcher.span\n    async def arun_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        step = validate_step_from_args(task_id, input, step, **kwargs)\n        return await self._arun_step(\n            task_id, step, input=input, mode=ChatResponseMode.WAIT, **kwargs\n        )\n\n    @dispatcher.span\n    def stream_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        step = validate_step_from_args(task_id, input, step, **kwargs)\n        return self._run_step(\n            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs\n        )\n\n    @dispatcher.span\n    async def astream_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        step = validate_step_from_args(task_id, input, step, **kwargs)\n        return await self._arun_step(\n            task_id, step, input=input, mode=ChatResponseMode.STREAM, **kwargs\n        )\n\n    @dispatcher.span\n    def finalize_response(\n        self,\n        task_id: str,\n        step_output: Optional[TaskStepOutput] = None,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Finalize response.\"\"\"\n        if step_output is None:\n            step_output = self.state.get_completed_steps(task_id)[-1]\n        if not step_output.is_last:\n            raise ValueError(\n                \"finalize_response can only be called on the last step output\"\n            )\n\n        if not isinstance(\n            step_output.output,\n            (AgentChatResponse, StreamingAgentChatResponse),\n        ):\n            raise ValueError(\n                \"When `is_last` is True, cur_step_output.output must be \"\n                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"\n            )\n\n        # finalize task\n        self.agent_worker.finalize_task(self.state.get_task(task_id))\n\n        if self.delete_task_on_finish:\n            self.delete_task(task_id)\n\n        # Attach all sources generated across all steps\n        step_output.output.sources = self.get_task(task_id).extra_state.get(\n            \"sources\", []\n        )\n        step_output.output.set_source_nodes()\n\n        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)\n\n    @dispatcher.span\n    async def afinalize_response(\n        self,\n        task_id: str,\n        step_output: Optional[TaskStepOutput] = None,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Finalize response.\"\"\"\n        if step_output is None:\n            step_output = self.state.get_completed_steps(task_id)[-1]\n        if not step_output.is_last:\n            raise ValueError(\n                \"finalize_response can only be called on the last step output\"\n            )\n\n        if not isinstance(\n            step_output.output,\n            (AgentChatResponse, StreamingAgentChatResponse),\n        ):\n            raise ValueError(\n                \"When `is_last` is True, cur_step_output.output must be \"\n                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"\n            )\n\n        # finalize task\n        await self.agent_worker.afinalize_task(self.state.get_task(task_id))\n\n        if self.delete_task_on_finish:\n            self.delete_task(task_id)\n\n        # Attach all sources generated across all steps\n        step_output.output.sources = self.get_task(task_id).extra_state.get(\n            \"sources\", []\n        )\n        step_output.output.set_source_nodes()\n\n        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)\n\n    @dispatcher.span\n    def _chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Chat with step executor.\"\"\"\n        if chat_history is not None:\n            self.memory.set(chat_history)\n        task = self.create_task(message)\n\n        result_output = None\n        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))\n        while True:\n            # pass step queue in as argument, assume step executor is stateless\n            cur_step_output = self._run_step(\n                task.task_id, mode=mode, tool_choice=tool_choice\n            )\n\n            if cur_step_output.is_last:\n                result_output = cur_step_output\n                break\n\n            # ensure tool_choice does not cause endless loops\n            tool_choice = \"auto\"\n\n        result = self.finalize_response(\n            task.task_id,\n            result_output,\n        )\n        dispatcher.event(AgentChatWithStepEndEvent(response=result))\n        return result"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/base.py",
    "filename": "base.py",
    "relpath": "agent/runner/base.py",
    "start_line": 640,
    "end_line": 781,
    "length": 142,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "create_task",
      "delete_task",
      "list_tasks",
      "get_completed_tasks",
      "get_task_output",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_completed_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "undo_step",
      "validate_step_from_args",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "reset",
      "__init__",
      "from_llm",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_task_output",
      "get_completed_tasks",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step",
      "create_plan",
      "get_next_tasks",
      "mark_task_complete",
      "refine_plan",
      "run_task",
      "acreate_plan",
      "arefine_plan",
      "arun_task",
      "_chat",
      "_achat"
    ],
    "document_class_names": [
      "BaseAgentRunner",
      "TaskState",
      "AgentState",
      "AgentRunner",
      "BasePlanningAgentRunner"
    ],
    "content": "@dispatcher.span\n    async def _achat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Chat with step executor.\"\"\"\n        if chat_history is not None:\n            self.memory.set(chat_history)\n        task = self.create_task(message)\n\n        result_output = None\n        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))\n        while True:\n            # pass step queue in as argument, assume step executor is stateless\n            cur_step_output = await self._arun_step(\n                task.task_id, mode=mode, tool_choice=tool_choice\n            )\n\n            if cur_step_output.is_last:\n                result_output = cur_step_output\n                break\n\n            # ensure tool_choice does not cause endless loops\n            tool_choice = \"auto\"\n\n        result = await self.afinalize_response(\n            task.task_id,\n            result_output,\n        )\n        dispatcher.event(AgentChatWithStepEndEvent(response=result))\n        return result\n\n    @dispatcher.span\n    @trace_method(\"chat\")\n    def chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Optional[Union[str, dict]] = None,\n    ) -> AgentChatResponse:\n        # override tool choice is provided as input.\n        if tool_choice is None:\n            tool_choice = self.default_tool_choice\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = self._chat(\n                message=message,\n                chat_history=chat_history,\n                tool_choice=tool_choice,\n                mode=ChatResponseMode.WAIT,\n            )\n            assert isinstance(chat_response, AgentChatResponse)\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n        return chat_response\n\n    @dispatcher.span\n    @trace_method(\"chat\")\n    async def achat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Optional[Union[str, dict]] = None,\n    ) -> AgentChatResponse:\n        # override tool choice is provided as input.\n        if tool_choice is None:\n            tool_choice = self.default_tool_choice\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = await self._achat(\n                message=message,\n                chat_history=chat_history,\n                tool_choice=tool_choice,\n                mode=ChatResponseMode.WAIT,\n            )\n            assert isinstance(chat_response, AgentChatResponse)\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n        return chat_response\n\n    @dispatcher.span\n    @trace_method(\"chat\")\n    def stream_chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Optional[Union[str, dict]] = None,\n    ) -> StreamingAgentChatResponse:\n        # override tool choice is provided as input.\n        if tool_choice is None:\n            tool_choice = self.default_tool_choice\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = self._chat(\n                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM\n            )\n            assert isinstance(chat_response, StreamingAgentChatResponse) or (\n                isinstance(chat_response, AgentChatResponse)\n                and chat_response.is_dummy_stream\n            )\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n\n        return chat_response  # type: ignore\n\n    @dispatcher.span\n    @trace_method(\"chat\")\n    async def astream_chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Optional[Union[str, dict]] = None,\n    ) -> StreamingAgentChatResponse:\n        # override tool choice is provided as input.\n        if tool_choice is None:\n            tool_choice = self.default_tool_choice\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = await self._achat(\n                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM\n            )\n            assert isinstance(chat_response, StreamingAgentChatResponse) or (\n                isinstance(chat_response, AgentChatResponse)\n                and chat_response.is_dummy_stream\n            )\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n\n        return chat_response  # type: ignore\n\n    def undo_step(self, task_id: str) -> None:\n        \"\"\"Undo previous step.\"\"\"\n        raise NotImplementedError(\"undo_step not implemented\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/base.py",
    "filename": "base.py",
    "relpath": "agent/runner/base.py",
    "start_line": 205,
    "end_line": 916,
    "length": 712,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "create_plan",
      "get_next_tasks",
      "mark_task_complete",
      "refine_plan",
      "run_task",
      "acreate_plan",
      "arefine_plan",
      "arun_task",
      "_chat",
      "_achat"
    ],
    "chunk_class_names": [
      "BasePlanningAgentRunner"
    ],
    "document_function_names": [
      "create_task",
      "delete_task",
      "list_tasks",
      "get_completed_tasks",
      "get_task_output",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_completed_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "undo_step",
      "validate_step_from_args",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "reset",
      "__init__",
      "from_llm",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "get_task_output",
      "get_completed_tasks",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "afinalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step",
      "create_plan",
      "get_next_tasks",
      "mark_task_complete",
      "refine_plan",
      "run_task",
      "acreate_plan",
      "arefine_plan",
      "arun_task",
      "_chat",
      "_achat"
    ],
    "document_class_names": [
      "BaseAgentRunner",
      "TaskState",
      "AgentState",
      "AgentRunner",
      "BasePlanningAgentRunner"
    ],
    "content": "class BasePlanningAgentRunner(AgentRunner):\n    @abstractmethod\n    def create_plan(self, input: str, **kwargs: Any) -> str:\n        \"\"\"Create plan. Returns the plan_id.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_next_tasks(self, plan_id: str, **kwargs: Any) -> List[str]:\n        \"\"\"Get next task ids for a given plan.\"\"\"\n        ...\n\n    @abstractmethod\n    def mark_task_complete(self, plan_id: str, task_id: str, **kwargs: Any) -> None:\n        \"\"\"Mark task complete for a given plan.\"\"\"\n        ...\n\n    @abstractmethod\n    def refine_plan(self, input: str, plan_id: str, **kwargs: Any) -> None:\n        \"\"\"Refine plan.\"\"\"\n        ...\n\n    @abstractmethod\n    def run_task(self, task_id: str, **kwargs: Any) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Run task.\"\"\"\n        ...\n\n    async def acreate_plan(self, input: str, **kwargs: Any) -> str:\n        \"\"\"Create plan (async). Returns the plan_id.\"\"\"\n        return self.create_plan(input, **kwargs)\n\n    async def arefine_plan(self, input: str, plan_id: str, **kwargs: Any) -> None:\n        \"\"\"Refine plan (async).\"\"\"\n        return self.refine_plan(input, plan_id, **kwargs)\n\n    async def arun_task(self, task_id: str, **kwargs: Any) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Run task (async).\"\"\"\n        return self.run_task(task_id, **kwargs)\n\n    @dispatcher.span\n    def _chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Chat with step executor.\"\"\"\n        if chat_history is not None:\n            self.memory.set(chat_history)\n\n        # create initial set of tasks\n        plan_id = self.create_plan(message)\n\n        results = []\n        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))\n        while True:\n            # EXIT CONDITION: check if all sub-tasks are completed\n            next_task_ids = self.get_next_tasks(plan_id)\n            if len(next_task_ids) == 0:\n                break\n\n            jobs = [\n                self.arun_task(sub_task_id, mode=mode, tool_choice=tool_choice)\n                for sub_task_id in next_task_ids\n            ]\n            results = asyncio_run(run_jobs(jobs, workers=len(jobs)))\n\n            for sub_task_id in next_task_ids:\n                self.mark_task_complete(plan_id, sub_task_id)\n\n            # EXIT CONDITION: check if all sub-tasks are completed now\n            # LLMs have a tendency to add more tasks, so we end if there are no more tasks\n            # next_sub_tasks = self.state.get_next_sub_tasks(plan_id)\n            # if len(next_sub_tasks) == 0:\n            #    break\n\n            # refine the plan\n            self.refine_plan(message, plan_id)\n\n        dispatcher.event(\n            AgentChatWithStepEndEvent(\n                response=results[-1] if len(results) > 0 else None\n            )\n        )\n        return results[-1]\n\n    @dispatcher.span\n    async def _achat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Chat with step executor.\"\"\"\n        if chat_history is not None:\n            self.memory.set(chat_history)\n\n        # create initial set of tasks\n        plan_id = self.create_plan(message)\n\n        results = []\n        dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))\n        while True:\n            # EXIT CONDITION: check if all sub-tasks are completed\n            next_task_ids = self.get_next_tasks(plan_id)\n            if len(next_task_ids) == 0:\n                break\n\n            jobs = [\n                self.arun_task(sub_task_id, mode=mode, tool_choice=tool_choice)\n                for sub_task_id in next_task_ids\n            ]\n            results = await run_jobs(jobs, workers=len(jobs))\n\n            for sub_task_id in next_task_ids:\n                self.mark_task_complete(plan_id, sub_task_id)\n\n            # EXIT CONDITION: check if all sub-tasks are completed now\n            # LLMs have a tendency to add more tasks, so we end if there are no more tasks\n            # next_sub_tasks = self.state.get_next_sub_tasks(plan_id)\n            # if len(next_sub_tasks) == 0:\n            #    break\n\n            # refine the plan\n            await self.arefine_plan(message, plan_id)\n\n        dispatcher.event(\n            AgentChatWithStepEndEvent(\n                response=results[-1] if len(results) > 0 else None\n            )\n        )\n        return results[-1]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/parallel.py",
    "filename": "parallel.py",
    "relpath": "agent/runner/parallel.py",
    "start_line": 1,
    "end_line": 69,
    "length": 69,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "task_id",
      "get_task",
      "get_completed_steps",
      "get_step_queue"
    ],
    "chunk_class_names": [
      "DAGTaskState",
      "DAGAgentState"
    ],
    "document_function_names": [
      "task_id",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "__init__",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "get_completed_tasks",
      "get_task_output",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "run_steps_in_queue",
      "arun_steps_in_queue",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step"
    ],
    "document_class_names": [
      "DAGTaskState",
      "DAGAgentState",
      "ParallelAgentRunner"
    ],
    "content": "\"\"\"Agent executor.\"\"\"\n\nimport asyncio\nfrom collections import deque\nfrom typing import Any, Deque, Dict, List, Optional, Union, cast\n\nfrom llama_index.core.agent.runner.base import BaseAgentRunner\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.async_utils import asyncio_run\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    CBEventType,\n    EventPayload,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n    AgentChatResponse,\n    ChatResponseMode,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\n\n\nclass DAGTaskState(BaseModel):\n    \"\"\"DAG Task state.\"\"\"\n\n    task: Task = Field(..., description=\"Task.\")\n    root_step: TaskStep = Field(..., description=\"Root step.\")\n    step_queue: Deque[TaskStep] = Field(\n        default_factory=deque, description=\"Task step queue.\"\n    )\n    completed_steps: List[TaskStepOutput] = Field(\n        default_factory=list, description=\"Completed step outputs.\"\n    )\n\n    @property\n    def task_id(self) -> str:\n        \"\"\"Task id.\"\"\"\n        return self.task.task_id\n\n\nclass DAGAgentState(BaseModel):\n    \"\"\"Agent state.\"\"\"\n\n    task_dict: Dict[str, DAGTaskState] = Field(\n        default_factory=dict, description=\"Task dictionary.\"\n    )\n\n    def get_task(self, task_id: str) -> Task:\n        \"\"\"Get task state.\"\"\"\n        return self.task_dict[task_id].task\n\n    def get_completed_steps(self, task_id: str) -> List[TaskStepOutput]:\n        \"\"\"Get completed steps.\"\"\"\n        return self.task_dict[task_id].completed_steps\n\n    def get_step_queue(self, task_id: str) -> Deque[TaskStep]:\n        \"\"\"Get step queue.\"\"\"\n        return self.task_dict[task_id].step_queue"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/parallel.py",
    "filename": "parallel.py",
    "relpath": "agent/runner/parallel.py",
    "start_line": 69,
    "end_line": 496,
    "length": 428,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "get_completed_tasks",
      "get_task_output",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "run_steps_in_queue",
      "arun_steps_in_queue",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step"
    ],
    "chunk_class_names": [
      "ParallelAgentRunner"
    ],
    "document_function_names": [
      "task_id",
      "get_task",
      "get_completed_steps",
      "get_step_queue",
      "__init__",
      "chat_history",
      "reset",
      "create_task",
      "delete_task",
      "get_completed_tasks",
      "get_task_output",
      "list_tasks",
      "get_task",
      "get_upcoming_steps",
      "get_completed_steps",
      "run_steps_in_queue",
      "arun_steps_in_queue",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_response",
      "_chat",
      "_achat",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "undo_step"
    ],
    "document_class_names": [
      "DAGTaskState",
      "DAGAgentState",
      "ParallelAgentRunner"
    ],
    "content": "class ParallelAgentRunner(BaseAgentRunner):\n    \"\"\"Parallel agent runner.\n\n    Executes steps in queue in parallel. Requires async support.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_worker: BaseAgentWorker,\n        chat_history: Optional[List[ChatMessage]] = None,\n        state: Optional[DAGAgentState] = None,\n        memory: Optional[BaseMemory] = None,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        init_task_state_kwargs: Optional[dict] = None,\n        delete_task_on_finish: bool = False,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)\n        self.state = state or DAGAgentState()\n        self.callback_manager = callback_manager or CallbackManager([])\n        self.init_task_state_kwargs = init_task_state_kwargs or {}\n        self.agent_worker = agent_worker\n        self.delete_task_on_finish = delete_task_on_finish\n\n    @property\n    def chat_history(self) -> List[ChatMessage]:\n        return self.memory.get_all()\n\n    def reset(self) -> None:\n        self.memory.reset()\n\n    def create_task(self, input: str, **kwargs: Any) -> Task:\n        \"\"\"Create task.\"\"\"\n        task = Task(\n            input=input,\n            memory=self.memory,\n            extra_state=self.init_task_state_kwargs,\n            **kwargs,\n        )\n        # # put input into memory\n        # self.memory.put(ChatMessage(content=input, role=MessageRole.USER))\n\n        # add it to state\n        # get initial step from task, and put it in the step queue\n        initial_step = self.agent_worker.initialize_step(task)\n        task_state = DAGTaskState(\n            task=task,\n            root_step=initial_step,\n            step_queue=deque([initial_step]),\n        )\n\n        self.state.task_dict[task.task_id] = task_state\n\n        return task\n\n    def delete_task(\n        self,\n        task_id: str,\n    ) -> None:\n        \"\"\"Delete task.\n\n        NOTE: this will not delete any previous executions from memory.\n\n        \"\"\"\n        self.state.task_dict.pop(task_id)\n\n    def get_completed_tasks(self, **kwargs: Any) -> List[Task]:\n        \"\"\"Get completed tasks.\"\"\"\n        task_states = list(self.state.task_dict.values())\n        return [\n            task_state.task\n            for task_state in task_states\n            if len(task_state.completed_steps) > 0\n            and task_state.completed_steps[-1].is_last\n        ]\n\n    def get_task_output(self, task_id: str, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Get task output.\"\"\"\n        task_state = self.state.task_dict[task_id]\n        if len(task_state.completed_steps) == 0:\n            raise ValueError(f\"No completed steps for task_id: {task_id}\")\n        return task_state.completed_steps[-1]\n\n    def list_tasks(self, **kwargs: Any) -> List[Task]:\n        \"\"\"List tasks.\"\"\"\n        task_states = list(self.state.task_dict.values())\n        return [task_state.task for task_state in task_states]\n\n    def get_task(self, task_id: str, **kwargs: Any) -> Task:\n        \"\"\"Get task.\"\"\"\n        return self.state.get_task(task_id)\n\n    def get_upcoming_steps(self, task_id: str, **kwargs: Any) -> List[TaskStep]:\n        \"\"\"Get upcoming steps.\"\"\"\n        return list(self.state.get_step_queue(task_id))\n\n    def get_completed_steps(self, task_id: str, **kwargs: Any) -> List[TaskStepOutput]:\n        \"\"\"Get completed steps.\"\"\"\n        return self.state.get_completed_steps(task_id)\n\n    def run_steps_in_queue(\n        self,\n        task_id: str,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        **kwargs: Any,\n    ) -> List[TaskStepOutput]:\n        \"\"\"Execute steps in queue.\n\n        Run all steps in queue, clearing it out.\n\n        Assume that all steps can be run in parallel.\n\n        \"\"\"\n        return asyncio_run(self.arun_steps_in_queue(task_id, mode=mode, **kwargs))\n\n    async def arun_steps_in_queue(\n        self,\n        task_id: str,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        **kwargs: Any,\n    ) -> List[TaskStepOutput]:\n        \"\"\"Execute all steps in queue.\n\n        All steps in queue are assumed to be ready.\n\n        \"\"\"\n        # first pop all steps from step_queue\n        steps: List[TaskStep] = []\n        while len(self.state.get_step_queue(task_id)) > 0:\n            steps.append(self.state.get_step_queue(task_id).popleft())\n\n        # take every item in the queue, and run it\n        tasks = []\n        for step in steps:\n            tasks.append(self._arun_step(task_id, step=step, mode=mode, **kwargs))\n\n        return await asyncio.gather(*tasks)\n\n    def _run_step(\n        self,\n        task_id: str,\n        step: Optional[TaskStep] = None,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Execute step.\"\"\"\n        task = self.state.get_task(task_id)\n        task_queue = self.state.get_step_queue(task_id)\n        step = step or task_queue.popleft()\n\n        if not step.is_ready:\n            raise ValueError(f\"Step {step.step_id} is not ready\")\n\n        if mode == ChatResponseMode.WAIT:\n            cur_step_output: TaskStepOutput = self.agent_worker.run_step(\n                step, task, **kwargs\n            )\n        elif mode == ChatResponseMode.STREAM:\n            cur_step_output = self.agent_worker.stream_step(step, task, **kwargs)\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        for next_step in cur_step_output.next_steps:\n            if next_step.is_ready:\n                task_queue.append(next_step)\n\n        # add cur_step_output to completed steps\n        completed_steps = self.state.get_completed_steps(task_id)\n        completed_steps.append(cur_step_output)\n\n        return cur_step_output\n\n    async def _arun_step(\n        self,\n        task_id: str,\n        step: Optional[TaskStep] = None,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Execute step.\"\"\"\n        task = self.state.get_task(task_id)\n        task_queue = self.state.get_step_queue(task_id)\n        step = step or task_queue.popleft()\n\n        if not step.is_ready:\n            raise ValueError(f\"Step {step.step_id} is not ready\")\n\n        if mode == ChatResponseMode.WAIT:\n            cur_step_output = await self.agent_worker.arun_step(step, task, **kwargs)\n        elif mode == ChatResponseMode.STREAM:\n            cur_step_output = await self.agent_worker.astream_step(step, task, **kwargs)\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        for next_step in cur_step_output.next_steps:\n            if next_step.is_ready:\n                task_queue.append(next_step)\n\n        # add cur_step_output to completed steps\n        completed_steps = self.state.get_completed_steps(task_id)\n        completed_steps.append(cur_step_output)\n\n        return cur_step_output\n\n    def run_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        return self._run_step(task_id, step, mode=ChatResponseMode.WAIT, **kwargs)\n\n    async def arun_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        return await self._arun_step(\n            task_id, step, mode=ChatResponseMode.WAIT, **kwargs\n        )\n\n    def stream_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        return self._run_step(task_id, step, mode=ChatResponseMode.STREAM, **kwargs)\n\n    async def astream_step(\n        self,\n        task_id: str,\n        input: Optional[str] = None,\n        step: Optional[TaskStep] = None,\n        **kwargs: Any,\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        return await self._arun_step(\n            task_id, step, mode=ChatResponseMode.STREAM, **kwargs\n        )\n\n    def finalize_response(\n        self,\n        task_id: str,\n        step_output: Optional[TaskStepOutput] = None,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Finalize response.\"\"\"\n        if step_output is None:\n            step_output = self.state.get_completed_steps(task_id)[-1]\n        if not step_output.is_last:\n            raise ValueError(\n                \"finalize_response can only be called on the last step output\"\n            )\n\n        if not isinstance(\n            step_output.output,\n            (AgentChatResponse, StreamingAgentChatResponse),\n        ):\n            raise ValueError(\n                \"When `is_last` is True, cur_step_output.output must be \"\n                f\"AGENT_CHAT_RESPONSE_TYPE: {step_output.output}\"\n            )\n\n        # finalize task\n        self.agent_worker.finalize_task(self.state.get_task(task_id))\n\n        if self.delete_task_on_finish:\n            self.delete_task(task_id)\n\n        return cast(AGENT_CHAT_RESPONSE_TYPE, step_output.output)\n\n    def _chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Chat with step executor.\"\"\"\n        if chat_history is not None:\n            self.memory.set(chat_history)\n        task = self.create_task(message)\n\n        result_output = None\n        while True:\n            # pass step queue in as argument, assume step executor is stateless\n            cur_step_outputs = self.run_steps_in_queue(task.task_id, mode=mode)\n\n            # check if a step output is_last\n            is_last = any(\n                cur_step_output.is_last for cur_step_output in cur_step_outputs\n            )\n            if is_last:\n                if len(cur_step_outputs) > 1:\n                    raise ValueError(\n                        \"More than one step output returned in final step.\"\n                    )\n                cur_step_output = cur_step_outputs[0]\n                result_output = cur_step_output\n                break\n\n        return self.finalize_response(\n            task.task_id,\n            result_output,\n        )\n\n    async def _achat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Chat with step executor.\"\"\"\n        if chat_history is not None:\n            self.memory.set(chat_history)\n        task = self.create_task(message)\n\n        result_output = None\n        while True:\n            # pass step queue in as argument, assume step executor is stateless\n            cur_step_outputs = await self.arun_steps_in_queue(task.task_id, mode=mode)\n\n            # check if a step output is_last\n            is_last = any(\n                cur_step_output.is_last for cur_step_output in cur_step_outputs\n            )\n            if is_last:\n                if len(cur_step_outputs) > 1:\n                    raise ValueError(\n                        \"More than one step output returned in final step.\"\n                    )\n                cur_step_output = cur_step_outputs[0]\n                result_output = cur_step_output\n                break\n\n        return self.finalize_response(\n            task.task_id,\n            result_output,\n        )\n\n    @trace_method(\"chat\")\n    def chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n    ) -> AgentChatResponse:\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = self._chat(\n                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT\n            )\n            assert isinstance(chat_response, AgentChatResponse)\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n        return chat_response\n\n    @trace_method(\"chat\")\n    async def achat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n    ) -> AgentChatResponse:\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = await self._achat(\n                message, chat_history, tool_choice, mode=ChatResponseMode.WAIT\n            )\n            assert isinstance(chat_response, AgentChatResponse)\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n        return chat_response\n\n    @trace_method(\"chat\")\n    def stream_chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n    ) -> StreamingAgentChatResponse:\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = self._chat(\n                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM\n            )\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n\n        return chat_response  # type: ignore\n\n    @trace_method(\"chat\")\n    async def astream_chat(\n        self,\n        message: str,\n        chat_history: Optional[List[ChatMessage]] = None,\n        tool_choice: Union[str, dict] = \"auto\",\n    ) -> StreamingAgentChatResponse:\n        with self.callback_manager.event(\n            CBEventType.AGENT_STEP,\n            payload={EventPayload.MESSAGES: [message]},\n        ) as e:\n            chat_response = await self._achat(\n                message, chat_history, tool_choice, mode=ChatResponseMode.STREAM\n            )\n\n            e.on_end(payload={EventPayload.RESPONSE: chat_response})\n        return chat_response  # type: ignore\n\n    def undo_step(self, task_id: str) -> None:\n        \"\"\"Undo previous step.\"\"\"\n        raise NotImplementedError(\"undo_step not implemented\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/__init__.py",
    "filename": "__init__.py",
    "relpath": "agent/runner/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/planner.py",
    "filename": "planner.py",
    "relpath": "agent/runner/planner.py",
    "start_line": 1,
    "end_line": 135,
    "length": 135,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_completed_sub_tasks",
      "add_completed_sub_task",
      "get_next_sub_tasks",
      "get_remaining_subtasks",
      "reset"
    ],
    "chunk_class_names": [
      "SubTask",
      "Plan",
      "PlannerAgentState"
    ],
    "document_function_names": [
      "get_completed_sub_tasks",
      "add_completed_sub_task",
      "get_next_sub_tasks",
      "get_remaining_subtasks",
      "reset",
      "__init__",
      "get_tools",
      "get_next_tasks",
      "mark_task_complete",
      "create_plan",
      "acreate_plan",
      "get_refine_plan_prompt_kwargs",
      "_update_plan",
      "refine_plan",
      "arefine_plan",
      "run_task",
      "arun_task"
    ],
    "document_class_names": [
      "SubTask",
      "Plan",
      "PlannerAgentState",
      "StructuredPlannerAgent"
    ],
    "content": "import uuid\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nfrom llama_index.core.agent.runner.base import BasePlanningAgentRunner, AgentState\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    TaskStepOutput,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ValidationError\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.chat_engine.types import (\n    ChatResponseMode,\n    AGENT_CHAT_RESPONSE_TYPE,\n)\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.prompts import PromptTemplate\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools.types import BaseTool\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass SubTask(BaseModel):\n    \"\"\"A single sub-task in a plan.\"\"\"\n\n    name: str = Field(..., description=\"The name of the sub-task.\")\n    input: str = Field(..., description=\"The input prompt for the sub-task.\")\n    expected_output: str = Field(\n        ..., description=\"The expected output of the sub-task.\"\n    )\n    dependencies: List[str] = Field(\n        ...,\n        description=\"The sub-task names that must be completed before this sub-task.\",\n    )\n\n\nclass Plan(BaseModel):\n    \"\"\"A series of sub-tasks to accomplish an overall task.\"\"\"\n\n    sub_tasks: List[SubTask] = Field(..., description=\"The sub-tasks in the plan.\")\n\n\nclass PlannerAgentState(AgentState):\n    \"\"\"Agent state.\"\"\"\n\n    plan_dict: Dict[str, Plan] = Field(\n        default_factory=dict, description=\"An id-plan lookup.\"\n    )\n    completed_sub_tasks: Dict[str, List[SubTask]] = Field(\n        default_factory=dict, description=\"A list of completed sub-tasks for each plan.\"\n    )\n\n    def get_completed_sub_tasks(self, plan_id: str) -> List[SubTask]:\n        return self.completed_sub_tasks.get(plan_id, [])\n\n    def add_completed_sub_task(self, plan_id: str, sub_task: SubTask) -> None:\n        if plan_id not in self.completed_sub_tasks:\n            self.completed_sub_tasks[plan_id] = []\n\n        self.completed_sub_tasks[plan_id].append(sub_task)\n\n    def get_next_sub_tasks(self, plan_id: str) -> List[SubTask]:\n        next_sub_tasks: List[SubTask] = []\n        plan = self.plan_dict[plan_id]\n\n        if plan_id not in self.completed_sub_tasks:\n            self.completed_sub_tasks[plan_id] = []\n\n        completed_sub_tasks = self.completed_sub_tasks[plan_id]\n        completed_sub_task_names = [sub_task.name for sub_task in completed_sub_tasks]\n\n        for sub_task in plan.sub_tasks:\n            dependencies_met = all(\n                dep in completed_sub_task_names for dep in sub_task.dependencies\n            )\n\n            if sub_task.name not in completed_sub_task_names and dependencies_met:\n                next_sub_tasks.append(sub_task)\n        return next_sub_tasks\n\n    def get_remaining_subtasks(self, plan_id: str) -> List[SubTask]:\n        remaining_subtasks = []\n        plan = self.plan_dict[plan_id]\n\n        if plan_id not in self.completed_sub_tasks:\n            self.completed_sub_tasks[plan_id] = []\n\n        completed_sub_tasks = self.completed_sub_tasks[plan_id]\n        completed_sub_task_names = [sub_task.name for sub_task in completed_sub_tasks]\n\n        for sub_task in plan.sub_tasks:\n            if sub_task.name not in completed_sub_task_names:\n                remaining_subtasks.append(sub_task)\n        return remaining_subtasks\n\n    def reset(self) -> None:\n        \"\"\"Reset.\"\"\"\n        self.task_dict = {}\n        self.completed_sub_tasks = {}\n        self.plan_dict = {}\n\n\nDEFAULT_INITIAL_PLAN_PROMPT = \"\"\"\\\nThink step-by-step. Given a task and a set of tools, create a comprehensive, end-to-end plan to accomplish the task.\nKeep in mind not every task needs to be decomposed into multiple sub-tasks if it is simple enough.\nThe plan should end with a sub-task that can achieve the overall task.\n\nThe tools available are:\n{tools_str}\n\nOverall Task: {task}\n\"\"\"\n\nDEFAULT_PLAN_REFINE_PROMPT = \"\"\"\\\nThink step-by-step. Given an overall task, a set of tools, and completed sub-tasks, update (if needed) the remaining sub-tasks so that the overall task can still be completed.\nThe plan should end with a sub-task that can achieve and satisfy the overall task.\nIf you do update the plan, only create new sub-tasks that will replace the remaining sub-tasks, do NOT repeat tasks that are already completed.\nIf the remaining sub-tasks are enough to achieve the overall task, it is ok to skip this step, and instead explain why the plan is complete.\n\nThe tools available are:\n{tools_str}\n\nCompleted Sub-Tasks + Outputs:\n{completed_outputs}\n\nRemaining Sub-Tasks:\n{remaining_sub_tasks}\n\nOverall Task: {task}\n\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/runner/planner.py",
    "filename": "planner.py",
    "relpath": "agent/runner/planner.py",
    "start_line": 135,
    "end_line": 479,
    "length": 345,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "get_tools",
      "get_next_tasks",
      "mark_task_complete",
      "create_plan",
      "acreate_plan",
      "get_refine_plan_prompt_kwargs",
      "_update_plan",
      "refine_plan",
      "arefine_plan",
      "run_task",
      "arun_task"
    ],
    "chunk_class_names": [
      "StructuredPlannerAgent"
    ],
    "document_function_names": [
      "get_completed_sub_tasks",
      "add_completed_sub_task",
      "get_next_sub_tasks",
      "get_remaining_subtasks",
      "reset",
      "__init__",
      "get_tools",
      "get_next_tasks",
      "mark_task_complete",
      "create_plan",
      "acreate_plan",
      "get_refine_plan_prompt_kwargs",
      "_update_plan",
      "refine_plan",
      "arefine_plan",
      "run_task",
      "arun_task"
    ],
    "document_class_names": [
      "SubTask",
      "Plan",
      "PlannerAgentState",
      "StructuredPlannerAgent"
    ],
    "content": "class StructuredPlannerAgent(BasePlanningAgentRunner):\n    \"\"\"Structured Planner Agent runner.\n\n    Top-level agent orchestrator that can create tasks, run each step in a task,\n    or run a task e2e. Stores state and keeps track of tasks.\n\n    Args:\n        agent_worker (BaseAgentWorker): step executor\n        chat_history (Optional[List[ChatMessage]], optional): chat history. Defaults to None.\n        state (Optional[PlannerAgentState], optional): agent state. Defaults to None.\n        memory (Optional[BaseMemory], optional): memory. Defaults to None.\n        llm (Optional[LLM], optional): LLM. Defaults to None.\n        callback_manager (Optional[CallbackManager], optional): callback manager. Defaults to None.\n        init_task_state_kwargs (Optional[dict], optional): init task state kwargs. Defaults to None.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_worker: BaseAgentWorker,\n        tools: Optional[List[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        state: Optional[PlannerAgentState] = None,\n        memory: Optional[BaseMemory] = None,\n        llm: Optional[LLM] = None,\n        initial_plan_prompt: Union[str, PromptTemplate] = DEFAULT_INITIAL_PLAN_PROMPT,\n        plan_refine_prompt: Union[str, PromptTemplate] = DEFAULT_PLAN_REFINE_PROMPT,\n        callback_manager: Optional[CallbackManager] = None,\n        init_task_state_kwargs: Optional[dict] = None,\n        delete_task_on_finish: bool = False,\n        default_tool_choice: str = \"auto\",\n        verbose: bool = False,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.agent_worker = agent_worker\n        self.state: PlannerAgentState = state or PlannerAgentState()\n        self.memory = memory or ChatMemoryBuffer.from_defaults(chat_history, llm=llm)\n        self.tools = tools\n        self.tool_retriever = tool_retriever\n        self.llm = llm or Settings.llm\n\n        if isinstance(initial_plan_prompt, str):\n            initial_plan_prompt = PromptTemplate(initial_plan_prompt)\n        self.initial_plan_prompt = initial_plan_prompt\n\n        if isinstance(plan_refine_prompt, str):\n            plan_refine_prompt = PromptTemplate(plan_refine_prompt)\n        self.plan_refine_prompt = plan_refine_prompt\n\n        # get and set callback manager\n        if callback_manager is not None:\n            self.agent_worker.set_callback_manager(callback_manager)\n            self.callback_manager = callback_manager\n        else:\n            # TODO: This is *temporary*\n            # Stopgap before having a callback on the BaseAgentWorker interface.\n            # Doing that requires a bit more refactoring to make sure existing code\n            # doesn't break.\n            if hasattr(self.agent_worker, \"callback_manager\"):\n                self.callback_manager = (\n                    self.agent_worker.callback_manager or CallbackManager()\n                )\n            else:\n                self.callback_manager = Settings.callback_manager\n        self.init_task_state_kwargs = init_task_state_kwargs or {}\n        self.delete_task_on_finish = delete_task_on_finish\n        self.default_tool_choice = default_tool_choice\n        self.verbose = verbose\n\n    def get_tools(self, input: str) -> List[BaseTool]:\n        \"\"\"Get tools.\"\"\"\n        if self.tools is not None:\n            return self.tools\n        if self.tool_retriever is not None:\n            return self.tool_retriever.retrieve(input)\n        raise ValueError(\"No tools provided or retriever set.\")\n\n    def get_next_tasks(self, plan_id: str, **kwargs: Any) -> List[str]:\n        \"\"\"Get next task ids for a given plan.\"\"\"\n        upcoming_sub_tasks = self.state.get_next_sub_tasks(plan_id)\n        return [sub_task.name for sub_task in upcoming_sub_tasks]\n\n    def mark_task_complete(self, plan_id: str, task_id: str, **kwargs: Any) -> None:\n        \"\"\"Mark task complete for a given plan.\"\"\"\n        sub_tasks_by_id = {\n            sub_task.name: sub_task\n            for sub_task in self.state.plan_dict[plan_id].sub_tasks\n        }\n        self.state.add_completed_sub_task(plan_id, sub_tasks_by_id[task_id])\n\n    def create_plan(self, input: str, **kwargs: Any) -> str:\n        \"\"\"Create plan. Returns the plan_id.\"\"\"\n        tools = self.get_tools(input)\n        tools_str = \"\"\n        for tool in tools:\n            tools_str += (\n                (tool.metadata.name or \"\") + \": \" + tool.metadata.description + \"\\n\"\n            )\n\n        try:\n            plan = self.llm.structured_predict(\n                Plan,\n                self.initial_plan_prompt,\n                tools_str=tools_str,\n                task=input,\n            )\n        except (ValueError, ValidationError):\n            # likely no complex plan predicted\n            # default to a single task plan\n            if self.verbose:\n                print(\"No complex plan predicted. Defaulting to a single task plan.\")\n            plan = Plan(\n                sub_tasks=[\n                    SubTask(\n                        name=\"default\", input=input, expected_output=\"\", dependencies=[]\n                    )\n                ]\n            )\n\n        if self.verbose:\n            print(f\"=== Initial plan ===\")\n            for sub_task in plan.sub_tasks:\n                print(\n                    f\"{sub_task.name}:\\n{sub_task.input} -> {sub_task.expected_output}\\ndeps: {sub_task.dependencies}\\n\\n\"\n                )\n\n        plan_id = str(uuid.uuid4())\n        self.state.plan_dict[plan_id] = plan\n\n        for sub_task in plan.sub_tasks:\n            self.create_task(sub_task.input, task_id=sub_task.name)\n\n        return plan_id\n\n    async def acreate_plan(self, input: str, **kwargs: Any) -> str:\n        \"\"\"Create plan (async). Returns the plan_id.\"\"\"\n        tools = self.get_tools(input)\n        tools_str = \"\"\n        for tool in tools:\n            tools_str += (\n                (tool.metadata.name or \"\") + \": \" + tool.metadata.description + \"\\n\"\n            )\n\n        try:\n            plan = await self.llm.astructured_predict(\n                Plan,\n                self.initial_plan_prompt,\n                tools_str=tools_str,\n                task=input,\n            )\n        except (ValueError, ValidationError):\n            # likely no complex plan predicted\n            # default to a single task plan\n            if self.verbose:\n                print(\"No complex plan predicted. Defaulting to a single task plan.\")\n            plan = Plan(\n                sub_tasks=[\n                    SubTask(\n                        name=\"default\", input=input, expected_output=\"\", dependencies=[]\n                    )\n                ]\n            )\n\n        if self.verbose:\n            print(f\"=== Initial plan ===\")\n            for sub_task in plan.sub_tasks:\n                print(\n                    f\"{sub_task.name}:\\n{sub_task.input} -> {sub_task.expected_output}\\ndeps: {sub_task.dependencies}\\n\\n\"\n                )\n\n        plan_id = str(uuid.uuid4())\n        self.state.plan_dict[plan_id] = plan\n\n        for sub_task in plan.sub_tasks:\n            self.create_task(sub_task.input, task_id=sub_task.name)\n\n        return plan_id\n\n    def get_refine_plan_prompt_kwargs(\n        self,\n        plan_id: str,\n        task: str,\n        completed_sub_task_pairs: List[Tuple[SubTask, TaskStepOutput]],\n    ) -> dict:\n        \"\"\"Get the refine plan prompt.\"\"\"\n        # gather completed sub-tasks and response pairs\n        completed_outputs_str = \"\"\n        for sub_task, task_output in completed_sub_task_pairs:\n            task_str = f\"{sub_task.name}:\\n\" f\"\\t{task_output.output!s}\\n\"\n            completed_outputs_str += task_str\n\n        # get a string for the remaining sub-tasks\n        remaining_sub_tasks = self.state.get_remaining_subtasks(plan_id)\n        remaining_sub_tasks_str = \"\" if len(remaining_sub_tasks) != 0 else \"None\"\n        for sub_task in remaining_sub_tasks:\n            task_str = (\n                f\"SubTask(name='{sub_task.name}', \"\n                f\"input='{sub_task.input}', \"\n                f\"expected_output='{sub_task.expected_output}', \"\n                f\"dependencies='{sub_task.dependencies}')\\n\"\n            )\n            remaining_sub_tasks_str += task_str\n\n        # get the tools string\n        tools = self.get_tools(remaining_sub_tasks_str)\n        tools_str = \"\"\n        for tool in tools:\n            tools_str += (\n                (tool.metadata.name or \"\") + \": \" + tool.metadata.description + \"\\n\"\n            )\n\n        # return the kwargs\n        return {\n            \"tools_str\": tools_str.strip(),\n            \"task\": task.strip(),\n            \"completed_outputs\": completed_outputs_str.strip(),\n            \"remaining_sub_tasks\": remaining_sub_tasks_str.strip(),\n        }\n\n    def _update_plan(self, plan_id: str, new_plan: Plan) -> None:\n        \"\"\"Update the plan.\"\"\"\n        # update state with new plan\n        self.state.plan_dict[plan_id] = new_plan\n        for sub_task in new_plan.sub_tasks:\n            # insert new tasks\n            if sub_task.name in self.state.task_dict:\n                continue\n            self.create_task(sub_task.input, task_id=sub_task.name)\n\n        if self.verbose:\n            print(f\"=== Refined plan ===\")\n            for sub_task in new_plan.sub_tasks:\n                print(\n                    f\"{sub_task.name}:\\n{sub_task.input} -> {sub_task.expected_output}\\ndeps: {sub_task.dependencies}\\n\\n\"\n                )\n\n    def refine_plan(\n        self,\n        input: str,\n        plan_id: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Refine a plan.\"\"\"\n        completed_sub_tasks = self.state.get_completed_sub_tasks(plan_id)\n        completed_sub_task_pairs = [\n            (sub_task, self.get_task_output(sub_task.name))\n            for sub_task in completed_sub_tasks\n        ]\n\n        prompt_kwargs = self.get_refine_plan_prompt_kwargs(\n            plan_id, input, completed_sub_task_pairs\n        )\n\n        try:\n            new_plan = self.llm.structured_predict(\n                Plan, self.plan_refine_prompt, **prompt_kwargs\n            )\n\n            self._update_plan(plan_id, new_plan)\n        except (ValueError, ValidationError):\n            # likely no new plan predicted\n            return\n\n    async def arefine_plan(\n        self,\n        input: str,\n        plan_id: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Refine a plan.\"\"\"\n        completed_sub_tasks = self.state.get_completed_sub_tasks(plan_id)\n        completed_sub_task_pairs = [\n            (sub_task, self.get_task_output(sub_task.name))\n            for sub_task in completed_sub_tasks\n        ]\n\n        prompt_args = self.get_refine_plan_prompt_kwargs(\n            plan_id, input, completed_sub_task_pairs\n        )\n\n        try:\n            new_plan = await self.llm.astructured_predict(\n                Plan, self.plan_refine_prompt, **prompt_args\n            )\n\n            self._update_plan(plan_id, new_plan)\n        except (ValueError, ValidationError):\n            # likely no new plan predicted\n            return\n\n    def run_task(\n        self,\n        task_id: str,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        tool_choice: Union[str, dict] = \"auto\",\n        **kwargs: Any,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Run a task.\"\"\"\n        while True:\n            # pass step queue in as argument, assume step executor is stateless\n            cur_step_output = self._run_step(\n                task_id, mode=mode, tool_choice=tool_choice\n            )\n\n            if cur_step_output.is_last:\n                result_output = cur_step_output\n                break\n\n            # ensure tool_choice does not cause endless loops\n            tool_choice = \"auto\"\n\n        return self.finalize_response(\n            task_id,\n            result_output,\n        )\n\n    async def arun_task(\n        self,\n        task_id: str,\n        mode: ChatResponseMode = ChatResponseMode.WAIT,\n        tool_choice: Union[str, dict] = \"auto\",\n        **kwargs: Any,\n    ) -> AGENT_CHAT_RESPONSE_TYPE:\n        \"\"\"Run a task.\"\"\"\n        while True:\n            # pass step queue in as argument, assume step executor is stateless\n            cur_step_output = await self._arun_step(\n                task_id, mode=mode, tool_choice=tool_choice\n            )\n\n            if cur_step_output.is_last:\n                result_output = cur_step_output\n                break\n\n            # ensure tool_choice does not cause endless loops\n            tool_choice = \"auto\"\n\n        return self.finalize_response(\n            task_id,\n            result_output,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/types.py",
    "filename": "types.py",
    "relpath": "agent/react/types.py",
    "start_line": 1,
    "end_line": 78,
    "length": 78,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_content",
      "is_done",
      "get_content",
      "is_done",
      "get_content",
      "is_done",
      "get_content",
      "is_done"
    ],
    "chunk_class_names": [
      "BaseReasoningStep",
      "ActionReasoningStep",
      "ObservationReasoningStep",
      "ResponseReasoningStep"
    ],
    "document_function_names": [
      "get_content",
      "is_done",
      "get_content",
      "is_done",
      "get_content",
      "is_done",
      "get_content",
      "is_done"
    ],
    "document_class_names": [
      "BaseReasoningStep",
      "ActionReasoningStep",
      "ObservationReasoningStep",
      "ResponseReasoningStep"
    ],
    "content": "\"\"\"Base types for ReAct agent.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Dict\n\nfrom llama_index.core.bridge.pydantic import BaseModel\n\n\nclass BaseReasoningStep(BaseModel):\n    \"\"\"Reasoning step.\"\"\"\n\n    @abstractmethod\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n\n    @property\n    @abstractmethod\n    def is_done(self) -> bool:\n        \"\"\"Is the reasoning step the last one.\"\"\"\n\n\nclass ActionReasoningStep(BaseReasoningStep):\n    \"\"\"Action Reasoning step.\"\"\"\n\n    thought: str\n    action: str\n    action_input: Dict\n\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n        return (\n            f\"Thought: {self.thought}\\nAction: {self.action}\\n\"\n            f\"Action Input: {self.action_input}\"\n        )\n\n    @property\n    def is_done(self) -> bool:\n        \"\"\"Is the reasoning step the last one.\"\"\"\n        return False\n\n\nclass ObservationReasoningStep(BaseReasoningStep):\n    \"\"\"Observation reasoning step.\"\"\"\n\n    observation: str\n    return_direct: bool = False\n\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n        return f\"Observation: {self.observation}\"\n\n    @property\n    def is_done(self) -> bool:\n        \"\"\"Is the reasoning step the last one.\"\"\"\n        return self.return_direct\n\n\nclass ResponseReasoningStep(BaseReasoningStep):\n    \"\"\"Response reasoning step.\"\"\"\n\n    thought: str\n    response: str\n    is_streaming: bool = False\n\n    def get_content(self) -> str:\n        \"\"\"Get content.\"\"\"\n        if self.is_streaming:\n            return (\n                f\"Thought: {self.thought}\\n\"\n                f\"Answer (Starts With): {self.response} ...\"\n            )\n        else:\n            return f\"Thought: {self.thought}\\n\" f\"Answer: {self.response}\"\n\n    @property\n    def is_done(self) -> bool:\n        \"\"\"Is the reasoning step the last one.\"\"\"\n        return True"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/base.py",
    "filename": "base.py",
    "relpath": "agent/react/base.py",
    "start_line": 1,
    "end_line": 151,
    "length": 151,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_tools",
      "_get_prompt_modules"
    ],
    "chunk_class_names": [
      "ReActAgent"
    ],
    "document_function_names": [
      "__init__",
      "from_tools",
      "_get_prompt_modules"
    ],
    "document_class_names": [
      "ReActAgent"
    ],
    "content": "\"\"\"ReAct agent.\n\nSimple wrapper around AgentRunner + ReActAgentWorker.\n\nFor the legacy implementation see:\n```python\nfrom llama_index.core.agent.legacy.react.base import ReActAgent\n```\n\n\"\"\"\nfrom typing import (\n    Any,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    Callable,\n)\n\nfrom llama_index.core.agent.react.formatter import ReActChatFormatter\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.agent.react.step import ReActAgentWorker\nfrom llama_index.core.agent.runner.base import AgentRunner\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools import BaseTool, ToolOutput\nfrom llama_index.core.prompts.mixin import PromptMixinType\n\n\nclass ReActAgent(AgentRunner):\n    \"\"\"ReAct agent.\n\n    Subclasses AgentRunner with a ReActAgentWorker.\n\n    For the legacy implementation see:\n    ```python\n    from llama_index.core.agent.legacy.react.base import ReActAgent\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tools: Sequence[BaseTool],\n        llm: LLM,\n        memory: BaseMemory,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        context: Optional[str] = None,\n        handle_reasoning_failure_fn: Optional[\n            Callable[[CallbackManager, Exception], ToolOutput]\n        ] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        callback_manager = callback_manager or llm.callback_manager\n        if context and react_chat_formatter:\n            raise ValueError(\"Cannot provide both context and react_chat_formatter\")\n        if context:\n            react_chat_formatter = ReActChatFormatter.from_context(context)\n\n        step_engine = ReActAgentWorker.from_tools(\n            tools=tools,\n            tool_retriever=tool_retriever,\n            llm=llm,\n            max_iterations=max_iterations,\n            react_chat_formatter=react_chat_formatter,\n            output_parser=output_parser,\n            callback_manager=callback_manager,\n            verbose=verbose,\n            handle_reasoning_failure_fn=handle_reasoning_failure_fn,\n        )\n        super().__init__(\n            step_engine,\n            memory=memory,\n            llm=llm,\n            callback_manager=callback_manager,\n            verbose=verbose,\n        )\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: Optional[List[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        llm: Optional[LLM] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        context: Optional[str] = None,\n        handle_reasoning_failure_fn: Optional[\n            Callable[[CallbackManager, Exception], ToolOutput]\n        ] = None,\n        **kwargs: Any,\n    ) -> \"ReActAgent\":\n        \"\"\"Convenience constructor method from set of BaseTools (Optional).\n\n        NOTE: kwargs should have been exhausted by this point. In other words\n        the various upstream components such as BaseSynthesizer (response synthesizer)\n        or BaseRetriever should have picked up off their respective kwargs in their\n        constructions.\n\n        If `handle_reasoning_failure_fn` is provided, when LLM fails to follow the response templates specified in\n        the System Prompt, this function will be called. This function should provide to the Agent, so that the Agent\n        can have a second chance to fix its mistakes.\n        To handle the exception yourself, you can provide a function that raises the `Exception`.\n\n        Note: If you modified any response template in the System Prompt, you should override the method\n        `_extract_reasoning_step` in `ReActAgentWorker`.\n\n        Returns:\n            ReActAgent\n        \"\"\"\n        llm = llm or Settings.llm\n        if callback_manager is not None:\n            llm.callback_manager = callback_manager\n        memory = memory or memory_cls.from_defaults(\n            chat_history=chat_history or [], llm=llm\n        )\n        return cls(\n            tools=tools or [],\n            tool_retriever=tool_retriever,\n            llm=llm,\n            memory=memory,\n            max_iterations=max_iterations,\n            react_chat_formatter=react_chat_formatter,\n            output_parser=output_parser,\n            callback_manager=callback_manager,\n            verbose=verbose,\n            context=context,\n            handle_reasoning_failure_fn=handle_reasoning_failure_fn,\n        )\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {\"agent_worker\": self.agent_worker}"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/agent.py",
    "filename": "agent.py",
    "relpath": "agent/react/agent.py",
    "start_line": 1,
    "end_line": 10,
    "length": 10,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"ReAct agent.\n\nSimple wrapper around AgentRunner + ReActAgentWorker.\n\nFor the legacy implementation see:\n```python\nfrom llama_index.core.agent.legacy.react.base import ReActAgent\n```\n\n\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/output_parser.py",
    "filename": "output_parser.py",
    "relpath": "agent/react/output_parser.py",
    "start_line": 1,
    "end_line": 114,
    "length": 114,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "extract_tool_use",
      "action_input_parser",
      "extract_final_response",
      "parse_action_reasoning_step",
      "parse",
      "format"
    ],
    "chunk_class_names": [
      "ReActOutputParser"
    ],
    "document_function_names": [
      "extract_tool_use",
      "action_input_parser",
      "extract_final_response",
      "parse_action_reasoning_step",
      "parse",
      "format"
    ],
    "document_class_names": [
      "ReActOutputParser"
    ],
    "content": "\"\"\"ReAct output parser.\"\"\"\n\n\nimport re\nfrom typing import Tuple\n\nfrom llama_index.core.agent.react.types import (\n    ActionReasoningStep,\n    BaseReasoningStep,\n    ResponseReasoningStep,\n)\nfrom llama_index.core.output_parsers.utils import extract_json_str\nfrom llama_index.core.types import BaseOutputParser\n\n\ndef extract_tool_use(input_text: str) -> Tuple[str, str, str]:\n    pattern = (\n        r\"\\s*Thought: (.*?)\\n+Action: ([^\\n\\(\\) ]+).*?\\n+Action Input: .*?(\\{.*\\})\"\n    )\n\n    match = re.search(pattern, input_text, re.DOTALL)\n    if not match:\n        raise ValueError(f\"Could not extract tool use from input text: {input_text}\")\n\n    thought = match.group(1).strip()\n    action = match.group(2).strip()\n    action_input = match.group(3).strip()\n    return thought, action, action_input\n\n\ndef action_input_parser(json_str: str) -> dict:\n    processed_string = re.sub(r\"(?<!\\w)\\'|\\'(?!\\w)\", '\"', json_str)\n    pattern = r'\"(\\w+)\":\\s*\"([^\"]*)\"'\n    matches = re.findall(pattern, processed_string)\n    return dict(matches)\n\n\ndef extract_final_response(input_text: str) -> Tuple[str, str]:\n    pattern = r\"\\s*Thought:(.*?)Answer:(.*?)(?:$)\"\n\n    match = re.search(pattern, input_text, re.DOTALL)\n    if not match:\n        raise ValueError(\n            f\"Could not extract final answer from input text: {input_text}\"\n        )\n\n    thought = match.group(1).strip()\n    answer = match.group(2).strip()\n    return thought, answer\n\n\ndef parse_action_reasoning_step(output: str) -> ActionReasoningStep:\n    \"\"\"\n    Parse an action reasoning step from the LLM output.\n    \"\"\"\n    # Weaker LLMs may generate ReActAgent steps whose Action Input are horrible JSON strings.\n    # `dirtyjson` is more lenient than `json` in parsing JSON strings.\n    import dirtyjson as json\n\n    thought, action, action_input = extract_tool_use(output)\n    json_str = extract_json_str(action_input)\n    # First we try json, if this fails we use ast\n    try:\n        action_input_dict = json.loads(json_str)\n    except Exception:\n        action_input_dict = action_input_parser(json_str)\n    return ActionReasoningStep(\n        thought=thought, action=action, action_input=action_input_dict\n    )\n\n\nclass ReActOutputParser(BaseOutputParser):\n    \"\"\"ReAct Output parser.\"\"\"\n\n    def parse(self, output: str, is_streaming: bool = False) -> BaseReasoningStep:\n        \"\"\"Parse output from ReAct agent.\n\n        We expect the output to be in one of the following formats:\n        1. If the agent need to use a tool to answer the question:\n            ```\n            Thought: <thought>\n            Action: <action>\n            Action Input: <action_input>\n            ```\n        2. If the agent can answer the question without any tools:\n            ```\n            Thought: <thought>\n            Answer: <answer>\n            ```\n        \"\"\"\n        if \"Thought:\" not in output:\n            # NOTE: handle the case where the agent directly outputs the answer\n            # instead of following the thought-answer format\n            return ResponseReasoningStep(\n                thought=\"(Implicit) I can answer without any more tools!\",\n                response=output,\n                is_streaming=is_streaming,\n            )\n\n        # An \"Action\" should take priority over an \"Answer\"\n        if \"Action:\" in output:\n            return parse_action_reasoning_step(output)\n\n        if \"Answer:\" in output:\n            thought, answer = extract_final_response(output)\n            return ResponseReasoningStep(\n                thought=thought, response=answer, is_streaming=is_streaming\n            )\n\n        raise ValueError(f\"Could not parse output: {output}\")\n\n    def format(self, output: str) -> str:\n        \"\"\"Format a query with structured output formatting instructions.\"\"\"\n        raise NotImplementedError"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/__init__.py",
    "filename": "__init__.py",
    "relpath": "agent/react/__init__.py",
    "start_line": 1,
    "end_line": 6,
    "length": 6,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.agent.react.base import ReActAgent\nfrom llama_index.core.agent.react.formatter import ReActChatFormatter\nfrom llama_index.core.agent.react.step import ReActAgentWorker\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\n\n__all__ = [\"ReActChatFormatter\", \"ReActAgentWorker\", \"ReActAgent\", \"ReActOutputParser\"]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/formatter.py",
    "filename": "formatter.py",
    "relpath": "agent/react/formatter.py",
    "start_line": 1,
    "end_line": 139,
    "length": 139,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_react_tool_descriptions",
      "format",
      "format",
      "from_defaults",
      "from_context"
    ],
    "chunk_class_names": [
      "BaseAgentChatFormatter",
      "ReActChatFormatter"
    ],
    "document_function_names": [
      "get_react_tool_descriptions",
      "format",
      "format",
      "from_defaults",
      "from_context"
    ],
    "document_class_names": [
      "BaseAgentChatFormatter",
      "ReActChatFormatter"
    ],
    "content": "# ReAct agent formatter\n\nimport logging\nfrom abc import abstractmethod\nfrom typing import List, Optional, Sequence\n\nfrom llama_index.core.agent.react.prompts import (\n    CONTEXT_REACT_CHAT_SYSTEM_HEADER,\n    REACT_CHAT_SYSTEM_HEADER,\n)\nfrom llama_index.core.agent.react.types import (\n    BaseReasoningStep,\n    ObservationReasoningStep,\n)\nfrom llama_index.core.base.llms.types import ChatMessage, MessageRole\nfrom llama_index.core.bridge.pydantic import BaseModel, ConfigDict, Field\nfrom llama_index.core.tools import BaseTool\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_react_tool_descriptions(tools: Sequence[BaseTool]) -> List[str]:\n    \"\"\"Tool.\"\"\"\n    tool_descs = []\n    for tool in tools:\n        tool_desc = (\n            f\"> Tool Name: {tool.metadata.name}\\n\"\n            f\"Tool Description: {tool.metadata.description}\\n\"\n            f\"Tool Args: {tool.metadata.fn_schema_str}\\n\"\n        )\n        tool_descs.append(tool_desc)\n    return tool_descs\n\n\n# TODO: come up with better name\nclass BaseAgentChatFormatter(BaseModel):\n    \"\"\"Base chat formatter.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def format(\n        self,\n        tools: Sequence[BaseTool],\n        chat_history: List[ChatMessage],\n        current_reasoning: Optional[List[BaseReasoningStep]] = None,\n    ) -> List[ChatMessage]:\n        \"\"\"Format chat history into list of ChatMessage.\"\"\"\n\n\nclass ReActChatFormatter(BaseAgentChatFormatter):\n    \"\"\"ReAct chat formatter.\"\"\"\n\n    system_header: str = REACT_CHAT_SYSTEM_HEADER  # default\n    context: str = \"\"  # not needed w/ default\n    observation_role: MessageRole = Field(\n        default=MessageRole.USER,\n        description=(\n            \"Message role of tool outputs. If the LLM you use supports function/tool \"\n            \"calling, you may set it to `MessageRole.TOOL` to avoid the tool outputs \"\n            \"being misinterpreted as new user messages.\"\n        ),\n    )\n\n    def format(\n        self,\n        tools: Sequence[BaseTool],\n        chat_history: List[ChatMessage],\n        current_reasoning: Optional[List[BaseReasoningStep]] = None,\n    ) -> List[ChatMessage]:\n        \"\"\"Format chat history into list of ChatMessage.\"\"\"\n        current_reasoning = current_reasoning or []\n\n        format_args = {\n            \"tool_desc\": \"\\n\".join(get_react_tool_descriptions(tools)),\n            \"tool_names\": \", \".join([tool.metadata.get_name() for tool in tools]),\n        }\n        if self.context:\n            format_args[\"context\"] = self.context\n\n        fmt_sys_header = self.system_header.format(**format_args)\n\n        # format reasoning history as alternating user and assistant messages\n        # where the assistant messages are thoughts and actions and the tool\n        # messages are observations\n        reasoning_history = []\n        for reasoning_step in current_reasoning:\n            if isinstance(reasoning_step, ObservationReasoningStep):\n                message = ChatMessage(\n                    role=self.observation_role,\n                    content=reasoning_step.get_content(),\n                )\n            else:\n                message = ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=reasoning_step.get_content(),\n                )\n            reasoning_history.append(message)\n\n        return [\n            ChatMessage(role=MessageRole.SYSTEM, content=fmt_sys_header),\n            *chat_history,\n            *reasoning_history,\n        ]\n\n    @classmethod\n    def from_defaults(\n        cls,\n        system_header: Optional[str] = None,\n        context: Optional[str] = None,\n        observation_role: MessageRole = MessageRole.USER,\n    ) -> \"ReActChatFormatter\":\n        \"\"\"Create ReActChatFormatter from defaults.\"\"\"\n        if not system_header:\n            system_header = (\n                REACT_CHAT_SYSTEM_HEADER\n                if not context\n                else CONTEXT_REACT_CHAT_SYSTEM_HEADER\n            )\n\n        return ReActChatFormatter(\n            system_header=system_header,\n            context=context or \"\",\n            observation_role=observation_role,\n        )\n\n    @classmethod\n    def from_context(cls, context: str) -> \"ReActChatFormatter\":\n        \"\"\"Create ReActChatFormatter from context.\n\n        NOTE: deprecated\n\n        \"\"\"\n        logger.warning(\n            \"ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\"\n        )\n        return ReActChatFormatter.from_defaults(\n            system_header=CONTEXT_REACT_CHAT_SYSTEM_HEADER, context=context\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/prompts.py",
    "filename": "prompts.py",
    "relpath": "agent/react/prompts.py",
    "start_line": 1,
    "end_line": 21,
    "length": 21,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Default prompt for ReAct agent.\"\"\"\nfrom pathlib import Path\n\n# TODO: have formatting instructions be a part of react output parser\nwith (\n    Path(__file__).parents[0] / Path(\"templates\") / Path(\"system_header_template.md\")\n).open(\"r\") as f:\n    __BASE_REACT_CHAT_SYSTEM_HEADER = f.read()\n\nREACT_CHAT_SYSTEM_HEADER = __BASE_REACT_CHAT_SYSTEM_HEADER.replace(\n    \"{context_prompt}\", \"\", 1\n)\n\nCONTEXT_REACT_CHAT_SYSTEM_HEADER = __BASE_REACT_CHAT_SYSTEM_HEADER.replace(\n    \"{context_prompt}\",\n    \"\"\"\nHere is some context to help you answer the question and plan:\n{context}\n\"\"\",\n    1,\n)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/step.py",
    "filename": "step.py",
    "relpath": "agent/react/step.py",
    "start_line": 1,
    "end_line": 105,
    "length": 105,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "add_user_step_to_reasoning",
      "tell_llm_about_failure_in_extract_reasoning_step"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "add_user_step_to_reasoning",
      "tell_llm_about_failure_in_extract_reasoning_step",
      "__init__",
      "from_tools",
      "_get_prompts",
      "_update_prompts",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_handle_nonexistent_tool_name",
      "_get_response",
      "_get_task_step_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "gen",
      "_async_add_back_chunk_to_stream",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ReActAgentWorker"
    ],
    "content": "\"\"\"ReAct agent worker.\"\"\"\n\nimport asyncio\nimport json\nimport uuid\nfrom functools import partial\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Dict,\n    Generator,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    cast,\n    Callable,\n)\n\nfrom llama_index.core.agent.react.formatter import ReActChatFormatter\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.agent.react.types import (\n    ActionReasoningStep,\n    BaseReasoningStep,\n    ObservationReasoningStep,\n    ResponseReasoningStep,\n)\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.base.llms.types import MessageRole\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    CBEventType,\n    EventPayload,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n    AgentChatResponse,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.base.llms.types import ChatMessage, ChatResponse\nfrom llama_index.core.instrumentation import get_dispatcher\nfrom llama_index.core.instrumentation.events.agent import AgentToolCallEvent\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools import BaseTool, ToolOutput, adapt_to_async_tool\nfrom llama_index.core.tools.types import AsyncBaseTool\nfrom llama_index.core.types import Thread\nfrom llama_index.core.utils import print_text\n\ndispatcher = get_dispatcher(__name__)\n\n\ndef add_user_step_to_reasoning(\n    step: TaskStep,\n    memory: BaseMemory,\n    current_reasoning: List[BaseReasoningStep],\n    verbose: bool = False,\n) -> None:\n    \"\"\"Add user step to memory.\"\"\"\n    if \"is_first\" in step.step_state and step.step_state[\"is_first\"]:\n        # add to new memory\n        memory.put(ChatMessage(content=step.input, role=MessageRole.USER))\n        step.step_state[\"is_first\"] = False\n    else:\n        reasoning_step = ObservationReasoningStep(observation=step.input)\n        current_reasoning.append(reasoning_step)\n        if verbose:\n            print(f\"Added user message to memory: {step.input}\")\n\n\ndef tell_llm_about_failure_in_extract_reasoning_step(\n    callback_manager: CallbackManager, _: ValueError\n) -> ToolOutput:\n    \"\"\"\n    If the developer has instructed to tell the Agent a complaint about its non-cooperation,\n    we will emit a Tool Output that we prepared (at initialization time) to the LLM, so that\n    the LLM can be more cooperative in its next generation.\n    \"\"\"\n    message = \"Error: Could not parse output. Please follow the thought-action-input format. Try again.\"\n    dummy_tool_output = ToolOutput(\n        content=message,\n        tool_name=\"unknown\",\n        raw_input={},\n        raw_output=message,\n    )\n    with callback_manager.event(\n        CBEventType.FUNCTION_CALL,\n        payload={\n            EventPayload.FUNCTION_CALL: \"unknown\",\n        },\n    ) as event:\n        event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(dummy_tool_output)})\n\n    return dummy_tool_output"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/step.py",
    "filename": "step.py",
    "relpath": "agent/react/step.py",
    "start_line": 105,
    "end_line": 108,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "ReActAgentWorker"
    ],
    "document_function_names": [
      "add_user_step_to_reasoning",
      "tell_llm_about_failure_in_extract_reasoning_step",
      "__init__",
      "from_tools",
      "_get_prompts",
      "_update_prompts",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_handle_nonexistent_tool_name",
      "_get_response",
      "_get_task_step_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "gen",
      "_async_add_back_chunk_to_stream",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ReActAgentWorker"
    ],
    "content": "class ReActAgentWorker(BaseAgentWorker):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/step.py",
    "filename": "step.py",
    "relpath": "agent/react/step.py",
    "start_line": 108,
    "end_line": 526,
    "length": 419,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_tools",
      "_get_prompts",
      "_update_prompts",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_handle_nonexistent_tool_name",
      "_get_response",
      "_get_task_step_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "gen"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "add_user_step_to_reasoning",
      "tell_llm_about_failure_in_extract_reasoning_step",
      "__init__",
      "from_tools",
      "_get_prompts",
      "_update_prompts",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_handle_nonexistent_tool_name",
      "_get_response",
      "_get_task_step_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "gen",
      "_async_add_back_chunk_to_stream",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ReActAgentWorker"
    ],
    "content": "\"\"\"OpenAI Agent worker.\"\"\"\n\n    def __init__(\n        self,\n        tools: Sequence[BaseTool],\n        llm: LLM,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        handle_reasoning_failure_fn: Optional[\n            Callable[[CallbackManager, Exception], ToolOutput]\n        ] = None,\n    ) -> None:\n        self._llm = llm\n        self.callback_manager = callback_manager or llm.callback_manager\n        self._max_iterations = max_iterations\n        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter()\n        self._output_parser = output_parser or ReActOutputParser()\n        self._verbose = verbose\n        self._handle_reasoning_failure_fn = (\n            handle_reasoning_failure_fn\n            or tell_llm_about_failure_in_extract_reasoning_step\n        )\n\n        if len(tools) > 0 and tool_retriever is not None:\n            raise ValueError(\"Cannot specify both tools and tool_retriever\")\n        elif len(tools) > 0:\n            self._get_tools = lambda _: tools\n        elif tool_retriever is not None:\n            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)\n            self._get_tools = lambda message: tool_retriever_c.retrieve(message)\n        else:\n            self._get_tools = lambda _: []\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: Optional[Sequence[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        llm: Optional[LLM] = None,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        handle_reasoning_failure_fn: Optional[\n            Callable[[CallbackManager, Exception], ToolOutput]\n        ] = None,\n        **kwargs: Any,\n    ) -> \"ReActAgentWorker\":\n        \"\"\"Convenience constructor method from set of BaseTools (Optional).\n\n        NOTE: kwargs should have been exhausted by this point. In other words\n        the various upstream components such as BaseSynthesizer (response synthesizer)\n        or BaseRetriever should have picked up off their respective kwargs in their\n        constructions.\n\n        Returns:\n            ReActAgentWorker\n        \"\"\"\n        llm = llm or Settings.llm\n        if callback_manager is not None:\n            llm.callback_manager = callback_manager\n        return cls(\n            tools=tools or [],\n            tool_retriever=tool_retriever,\n            llm=llm,\n            max_iterations=max_iterations,\n            react_chat_formatter=react_chat_formatter,\n            output_parser=output_parser,\n            callback_manager=callback_manager,\n            verbose=verbose,\n            handle_reasoning_failure_fn=handle_reasoning_failure_fn,\n        )\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        # TODO: the ReAct formatter does not explicitly specify PromptTemplate\n        # objects, but wrap it in this to obey the interface\n        sys_header = self._react_chat_formatter.system_header\n        return {\"system_prompt\": PromptTemplate(sys_header)}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"system_prompt\" in prompts:\n            sys_prompt = cast(PromptTemplate, prompts[\"system_prompt\"])\n            self._react_chat_formatter.system_header = sys_prompt.template\n\n    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:\n        \"\"\"Initialize step from task.\"\"\"\n        sources: List[ToolOutput] = []\n        current_reasoning: List[BaseReasoningStep] = []\n        # temporary memory for new messages\n        new_memory = ChatMemoryBuffer.from_defaults()\n\n        # initialize task state\n        task_state = {\n            \"sources\": sources,\n            \"current_reasoning\": current_reasoning,\n            \"new_memory\": new_memory,\n        }\n        task.extra_state.update(task_state)\n\n        return TaskStep(\n            task_id=task.task_id,\n            step_id=str(uuid.uuid4()),\n            input=task.input,\n            step_state={\"is_first\": True},\n        )\n\n    def get_tools(self, input: str) -> List[AsyncBaseTool]:\n        \"\"\"Get tools.\"\"\"\n        return [adapt_to_async_tool(t) for t in self._get_tools(input)]\n\n    def _extract_reasoning_step(\n        self, output: ChatResponse, is_streaming: bool = False\n    ) -> Tuple[str, List[BaseReasoningStep], bool]:\n        \"\"\"\n        Extracts the reasoning step from the given output.\n\n        This method parses the message content from the output,\n        extracts the reasoning step, and determines whether the processing is\n        complete. It also performs validation checks on the output and\n        handles possible errors.\n        \"\"\"\n        if output.message.content is None:\n            raise ValueError(\"Got empty message.\")\n        message_content = output.message.content\n\n        current_reasoning = []\n        try:\n            reasoning_step = self._output_parser.parse(message_content, is_streaming)\n        except BaseException as exc:\n            raise ValueError(f\"Could not parse output: {message_content}\") from exc\n        if self._verbose:\n            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")\n        current_reasoning.append(reasoning_step)\n\n        if reasoning_step.is_done:\n            return message_content, current_reasoning, True\n\n        reasoning_step = cast(ActionReasoningStep, reasoning_step)\n        if not isinstance(reasoning_step, ActionReasoningStep):\n            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")\n\n        return message_content, current_reasoning, False\n\n    def _process_actions(\n        self,\n        task: Task,\n        tools: Sequence[AsyncBaseTool],\n        output: ChatResponse,\n        is_streaming: bool = False,\n    ) -> Tuple[List[BaseReasoningStep], bool]:\n        tools_dict: Dict[str, AsyncBaseTool] = {\n            tool.metadata.get_name(): tool for tool in tools\n        }\n        tool = None\n\n        try:\n            _, current_reasoning, is_done = self._extract_reasoning_step(\n                output, is_streaming\n            )\n        except ValueError as exp:\n            current_reasoning = []\n            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)\n        else:\n            if is_done:\n                return current_reasoning, True\n\n            # call tool with input\n            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])\n            if reasoning_step.action in tools_dict:\n                tool = tools_dict[reasoning_step.action]\n                with self.callback_manager.event(\n                    CBEventType.FUNCTION_CALL,\n                    payload={\n                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,\n                        EventPayload.TOOL: tool.metadata,\n                    },\n                ) as event:\n                    try:\n                        dispatcher.event(\n                            AgentToolCallEvent(\n                                arguments=json.dumps({**reasoning_step.action_input}),\n                                tool=tool.metadata,\n                            )\n                        )\n                        tool_output = tool.call(**reasoning_step.action_input)\n                    except Exception as e:\n                        tool_output = ToolOutput(\n                            content=f\"Error: {e!s}\",\n                            tool_name=tool.metadata.name,\n                            raw_input={\"kwargs\": reasoning_step.action_input},\n                            raw_output=e,\n                            is_error=True,\n                        )\n                    event.on_end(\n                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}\n                    )\n            else:\n                tool_output = self._handle_nonexistent_tool_name(reasoning_step)\n\n        task.extra_state[\"sources\"].append(tool_output)\n\n        observation_step = ObservationReasoningStep(\n            observation=str(tool_output),\n            return_direct=(\n                tool.metadata.return_direct and not tool_output.is_error\n                if tool\n                else False\n            ),\n        )\n        current_reasoning.append(observation_step)\n        if self._verbose:\n            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")\n        return (\n            current_reasoning,\n            tool.metadata.return_direct and not tool_output.is_error if tool else False,\n        )\n\n    async def _aprocess_actions(\n        self,\n        task: Task,\n        tools: Sequence[AsyncBaseTool],\n        output: ChatResponse,\n        is_streaming: bool = False,\n    ) -> Tuple[List[BaseReasoningStep], bool]:\n        tools_dict = {tool.metadata.name: tool for tool in tools}\n        tool = None\n\n        try:\n            _, current_reasoning, is_done = self._extract_reasoning_step(\n                output, is_streaming\n            )\n        except ValueError as exp:\n            current_reasoning = []\n            tool_output = self._handle_reasoning_failure_fn(self.callback_manager, exp)\n        else:\n            if is_done:\n                return current_reasoning, True\n\n            # call tool with input\n            reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])\n            if reasoning_step.action in tools_dict:\n                tool = tools_dict[reasoning_step.action]\n                with self.callback_manager.event(\n                    CBEventType.FUNCTION_CALL,\n                    payload={\n                        EventPayload.FUNCTION_CALL: reasoning_step.action_input,\n                        EventPayload.TOOL: tool.metadata,\n                    },\n                ) as event:\n                    try:\n                        dispatcher.event(\n                            AgentToolCallEvent(\n                                arguments=json.dumps({**reasoning_step.action_input}),\n                                tool=tool.metadata,\n                            )\n                        )\n                        tool_output = await tool.acall(**reasoning_step.action_input)\n                    except Exception as e:\n                        tool_output = ToolOutput(\n                            content=f\"Error: {e!s}\",\n                            tool_name=tool.metadata.name,\n                            raw_input={\"kwargs\": reasoning_step.action_input},\n                            raw_output=e,\n                            is_error=True,\n                        )\n                    event.on_end(\n                        payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)}\n                    )\n            else:\n                tool_output = self._handle_nonexistent_tool_name(reasoning_step)\n\n        task.extra_state[\"sources\"].append(tool_output)\n\n        observation_step = ObservationReasoningStep(\n            observation=str(tool_output),\n            return_direct=(\n                tool.metadata.return_direct and not tool_output.is_error\n                if tool\n                else False\n            ),\n        )\n        current_reasoning.append(observation_step)\n        if self._verbose:\n            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")\n        return (\n            current_reasoning,\n            tool.metadata.return_direct and not tool_output.is_error if tool else False,\n        )\n\n    def _handle_nonexistent_tool_name(\n        self, reasoning_step: ActionReasoningStep\n    ) -> ToolOutput:\n        # We still emit a `tool_output` object to the task, so that the LLM can know\n        # it has hallucinated in the next reasoning step.\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: reasoning_step.action_input,\n            },\n        ) as event:\n            # TODO(L10N): This should be localized.\n            content = f\"Error: No such tool named `{reasoning_step.action}`.\"\n            tool_output = ToolOutput(\n                content=content,\n                tool_name=reasoning_step.action,\n                raw_input={\"kwargs\": reasoning_step.action_input},\n                raw_output=content,\n                is_error=True,\n            )\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n        return tool_output\n\n    def _get_response(\n        self,\n        current_reasoning: List[BaseReasoningStep],\n        sources: List[ToolOutput],\n    ) -> AgentChatResponse:\n        \"\"\"Get response from reasoning steps.\"\"\"\n        if len(current_reasoning) == 0:\n            raise ValueError(\"No reasoning steps were taken.\")\n        elif len(current_reasoning) == self._max_iterations:\n            raise ValueError(\"Reached max iterations.\")\n\n        if isinstance(current_reasoning[-1], ResponseReasoningStep):\n            response_step = cast(ResponseReasoningStep, current_reasoning[-1])\n            response_str = response_step.response\n        elif (\n            isinstance(current_reasoning[-1], ObservationReasoningStep)\n            and current_reasoning[-1].return_direct\n        ):\n            response_str = current_reasoning[-1].observation\n        else:\n            response_str = current_reasoning[-1].get_content()\n\n        # TODO: add sources from reasoning steps\n        return AgentChatResponse(response=response_str, sources=sources)\n\n    def _get_task_step_response(\n        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool\n    ) -> TaskStepOutput:\n        \"\"\"Get task step response.\"\"\"\n        if is_done:\n            new_steps = []\n        else:\n            new_steps = [\n                step.get_next_step(\n                    step_id=str(uuid.uuid4()),\n                    # NOTE: input is unused\n                    input=None,\n                )\n            ]\n\n        return TaskStepOutput(\n            output=agent_response,\n            task_step=step,\n            is_last=is_done,\n            next_steps=new_steps,\n        )\n\n    def _infer_stream_chunk_is_final(\n        self, chunk: ChatResponse, missed_chunks_storage: list\n    ) -> bool:\n        \"\"\"Infers if a chunk from a live stream is the start of the final\n        reasoning step. (i.e., and should eventually become\n        ResponseReasoningStep \u2014 not part of this function's logic tho.).\n\n        Args:\n            chunk (ChatResponse): the current chunk stream to check\n            missed_chunks_storage (list): list to store missed chunks\n\n        Returns:\n            bool: Boolean on whether the chunk is the start of the final response\n        \"\"\"\n        latest_content = (\n            None if chunk.message.content is None else chunk.message.content.strip()\n        )\n        if latest_content:\n            # doesn't follow thought-action format\n            # keep first chunks\n            if len(latest_content) < len(\"Thought\"):\n                missed_chunks_storage.append(chunk)\n            elif not latest_content.startswith(\"Thought\"):\n                return True\n            elif \"Answer:\" in latest_content:\n                missed_chunks_storage.clear()\n                return True\n        return False\n\n    def _add_back_chunk_to_stream(\n        self,\n        chunks: List[ChatResponse],\n        chat_stream: Generator[ChatResponse, None, None],\n    ) -> Generator[ChatResponse, None, None]:\n        \"\"\"Helper method for adding back initial chunk stream of final response\n        back to the rest of the chat_stream.\n\n        Args:\n            chunks List[ChatResponse]: the chunks to add back to the beginning of the\n                                    chat_stream.\n\n        Return:\n            Generator[ChatResponse, None, None]: the updated chat_stream\n        \"\"\"\n\n        def gen() -> Generator[ChatResponse, None, None]:\n            yield from chunks\n            yield from chat_stream\n\n        return gen()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react/step.py",
    "filename": "step.py",
    "relpath": "agent/react/step.py",
    "start_line": 526,
    "end_line": 854,
    "length": 329,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_async_add_back_chunk_to_stream",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "add_user_step_to_reasoning",
      "tell_llm_about_failure_in_extract_reasoning_step",
      "__init__",
      "from_tools",
      "_get_prompts",
      "_update_prompts",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_handle_nonexistent_tool_name",
      "_get_response",
      "_get_task_step_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "gen",
      "_async_add_back_chunk_to_stream",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ReActAgentWorker"
    ],
    "content": "async def _async_add_back_chunk_to_stream(\n        self,\n        chunks: List[ChatResponse],\n        chat_stream: AsyncGenerator[ChatResponse, None],\n    ) -> AsyncGenerator[ChatResponse, None]:\n        \"\"\"Helper method for adding back initial chunk stream of final response\n        back to the rest of the chat_stream.\n\n        NOTE: this itself is not an async function.\n\n        Args:\n            chunks List[ChatResponse]: the chunks to add back to the beginning of the\n                                    chat_stream.\n\n        Return:\n            AsyncGenerator[ChatResponse, None]: the updated async chat_stream\n        \"\"\"\n        for chunk in chunks:\n            yield chunk\n\n        async for item in chat_stream:\n            yield item\n\n    def _run_step(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        if step.input is not None:\n            add_user_step_to_reasoning(\n                step,\n                task.extra_state[\"new_memory\"],\n                task.extra_state[\"current_reasoning\"],\n                verbose=self._verbose,\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n        input_chat = self._react_chat_formatter.format(\n            tools,\n            chat_history=task.memory.get(input=task.input)\n            + task.extra_state[\"new_memory\"].get_all(),\n            current_reasoning=task.extra_state[\"current_reasoning\"],\n        )\n\n        # send prompt\n        chat_response = self._llm.chat(input_chat)\n        # given react prompt outputs, call tools or return response\n        reasoning_steps, is_done = self._process_actions(\n            task, tools, output=chat_response\n        )\n        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)\n        agent_response = self._get_response(\n            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]\n        )\n        if is_done:\n            task.extra_state[\"new_memory\"].put(\n                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)\n            )\n\n        return self._get_task_step_response(agent_response, step, is_done)\n\n    async def _arun_step(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        if step.input is not None:\n            add_user_step_to_reasoning(\n                step,\n                task.extra_state[\"new_memory\"],\n                task.extra_state[\"current_reasoning\"],\n                verbose=self._verbose,\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n\n        input_chat = self._react_chat_formatter.format(\n            tools,\n            chat_history=task.memory.get(input=task.input)\n            + task.extra_state[\"new_memory\"].get_all(),\n            current_reasoning=task.extra_state[\"current_reasoning\"],\n        )\n        # send prompt\n        chat_response = await self._llm.achat(input_chat)\n        # given react prompt outputs, call tools or return response\n        reasoning_steps, is_done = await self._aprocess_actions(\n            task, tools, output=chat_response\n        )\n        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)\n        agent_response = self._get_response(\n            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]\n        )\n        if is_done:\n            task.extra_state[\"new_memory\"].put(\n                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)\n            )\n\n        return self._get_task_step_response(agent_response, step, is_done)\n\n    def _run_step_stream(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        if step.input is not None:\n            add_user_step_to_reasoning(\n                step,\n                task.extra_state[\"new_memory\"],\n                task.extra_state[\"current_reasoning\"],\n                verbose=self._verbose,\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n\n        input_chat = self._react_chat_formatter.format(\n            tools,\n            chat_history=task.memory.get(input=task.input)\n            + task.extra_state[\"new_memory\"].get_all(),\n            current_reasoning=task.extra_state[\"current_reasoning\"],\n        )\n\n        chat_stream = self._llm.stream_chat(input_chat)\n\n        # iterate over stream, break out if is final answer after the \"Answer: \"\n        full_response = ChatResponse(\n            message=ChatMessage(content=None, role=\"assistant\")\n        )\n        missed_chunks_storage: List[ChatResponse] = []\n        is_done = False\n        for latest_chunk in chat_stream:\n            full_response = latest_chunk\n            is_done = self._infer_stream_chunk_is_final(\n                latest_chunk, missed_chunks_storage\n            )\n            if is_done:\n                break\n\n        non_streaming_agent_response = None\n        agent_response_stream = None\n        if not is_done:\n            # given react prompt outputs, call tools or return response\n            reasoning_steps, is_done = self._process_actions(\n                task, tools=tools, output=full_response, is_streaming=True\n            )\n            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)\n            # use _get_response to return intermediate response\n            non_streaming_agent_response = self._get_response(\n                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]\n            )\n            if is_done:\n                non_streaming_agent_response.is_dummy_stream = True\n                task.extra_state[\"new_memory\"].put(\n                    ChatMessage(\n                        content=non_streaming_agent_response.response,\n                        role=MessageRole.ASSISTANT,\n                    )\n                )\n        else:\n            # remove \"Answer: \" from the response, and anything before it\n            start_idx = (latest_chunk.message.content or \"\").find(\"Answer:\")\n            if start_idx != -1 and latest_chunk.message.content:\n                latest_chunk.message.content = latest_chunk.message.content[\n                    start_idx + len(\"Answer:\") :\n                ].strip()\n\n                # set delta to the content, minus the \"Answer: \"\n                latest_chunk.delta = latest_chunk.message.content\n\n            # add back the chunks that were missed\n            response_stream = self._add_back_chunk_to_stream(\n                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream\n            )\n\n            # Get the response in a separate thread so we can yield the response\n            agent_response_stream = StreamingAgentChatResponse(\n                chat_stream=response_stream,\n                sources=task.extra_state[\"sources\"],\n            )\n            thread = Thread(\n                target=agent_response_stream.write_response_to_history,\n                args=(task.extra_state[\"new_memory\"],),\n                kwargs={\"on_stream_end_fn\": partial(self.finalize_task, task)},\n            )\n            thread.start()\n\n        response = agent_response_stream or non_streaming_agent_response\n        assert response is not None\n\n        return self._get_task_step_response(response, step, is_done)\n\n    async def _arun_step_stream(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        if step.input is not None:\n            add_user_step_to_reasoning(\n                step,\n                task.extra_state[\"new_memory\"],\n                task.extra_state[\"current_reasoning\"],\n                verbose=self._verbose,\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n\n        input_chat = self._react_chat_formatter.format(\n            tools,\n            chat_history=task.memory.get(input=task.input)\n            + task.extra_state[\"new_memory\"].get_all(),\n            current_reasoning=task.extra_state[\"current_reasoning\"],\n        )\n\n        chat_stream = await self._llm.astream_chat(input_chat)\n\n        # iterate over stream, break out if is final answer after the \"Answer: \"\n        full_response = ChatResponse(\n            message=ChatMessage(content=None, role=\"assistant\")\n        )\n        missed_chunks_storage: List[ChatResponse] = []\n        is_done = False\n        async for latest_chunk in chat_stream:\n            full_response = latest_chunk\n            is_done = self._infer_stream_chunk_is_final(\n                latest_chunk, missed_chunks_storage\n            )\n            if is_done:\n                break\n\n        non_streaming_agent_response = None\n        agent_response_stream = None\n        if not is_done:\n            # given react prompt outputs, call tools or return response\n            reasoning_steps, is_done = await self._aprocess_actions(\n                task, tools=tools, output=full_response, is_streaming=True\n            )\n            task.extra_state[\"current_reasoning\"].extend(reasoning_steps)\n            # use _get_response to return intermediate response\n            non_streaming_agent_response = self._get_response(\n                task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]\n            )\n\n            if is_done:\n                non_streaming_agent_response.is_dummy_stream = True\n                task.extra_state[\"new_memory\"].put(\n                    ChatMessage(\n                        content=non_streaming_agent_response.response,\n                        role=MessageRole.ASSISTANT,\n                    )\n                )\n        else:\n            # remove \"Answer: \" from the response, and anything before it\n            start_idx = (latest_chunk.message.content or \"\").find(\"Answer:\")\n            if start_idx != -1 and latest_chunk.message.content:\n                latest_chunk.message.content = latest_chunk.message.content[\n                    start_idx + len(\"Answer:\") :\n                ].strip()\n\n            # set delta to the content, minus the \"Answer: \"\n            latest_chunk.delta = latest_chunk.message.content\n\n            # add back the chunks that were missed\n            response_stream = self._async_add_back_chunk_to_stream(\n                chunks=[*missed_chunks_storage, latest_chunk], chat_stream=chat_stream\n            )\n\n            agent_response_stream = StreamingAgentChatResponse(\n                achat_stream=response_stream,\n                sources=task.extra_state[\"sources\"],\n            )\n            # create task to write chat response to history\n            agent_response_stream.awrite_response_to_history_task = asyncio.create_task(\n                agent_response_stream.awrite_response_to_history(\n                    task.extra_state[\"new_memory\"],\n                    on_stream_end_fn=partial(self.finalize_task, task),\n                )\n            )\n            # wait until response writing is done\n            agent_response_stream._ensure_async_setup()\n\n            assert agent_response_stream.is_function_false_event is not None\n            await agent_response_stream.is_function_false_event.wait()\n\n        response = agent_response_stream or non_streaming_agent_response\n        assert response is not None\n\n        return self._get_task_step_response(response, step, is_done)\n\n    @trace_method(\"run_step\")\n    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        return self._run_step(step, task)\n\n    @trace_method(\"run_step\")\n    async def arun_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        return await self._arun_step(step, task)\n\n    @trace_method(\"run_step\")\n    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        # TODO: figure out if we need a different type for TaskStepOutput\n        return self._run_step_stream(step, task)\n\n    @trace_method(\"run_step\")\n    async def astream_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        return await self._arun_step_stream(step, task)\n\n    def finalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n        # add new messages to memory\n        task.memory.set(\n            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()\n        )\n        # reset new memory\n        task.extra_state[\"new_memory\"].reset()\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: make this abstractmethod (right now will break some agent impls)\n        self.callback_manager = callback_manager"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/legacy/__init__.py",
    "filename": "__init__.py",
    "relpath": "agent/legacy/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/legacy/react/base.py",
    "filename": "base.py",
    "relpath": "agent/legacy/react/base.py",
    "start_line": 1,
    "end_line": 45,
    "length": 45,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_tools",
      "chat_history",
      "reset",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "_async_add_back_chunk_to_stream",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "get_tools"
    ],
    "document_class_names": [
      "ReActAgent"
    ],
    "content": "import asyncio\nfrom itertools import chain\nfrom typing import (\n    Any,\n    AsyncGenerator,\n    Dict,\n    Generator,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    cast,\n)\n\nfrom llama_index.core.agent.react.formatter import ReActChatFormatter\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.agent.react.types import (\n    ActionReasoningStep,\n    BaseReasoningStep,\n    ObservationReasoningStep,\n    ResponseReasoningStep,\n)\nfrom llama_index.core.agent.types import BaseAgent\nfrom llama_index.core.base.llms.types import MessageRole\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    CBEventType,\n    EventPayload,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AgentChatResponse,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.base.llms.types import ChatMessage, ChatResponse\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools import BaseTool, ToolOutput, adapt_to_async_tool\nfrom llama_index.core.tools.types import AsyncBaseTool\nfrom llama_index.core.types import Thread\nfrom llama_index.core.utils import print_text, unit_generator"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/legacy/react/base.py",
    "filename": "base.py",
    "relpath": "agent/legacy/react/base.py",
    "start_line": 45,
    "end_line": 48,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "ReActAgent"
    ],
    "document_function_names": [
      "__init__",
      "from_tools",
      "chat_history",
      "reset",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "_async_add_back_chunk_to_stream",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "get_tools"
    ],
    "document_class_names": [
      "ReActAgent"
    ],
    "content": "class ReActAgent(BaseAgent):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/legacy/react/base.py",
    "filename": "base.py",
    "relpath": "agent/legacy/react/base.py",
    "start_line": 48,
    "end_line": 466,
    "length": 419,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_tools",
      "chat_history",
      "reset",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "_async_add_back_chunk_to_stream",
      "chat",
      "achat",
      "stream_chat"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_tools",
      "chat_history",
      "reset",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "_async_add_back_chunk_to_stream",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "get_tools"
    ],
    "document_class_names": [
      "ReActAgent"
    ],
    "content": "\"\"\"ReAct agent.\n\n    Uses a ReAct prompt that can be used in both chat and text\n    completion endpoints.\n\n    Can take in a set of tools that require structured inputs.\n    \"\"\"\n\n    def __init__(\n        self,\n        tools: Sequence[BaseTool],\n        llm: LLM,\n        memory: BaseMemory,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n    ) -> None:\n        super().__init__(callback_manager=callback_manager or llm.callback_manager)\n        self._llm = llm\n        self._memory = memory\n        self._max_iterations = max_iterations\n        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter()\n        self._output_parser = output_parser or ReActOutputParser()\n        self._verbose = verbose\n        self.sources: List[ToolOutput] = []\n\n        if len(tools) > 0 and tool_retriever is not None:\n            raise ValueError(\"Cannot specify both tools and tool_retriever\")\n        elif len(tools) > 0:\n            self._get_tools = lambda _: tools\n        elif tool_retriever is not None:\n            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)\n            self._get_tools = lambda message: tool_retriever_c.retrieve(message)\n        else:\n            self._get_tools = lambda _: []\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: Optional[List[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        llm: Optional[LLM] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        memory_cls: Type[BaseMemory] = ChatMemoryBuffer,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"ReActAgent\":\n        \"\"\"Convenience constructor method from set of BaseTools (Optional).\n\n        NOTE: kwargs should have been exhausted by this point. In other words\n        the various upstream components such as BaseSynthesizer (response synthesizer)\n        or BaseRetriever should have picked up off their respective kwargs in their\n        constructions.\n\n        Returns:\n            ReActAgent\n        \"\"\"\n        llm = llm or Settings.llm\n        if callback_manager is not None:\n            llm.callback_manager = callback_manager\n        memory = memory or memory_cls.from_defaults(\n            chat_history=chat_history or [], llm=llm\n        )\n        return cls(\n            tools=tools or [],\n            tool_retriever=tool_retriever,\n            llm=llm,\n            memory=memory,\n            max_iterations=max_iterations,\n            react_chat_formatter=react_chat_formatter,\n            output_parser=output_parser,\n            callback_manager=callback_manager,\n            verbose=verbose,\n        )\n\n    @property\n    def chat_history(self) -> List[ChatMessage]:\n        \"\"\"Chat history.\"\"\"\n        return self._memory.get_all()\n\n    def reset(self) -> None:\n        self._memory.reset()\n\n    def _extract_reasoning_step(\n        self, output: ChatResponse, is_streaming: bool = False\n    ) -> Tuple[str, List[BaseReasoningStep], bool]:\n        \"\"\"\n        Extracts the reasoning step from the given output.\n\n        This method parses the message content from the output,\n        extracts the reasoning step, and determines whether the processing is\n        complete. It also performs validation checks on the output and\n        handles possible errors.\n        \"\"\"\n        if output.message.content is None:\n            raise ValueError(\"Got empty message.\")\n        message_content = output.message.content\n\n        current_reasoning = []\n        try:\n            reasoning_step = self._output_parser.parse(message_content, is_streaming)\n        except BaseException as exc:\n            raise ValueError(f\"Could not parse output: {message_content}\") from exc\n        if self._verbose:\n            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")\n        current_reasoning.append(reasoning_step)\n\n        if reasoning_step.is_done:\n            return message_content, current_reasoning, True\n\n        reasoning_step = cast(ActionReasoningStep, reasoning_step)\n        if not isinstance(reasoning_step, ActionReasoningStep):\n            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")\n\n        return message_content, current_reasoning, False\n\n    def _process_actions(\n        self,\n        tools: Sequence[AsyncBaseTool],\n        output: ChatResponse,\n        is_streaming: bool = False,\n    ) -> Tuple[List[BaseReasoningStep], bool]:\n        tools_dict: Dict[str, AsyncBaseTool] = {\n            tool.metadata.get_name(): tool for tool in tools\n        }\n        _, current_reasoning, is_done = self._extract_reasoning_step(\n            output, is_streaming\n        )\n\n        if is_done:\n            return current_reasoning, True\n\n        # call tool with input\n        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])\n        tool = tools_dict[reasoning_step.action]\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: reasoning_step.action_input,\n                EventPayload.TOOL: tool.metadata,\n            },\n        ) as event:\n            tool_output = tool.call(**reasoning_step.action_input)\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n\n        self.sources.append(tool_output)\n\n        observation_step = ObservationReasoningStep(observation=str(tool_output))\n        current_reasoning.append(observation_step)\n        if self._verbose:\n            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")\n        return current_reasoning, False\n\n    async def _aprocess_actions(\n        self,\n        tools: Sequence[AsyncBaseTool],\n        output: ChatResponse,\n        is_streaming: bool = False,\n    ) -> Tuple[List[BaseReasoningStep], bool]:\n        tools_dict = {tool.metadata.name: tool for tool in tools}\n        _, current_reasoning, is_done = self._extract_reasoning_step(\n            output, is_streaming\n        )\n\n        if is_done:\n            return current_reasoning, True\n\n        # call tool with input\n        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])\n        tool = tools_dict[reasoning_step.action]\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: reasoning_step.action_input,\n                EventPayload.TOOL: tool.metadata,\n            },\n        ) as event:\n            tool_output = await tool.acall(**reasoning_step.action_input)\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n\n        self.sources.append(tool_output)\n\n        observation_step = ObservationReasoningStep(observation=str(tool_output))\n        current_reasoning.append(observation_step)\n        if self._verbose:\n            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")\n        return current_reasoning, False\n\n    def _get_response(\n        self,\n        current_reasoning: List[BaseReasoningStep],\n    ) -> AgentChatResponse:\n        \"\"\"Get response from reasoning steps.\"\"\"\n        if len(current_reasoning) == 0:\n            raise ValueError(\"No reasoning steps were taken.\")\n        elif len(current_reasoning) == self._max_iterations:\n            raise ValueError(\"Reached max iterations.\")\n\n        response_step = cast(ResponseReasoningStep, current_reasoning[-1])\n\n        # TODO: add sources from reasoning steps\n        return AgentChatResponse(response=response_step.response, sources=self.sources)\n\n    def _infer_stream_chunk_is_final(self, chunk: ChatResponse) -> bool:\n        \"\"\"Infers if a chunk from a live stream is the start of the final\n        reasoning step. (i.e., and should eventually become\n        ResponseReasoningStep \u2014 not part of this function's logic tho.).\n\n        Args:\n            chunk (ChatResponse): the current chunk stream to check\n\n        Returns:\n            bool: Boolean on whether the chunk is the start of the final response\n        \"\"\"\n        latest_content = chunk.message.content\n\n        if latest_content:\n            if not latest_content.startswith(\n                \"Thought\"\n            ):  # doesn't follow thought-action format\n                return True\n            else:\n                if \"Answer: \" in latest_content:\n                    return True\n        return False\n\n    def _add_back_chunk_to_stream(\n        self, chunk: ChatResponse, chat_stream: Generator[ChatResponse, None, None]\n    ) -> Generator[ChatResponse, None, None]:\n        \"\"\"Helper method for adding back initial chunk stream of final response\n        back to the rest of the chat_stream.\n\n        Args:\n            chunk (ChatResponse): the chunk to add back to the beginning of the\n                                    chat_stream.\n\n        Return:\n            Generator[ChatResponse, None, None]: the updated chat_stream\n        \"\"\"\n        updated_stream = chain.from_iterable(  # need to add back partial response chunk\n            [\n                unit_generator(chunk),\n                chat_stream,\n            ]\n        )\n        # use cast to avoid mypy issue with chain and Generator\n        updated_stream_c: Generator[ChatResponse, None, None] = cast(\n            Generator[ChatResponse, None, None], updated_stream\n        )\n        return updated_stream_c\n\n    async def _async_add_back_chunk_to_stream(\n        self, chunk: ChatResponse, chat_stream: AsyncGenerator[ChatResponse, None]\n    ) -> AsyncGenerator[ChatResponse, None]:\n        \"\"\"Helper method for adding back initial chunk stream of final response\n        back to the rest of the chat_stream.\n\n        NOTE: this itself is not an async function.\n\n        Args:\n            chunk (ChatResponse): the chunk to add back to the beginning of the\n                                    chat_stream.\n\n        Return:\n            AsyncGenerator[ChatResponse, None]: the updated async chat_stream\n        \"\"\"\n        yield chunk\n        async for item in chat_stream:\n            yield item\n\n    @trace_method(\"chat\")\n    def chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        \"\"\"Chat.\"\"\"\n        # get tools\n        # TODO: do get tools dynamically at every iteration of the agent loop\n        self.sources = []\n        tools = self.get_tools(message)\n\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))\n\n        current_reasoning: List[BaseReasoningStep] = []\n        # start loop\n        for _ in range(self._max_iterations):\n            # prepare inputs\n            input_chat = self._react_chat_formatter.format(\n                tools,\n                chat_history=self._memory.get(),\n                current_reasoning=current_reasoning,\n            )\n            # send prompt\n            chat_response = self._llm.chat(input_chat)\n            # given react prompt outputs, call tools or return response\n            reasoning_steps, is_done = self._process_actions(\n                tools, output=chat_response\n            )\n            current_reasoning.extend(reasoning_steps)\n            if is_done:\n                break\n\n        response = self._get_response(current_reasoning)\n        self._memory.put(\n            ChatMessage(content=response.response, role=MessageRole.ASSISTANT)\n        )\n        return response\n\n    @trace_method(\"chat\")\n    async def achat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> AgentChatResponse:\n        # get tools\n        # TODO: do get tools dynamically at every iteration of the agent loop\n        self.sources = []\n        tools = self.get_tools(message)\n\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))\n\n        current_reasoning: List[BaseReasoningStep] = []\n        # start loop\n        for _ in range(self._max_iterations):\n            # prepare inputs\n            input_chat = self._react_chat_formatter.format(\n                tools,\n                chat_history=self._memory.get(),\n                current_reasoning=current_reasoning,\n            )\n            # send prompt\n            chat_response = await self._llm.achat(input_chat)\n            # given react prompt outputs, call tools or return response\n            reasoning_steps, is_done = await self._aprocess_actions(\n                tools, output=chat_response\n            )\n            current_reasoning.extend(reasoning_steps)\n            if is_done:\n                break\n\n        response = self._get_response(current_reasoning)\n        self._memory.put(\n            ChatMessage(content=response.response, role=MessageRole.ASSISTANT)\n        )\n        return response\n\n    @trace_method(\"chat\")\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        # get tools\n        # TODO: do get tools dynamically at every iteration of the agent loop\n        self.sources = []\n        tools = self.get_tools(message)\n\n        if chat_history is not None:\n            self._memory.set(chat_history)\n        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))\n\n        current_reasoning: List[BaseReasoningStep] = []\n        # start loop\n        is_done, ix = False, 0\n        while (not is_done) and (ix < self._max_iterations):\n            ix += 1\n\n            # prepare inputs\n            input_chat = self._react_chat_formatter.format(\n                tools,\n                chat_history=self._memory.get(),\n                current_reasoning=current_reasoning,\n            )\n            # send prompt\n            chat_stream = self._llm.stream_chat(input_chat)\n\n            # iterate over stream, break out if is final answer after the \"Answer: \"\n            full_response = ChatResponse(\n                message=ChatMessage(content=None, role=MessageRole.ASSISTANT)\n            )\n            for latest_chunk in chat_stream:\n                full_response = latest_chunk\n                is_done = self._infer_stream_chunk_is_final(latest_chunk)\n                if is_done:\n                    break\n\n            # given react prompt outputs, call tools or return response\n            reasoning_steps, _ = self._process_actions(\n                tools=tools, output=full_response, is_streaming=True\n            )\n            current_reasoning.extend(reasoning_steps)\n\n        # Get the response in a separate thread so we can yield the response\n        response_stream = self._add_back_chunk_to_stream(\n            chunk=latest_chunk, chat_stream=chat_stream\n        )\n\n        chat_stream_response = StreamingAgentChatResponse(\n            chat_stream=response_stream,\n            sources=self.sources,\n        )\n        thread = Thread(\n            target=chat_stream_response.write_response_to_history,\n            args=(self._memory,),\n        )\n        thread.start()\n        return chat_stream_response"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/legacy/react/base.py",
    "filename": "base.py",
    "relpath": "agent/legacy/react/base.py",
    "start_line": 466,
    "end_line": 529,
    "length": 64,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "astream_chat",
      "get_tools"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__init__",
      "from_tools",
      "chat_history",
      "reset",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_infer_stream_chunk_is_final",
      "_add_back_chunk_to_stream",
      "_async_add_back_chunk_to_stream",
      "chat",
      "achat",
      "stream_chat",
      "astream_chat",
      "get_tools"
    ],
    "document_class_names": [
      "ReActAgent"
    ],
    "content": "@trace_method(\"chat\")\n    async def astream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        # get tools\n        # TODO: do get tools dynamically at every iteration of the agent loop\n        self.sources = []\n        tools = self.get_tools(message)\n\n        if chat_history is not None:\n            self._memory.set(chat_history)\n\n        self._memory.put(ChatMessage(content=message, role=MessageRole.USER))\n\n        current_reasoning: List[BaseReasoningStep] = []\n        # start loop\n        is_done, ix = False, 0\n        while (not is_done) and (ix < self._max_iterations):\n            ix += 1\n\n            # prepare inputs\n            input_chat = self._react_chat_formatter.format(\n                tools,\n                chat_history=self._memory.get(),\n                current_reasoning=current_reasoning,\n            )\n            # send prompt\n            chat_stream = await self._llm.astream_chat(input_chat)\n\n            # iterate over stream, break out if is final answer\n            is_done = False\n            full_response = ChatResponse(\n                message=ChatMessage(content=None, role=MessageRole.ASSISTANT)\n            )\n            async for latest_chunk in chat_stream:\n                full_response = latest_chunk\n                is_done = self._infer_stream_chunk_is_final(latest_chunk)\n                if is_done:\n                    break\n\n            # given react prompt outputs, call tools or return response\n            reasoning_steps, _ = self._process_actions(\n                tools=tools, output=full_response, is_streaming=True\n            )\n            current_reasoning.extend(reasoning_steps)\n\n        # Get the response in a separate thread so we can yield the response\n        response_stream = self._async_add_back_chunk_to_stream(\n            chunk=latest_chunk, chat_stream=chat_stream\n        )\n\n        chat_stream_response = StreamingAgentChatResponse(\n            achat_stream=response_stream, sources=self.sources\n        )\n        # create task to write chat response to history\n        chat_stream_response.awrite_response_to_history_task = asyncio.create_task(\n            chat_stream_response.awrite_response_to_history(self._memory)\n        )\n\n        return chat_stream_response\n\n    def get_tools(self, message: str) -> List[AsyncBaseTool]:\n        \"\"\"Get tools.\"\"\"\n        return [adapt_to_async_tool(t) for t in self._get_tools(message)]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/legacy/react/__init__.py",
    "filename": "__init__.py",
    "relpath": "agent/legacy/react/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py",
    "filename": "multi_agent_workflow.py",
    "relpath": "agent/workflow/multi_agent_workflow.py",
    "start_line": 1,
    "end_line": 74,
    "length": 74,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "handoff"
    ],
    "chunk_class_names": [
      "AgentWorkflowMeta",
      "for"
    ],
    "document_function_names": [
      "handoff",
      "__init__",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_ensure_tools_are_async",
      "_get_handoff_tool",
      "get_tools",
      "_init_context",
      "_call_tool",
      "init_run",
      "setup_agent",
      "run_agent_step",
      "parse_agent_output",
      "call_tool",
      "aggregate_tool_results",
      "run",
      "from_tools_or_functions"
    ],
    "document_class_names": [
      "AgentWorkflowMeta",
      "for",
      "AgentWorkflow"
    ],
    "content": "from abc import ABCMeta\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Union\n\nfrom llama_index.core.agent.workflow.base_agent import BaseWorkflowAgent\nfrom llama_index.core.agent.workflow.function_agent import FunctionAgent\nfrom llama_index.core.agent.workflow.react_agent import ReActAgent\nfrom llama_index.core.agent.workflow.workflow_events import (\n    ToolCall,\n    ToolCallResult,\n    AgentInput,\n    AgentSetup,\n    AgentOutput,\n)\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory import BaseMemory, ChatMemoryBuffer\nfrom llama_index.core.prompts import BasePromptTemplate, PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptMixin, PromptMixinType, PromptDictType\nfrom llama_index.core.tools import (\n    BaseTool,\n    AsyncBaseTool,\n    FunctionTool,\n    ToolOutput,\n    ToolSelection,\n    adapt_to_async_tool,\n)\nfrom llama_index.core.workflow import (\n    Context,\n    StartEvent,\n    StopEvent,\n    Workflow,\n    step,\n)\nfrom llama_index.core.workflow.checkpointer import CheckpointCallback\nfrom llama_index.core.workflow.handler import WorkflowHandler\nfrom llama_index.core.workflow.workflow import WorkflowMeta\nfrom llama_index.core.settings import Settings\n\n\nDEFAULT_HANDOFF_PROMPT = \"\"\"Useful for handing off to another agent.\nIf you are currently not equipped to handle the user's request, or another agent is better suited to handle the request, please hand off to the appropriate agent.\n\nCurrently available agents:\n{agent_info}\n\"\"\"\n\nDEFAULT_STATE_PROMPT = \"\"\"Current state:\n{state}\n\nCurrent message:\n{msg}\n\"\"\"\n\nDEFAULT_HANDOFF_OUTPUT_PROMPT = \"Agent {to_agent} is now handling the request due to the following reason: {reason}.\\nPlease continue with the current request.\"\n\n\nasync def handoff(ctx: Context, to_agent: str, reason: str) -> str:\n    \"\"\"Handoff control of that chat to the given agent.\"\"\"\n    agents: list[str] = await ctx.get(\"agents\")\n    current_agent_name: str = await ctx.get(\"current_agent_name\")\n    if to_agent not in agents:\n        valid_agents = \", \".join([x for x in agents if x != current_agent_name])\n        return f\"Agent {to_agent} not found. Please select a valid agent to hand off to. Valid agents: {valid_agents}\"\n\n    await ctx.set(\"next_agent\", to_agent)\n    handoff_output_prompt = await ctx.get(\n        \"handoff_output_prompt\", default=DEFAULT_HANDOFF_OUTPUT_PROMPT\n    )\n\n    return handoff_output_prompt.format(to_agent=to_agent, reason=reason)\n\n\nclass AgentWorkflowMeta(WorkflowMeta, ABCMeta):\n    \"\"\"Metaclass for AgentWorkflow that inherits from WorkflowMeta.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py",
    "filename": "multi_agent_workflow.py",
    "relpath": "agent/workflow/multi_agent_workflow.py",
    "start_line": 74,
    "end_line": 77,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "AgentWorkflow"
    ],
    "document_function_names": [
      "handoff",
      "__init__",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_ensure_tools_are_async",
      "_get_handoff_tool",
      "get_tools",
      "_init_context",
      "_call_tool",
      "init_run",
      "setup_agent",
      "run_agent_step",
      "parse_agent_output",
      "call_tool",
      "aggregate_tool_results",
      "run",
      "from_tools_or_functions"
    ],
    "document_class_names": [
      "AgentWorkflowMeta",
      "for",
      "AgentWorkflow"
    ],
    "content": "class AgentWorkflow(Workflow, PromptMixin, metaclass=AgentWorkflowMeta):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py",
    "filename": "multi_agent_workflow.py",
    "relpath": "agent/workflow/multi_agent_workflow.py",
    "start_line": 77,
    "end_line": 502,
    "length": 426,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_ensure_tools_are_async",
      "_get_handoff_tool",
      "get_tools",
      "_init_context",
      "_call_tool",
      "init_run",
      "setup_agent",
      "run_agent_step",
      "parse_agent_output",
      "call_tool",
      "aggregate_tool_results",
      "run"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "handoff",
      "__init__",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_ensure_tools_are_async",
      "_get_handoff_tool",
      "get_tools",
      "_init_context",
      "_call_tool",
      "init_run",
      "setup_agent",
      "run_agent_step",
      "parse_agent_output",
      "call_tool",
      "aggregate_tool_results",
      "run",
      "from_tools_or_functions"
    ],
    "document_class_names": [
      "AgentWorkflowMeta",
      "for",
      "AgentWorkflow"
    ],
    "content": "\"\"\"A workflow for managing multiple agents with handoffs.\"\"\"\n\n    def __init__(\n        self,\n        agents: List[BaseWorkflowAgent],\n        initial_state: Optional[Dict] = None,\n        root_agent: Optional[str] = None,\n        handoff_prompt: Optional[Union[str, BasePromptTemplate]] = None,\n        handoff_output_prompt: Optional[Union[str, BasePromptTemplate]] = None,\n        state_prompt: Optional[Union[str, BasePromptTemplate]] = None,\n        timeout: Optional[float] = None,\n        **workflow_kwargs: Any,\n    ):\n        super().__init__(timeout=timeout, **workflow_kwargs)\n        if not agents:\n            raise ValueError(\"At least one agent must be provided\")\n\n        self.agents = {cfg.name: cfg for cfg in agents}\n        if len(agents) == 1:\n            root_agent = agents[0].name\n        elif root_agent is None:\n            raise ValueError(\"Exactly one root agent must be provided\")\n        else:\n            root_agent = root_agent\n\n        if root_agent not in self.agents:\n            raise ValueError(f\"Root agent {root_agent} not found in provided agents\")\n\n        self.root_agent = root_agent\n        self.initial_state = initial_state or {}\n\n        handoff_prompt = handoff_prompt or DEFAULT_HANDOFF_PROMPT\n        if isinstance(handoff_prompt, str):\n            handoff_prompt = PromptTemplate(handoff_prompt)\n            if \"{agent_info}\" not in handoff_prompt.get_template():\n                raise ValueError(\"Handoff prompt must contain {agent_info}\")\n        self.handoff_prompt = handoff_prompt\n\n        handoff_output_prompt = handoff_output_prompt or DEFAULT_HANDOFF_OUTPUT_PROMPT\n        if isinstance(handoff_output_prompt, str):\n            handoff_output_prompt = PromptTemplate(handoff_output_prompt)\n            if (\n                \"{to_agent}\" not in handoff_output_prompt.get_template()\n                or \"{reason}\" not in handoff_output_prompt.get_template()\n            ):\n                raise ValueError(\n                    \"Handoff output prompt must contain {to_agent} and {reason}\"\n                )\n        self.handoff_output_prompt = handoff_output_prompt\n\n        state_prompt = state_prompt or DEFAULT_STATE_PROMPT\n        if isinstance(state_prompt, str):\n            state_prompt = PromptTemplate(state_prompt)\n            if (\n                \"{state}\" not in state_prompt.get_template()\n                or \"{msg}\" not in state_prompt.get_template()\n            ):\n                raise ValueError(\"State prompt must contain {state} and {msg}\")\n        self.state_prompt = state_prompt\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {\n            \"handoff_prompt\": self.handoff_prompt,\n            \"handoff_output_prompt\": self.handoff_output_prompt,\n            \"state_prompt\": self.state_prompt,\n        }\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {agent.name: agent for agent in self.agents.values()}\n\n    def _update_prompts(self, prompts_dict: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"handoff_prompt\" in prompts_dict:\n            self.handoff_prompt = prompts_dict[\"handoff_prompt\"]\n        if \"handoff_output_prompt\" in prompts_dict:\n            self.handoff_output_prompt = prompts_dict[\"handoff_output_prompt\"]\n        if \"state_prompt\" in prompts_dict:\n            self.state_prompt = prompts_dict[\"state_prompt\"]\n\n    def _ensure_tools_are_async(\n        self, tools: Sequence[BaseTool]\n    ) -> Sequence[AsyncBaseTool]:\n        \"\"\"Ensure all tools are async.\"\"\"\n        return [adapt_to_async_tool(tool) for tool in tools]\n\n    def _get_handoff_tool(\n        self, current_agent: BaseWorkflowAgent\n    ) -> Optional[AsyncBaseTool]:\n        \"\"\"Creates a handoff tool for the given agent.\"\"\"\n        agent_info = {cfg.name: cfg.description for cfg in self.agents.values()}\n\n        # Filter out agents that the current agent cannot handoff to\n        configs_to_remove = []\n        for name in agent_info:\n            if name == current_agent.name:\n                configs_to_remove.append(name)\n            elif (\n                current_agent.can_handoff_to is not None\n                and name not in current_agent.can_handoff_to\n            ):\n                configs_to_remove.append(name)\n\n        for name in configs_to_remove:\n            agent_info.pop(name)\n\n        if not agent_info:\n            return None\n\n        fn_tool_prompt = self.handoff_prompt.format(agent_info=str(agent_info))\n        return FunctionTool.from_defaults(\n            async_fn=handoff, description=fn_tool_prompt, return_direct=True\n        )\n\n    async def get_tools(\n        self, agent_name: str, input_str: Optional[str] = None\n    ) -> Sequence[AsyncBaseTool]:\n        \"\"\"Get tools for the given agent.\"\"\"\n        agent_tools = self.agents[agent_name].tools or []\n        tools = [*agent_tools]\n        retriever = self.agents[agent_name].tool_retriever\n        if retriever is not None:\n            retrieved_tools = await retriever.aretrieve(input_str or \"\")\n            tools.extend(retrieved_tools)\n\n        if (\n            self.agents[agent_name].can_handoff_to\n            or self.agents[agent_name].can_handoff_to is None\n        ):\n            handoff_tool = self._get_handoff_tool(self.agents[agent_name])\n            if handoff_tool:\n                tools.append(handoff_tool)\n\n        return self._ensure_tools_are_async(tools)\n\n    async def _init_context(self, ctx: Context, ev: StartEvent) -> None:\n        \"\"\"Initialize the context once, if needed.\"\"\"\n        if not await ctx.get(\"memory\", default=None):\n            default_memory = ev.get(\"memory\", default=None)\n            default_memory = default_memory or ChatMemoryBuffer.from_defaults(\n                llm=self.agents[self.root_agent].llm or Settings.llm\n            )\n            await ctx.set(\"memory\", default_memory)\n        if not await ctx.get(\"agents\", default=None):\n            await ctx.set(\"agents\", list(self.agents.keys()))\n        if not await ctx.get(\"state\", default=None):\n            await ctx.set(\"state\", self.initial_state)\n        if not await ctx.get(\"current_agent_name\", default=None):\n            await ctx.set(\"current_agent_name\", self.root_agent)\n        if not await ctx.get(\"handoff_output_prompt\", default=None):\n            await ctx.set(\n                \"handoff_output_prompt\", self.handoff_output_prompt.get_template()\n            )\n\n    async def _call_tool(\n        self,\n        ctx: Context,\n        tool: AsyncBaseTool,\n        tool_input: dict,\n    ) -> ToolOutput:\n        \"\"\"Call the given tool with the given input.\"\"\"\n        try:\n            if isinstance(tool, FunctionTool) and tool.requires_context:\n                tool_output = await tool.acall(ctx=ctx, **tool_input)\n            else:\n                tool_output = await tool.acall(**tool_input)\n        except Exception as e:\n            tool_output = ToolOutput(\n                content=str(e),\n                tool_name=tool.metadata.name,\n                raw_input=tool_input,\n                raw_output=str(e),\n                is_error=True,\n            )\n\n        return tool_output\n\n    @step\n    async def init_run(self, ctx: Context, ev: StartEvent) -> AgentInput:\n        \"\"\"Sets up the workflow and validates inputs.\"\"\"\n        await self._init_context(ctx, ev)\n\n        user_msg: Optional[Union[str, ChatMessage]] = ev.get(\"user_msg\")\n        chat_history: Optional[List[ChatMessage]] = ev.get(\"chat_history\", [])\n\n        # Convert string user_msg to ChatMessage\n        if isinstance(user_msg, str):\n            user_msg = ChatMessage(role=\"user\", content=user_msg)\n\n        # Add messages to memory\n        memory: BaseMemory = await ctx.get(\"memory\")\n\n        # First set chat history if it exists\n        if chat_history:\n            memory.set(chat_history)\n\n        # Then add user message if it exists\n        current_state = await ctx.get(\"state\")\n        if user_msg:\n            # Add the state to the user message if it exists\n            if current_state:\n                user_msg.content = self.state_prompt.format(\n                    state=current_state, msg=user_msg.content\n                )\n            await memory.aput(user_msg)\n            await ctx.set(\"user_msg_str\", user_msg.content)\n        elif chat_history:\n            # If no user message, use the last message from chat history as user_msg_str\n            last_msg = chat_history[-1].content or \"\"\n            await ctx.set(\"user_msg_str\", last_msg)\n\n            if current_state:\n                chat_history[-1].content = self.state_prompt.format(\n                    state=current_state, msg=chat_history[-1].content\n                )\n        else:\n            raise ValueError(\"Must provide either user_msg or chat_history\")\n\n        # Get all messages from memory\n        input_messages = memory.get()\n\n        # send to the current agent\n        current_agent_name: str = await ctx.get(\"current_agent_name\")\n        return AgentInput(input=input_messages, current_agent_name=current_agent_name)\n\n    @step\n    async def setup_agent(self, ctx: Context, ev: AgentInput) -> AgentSetup:\n        \"\"\"Main agent handling logic.\"\"\"\n        current_agent_name = ev.current_agent_name\n        agent = self.agents[current_agent_name]\n        llm_input = ev.input\n\n        if agent.system_prompt:\n            llm_input = [\n                ChatMessage(role=\"system\", content=agent.system_prompt),\n                *llm_input,\n            ]\n\n        return AgentSetup(\n            input=llm_input,\n            current_agent_name=ev.current_agent_name,\n        )\n\n    @step\n    async def run_agent_step(self, ctx: Context, ev: AgentSetup) -> AgentOutput:\n        \"\"\"Run the agent.\"\"\"\n        memory: BaseMemory = await ctx.get(\"memory\")\n        agent = self.agents[ev.current_agent_name]\n        tools = await self.get_tools(ev.current_agent_name, ev.input[-1].content or \"\")\n\n        agent_output = await agent.take_step(\n            ctx,\n            ev.input,\n            tools,\n            memory,\n        )\n\n        ctx.write_event_to_stream(agent_output)\n        return agent_output\n\n    @step\n    async def parse_agent_output(\n        self, ctx: Context, ev: AgentOutput\n    ) -> Union[StopEvent, ToolCall, None]:\n        if not ev.tool_calls:\n            agent = self.agents[ev.current_agent_name]\n            memory: BaseMemory = await ctx.get(\"memory\")\n            output = await agent.finalize(ctx, ev, memory)\n\n            cur_tool_calls: List[ToolCallResult] = await ctx.get(\n                \"current_tool_calls\", default=[]\n            )\n            output.tool_calls.extend(cur_tool_calls)  # type: ignore\n            await ctx.set(\"current_tool_calls\", [])\n\n            return StopEvent(result=output)\n\n        await ctx.set(\"num_tool_calls\", len(ev.tool_calls))\n\n        for tool_call in ev.tool_calls:\n            ctx.send_event(\n                ToolCall(\n                    tool_name=tool_call.tool_name,\n                    tool_kwargs=tool_call.tool_kwargs,\n                    tool_id=tool_call.tool_id,\n                )\n            )\n\n        return None\n\n    @step\n    async def call_tool(self, ctx: Context, ev: ToolCall) -> ToolCallResult:\n        \"\"\"Calls the tool and handles the result.\"\"\"\n        ctx.write_event_to_stream(\n            ToolCall(\n                tool_name=ev.tool_name,\n                tool_kwargs=ev.tool_kwargs,\n                tool_id=ev.tool_id,\n            )\n        )\n\n        current_agent_name = await ctx.get(\"current_agent_name\")\n        tools = await self.get_tools(current_agent_name, ev.tool_name)\n        tools_by_name = {tool.metadata.name: tool for tool in tools}\n        if ev.tool_name not in tools_by_name:\n            tool = None\n            result = ToolOutput(\n                content=f\"Tool {ev.tool_name} not found. Please select a tool that is available.\",\n                tool_name=ev.tool_name,\n                raw_input=ev.tool_kwargs,\n                raw_output=None,\n                is_error=True,\n            )\n        else:\n            tool = tools_by_name[ev.tool_name]\n            result = await self._call_tool(ctx, tool, ev.tool_kwargs)\n\n        result_ev = ToolCallResult(\n            tool_name=ev.tool_name,\n            tool_kwargs=ev.tool_kwargs,\n            tool_id=ev.tool_id,\n            tool_output=result,\n            return_direct=tool.metadata.return_direct if tool else False,\n        )\n\n        ctx.write_event_to_stream(result_ev)\n        return result_ev\n\n    @step\n    async def aggregate_tool_results(\n        self, ctx: Context, ev: ToolCallResult\n    ) -> Union[AgentInput, StopEvent, None]:\n        \"\"\"Aggregate tool results and return the next agent input.\"\"\"\n        num_tool_calls = await ctx.get(\"num_tool_calls\", default=0)\n        if num_tool_calls == 0:\n            raise ValueError(\"No tool calls found, cannot aggregate results.\")\n\n        tool_call_results: list[ToolCallResult] = ctx.collect_events(  # type: ignore\n            ev, expected=[ToolCallResult] * num_tool_calls\n        )\n        if not tool_call_results:\n            return None\n\n        memory: BaseMemory = await ctx.get(\"memory\")\n        agent_name: str = await ctx.get(\"current_agent_name\")\n        agent: BaseWorkflowAgent = self.agents[agent_name]\n\n        # track tool calls made during a .run() call\n        cur_tool_calls: List[ToolCallResult] = await ctx.get(\n            \"current_tool_calls\", default=[]\n        )\n        cur_tool_calls.extend(tool_call_results)\n        await ctx.set(\"current_tool_calls\", cur_tool_calls)\n\n        await agent.handle_tool_call_results(ctx, tool_call_results, memory)\n\n        # set the next agent, if needed\n        # the handoff tool sets this\n        next_agent_name = await ctx.get(\"next_agent\", default=None)\n        if next_agent_name:\n            await ctx.set(\"current_agent_name\", next_agent_name)\n\n        if any(\n            tool_call_result.return_direct for tool_call_result in tool_call_results\n        ):\n            # if any tool calls return directly, take the first one\n            return_direct_tool = next(\n                tool_call_result\n                for tool_call_result in tool_call_results\n                if tool_call_result.return_direct\n            )\n\n            # always finalize the agent, even if we're just handing off\n            result = AgentOutput(\n                response=ChatMessage(\n                    role=\"assistant\",\n                    content=return_direct_tool.tool_output.content or \"\",\n                ),\n                tool_calls=[\n                    ToolSelection(\n                        tool_id=t.tool_id,\n                        tool_name=t.tool_name,\n                        tool_kwargs=t.tool_kwargs,\n                    )\n                    for t in cur_tool_calls\n                ],\n                raw=return_direct_tool.tool_output.raw_output,\n                current_agent_name=agent.name,\n            )\n            result = await agent.finalize(ctx, result, memory)\n\n            # we don't want to stop the system if we're just handing off\n            if return_direct_tool.tool_name != \"handoff\":\n                await ctx.set(\"current_tool_calls\", [])\n                return StopEvent(result=result)\n\n        user_msg_str = await ctx.get(\"user_msg_str\")\n        input_messages = memory.get(input=user_msg_str)\n\n        # get this again, in case it changed\n        agent_name = await ctx.get(\"current_agent_name\")\n        agent = self.agents[agent_name]\n\n        return AgentInput(input=input_messages, current_agent_name=agent.name)\n\n    def run(\n        self,\n        user_msg: Optional[Union[str, ChatMessage]] = None,\n        chat_history: Optional[List[ChatMessage]] = None,\n        memory: Optional[BaseMemory] = None,\n        ctx: Optional[Context] = None,\n        stepwise: bool = False,\n        checkpoint_callback: Optional[CheckpointCallback] = None,\n        **kwargs: Any,\n    ) -> WorkflowHandler:\n        return super().run(\n            user_msg=user_msg,\n            chat_history=chat_history,\n            memory=memory,\n            ctx=ctx,\n            stepwise=stepwise,\n            checkpoint_callback=checkpoint_callback,\n            **kwargs,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/multi_agent_workflow.py",
    "filename": "multi_agent_workflow.py",
    "relpath": "agent/workflow/multi_agent_workflow.py",
    "start_line": 502,
    "end_line": 547,
    "length": 46,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "from_tools_or_functions"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "handoff",
      "__init__",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_ensure_tools_are_async",
      "_get_handoff_tool",
      "get_tools",
      "_init_context",
      "_call_tool",
      "init_run",
      "setup_agent",
      "run_agent_step",
      "parse_agent_output",
      "call_tool",
      "aggregate_tool_results",
      "run",
      "from_tools_or_functions"
    ],
    "document_class_names": [
      "AgentWorkflowMeta",
      "for",
      "AgentWorkflow"
    ],
    "content": "@classmethod\n    def from_tools_or_functions(\n        cls,\n        tools_or_functions: List[Union[BaseTool, Callable]],\n        llm: Optional[LLM] = None,\n        system_prompt: Optional[str] = None,\n        state_prompt: Optional[Union[str, BasePromptTemplate]] = None,\n        initial_state: Optional[dict] = None,\n        timeout: Optional[float] = None,\n        verbose: bool = False,\n    ) -> \"AgentWorkflow\":\n        \"\"\"Initializes an AgentWorkflow from a list of tools or functions.\n\n        The workflow will be initialized with a single agent that uses the provided tools or functions.\n\n        If the LLM is a function calling model, the workflow will use the FunctionAgent.\n        Otherwise, it will use the ReActAgent.\n        \"\"\"\n        llm = llm or Settings.llm\n        agent_cls = (\n            FunctionAgent if llm.metadata.is_function_calling_model else ReActAgent\n        )\n\n        tools = [\n            FunctionTool.from_defaults(fn=tool)\n            if not isinstance(tool, BaseTool)\n            else tool\n            for tool in tools_or_functions\n        ]\n        return cls(\n            agents=[\n                agent_cls(\n                    name=\"Agent\",\n                    description=\"A single agent that uses the provided tools or functions.\",\n                    tools=tools,\n                    llm=llm,\n                    system_prompt=system_prompt,\n                )\n            ],\n            state_prompt=state_prompt,\n            initial_state=initial_state,\n            timeout=timeout,\n            verbose=verbose,\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/workflow_events.py",
    "filename": "workflow_events.py",
    "relpath": "agent/workflow/workflow_events.py",
    "start_line": 1,
    "end_line": 56,
    "length": 56,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__str__"
    ],
    "chunk_class_names": [
      "AgentInput",
      "AgentSetup",
      "AgentStream",
      "AgentOutput",
      "ToolCall",
      "ToolCallResult"
    ],
    "document_function_names": [
      "__str__"
    ],
    "document_class_names": [
      "AgentInput",
      "AgentSetup",
      "AgentStream",
      "AgentOutput",
      "ToolCall",
      "ToolCallResult"
    ],
    "content": "from typing import Any\n\nfrom llama_index.core.tools import ToolSelection, ToolOutput\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.workflow import Event\n\n\nclass AgentInput(Event):\n    \"\"\"LLM input.\"\"\"\n\n    input: list[ChatMessage]\n    current_agent_name: str\n\n\nclass AgentSetup(Event):\n    \"\"\"Agent setup.\"\"\"\n\n    input: list[ChatMessage]\n    current_agent_name: str\n\n\nclass AgentStream(Event):\n    \"\"\"Agent stream.\"\"\"\n\n    delta: str\n    response: str\n    current_agent_name: str\n    tool_calls: list[ToolSelection]\n    raw: Any\n\n\nclass AgentOutput(Event):\n    \"\"\"LLM output.\"\"\"\n\n    response: ChatMessage\n    tool_calls: list[ToolSelection]\n    raw: Any\n    current_agent_name: str\n\n    def __str__(self) -> str:\n        return self.response.content or \"\"\n\n\nclass ToolCall(Event):\n    \"\"\"All tool calls are surfaced.\"\"\"\n\n    tool_name: str\n    tool_kwargs: dict\n    tool_id: str\n\n\nclass ToolCallResult(ToolCall):\n    \"\"\"Tool call result.\"\"\"\n\n    tool_output: ToolOutput\n    return_direct: bool"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/base_agent.py",
    "filename": "base_agent.py",
    "relpath": "agent/workflow/base_agent.py",
    "start_line": 1,
    "end_line": 111,
    "length": 111,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "get_default_llm",
      "validate_tools",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "take_step",
      "handle_tool_call_results",
      "finalize"
    ],
    "chunk_class_names": [
      "BaseWorkflowAgent",
      "for"
    ],
    "document_function_names": [
      "get_default_llm",
      "validate_tools",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "take_step",
      "handle_tool_call_results",
      "finalize"
    ],
    "document_class_names": [
      "BaseWorkflowAgent",
      "for"
    ],
    "content": "from abc import ABC, abstractmethod\nfrom typing import Callable, List, Sequence, Optional, Union\n\nfrom llama_index.core.agent.workflow.workflow_events import (\n    AgentOutput,\n    ToolCallResult,\n)\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    ConfigDict,\n    field_validator,\n)\nfrom llama_index.core.llms import ChatMessage, LLM\nfrom llama_index.core.memory import BaseMemory\nfrom llama_index.core.prompts.mixin import PromptMixin, PromptMixinType, PromptDictType\nfrom llama_index.core.tools import BaseTool, AsyncBaseTool, FunctionTool\nfrom llama_index.core.workflow import Context\nfrom llama_index.core.objects import ObjectRetriever\nfrom llama_index.core.settings import Settings\n\n\ndef get_default_llm() -> LLM:\n    return Settings.llm\n\n\nclass BaseWorkflowAgent(BaseModel, PromptMixin, ABC):\n    \"\"\"Base class for all agents, combining config and logic.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    name: str = Field(description=\"The name of the agent\")\n    description: str = Field(\n        description=\"The description of what the agent does and is responsible for\"\n    )\n    system_prompt: Optional[str] = Field(\n        default=None, description=\"The system prompt for the agent\"\n    )\n    tools: Optional[List[BaseTool]] = Field(\n        default=None, description=\"The tools that the agent can use\"\n    )\n    tool_retriever: Optional[ObjectRetriever] = Field(\n        default=None,\n        description=\"The tool retriever for the agent, can be provided instead of tools\",\n    )\n    can_handoff_to: Optional[List[str]] = Field(\n        default=None, description=\"The agent names that this agent can hand off to\"\n    )\n    llm: LLM = Field(\n        default_factory=get_default_llm, description=\"The LLM that the agent uses\"\n    )\n\n    @field_validator(\"tools\", mode=\"before\")\n    def validate_tools(\n        cls, v: Optional[Sequence[Union[BaseTool, Callable]]]\n    ) -> Optional[Sequence[BaseTool]]:\n        \"\"\"Validate tools.\n\n        If tools are not of type BaseTool, they will be converted to FunctionTools.\n        This assumes the inputs are tools or callable functions.\n        \"\"\"\n        if v is None:\n            return None\n\n        validated_tools: List[BaseTool] = []\n        for tool in v:\n            if not isinstance(tool, BaseTool):\n                validated_tools.append(FunctionTool.from_defaults(tool))\n            else:\n                validated_tools.append(tool)\n\n        for tool in validated_tools:\n            if tool.metadata.name == \"handoff\":\n                raise ValueError(\n                    \"'handoff' is a reserved tool name. Please use a different name.\"\n                )\n\n        return validated_tools  # type: ignore[return-value]\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts_dict: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    @abstractmethod\n    async def take_step(\n        self,\n        ctx: Context,\n        llm_input: List[ChatMessage],\n        tools: Sequence[AsyncBaseTool],\n        memory: BaseMemory,\n    ) -> AgentOutput:\n        \"\"\"Take a single step with the agent.\"\"\"\n\n    @abstractmethod\n    async def handle_tool_call_results(\n        self, ctx: Context, results: List[ToolCallResult], memory: BaseMemory\n    ) -> None:\n        \"\"\"Handle tool call results.\"\"\"\n\n    @abstractmethod\n    async def finalize(\n        self, ctx: Context, output: AgentOutput, memory: BaseMemory\n    ) -> AgentOutput:\n        \"\"\"Finalize the agent's execution.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/__init__.py",
    "filename": "__init__.py",
    "relpath": "agent/workflow/__init__.py",
    "start_line": 1,
    "end_line": 26,
    "length": 26,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "from llama_index.core.agent.workflow.multi_agent_workflow import AgentWorkflow\nfrom llama_index.core.agent.workflow.base_agent import BaseWorkflowAgent\nfrom llama_index.core.agent.workflow.function_agent import FunctionAgent\nfrom llama_index.core.agent.workflow.react_agent import ReActAgent\nfrom llama_index.core.agent.workflow.workflow_events import (\n    AgentInput,\n    AgentSetup,\n    AgentStream,\n    AgentOutput,\n    ToolCall,\n    ToolCallResult,\n)\n\n\n__all__ = [\n    \"AgentInput\",\n    \"AgentSetup\",\n    \"AgentStream\",\n    \"AgentOutput\",\n    \"BaseWorkflowAgent\",\n    \"FunctionAgent\",\n    \"AgentWorkflow\",\n    \"ReActAgent\",\n    \"ToolCall\",\n    \"ToolCallResult\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/function_agent.py",
    "filename": "function_agent.py",
    "relpath": "agent/workflow/function_agent.py",
    "start_line": 1,
    "end_line": 130,
    "length": 130,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "take_step",
      "handle_tool_call_results",
      "finalize"
    ],
    "chunk_class_names": [
      "FunctionAgent"
    ],
    "document_function_names": [
      "take_step",
      "handle_tool_call_results",
      "finalize"
    ],
    "document_class_names": [
      "FunctionAgent"
    ],
    "content": "from typing import List, Sequence\n\nfrom llama_index.core.agent.workflow.base_agent import BaseWorkflowAgent\nfrom llama_index.core.agent.workflow.workflow_events import (\n    AgentInput,\n    AgentOutput,\n    AgentStream,\n    ToolCallResult,\n)\nfrom llama_index.core.base.llms.types import ChatResponse\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.memory import BaseMemory\nfrom llama_index.core.tools import AsyncBaseTool\nfrom llama_index.core.workflow import Context\n\n\nclass FunctionAgent(BaseWorkflowAgent):\n    \"\"\"Function calling agent implementation.\"\"\"\n\n    scratchpad_key: str = \"scratchpad\"\n\n    async def take_step(\n        self,\n        ctx: Context,\n        llm_input: List[ChatMessage],\n        tools: Sequence[AsyncBaseTool],\n        memory: BaseMemory,\n    ) -> AgentOutput:\n        \"\"\"Take a single step with the function calling agent.\"\"\"\n        if not self.llm.metadata.is_function_calling_model:\n            raise ValueError(\"LLM must be a FunctionCallingLLM\")\n\n        scratchpad: List[ChatMessage] = await ctx.get(self.scratchpad_key, default=[])\n        current_llm_input = [*llm_input, *scratchpad]\n\n        ctx.write_event_to_stream(\n            AgentInput(input=current_llm_input, current_agent_name=self.name)\n        )\n\n        response = await self.llm.astream_chat_with_tools(  # type: ignore\n            tools, chat_history=current_llm_input, allow_parallel_tool_calls=True\n        )\n        # last_chat_response will be used later, after the loop.\n        # We initialize it so it's valid even when 'response' is empty\n        last_chat_response = ChatResponse(message=ChatMessage())\n        async for last_chat_response in response:\n            tool_calls = self.llm.get_tool_calls_from_response(  # type: ignore\n                last_chat_response, error_on_no_tool_call=False\n            )\n            raw = (\n                last_chat_response.raw.model_dump()\n                if isinstance(last_chat_response.raw, BaseModel)\n                else last_chat_response.raw\n            )\n            ctx.write_event_to_stream(\n                AgentStream(\n                    delta=last_chat_response.delta or \"\",\n                    response=last_chat_response.message.content or \"\",\n                    tool_calls=tool_calls or [],\n                    raw=raw,\n                    current_agent_name=self.name,\n                )\n            )\n\n        tool_calls = self.llm.get_tool_calls_from_response(  # type: ignore\n            last_chat_response, error_on_no_tool_call=False\n        )\n\n        # only add to scratchpad if we didn't select the handoff tool\n        scratchpad.append(last_chat_response.message)\n        await ctx.set(self.scratchpad_key, scratchpad)\n\n        raw = (\n            last_chat_response.raw.model_dump()\n            if isinstance(last_chat_response.raw, BaseModel)\n            else last_chat_response.raw\n        )\n        return AgentOutput(\n            response=last_chat_response.message,\n            tool_calls=tool_calls or [],\n            raw=raw,\n            current_agent_name=self.name,\n        )\n\n    async def handle_tool_call_results(\n        self, ctx: Context, results: List[ToolCallResult], memory: BaseMemory\n    ) -> None:\n        \"\"\"Handle tool call results for function calling agent.\"\"\"\n        scratchpad: List[ChatMessage] = await ctx.get(self.scratchpad_key, default=[])\n\n        for tool_call_result in results:\n            scratchpad.append(\n                ChatMessage(\n                    role=\"tool\",\n                    content=str(tool_call_result.tool_output.content),\n                    additional_kwargs={\"tool_call_id\": tool_call_result.tool_id},\n                )\n            )\n\n            if (\n                tool_call_result.return_direct\n                and tool_call_result.tool_name != \"handoff\"\n            ):\n                scratchpad.append(\n                    ChatMessage(\n                        role=\"assistant\",\n                        content=str(tool_call_result.tool_output.content),\n                        additional_kwargs={\"tool_call_id\": tool_call_result.tool_id},\n                    )\n                )\n                break\n\n        await ctx.set(self.scratchpad_key, scratchpad)\n\n    async def finalize(\n        self, ctx: Context, output: AgentOutput, memory: BaseMemory\n    ) -> AgentOutput:\n        \"\"\"Finalize the function calling agent.\n\n        Adds all in-progress messages to memory.\n        \"\"\"\n        scratchpad: List[ChatMessage] = await ctx.get(self.scratchpad_key, default=[])\n        for msg in scratchpad:\n            await memory.aput(msg)\n\n        # reset scratchpad\n        await ctx.set(self.scratchpad_key, [])\n\n        return output"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/workflow/react_agent.py",
    "filename": "react_agent.py",
    "relpath": "agent/workflow/react_agent.py",
    "start_line": 1,
    "end_line": 233,
    "length": 233,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "default_formatter",
      "_get_prompts",
      "_update_prompts",
      "take_step",
      "handle_tool_call_results",
      "finalize"
    ],
    "chunk_class_names": [
      "ReActAgent"
    ],
    "document_function_names": [
      "default_formatter",
      "_get_prompts",
      "_update_prompts",
      "take_step",
      "handle_tool_call_results",
      "finalize"
    ],
    "document_class_names": [
      "ReActAgent"
    ],
    "content": "import uuid\nfrom typing import List, Sequence, cast\n\nfrom llama_index.core.agent.react.formatter import ReActChatFormatter\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.agent.react.types import (\n    ActionReasoningStep,\n    BaseReasoningStep,\n    ObservationReasoningStep,\n    ResponseReasoningStep,\n)\nfrom llama_index.core.agent.workflow.base_agent import BaseWorkflowAgent\nfrom llama_index.core.agent.workflow.workflow_events import (\n    AgentInput,\n    AgentOutput,\n    AgentStream,\n    ToolCallResult,\n)\nfrom llama_index.core.base.llms.types import ChatResponse\nfrom llama_index.core.bridge.pydantic import BaseModel, Field\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.llms.llm import ToolSelection\nfrom llama_index.core.memory import BaseMemory\nfrom llama_index.core.prompts.base import PromptTemplate\nfrom llama_index.core.prompts.mixin import PromptDictType\nfrom llama_index.core.tools import AsyncBaseTool\nfrom llama_index.core.workflow import Context\n\n\ndef default_formatter() -> ReActChatFormatter:\n    \"\"\"Sets up a default formatter so that the proper react header is set.\"\"\"\n    return ReActChatFormatter.from_defaults(context=\"some context\")\n\n\nclass ReActAgent(BaseWorkflowAgent):\n    \"\"\"React agent implementation.\"\"\"\n\n    reasoning_key: str = \"current_reasoning\"\n    output_parser: ReActOutputParser = Field(\n        default_factory=ReActOutputParser, description=\"The react output parser\"\n    )\n    formatter: ReActChatFormatter = Field(\n        default_factory=default_formatter,\n        description=\"The react chat formatter to format the reasoning steps and chat history into an llm input.\",\n    )\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        # TODO: the ReAct formatter does not explicitly specify PromptTemplate\n        # objects, but wrap it in this to obey the interface\n        react_header = self.formatter.system_header\n        return {\"react_header\": PromptTemplate(react_header)}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n        if \"system_prompt\" in prompts:\n            react_header = cast(PromptTemplate, prompts[\"react_header\"])\n            self.formatter.system_header = react_header.template\n\n    async def take_step(\n        self,\n        ctx: Context,\n        llm_input: List[ChatMessage],\n        tools: Sequence[AsyncBaseTool],\n        memory: BaseMemory,\n    ) -> AgentOutput:\n        \"\"\"Take a single step with the React agent.\"\"\"\n        # remove system prompt, since the react prompt will be combined with it\n        if llm_input[0].role == \"system\":\n            system_prompt = llm_input[0].content or \"\"\n            llm_input = llm_input[1:]\n        else:\n            system_prompt = \"\"\n\n        output_parser = self.output_parser\n        react_chat_formatter = self.formatter\n        react_chat_formatter.context = system_prompt\n\n        # Format initial chat input\n        current_reasoning: list[BaseReasoningStep] = await ctx.get(\n            self.reasoning_key, default=[]\n        )\n        input_chat = react_chat_formatter.format(\n            tools,\n            chat_history=llm_input,\n            current_reasoning=current_reasoning,\n        )\n        ctx.write_event_to_stream(\n            AgentInput(input=input_chat, current_agent_name=self.name)\n        )\n\n        # Initial LLM call\n        response = await self.llm.astream_chat(input_chat)\n        # last_chat_response will be used later, after the loop.\n        # We initialize it so it's valid even when 'response' is empty\n        last_chat_response = ChatResponse(message=ChatMessage())\n        async for last_chat_response in response:\n            raw = (\n                last_chat_response.raw.model_dump()\n                if isinstance(last_chat_response.raw, BaseModel)\n                else last_chat_response.raw\n            )\n            ctx.write_event_to_stream(\n                AgentStream(\n                    delta=last_chat_response.delta or \"\",\n                    response=last_chat_response.message.content or \"\",\n                    tool_calls=[],\n                    raw=raw,\n                    current_agent_name=self.name,\n                )\n            )\n\n        # Parse reasoning step and check if done\n        message_content = last_chat_response.message.content\n        if not message_content:\n            raise ValueError(\"Got empty message\")\n\n        try:\n            reasoning_step = output_parser.parse(message_content, is_streaming=False)\n        except ValueError as e:\n            error_msg = f\"Error: Could not parse output. Please follow the thought-action-input format. Try again. Details: {e!s}\"\n            await memory.aput(last_chat_response.message)\n            await memory.aput(ChatMessage(role=\"user\", content=error_msg))\n\n            raw = (\n                last_chat_response.raw.model_dump()\n                if isinstance(last_chat_response.raw, BaseModel)\n                else last_chat_response.raw\n            )\n            return AgentOutput(\n                response=last_chat_response.message,\n                tool_calls=[],\n                raw=raw,\n                current_agent_name=self.name,\n            )\n\n        # add to reasoning if not a handoff\n        current_reasoning.append(reasoning_step)\n        await ctx.set(self.reasoning_key, current_reasoning)\n\n        # If response step, we're done\n        raw = (\n            last_chat_response.raw.model_dump()\n            if isinstance(last_chat_response.raw, BaseModel)\n            else last_chat_response.raw\n        )\n        if reasoning_step.is_done:\n            return AgentOutput(\n                response=last_chat_response.message,\n                tool_calls=[],\n                raw=raw,\n                current_agent_name=self.name,\n            )\n\n        reasoning_step = cast(ActionReasoningStep, reasoning_step)\n        if not isinstance(reasoning_step, ActionReasoningStep):\n            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")\n\n        # Create tool call\n        tool_calls = [\n            ToolSelection(\n                tool_id=str(uuid.uuid4()),\n                tool_name=reasoning_step.action,\n                tool_kwargs=reasoning_step.action_input,\n            )\n        ]\n\n        return AgentOutput(\n            response=last_chat_response.message,\n            tool_calls=tool_calls,\n            raw=raw,\n            current_agent_name=self.name,\n        )\n\n    async def handle_tool_call_results(\n        self, ctx: Context, results: List[ToolCallResult], memory: BaseMemory\n    ) -> None:\n        \"\"\"Handle tool call results for React agent.\"\"\"\n        current_reasoning: list[BaseReasoningStep] = await ctx.get(\n            self.reasoning_key, default=[]\n        )\n        for tool_call_result in results:\n            obs_step = ObservationReasoningStep(\n                observation=str(tool_call_result.tool_output.content),\n                return_direct=tool_call_result.return_direct,\n            )\n            current_reasoning.append(obs_step)\n\n            if (\n                tool_call_result.return_direct\n                and tool_call_result.tool_name != \"handoff\"\n            ):\n                current_reasoning.append(\n                    ResponseReasoningStep(\n                        thought=obs_step.observation,\n                        response=obs_step.observation,\n                        is_streaming=False,\n                    )\n                )\n                break\n\n        await ctx.set(self.reasoning_key, current_reasoning)\n\n    async def finalize(\n        self, ctx: Context, output: AgentOutput, memory: BaseMemory\n    ) -> AgentOutput:\n        \"\"\"Finalize the React agent.\"\"\"\n        current_reasoning: list[BaseReasoningStep] = await ctx.get(\n            self.reasoning_key, default=[]\n        )\n\n        if len(current_reasoning) > 0 and isinstance(\n            current_reasoning[-1], ResponseReasoningStep\n        ):\n            reasoning_str = \"\\n\".join([x.get_content() for x in current_reasoning])\n\n            if reasoning_str:\n                reasoning_msg = ChatMessage(role=\"assistant\", content=reasoning_str)\n                await memory.aput(reasoning_msg)\n                await ctx.set(self.reasoning_key, [])\n\n            # remove \"Answer:\" from the response\n            if output.response.content and \"Answer:\" in output.response.content:\n                start_idx = output.response.content.find(\"Answer:\")\n                if start_idx != -1:\n                    output.response.content = output.response.content[\n                        start_idx + len(\"Answer:\") :\n                    ].strip()\n\n            # clear scratchpad\n            await ctx.set(self.reasoning_key, [])\n\n        return output"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/custom/pipeline_worker.py",
    "filename": "pipeline_worker.py",
    "relpath": "agent/custom/pipeline_worker.py",
    "start_line": 1,
    "end_line": 256,
    "length": 256,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_agent_components",
      "__init__",
      "agent_input_component",
      "agent_components",
      "preprocess",
      "initialize_step",
      "_get_task_step_response",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "chunk_class_names": [
      "QueryPipelineAgentWorker"
    ],
    "document_function_names": [
      "_get_agent_components",
      "__init__",
      "agent_input_component",
      "agent_components",
      "preprocess",
      "initialize_step",
      "_get_task_step_response",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "QueryPipelineAgentWorker"
    ],
    "content": "\"\"\"Agent worker that takes in a query pipeline.\"\"\"\n\nimport uuid\nfrom typing import (\n    Any,\n    List,\n    Optional,\n    Sequence,\n    cast,\n)\n\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.base.query_pipeline.query import QueryComponent\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n)\nfrom llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer\nfrom llama_index.core.query_pipeline.components.agent import (\n    AgentInputComponent,\n    BaseAgentComponent,\n)\nfrom llama_index.core.query_pipeline.query import QueryPipeline\nfrom llama_index.core.tools import ToolOutput\nfrom deprecated import deprecated\n\nDEFAULT_MODEL_NAME = \"gpt-3.5-turbo-0613\"\n\n\ndef _get_agent_components(\n    query_component: QueryComponent,\n) -> Sequence[BaseAgentComponent]:\n    \"\"\"Get agent components.\"\"\"\n    agent_components: List[BaseAgentComponent] = []\n    for c in query_component.sub_query_components:\n        if isinstance(c, BaseAgentComponent):\n            agent_components.append(cast(BaseAgentComponent, c))\n\n        if len(c.sub_query_components) > 0:\n            agent_components.extend(_get_agent_components(c))\n\n    return agent_components\n\n\n@deprecated(\"Use `FnAgentWorker` instead to build a stateful agent.\")\nclass QueryPipelineAgentWorker(BaseModel, BaseAgentWorker):\n    \"\"\"Query Pipeline agent worker.\n\n    NOTE: This is now deprecated. Use `FnAgentWorker` instead to build a stateful agent.\n\n    Barebones agent worker that takes in a query pipeline.\n\n    **Default Workflow**: The default workflow assumes that you compose\n    a query pipeline with `StatefulFnComponent` objects. This allows you to store, update\n    and retrieve state throughout the executions of the query pipeline by the agent.\n\n    The task and step state of the agent are stored in this `state` variable via a special key.\n    Of course you can choose to store other variables in this state as well.\n\n    **Deprecated Workflow**: The deprecated workflow assumes that the first component in the\n    query pipeline is an `AgentInputComponent` and last is `AgentFnComponent`.\n\n    Args:\n        pipeline (QueryPipeline): Query pipeline\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    pipeline: QueryPipeline = Field(..., description=\"Query pipeline\")\n    callback_manager: CallbackManager = Field(..., exclude=True)\n    task_key: str = Field(\"task\", description=\"Key to store task in state\")\n    step_state_key: str = Field(\"step_state\", description=\"Key to store step in state\")\n\n    def __init__(\n        self,\n        pipeline: QueryPipeline,\n        callback_manager: Optional[CallbackManager] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        if callback_manager is not None:\n            # set query pipeline callback\n            pipeline.set_callback_manager(callback_manager)\n        else:\n            callback_manager = pipeline.callback_manager\n        super().__init__(\n            pipeline=pipeline,\n            callback_manager=callback_manager,\n            **kwargs,\n        )\n        # validate query pipeline\n        # self.agent_input_component\n        self.agent_components\n\n    @property\n    def agent_input_component(self) -> AgentInputComponent:\n        \"\"\"Get agent input component.\n\n        NOTE: This is deprecated and will be removed in the future.\n\n        \"\"\"\n        root_key = self.pipeline.get_root_keys()[0]\n        if not isinstance(self.pipeline.module_dict[root_key], AgentInputComponent):\n            raise ValueError(\n                \"Query pipeline first component must be AgentInputComponent, got \"\n                f\"{self.pipeline.module_dict[root_key]}\"\n            )\n\n        return cast(AgentInputComponent, self.pipeline.module_dict[root_key])\n\n    @property\n    def agent_components(self) -> Sequence[BaseAgentComponent]:\n        \"\"\"Get agent output component.\"\"\"\n        return _get_agent_components(self.pipeline)\n\n    def preprocess(self, task: Task, step: TaskStep) -> None:\n        \"\"\"Preprocessing flow.\n\n        This runs preprocessing to propagate the task and step as variables\n        to relevant components in the query pipeline.\n\n        Contains deprecated flow of updating agent components.\n        But also contains main flow of updating StatefulFnComponent components.\n\n        \"\"\"\n        # NOTE: this is deprecated\n        # partial agent output component with task and step\n        for agent_fn_component in self.agent_components:\n            agent_fn_component.partial(task=task, state=step.step_state)\n\n        # update stateful components\n        self.pipeline.update_state(\n            {self.task_key: task, self.step_state_key: step.step_state}\n        )\n\n    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:\n        \"\"\"Initialize step from task.\"\"\"\n        sources: List[ToolOutput] = []\n        # temporary memory for new messages\n        new_memory = ChatMemoryBuffer.from_defaults()\n\n        # initialize initial state\n        initial_state = {\n            \"sources\": sources,\n            \"memory\": new_memory,\n        }\n\n        return TaskStep(\n            task_id=task.task_id,\n            step_id=str(uuid.uuid4()),\n            input=task.input,\n            step_state=initial_state,\n        )\n\n    def _get_task_step_response(\n        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool\n    ) -> TaskStepOutput:\n        \"\"\"Get task step response.\"\"\"\n        if is_done:\n            new_steps = []\n        else:\n            new_steps = [\n                step.get_next_step(\n                    step_id=str(uuid.uuid4()),\n                    # NOTE: input is unused\n                    input=None,\n                )\n            ]\n\n        return TaskStepOutput(\n            output=agent_response,\n            task_step=step,\n            is_last=is_done,\n            next_steps=new_steps,\n        )\n\n    @trace_method(\"run_step\")\n    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        self.preprocess(task, step)\n\n        # HACK: do a try/except for now. Fine since old agent components are deprecated\n        try:\n            self.agent_input_component\n            uses_deprecated = True\n        except ValueError:\n            uses_deprecated = False\n\n        if uses_deprecated:\n            agent_response, is_done = self.pipeline.run(\n                state=step.step_state, task=task\n            )\n        else:\n            agent_response, is_done = self.pipeline.run()\n        response = self._get_task_step_response(agent_response, step, is_done)\n        # sync step state with task state\n        task.extra_state.update(step.step_state)\n        return response\n\n    @trace_method(\"run_step\")\n    async def arun_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        self.preprocess(task, step)\n\n        # HACK: do a try/except for now. Fine since old agent components are deprecated\n        try:\n            self.agent_input_component\n            uses_deprecated = True\n        except ValueError:\n            uses_deprecated = False\n\n        if uses_deprecated:\n            agent_response, is_done = await self.pipeline.arun(\n                state=step.step_state, task=task\n            )\n        else:\n            agent_response, is_done = await self.pipeline.arun()\n        response = self._get_task_step_response(agent_response, step, is_done)\n        task.extra_state.update(step.step_state)\n        return response\n\n    @trace_method(\"run_step\")\n    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        raise NotImplementedError(\"This agent does not support streaming.\")\n\n    @trace_method(\"run_step\")\n    async def astream_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        raise NotImplementedError(\"This agent does not support streaming.\")\n\n    def finalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n        # add new messages to memory\n        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())\n        # reset new memory\n        task.extra_state[\"memory\"].reset()\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: make this abstractmethod (right now will break some agent impls)\n        self.callback_manager = callback_manager\n        self.pipeline.set_callback_manager(callback_manager)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/custom/__init__.py",
    "filename": "__init__.py",
    "relpath": "agent/custom/__init__.py",
    "start_line": 1,
    "end_line": 1,
    "length": 1,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Init params.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/custom/simple_function.py",
    "filename": "simple_function.py",
    "relpath": "agent/custom/simple_function.py",
    "start_line": 1,
    "end_line": 187,
    "length": 187,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "initialize_step",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "chunk_class_names": [
      "FnAgentWorker",
      "this"
    ],
    "document_function_names": [
      "__init__",
      "initialize_step",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "FnAgentWorker",
      "this"
    ],
    "content": "\"\"\"Custom function agent worker.\"\"\"\n\nimport uuid\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Optional,\n    Tuple,\n)\n\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n    AgentChatResponse,\n)\nfrom llama_index.core.query_pipeline.components.function import get_parameters\n\n\nclass FnAgentWorker(BaseModel, BaseAgentWorker):\n    \"\"\"Function Agent Worker.\n\n    Define an agent worker over a stateful function (takes in a `state` variable).\n    The stateful function expects a tuple of (`AgentChatResponse`, bool) as the response.\n\n    Subclass this to define your own agent worker.\n\n    Args:\n        fn (Callable): The function to use. Must contain a `state` dictionary.\n        initial_state (Dict[str, Any]): The initial state\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    fn: Callable = Field(..., description=\"Function to run.\")\n    async_fn: Optional[Callable] = Field(\n        None, description=\"Async function to run. If not provided, will run `fn`.\"\n    )\n    initial_state: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Initial state dictionary.\"\n    )\n    task_input_key: str = Field(default=\"__task__\", description=\"Task\")\n    output_key: str = Field(default=\"__output__\", description=\"output\")\n\n    verbose: bool = Field(False, description=\"Verbose mode.\")\n\n    def __init__(\n        self,\n        fn: Callable,\n        async_fn: Optional[Callable] = None,\n        initial_state: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        # determine parameters\n        default_req_params, default_opt_params = get_parameters(fn)\n        # make sure task and step are part of the list, and remove them from the list\n        if \"state\" not in default_req_params:\n            raise ValueError(\n                \"StatefulFnComponent must have 'state' as required parameters\"\n            )\n\n        super().__init__(fn=fn, async_fn=async_fn, initial_state=initial_state)\n\n    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:\n        \"\"\"Initialize step from task.\"\"\"\n        step_state = {\n            **self.initial_state,\n            self.task_input_key: task,\n        }\n\n        return TaskStep(\n            task_id=task.task_id,\n            step_id=str(uuid.uuid4()),\n            input=task.input,\n            step_state=step_state,\n        )\n\n    def _get_task_step_response(\n        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool\n    ) -> TaskStepOutput:\n        \"\"\"Get task step response.\"\"\"\n        if is_done:\n            new_steps = []\n        else:\n            new_steps = [\n                step.get_next_step(\n                    step_id=str(uuid.uuid4()),\n                    # NOTE: input is unused\n                    input=None,\n                )\n            ]\n\n        return TaskStepOutput(\n            output=agent_response,\n            task_step=step,\n            is_last=is_done,\n            next_steps=new_steps,\n        )\n\n    def _run_step(\n        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n    ) -> Tuple[AgentChatResponse, bool]:\n        \"\"\"Run step.\n\n        Returns:\n            Tuple of (agent_response, is_done)\n\n        \"\"\"\n        current_state, is_done = self.fn(state)\n        # TODO: return auxiliary response\n        output = state[self.output_key] if self.output_key in state else \"\"\n        return (\n            AgentChatResponse(response=output, metadata=current_state),\n            is_done,\n        )\n\n    async def _arun_step(\n        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n    ) -> Tuple[AgentChatResponse, bool]:\n        \"\"\"Run step (async).\n\n        Can override this method if you want to run the step asynchronously.\n\n        Returns:\n            Tuple of (agent_response, is_done)\n\n        \"\"\"\n        if self.async_fn is None:\n            current_state, is_done = self.fn(state)\n        else:\n            current_state, is_done = await self.async_fn(state)\n        # TODO: return auxiliary response\n        return (\n            AgentChatResponse(response=state[self.output_key], metadata=current_state),\n            is_done,\n        )\n\n    @trace_method(\"run_step\")\n    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        agent_response, is_done = self._run_step(\n            step.step_state, task, input=step.input\n        )\n        response = self._get_task_step_response(agent_response, step, is_done)\n        # sync step state with task state\n        task.extra_state.update(step.step_state)\n        return response\n\n    @trace_method(\"run_step\")\n    async def arun_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        agent_response, is_done = await self._arun_step(\n            step.step_state, task, input=step.input\n        )\n        response = self._get_task_step_response(agent_response, step, is_done)\n        task.extra_state.update(step.step_state)\n        return response\n\n    @trace_method(\"run_step\")\n    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        raise NotImplementedError(\"This agent does not support streaming.\")\n\n    @trace_method(\"run_step\")\n    async def astream_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        raise NotImplementedError(\"This agent does not support streaming.\")\n\n    def finalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/custom/simple.py",
    "filename": "simple.py",
    "relpath": "agent/custom/simple.py",
    "start_line": 1,
    "end_line": 261,
    "length": 261,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_tools",
      "_initialize_state",
      "initialize_step",
      "get_tools",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "_finalize_task",
      "finalize_task",
      "set_callback_manager"
    ],
    "chunk_class_names": [
      "CustomSimpleAgentWorker"
    ],
    "document_function_names": [
      "__init__",
      "from_tools",
      "_initialize_state",
      "initialize_step",
      "get_tools",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "_finalize_task",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "CustomSimpleAgentWorker"
    ],
    "content": "\"\"\"Custom agent worker.\"\"\"\n\nimport uuid\nfrom abc import abstractmethod\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    cast,\n)\n\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, PrivateAttr, ConfigDict\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n    AgentChatResponse,\n)\nfrom llama_index.core.llms.llm import LLM\nfrom llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.tools import BaseTool, ToolOutput, adapt_to_async_tool\nfrom llama_index.core.tools.types import AsyncBaseTool\n\n\nclass CustomSimpleAgentWorker(BaseModel, BaseAgentWorker):\n    \"\"\"Custom simple agent worker.\n\n    This is \"simple\" in the sense that some of the scaffolding is setup already.\n    Assumptions:\n    - assumes that the agent has tools, llm, callback manager, and tool retriever\n    - has a `from_tools` convenience function\n    - assumes that the agent is sequential, and doesn't take in any additional\n    intermediate inputs.\n\n    Args:\n        tools (Sequence[BaseTool]): Tools to use for reasoning\n        llm (LLM): LLM to use\n        callback_manager (CallbackManager): Callback manager\n        tool_retriever (Optional[ObjectRetriever[BaseTool]]): Tool retriever\n        verbose (bool): Whether to print out reasoning steps\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    tools: Sequence[BaseTool] = Field(..., description=\"Tools to use for reasoning\")\n    llm: LLM = Field(..., description=\"LLM to use\")\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n    tool_retriever: Optional[ObjectRetriever[BaseTool]] = Field(\n        default=None, description=\"Tool retriever\"\n    )\n    verbose: bool = Field(False, description=\"Whether to print out reasoning steps\")\n\n    _get_tools: Callable[[str], Sequence[BaseTool]] = PrivateAttr()\n\n    def __init__(\n        self,\n        tools: Sequence[BaseTool],\n        llm: LLM,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        **kwargs: Any,\n    ) -> None:\n        callback_manager = callback_manager or CallbackManager([])\n        super().__init__(\n            tools=tools,\n            llm=llm,\n            callback_manager=callback_manager or CallbackManager([]),\n            tool_retriever=tool_retriever,\n            verbose=verbose,\n            **kwargs,\n        )\n\n        if len(tools) > 0 and tool_retriever is not None:\n            raise ValueError(\"Cannot specify both tools and tool_retriever\")\n        elif len(tools) > 0:\n            self._get_tools = lambda _: tools\n        elif tool_retriever is not None:\n            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)\n            self._get_tools = lambda message: tool_retriever_c.retrieve(message)\n        else:\n            self._get_tools = lambda _: []\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: Optional[Sequence[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        llm: Optional[LLM] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"CustomSimpleAgentWorker\":\n        \"\"\"Convenience constructor method from set of BaseTools (Optional).\"\"\"\n        llm = llm or Settings.llm\n        if callback_manager is not None:\n            llm.callback_manager = callback_manager\n        return cls(\n            tools=tools or [],\n            tool_retriever=tool_retriever,\n            llm=llm,\n            callback_manager=callback_manager or CallbackManager([]),\n            verbose=verbose,\n            **kwargs,\n        )\n\n    @abstractmethod\n    def _initialize_state(self, task: Task, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Initialize state.\"\"\"\n\n    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:\n        \"\"\"Initialize step from task.\"\"\"\n        sources: List[ToolOutput] = []\n        # temporary memory for new messages\n        new_memory = ChatMemoryBuffer.from_defaults()\n\n        # initialize initial state\n        initial_state = {\n            \"sources\": sources,\n            \"memory\": new_memory,\n        }\n\n        step_state = self._initialize_state(task, **kwargs)\n        # if intersecting keys, error\n        if set(step_state.keys()).intersection(set(initial_state.keys())):\n            raise ValueError(\n                f\"Step state keys {step_state.keys()} and initial state keys {initial_state.keys()} intersect.\"\n                f\"*NOTE*: initial state keys {initial_state.keys()} are reserved.\"\n            )\n        step_state.update(initial_state)\n\n        return TaskStep(\n            task_id=task.task_id,\n            step_id=str(uuid.uuid4()),\n            input=task.input,\n            step_state=step_state,\n        )\n\n    def get_tools(self, input: str) -> List[AsyncBaseTool]:\n        \"\"\"Get tools.\"\"\"\n        return [adapt_to_async_tool(t) for t in self._get_tools(input)]\n\n    def _get_task_step_response(\n        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool\n    ) -> TaskStepOutput:\n        \"\"\"Get task step response.\"\"\"\n        if is_done:\n            new_steps = []\n        else:\n            new_steps = [\n                step.get_next_step(\n                    step_id=str(uuid.uuid4()),\n                    # NOTE: input is unused\n                    input=None,\n                )\n            ]\n\n        return TaskStepOutput(\n            output=agent_response,\n            task_step=step,\n            is_last=is_done,\n            next_steps=new_steps,\n        )\n\n    @abstractmethod\n    def _run_step(\n        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n    ) -> Tuple[AgentChatResponse, bool]:\n        \"\"\"Run step.\n\n        Returns:\n            Tuple of (agent_response, is_done)\n\n        \"\"\"\n\n    async def _arun_step(\n        self, state: Dict[str, Any], task: Task, input: Optional[str] = None\n    ) -> Tuple[AgentChatResponse, bool]:\n        \"\"\"Run step (async).\n\n        Can override this method if you want to run the step asynchronously.\n\n        Returns:\n            Tuple of (agent_response, is_done)\n\n        \"\"\"\n        raise NotImplementedError(\n            \"This agent does not support async.\" \"Please implement _arun_step.\"\n        )\n\n    @trace_method(\"run_step\")\n    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        agent_response, is_done = self._run_step(\n            step.step_state, task, input=step.input\n        )\n        response = self._get_task_step_response(agent_response, step, is_done)\n        # sync step state with task state\n        task.extra_state.update(step.step_state)\n        return response\n\n    @trace_method(\"run_step\")\n    async def arun_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        agent_response, is_done = await self._arun_step(\n            step.step_state, task, input=step.input\n        )\n        response = self._get_task_step_response(agent_response, step, is_done)\n        task.extra_state.update(step.step_state)\n        return response\n\n    @trace_method(\"run_step\")\n    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        raise NotImplementedError(\"This agent does not support streaming.\")\n\n    @trace_method(\"run_step\")\n    async def astream_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        raise NotImplementedError(\"This agent does not support streaming.\")\n\n    @abstractmethod\n    def _finalize_task(self, state: Dict[str, Any], **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\n\n        State is all the step states.\n\n        \"\"\"\n\n    def finalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n        # add new messages to memory\n        task.memory.set(task.memory.get() + task.extra_state[\"memory\"].get_all())\n        # reset new memory\n        task.extra_state[\"memory\"].reset()\n        self._finalize_task(task.extra_state, **kwargs)\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: make this abstractmethod (right now will break some agent impls)\n        self.callback_manager = callback_manager"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react_multimodal/prompts.py",
    "filename": "prompts.py",
    "relpath": "agent/react_multimodal/prompts.py",
    "start_line": 1,
    "end_line": 87,
    "length": 87,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Default prompt for ReAct agent.\"\"\"\n\n\n# ReAct multimodal chat prompt\n# TODO: have formatting instructions be a part of react output parser\n\nREACT_MM_CHAT_SYSTEM_HEADER = \"\"\"\\\n\nYou are designed to help with a variety of tasks, from answering questions \\\n    to providing summaries to other types of analyses. You can take in both text \\\n    and images.\n\n\n## Tools\nYou have access to a wide variety of tools. You are responsible for using\nthe tools in any sequence you deem appropriate to complete the task at hand.\nThis may require breaking the task into subtasks and using different tools\nto complete each subtask.\n\nNOTE: you do NOT need to use a tool to understand the provided images. You can\nuse both the input text and images as context to decide which tool to use.\n\nYou have access to the following tools:\n{tool_desc}\n\n## Input\nThe user will specify a task (in text) and a set of images. Treat\nthe images as additional context for the task.\n\n## Output Format\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names}) if using a tool.\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n```\n\nPlease ALWAYS start with a Thought.\n\nPlease use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, the tool will respond in the following format:\n\n```\nObservation: tool response\n```\n\nHere's a concrete example. Again, you can take in both text and images as input. This can generate a thought which can be used to decide which tool to use.\nThe input to the tool should not assume knowledge of the image. Therefore it is your responsibility \\\n    to translate the input text/images into a format that the tool can understand.\n\nFor example:\n```\nThought: This image is a picture of a brown dog. The text asked me to identify its name, so I need to use a tool to lookup its name.\nAction: churchill_bio_tool\nAction Input: {{\"input\": \"brown dog name\"}}\n\n```\nExample user response:\n\n```\nObservation: The name of the brown dog is Rufus.\n```\n\n\nYou should keep repeating the above format until you have enough information\nto answer the question without using any more tools. At that point, you MUST respond\nin the one of the following two formats:\n\n```\nThought: I can answer without using any more tools.\nAnswer: [your answer here]\n```\n\n```\nThought: I cannot answer the question with the provided tools.\nAnswer: Sorry, I cannot answer your query.\n```\n\nThe answer MUST be grounded in the input text and images. Do not give an answer that is irrelevant to the image\nprovided.\n\n## Current Conversation\nBelow is the current conversation consisting of interleaving human and assistant messages.\n\n\"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react_multimodal/step.py",
    "filename": "step.py",
    "relpath": "agent/react_multimodal/step.py",
    "start_line": 1,
    "end_line": 99,
    "length": 99,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__call__",
      "add_user_step_to_reasoning"
    ],
    "chunk_class_names": [
      "ChatMessageCallable"
    ],
    "document_function_names": [
      "__call__",
      "add_user_step_to_reasoning",
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ChatMessageCallable",
      "MultimodalReActAgentWorker"
    ],
    "content": "\"\"\"ReAct multimodal agent.\"\"\"\n\nimport uuid\nfrom functools import partial\nfrom typing import Any, Dict, List, Optional, Protocol, Sequence, Tuple, cast\n\nfrom llama_index.core.agent.react.formatter import ReActChatFormatter\nfrom llama_index.core.agent.react.output_parser import ReActOutputParser\nfrom llama_index.core.agent.react.types import (\n    ActionReasoningStep,\n    BaseReasoningStep,\n    ObservationReasoningStep,\n    ResponseReasoningStep,\n)\nfrom llama_index.core.agent.react_multimodal.prompts import (\n    REACT_MM_CHAT_SYSTEM_HEADER,\n)\nfrom llama_index.core.agent.types import (\n    BaseAgentWorker,\n    Task,\n    TaskStep,\n    TaskStepOutput,\n)\nfrom llama_index.core.base.llms.types import MessageRole\nfrom llama_index.core.callbacks import (\n    CallbackManager,\n    CBEventType,\n    EventPayload,\n    trace_method,\n)\nfrom llama_index.core.chat_engine.types import (\n    AGENT_CHAT_RESPONSE_TYPE,\n    AgentChatResponse,\n)\nfrom llama_index.core.base.llms.types import ChatMessage, ChatResponse\nfrom llama_index.core.memory.chat_memory_buffer import ChatMemoryBuffer\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.multi_modal_llms.base import MultiModalLLM\nfrom llama_index.core.objects.base import ObjectRetriever\nfrom llama_index.core.schema import ImageDocument\nfrom llama_index.core.tools import BaseTool, ToolOutput, adapt_to_async_tool\nfrom llama_index.core.tools.types import AsyncBaseTool\nfrom llama_index.core.utils import print_text\n\nDEFAULT_MODEL_NAME = \"gpt-3.5-turbo-0613\"\n\n\nclass ChatMessageCallable(Protocol):\n    \"\"\"ChatMessage Callable Protocol.\"\"\"\n\n    def __call__(\n        self,\n        prompt: str,\n        role: str,\n        image_documents: Optional[Sequence[ImageDocument]],\n        **kwargs: Any,\n    ) -> ChatMessage:\n        ...\n\n\ndef add_user_step_to_reasoning(\n    generate_chat_message_fn: ChatMessageCallable,\n    step: TaskStep,\n    memory: BaseMemory,\n    current_reasoning: List[BaseReasoningStep],\n    verbose: bool = False,\n) -> None:\n    \"\"\"Add user step to reasoning.\n\n    Adds both text input and image input to reasoning.\n\n    \"\"\"\n    # raise error if step.input is None\n    if step.input is None:\n        raise ValueError(\"Step input is None.\")\n\n    # TODO: currently assume that you can't generate images in the loop,\n    # so step_state contains the original image_docs from the task\n    # (it doesn't change)\n    image_docs = step.step_state[\"image_docs\"]\n    image_kwargs = step.step_state.get(\"image_kwargs\", {})\n\n    if \"is_first\" in step.step_state and step.step_state[\"is_first\"]:\n        mm_message = generate_chat_message_fn(\n            prompt=step.input,\n            role=MessageRole.USER,\n            image_documents=image_docs,\n            **image_kwargs,\n        )\n        # add to new memory\n        memory.put(mm_message)\n        step.step_state[\"is_first\"] = False\n    else:\n        # NOTE: this is where the user specifies an intermediate step in the middle\n        # TODO: don't support specifying image_docs here for now\n        reasoning_step = ObservationReasoningStep(observation=step.input)\n        current_reasoning.append(reasoning_step)\n        if verbose:\n            print(f\"Added user message to memory: {step.input}\")"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react_multimodal/step.py",
    "filename": "step.py",
    "relpath": "agent/react_multimodal/step.py",
    "start_line": 99,
    "end_line": 102,
    "length": 4,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [
      "MultimodalReActAgentWorker"
    ],
    "document_function_names": [
      "__call__",
      "add_user_step_to_reasoning",
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ChatMessageCallable",
      "MultimodalReActAgentWorker"
    ],
    "content": "class MultimodalReActAgentWorker(BaseAgentWorker):"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react_multimodal/step.py",
    "filename": "step.py",
    "relpath": "agent/react_multimodal/step.py",
    "start_line": 102,
    "end_line": 526,
    "length": 425,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__call__",
      "add_user_step_to_reasoning",
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ChatMessageCallable",
      "MultimodalReActAgentWorker"
    ],
    "content": "\"\"\"Multimodal ReAct Agent worker.\n\n    **NOTE**: This is a BETA feature.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        tools: Sequence[BaseTool],\n        multi_modal_llm: MultiModalLLM,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        generate_chat_message_fn: Optional[ChatMessageCallable] = None,\n    ) -> None:\n        self._multi_modal_llm = multi_modal_llm\n        self.callback_manager = callback_manager or CallbackManager([])\n        self._max_iterations = max_iterations\n        self._react_chat_formatter = react_chat_formatter or ReActChatFormatter(\n            system_header=REACT_MM_CHAT_SYSTEM_HEADER\n        )\n        self._output_parser = output_parser or ReActOutputParser()\n        self._verbose = verbose\n        self._generate_chat_message_fn = generate_chat_message_fn\n        if self._generate_chat_message_fn is None:\n            try:\n                from llama_index.multi_modal_llms.openai.utils import (\n                    generate_openai_multi_modal_chat_message,\n                )  # pants: no-infer-dep\n\n                self._generate_chat_message_fn = (\n                    generate_openai_multi_modal_chat_message\n                )\n\n            except ImportError:\n                raise ImportError(\n                    \"`llama-index-multi-modal-llms-openai` package cannot be found. \"\n                    \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"\n                )\n\n        self._add_user_step_to_reasoning = partial(\n            add_user_step_to_reasoning,\n            generate_chat_message_fn=self._generate_chat_message_fn,  # type: ignore\n        )\n\n        if len(tools) > 0 and tool_retriever is not None:\n            raise ValueError(\"Cannot specify both tools and tool_retriever\")\n        elif len(tools) > 0:\n            self._get_tools = lambda _: tools\n        elif tool_retriever is not None:\n            tool_retriever_c = cast(ObjectRetriever[BaseTool], tool_retriever)\n            self._get_tools = lambda message: tool_retriever_c.retrieve(message)\n        else:\n            self._get_tools = lambda _: []\n\n    @classmethod\n    def from_tools(\n        cls,\n        tools: Optional[Sequence[BaseTool]] = None,\n        tool_retriever: Optional[ObjectRetriever[BaseTool]] = None,\n        multi_modal_llm: Optional[MultiModalLLM] = None,\n        max_iterations: int = 10,\n        react_chat_formatter: Optional[ReActChatFormatter] = None,\n        output_parser: Optional[ReActOutputParser] = None,\n        callback_manager: Optional[CallbackManager] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -> \"MultimodalReActAgentWorker\":\n        \"\"\"Convenience constructor method from set of BaseTools (Optional).\n\n        NOTE: kwargs should have been exhausted by this point. In other words\n        the various upstream components such as BaseSynthesizer (response synthesizer)\n        or BaseRetriever should have picked up off their respective kwargs in their\n        constructions.\n\n        Returns:\n            ReActAgent\n        \"\"\"\n        if multi_modal_llm is None:\n            try:\n                from llama_index.multi_modal_llms.openai import (\n                    OpenAIMultiModal,\n                )  # pants: no-infer-dep\n\n                multi_modal_llm = multi_modal_llm or OpenAIMultiModal(\n                    model=\"gpt-4-vision-preview\", max_new_tokens=1000\n                )\n            except ImportError:\n                raise ImportError(\n                    \"`llama-index-multi-modal-llms-openai` package cannot be found. \"\n                    \"Please install it by using `pip install `llama-index-multi-modal-llms-openai`\"\n                )\n        return cls(\n            tools=tools or [],\n            tool_retriever=tool_retriever,\n            multi_modal_llm=multi_modal_llm,\n            max_iterations=max_iterations,\n            react_chat_formatter=react_chat_formatter,\n            output_parser=output_parser,\n            callback_manager=callback_manager,\n            verbose=verbose,\n        )\n\n    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:\n        \"\"\"Initialize step from task.\"\"\"\n        sources: List[ToolOutput] = []\n        current_reasoning: List[BaseReasoningStep] = []\n        # temporary memory for new messages\n        new_memory = ChatMemoryBuffer.from_defaults()\n\n        # validation\n        if \"image_docs\" not in task.extra_state:\n            raise ValueError(\"Image docs not found in task extra state.\")\n\n        # initialize task state\n        task_state = {\n            \"sources\": sources,\n            \"current_reasoning\": current_reasoning,\n            \"new_memory\": new_memory,\n        }\n        task.extra_state.update(task_state)\n\n        return TaskStep(\n            task_id=task.task_id,\n            step_id=str(uuid.uuid4()),\n            input=task.input,\n            step_state={\"is_first\": True, \"image_docs\": task.extra_state[\"image_docs\"]},\n        )\n\n    def get_tools(self, input: str) -> List[AsyncBaseTool]:\n        \"\"\"Get tools.\"\"\"\n        return [adapt_to_async_tool(t) for t in self._get_tools(input)]\n\n    def _extract_reasoning_step(\n        self, output: ChatResponse, is_streaming: bool = False\n    ) -> Tuple[str, List[BaseReasoningStep], bool]:\n        \"\"\"\n        Extracts the reasoning step from the given output.\n\n        This method parses the message content from the output,\n        extracts the reasoning step, and determines whether the processing is\n        complete. It also performs validation checks on the output and\n        handles possible errors.\n        \"\"\"\n        if output.message.content is None:\n            raise ValueError(\"Got empty message.\")\n        message_content = output.message.content\n\n        current_reasoning = []\n        try:\n            reasoning_step = self._output_parser.parse(message_content, is_streaming)\n        except BaseException as exc:\n            raise ValueError(f\"Could not parse output: {message_content}\") from exc\n        if self._verbose:\n            print_text(f\"{reasoning_step.get_content()}\\n\", color=\"pink\")\n        current_reasoning.append(reasoning_step)\n\n        if reasoning_step.is_done:\n            return message_content, current_reasoning, True\n\n        reasoning_step = cast(ActionReasoningStep, reasoning_step)\n        if not isinstance(reasoning_step, ActionReasoningStep):\n            raise ValueError(f\"Expected ActionReasoningStep, got {reasoning_step}\")\n\n        return message_content, current_reasoning, False\n\n    def _process_actions(\n        self,\n        task: Task,\n        tools: Sequence[AsyncBaseTool],\n        output: ChatResponse,\n        is_streaming: bool = False,\n    ) -> Tuple[List[BaseReasoningStep], bool]:\n        tools_dict: Dict[str, AsyncBaseTool] = {\n            tool.metadata.get_name(): tool for tool in tools\n        }\n        _, current_reasoning, is_done = self._extract_reasoning_step(\n            output, is_streaming\n        )\n\n        if is_done:\n            return current_reasoning, True\n\n        # call tool with input\n        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])\n        tool = tools_dict[reasoning_step.action]\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: reasoning_step.action_input,\n                EventPayload.TOOL: tool.metadata,\n            },\n        ) as event:\n            tool_output = tool.call(**reasoning_step.action_input)\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n\n        task.extra_state[\"sources\"].append(tool_output)\n\n        observation_step = ObservationReasoningStep(\n            observation=str(tool_output), return_direct=tool.metadata.return_direct\n        )\n        current_reasoning.append(observation_step)\n        if self._verbose:\n            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")\n        return current_reasoning, tool.metadata.return_direct\n\n    async def _aprocess_actions(\n        self,\n        task: Task,\n        tools: Sequence[AsyncBaseTool],\n        output: ChatResponse,\n        is_streaming: bool = False,\n    ) -> Tuple[List[BaseReasoningStep], bool]:\n        tools_dict = {tool.metadata.name: tool for tool in tools}\n        _, current_reasoning, is_done = self._extract_reasoning_step(\n            output, is_streaming\n        )\n\n        if is_done:\n            return current_reasoning, True\n\n        # call tool with input\n        reasoning_step = cast(ActionReasoningStep, current_reasoning[-1])\n        tool = tools_dict[reasoning_step.action]\n        with self.callback_manager.event(\n            CBEventType.FUNCTION_CALL,\n            payload={\n                EventPayload.FUNCTION_CALL: reasoning_step.action_input,\n                EventPayload.TOOL: tool.metadata,\n            },\n        ) as event:\n            tool_output = await tool.acall(**reasoning_step.action_input)\n            event.on_end(payload={EventPayload.FUNCTION_OUTPUT: str(tool_output)})\n\n        task.extra_state[\"sources\"].append(tool_output)\n\n        observation_step = ObservationReasoningStep(\n            observation=str(tool_output), return_direct=tool.metadata.return_direct\n        )\n        current_reasoning.append(observation_step)\n        if self._verbose:\n            print_text(f\"{observation_step.get_content()}\\n\", color=\"blue\")\n        return current_reasoning, tool.metadata.return_direct\n\n    def _get_response(\n        self,\n        current_reasoning: List[BaseReasoningStep],\n        sources: List[ToolOutput],\n    ) -> AgentChatResponse:\n        \"\"\"Get response from reasoning steps.\"\"\"\n        if len(current_reasoning) == 0:\n            raise ValueError(\"No reasoning steps were taken.\")\n        elif len(current_reasoning) == self._max_iterations:\n            raise ValueError(\"Reached max iterations.\")\n\n        if isinstance(current_reasoning[-1], ResponseReasoningStep):\n            response_step = cast(ResponseReasoningStep, current_reasoning[-1])\n            response_str = response_step.response\n        elif (\n            isinstance(current_reasoning[-1], ObservationReasoningStep)\n            and current_reasoning[-1].return_direct\n        ):\n            response_str = current_reasoning[-1].observation\n        else:\n            response_str = current_reasoning[-1].get_content()\n\n        # TODO: add sources from reasoning steps\n        return AgentChatResponse(response=response_str, sources=sources)\n\n    def _get_task_step_response(\n        self, agent_response: AGENT_CHAT_RESPONSE_TYPE, step: TaskStep, is_done: bool\n    ) -> TaskStepOutput:\n        \"\"\"Get task step response.\"\"\"\n        if is_done:\n            new_steps = []\n        else:\n            new_steps = [\n                step.get_next_step(\n                    step_id=str(uuid.uuid4()),\n                    # NOTE: input is unused\n                    input=None,\n                )\n            ]\n\n        return TaskStepOutput(\n            output=agent_response,\n            task_step=step,\n            is_last=is_done,\n            next_steps=new_steps,\n        )\n\n    def _run_step(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        # This is either not None on the first step or if the user specifies\n        # an intermediate step in the middle\n        if step.input is not None:\n            self._add_user_step_to_reasoning(\n                step=step,\n                memory=task.extra_state[\"new_memory\"],\n                current_reasoning=task.extra_state[\"current_reasoning\"],\n                verbose=self._verbose,\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n\n        input_chat = self._react_chat_formatter.format(\n            tools,\n            chat_history=task.memory.get_all()\n            + task.extra_state[\"new_memory\"].get_all(),\n            current_reasoning=task.extra_state[\"current_reasoning\"],\n        )\n\n        # send prompt\n        chat_response = self._multi_modal_llm.chat(input_chat)\n        # given react prompt outputs, call tools or return response\n        reasoning_steps, is_done = self._process_actions(\n            task, tools, output=chat_response\n        )\n        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)\n        agent_response = self._get_response(\n            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]\n        )\n        if is_done:\n            task.extra_state[\"new_memory\"].put(\n                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)\n            )\n\n        return self._get_task_step_response(agent_response, step, is_done)\n\n    async def _arun_step(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        if step.input is not None:\n            self._add_user_step_to_reasoning(\n                step=step,\n                memory=task.extra_state[\"new_memory\"],\n                current_reasoning=task.extra_state[\"current_reasoning\"],\n                verbose=self._verbose,\n            )\n        # TODO: see if we want to do step-based inputs\n        tools = self.get_tools(task.input)\n\n        input_chat = self._react_chat_formatter.format(\n            tools,\n            chat_history=task.memory.get_all()\n            + task.extra_state[\"new_memory\"].get_all(),\n            current_reasoning=task.extra_state[\"current_reasoning\"],\n        )\n        # send prompt\n        chat_response = await self._multi_modal_llm.achat(input_chat)\n        # given react prompt outputs, call tools or return response\n        reasoning_steps, is_done = await self._aprocess_actions(\n            task, tools, output=chat_response\n        )\n        task.extra_state[\"current_reasoning\"].extend(reasoning_steps)\n        agent_response = self._get_response(\n            task.extra_state[\"current_reasoning\"], task.extra_state[\"sources\"]\n        )\n        if is_done:\n            task.extra_state[\"new_memory\"].put(\n                ChatMessage(content=agent_response.response, role=MessageRole.ASSISTANT)\n            )\n\n        return self._get_task_step_response(agent_response, step, is_done)\n\n    def _run_step_stream(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        raise NotImplementedError(\"Stream step not implemented yet.\")\n\n    async def _arun_step_stream(\n        self,\n        step: TaskStep,\n        task: Task,\n    ) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        raise NotImplementedError(\"Stream step not implemented yet.\")\n\n    @trace_method(\"run_step\")\n    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n        return self._run_step(step, task)\n\n    @trace_method(\"run_step\")\n    async def arun_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        return await self._arun_step(step, task)\n\n    @trace_method(\"run_step\")\n    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        # TODO: figure out if we need a different type for TaskStepOutput\n        return self._run_step_stream(step, task)\n\n    @trace_method(\"run_step\")\n    async def astream_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        return await self._arun_step_stream(step, task)\n\n    def finalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n        # add new messages to memory\n        task.memory.set(\n            task.memory.get_all() + task.extra_state[\"new_memory\"].get_all()\n        )\n        # reset new memory\n        task.extra_state[\"new_memory\"].reset()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/agent/react_multimodal/step.py",
    "filename": "step.py",
    "relpath": "agent/react_multimodal/step.py",
    "start_line": 526,
    "end_line": 531,
    "length": 6,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "set_callback_manager"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "__call__",
      "add_user_step_to_reasoning",
      "__init__",
      "from_tools",
      "initialize_step",
      "get_tools",
      "_extract_reasoning_step",
      "_process_actions",
      "_aprocess_actions",
      "_get_response",
      "_get_task_step_response",
      "_run_step",
      "_arun_step",
      "_run_step_stream",
      "_arun_step_stream",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "set_callback_manager"
    ],
    "document_class_names": [
      "ChatMessageCallable",
      "MultimodalReActAgentWorker"
    ],
    "content": "def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: make this abstractmethod (right now will break some agent impls)\n        self.callback_manager = callback_manager"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/base_selector.py",
    "filename": "base_selector.py",
    "relpath": "base/base_selector.py",
    "start_line": 1,
    "end_line": 115,
    "length": 115,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "ind",
      "reason",
      "inds",
      "reasons",
      "_wrap_choice",
      "_wrap_query",
      "_get_prompt_modules",
      "select",
      "aselect",
      "_select",
      "_aselect",
      "_as_query_component"
    ],
    "chunk_class_names": [
      "SingleSelection",
      "MultiSelection",
      "BaseSelector"
    ],
    "document_function_names": [
      "ind",
      "reason",
      "inds",
      "reasons",
      "_wrap_choice",
      "_wrap_query",
      "_get_prompt_modules",
      "select",
      "aselect",
      "_select",
      "_aselect",
      "_as_query_component"
    ],
    "document_class_names": [
      "SingleSelection",
      "MultiSelection",
      "BaseSelector"
    ],
    "content": "from abc import abstractmethod\nfrom typing import Any, List, Sequence, Union\n\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    QueryComponent,\n)\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.prompts.mixin import PromptMixin, PromptMixinType\nfrom llama_index.core.schema import QueryBundle, QueryType\nfrom llama_index.core.tools.types import ToolMetadata\n\nMetadataType = Union[str, ToolMetadata]\n\n\nclass SingleSelection(BaseModel):\n    \"\"\"A single selection of a choice.\"\"\"\n\n    index: int\n    reason: str\n\n\nclass MultiSelection(BaseModel):\n    \"\"\"A multi-selection of choices.\"\"\"\n\n    selections: List[SingleSelection]\n\n    @property\n    def ind(self) -> int:\n        if len(self.selections) != 1:\n            raise ValueError(\n                f\"There are {len(self.selections)} selections, \" \"please use .inds.\"\n            )\n        return self.selections[0].index\n\n    @property\n    def reason(self) -> str:\n        if len(self.reasons) != 1:\n            raise ValueError(\n                f\"There are {len(self.reasons)} selections, \" \"please use .reasons.\"\n            )\n        return self.selections[0].reason\n\n    @property\n    def inds(self) -> List[int]:\n        return [x.index for x in self.selections]\n\n    @property\n    def reasons(self) -> List[str]:\n        return [x.reason for x in self.selections]\n\n\n# separate name for clarity and to not confuse function calling model\nSelectorResult = MultiSelection\n\n\ndef _wrap_choice(choice: MetadataType) -> ToolMetadata:\n    if isinstance(choice, ToolMetadata):\n        return choice\n    elif isinstance(choice, str):\n        return ToolMetadata(description=choice)\n    else:\n        raise ValueError(f\"Unexpected type: {type(choice)}\")\n\n\ndef _wrap_query(query: QueryType) -> QueryBundle:\n    if isinstance(query, QueryBundle):\n        return query\n    elif isinstance(query, str):\n        return QueryBundle(query_str=query)\n    else:\n        raise ValueError(f\"Unexpected type: {type(query)}\")\n\n\nclass BaseSelector(PromptMixin, ChainableMixin, DispatcherSpanMixin):\n    \"\"\"Base selector.\"\"\"\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt sub-modules.\"\"\"\n        return {}\n\n    def select(\n        self, choices: Sequence[MetadataType], query: QueryType\n    ) -> SelectorResult:\n        metadatas = [_wrap_choice(choice) for choice in choices]\n        query_bundle = _wrap_query(query)\n        return self._select(choices=metadatas, query=query_bundle)\n\n    async def aselect(\n        self, choices: Sequence[MetadataType], query: QueryType\n    ) -> SelectorResult:\n        metadatas = [_wrap_choice(choice) for choice in choices]\n        query_bundle = _wrap_query(query)\n        return await self._aselect(choices=metadatas, query=query_bundle)\n\n    @abstractmethod\n    def _select(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        pass\n\n    @abstractmethod\n    async def _aselect(\n        self, choices: Sequence[ToolMetadata], query: QueryBundle\n    ) -> SelectorResult:\n        pass\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"As query component.\"\"\"\n        from llama_index.core.query_pipeline.components.router import (\n            SelectorComponent,\n        )\n\n        return SelectorComponent(selector=self)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/base_query_engine.py",
    "filename": "base_query_engine.py",
    "relpath": "base/base_query_engine.py",
    "start_line": 1,
    "end_line": 144,
    "length": 144,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "query",
      "aquery",
      "retrieve",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "BaseQueryEngine",
      "QueryEngineComponent"
    ],
    "document_function_names": [
      "__init__",
      "_get_prompts",
      "_update_prompts",
      "query",
      "aquery",
      "retrieve",
      "synthesize",
      "asynthesize",
      "_query",
      "_aquery",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "BaseQueryEngine",
      "QueryEngineComponent"
    ],
    "content": "\"\"\"Base query engine.\"\"\"\n\nimport logging\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, Sequence\n\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict, SerializeAsAny\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixin\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, QueryType\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.instrumentation.events.query import (\n    QueryEndEvent,\n    QueryStartEvent,\n)\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\nlogger = logging.getLogger(__name__)\n\n\nclass BaseQueryEngine(ChainableMixin, PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base query engine.\"\"\"\n\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager],\n    ) -> None:\n        self.callback_manager = callback_manager or CallbackManager([])\n\n    def _get_prompts(self) -> Dict[str, Any]:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    @dispatcher.span\n    def query(self, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:\n        dispatcher.event(QueryStartEvent(query=str_or_query_bundle))\n        with self.callback_manager.as_trace(\"query\"):\n            if isinstance(str_or_query_bundle, str):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n            query_result = self._query(str_or_query_bundle)\n        dispatcher.event(\n            QueryEndEvent(query=str_or_query_bundle, response=query_result)\n        )\n        return query_result\n\n    @dispatcher.span\n    async def aquery(self, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:\n        dispatcher.event(QueryStartEvent(query=str_or_query_bundle))\n        with self.callback_manager.as_trace(\"query\"):\n            if isinstance(str_or_query_bundle, str):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n            query_result = await self._aquery(str_or_query_bundle)\n        dispatcher.event(\n            QueryEndEvent(query=str_or_query_bundle, response=query_result)\n        )\n        return query_result\n\n    def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        raise NotImplementedError(\n            \"This query engine does not support retrieve, use query directly\"\n        )\n\n    def synthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        raise NotImplementedError(\n            \"This query engine does not support synthesize, use query directly\"\n        )\n\n    async def asynthesize(\n        self,\n        query_bundle: QueryBundle,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n    ) -> RESPONSE_TYPE:\n        raise NotImplementedError(\n            \"This query engine does not support asynthesize, use aquery directly\"\n        )\n\n    @abstractmethod\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        pass\n\n    @abstractmethod\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        pass\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"Return a query component.\"\"\"\n        return QueryEngineComponent(query_engine=self)\n\n\nclass QueryEngineComponent(QueryComponent):\n    \"\"\"Query engine component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    query_engine: SerializeAsAny[BaseQueryEngine] = Field(\n        ..., description=\"Query engine\"\n    )\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.query_engine.callback_manager = callback_manager\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # make sure input is a string\n        input[\"input\"] = validate_and_convert_stringable(input[\"input\"])\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = self.query_engine.query(kwargs[\"input\"])\n        return {\"output\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = await self.query_engine.aquery(kwargs[\"input\"])\n        return {\"output\": output}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"input\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/base_multi_modal_retriever.py",
    "filename": "base_multi_modal_retriever.py",
    "relpath": "base/base_multi_modal_retriever.py",
    "start_line": 1,
    "end_line": 70,
    "length": 70,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "text_retrieve",
      "text_to_image_retrieve",
      "image_to_image_retrieve",
      "atext_retrieve",
      "atext_to_image_retrieve",
      "aimage_to_image_retrieve"
    ],
    "chunk_class_names": [
      "MultiModalRetriever"
    ],
    "document_function_names": [
      "text_retrieve",
      "text_to_image_retrieve",
      "image_to_image_retrieve",
      "atext_retrieve",
      "atext_to_image_retrieve",
      "aimage_to_image_retrieve"
    ],
    "document_class_names": [
      "MultiModalRetriever"
    ],
    "content": "\"\"\"base multi modal retriever.\"\"\"\nfrom abc import abstractmethod\nfrom typing import List\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.image_retriever import BaseImageRetriever\nfrom llama_index.core.indices.query.schema import QueryType\nfrom llama_index.core.schema import NodeWithScore\n\n\nclass MultiModalRetriever(BaseRetriever, BaseImageRetriever):\n    \"\"\"Multi Modal base retriever.\"\"\"\n\n    @abstractmethod\n    def text_retrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n        \"\"\"Retrieve text nodes given text query.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    @abstractmethod\n    def text_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve image nodes given text query.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    @abstractmethod\n    def image_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve image nodes given image query.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    @abstractmethod\n    async def atext_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"Async Retrieve text nodes given text query.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    @abstractmethod\n    async def atext_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"Async Retrieve image nodes given text query.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    @abstractmethod\n    async def aimage_to_image_retrieve(\n        self, str_or_query_bundle: QueryType\n    ) -> List[NodeWithScore]:\n        \"\"\"Async Retrieve image nodes given image query.\n\n        Implemented by the user.\n\n        \"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/base_auto_retriever.py",
    "filename": "base_auto_retriever.py",
    "relpath": "base/base_auto_retriever.py",
    "start_line": 1,
    "end_line": 43,
    "length": 43,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "generate_retrieval_spec",
      "agenerate_retrieval_spec",
      "_build_retriever_from_spec",
      "_retrieve",
      "_aretrieve"
    ],
    "chunk_class_names": [
      "BaseAutoRetriever"
    ],
    "document_function_names": [
      "generate_retrieval_spec",
      "agenerate_retrieval_spec",
      "_build_retriever_from_spec",
      "_retrieve",
      "_aretrieve"
    ],
    "document_class_names": [
      "BaseAutoRetriever"
    ],
    "content": "from abc import abstractmethod\nfrom typing import Any, List, Tuple\n\nfrom llama_index.core.base.base_retriever import BaseRetriever\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.schema import NodeWithScore, QueryBundle\n\n\nclass BaseAutoRetriever(BaseRetriever):\n    \"\"\"Base auto retriever.\"\"\"\n\n    @abstractmethod\n    def generate_retrieval_spec(\n        self, query_bundle: QueryBundle, **kwargs: Any\n    ) -> BaseModel:\n        \"\"\"Generate retrieval spec synchronously.\"\"\"\n        ...\n\n    @abstractmethod\n    async def agenerate_retrieval_spec(\n        self, query_bundle: QueryBundle, **kwargs: Any\n    ) -> BaseModel:\n        \"\"\"Generate retrieval spec asynchronously.\"\"\"\n        ...\n\n    @abstractmethod\n    def _build_retriever_from_spec(\n        self, retrieval_spec: BaseModel\n    ) -> Tuple[BaseRetriever, QueryBundle]:\n        \"\"\"Build retriever from spec and provide query bundle.\"\"\"\n        ...\n\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec.\"\"\"\n        retrieval_spec = self.generate_retrieval_spec(query_bundle)\n        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)\n        return retriever.retrieve(new_query_bundle)\n\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve using generated spec asynchronously.\"\"\"\n        retrieval_spec = await self.agenerate_retrieval_spec(query_bundle)\n        retriever, new_query_bundle = self._build_retriever_from_spec(retrieval_spec)\n        return await retriever.aretrieve(new_query_bundle)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/base_retriever.py",
    "filename": "base_retriever.py",
    "relpath": "base/base_retriever.py",
    "start_line": 1,
    "end_line": 348,
    "length": 348,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "_check_callback_manager",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_retrieve_from_object",
      "_aretrieve_from_object",
      "_handle_recursive_retrieval",
      "_ahandle_recursive_retrieval",
      "retrieve",
      "aretrieve",
      "_retrieve",
      "_aretrieve",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "BaseRetriever",
      "RetrieverComponent"
    ],
    "document_function_names": [
      "__init__",
      "_check_callback_manager",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_retrieve_from_object",
      "_aretrieve_from_object",
      "_handle_recursive_retrieval",
      "_ahandle_recursive_retrieval",
      "retrieve",
      "aretrieve",
      "_retrieve",
      "_aretrieve",
      "_as_query_component",
      "set_callback_manager",
      "_validate_component_inputs",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "BaseRetriever",
      "RetrieverComponent"
    ],
    "content": "\"\"\"Base retriever.\"\"\"\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.prompts.mixin import (\n    PromptDictType,\n    PromptMixin,\n    PromptMixinType,\n)\nfrom llama_index.core.schema import (\n    BaseNode,\n    IndexNode,\n    NodeWithScore,\n    QueryBundle,\n    QueryType,\n    TextNode,\n)\nfrom llama_index.core.settings import Settings\nfrom llama_index.core.utils import print_text\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.instrumentation.events.retrieval import (\n    RetrievalEndEvent,\n    RetrievalStartEvent,\n)\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass BaseRetriever(ChainableMixin, PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base retriever.\"\"\"\n\n    def __init__(\n        self,\n        callback_manager: Optional[CallbackManager] = None,\n        object_map: Optional[Dict] = None,\n        objects: Optional[List[IndexNode]] = None,\n        verbose: bool = False,\n    ) -> None:\n        self.callback_manager = callback_manager or CallbackManager()\n\n        if objects is not None:\n            object_map = {obj.index_id: obj.obj for obj in objects}\n\n        self.object_map = object_map or {}\n        self._verbose = verbose\n\n    def _check_callback_manager(self) -> None:\n        \"\"\"Check callback manager.\"\"\"\n        if not hasattr(self, \"callback_manager\"):\n            self.callback_manager = Settings.callback_manager\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        return {}\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    def _retrieve_from_object(\n        self,\n        obj: Any,\n        query_bundle: QueryBundle,\n        score: float,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes from object.\"\"\"\n        if self._verbose:\n            print_text(\n                f\"Retrieving from object {obj.__class__.__name__} with query {query_bundle.query_str}\\n\",\n                color=\"llama_pink\",\n            )\n        if isinstance(obj, NodeWithScore):\n            return [obj]\n        elif isinstance(obj, BaseNode):\n            return [NodeWithScore(node=obj, score=score)]\n        elif isinstance(obj, BaseQueryEngine):\n            response = obj.query(query_bundle)\n            return [\n                NodeWithScore(\n                    node=TextNode(text=str(response), metadata=response.metadata or {}),\n                    score=score,\n                )\n            ]\n        elif isinstance(obj, BaseRetriever):\n            return obj.retrieve(query_bundle)\n        elif isinstance(obj, QueryComponent):\n            component_keys = obj.input_keys.required_keys\n            if len(component_keys) > 1:\n                raise ValueError(\n                    f\"QueryComponent {obj} has more than one input key: {component_keys}\"\n                )\n            elif len(component_keys) == 0:\n                component_response = obj.run_component()\n            else:\n                kwargs = {next(iter(component_keys)): query_bundle.query_str}\n                component_response = obj.run_component(**kwargs)\n\n            result_output = str(next(iter(component_response.values())))\n            return [NodeWithScore(node=TextNode(text=result_output), score=score)]\n        else:\n            raise ValueError(f\"Object {obj} is not retrievable.\")\n\n    async def _aretrieve_from_object(\n        self,\n        obj: Any,\n        query_bundle: QueryBundle,\n        score: float,\n    ) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes from object.\"\"\"\n        if isinstance(obj, NodeWithScore):\n            return [obj]\n        elif isinstance(obj, BaseNode):\n            return [NodeWithScore(node=obj, score=score)]\n        elif isinstance(obj, BaseQueryEngine):\n            response = await obj.aquery(query_bundle)\n            return [NodeWithScore(node=TextNode(text=str(response)), score=score)]\n        elif isinstance(obj, BaseRetriever):\n            return await obj.aretrieve(query_bundle)\n        elif isinstance(obj, QueryComponent):\n            component_keys = obj.input_keys.required_keys\n            if len(component_keys) > 1:\n                raise ValueError(\n                    f\"QueryComponent {obj} has more than one input key: {component_keys}\"\n                )\n            elif len(component_keys) == 0:\n                component_response = await obj.arun_component()\n            else:\n                kwargs = {next(iter(component_keys)): query_bundle.query_str}\n                component_response = await obj.arun_component(**kwargs)\n\n            result_output = str(next(iter(component_response.values())))\n            return [NodeWithScore(node=TextNode(text=result_output), score=score)]\n        else:\n            raise ValueError(f\"Object {obj} is not retrievable.\")\n\n    def _handle_recursive_retrieval(\n        self, query_bundle: QueryBundle, nodes: List[NodeWithScore]\n    ) -> List[NodeWithScore]:\n        retrieved_nodes: List[NodeWithScore] = []\n        for n in nodes:\n            node = n.node\n            score = n.score or 1.0\n            if isinstance(node, IndexNode):\n                obj = node.obj or self.object_map.get(node.index_id, None)\n                if obj is not None:\n                    if self._verbose:\n                        print_text(\n                            f\"Retrieval entering {node.index_id}: {obj.__class__.__name__}\\n\",\n                            color=\"llama_turquoise\",\n                        )\n                    retrieved_nodes.extend(\n                        self._retrieve_from_object(\n                            obj, query_bundle=query_bundle, score=score\n                        )\n                    )\n                else:\n                    retrieved_nodes.append(n)\n            else:\n                retrieved_nodes.append(n)\n\n        seen = set()\n        return [\n            n\n            for n in retrieved_nodes\n            if not (n.node.hash in seen or seen.add(n.node.hash))  # type: ignore[func-returns-value]\n        ]\n\n    async def _ahandle_recursive_retrieval(\n        self, query_bundle: QueryBundle, nodes: List[NodeWithScore]\n    ) -> List[NodeWithScore]:\n        retrieved_nodes: List[NodeWithScore] = []\n        for n in nodes:\n            node = n.node\n            score = n.score or 1.0\n            if isinstance(node, IndexNode):\n                obj = node.obj or self.object_map.get(node.index_id, None)\n                if obj is not None:\n                    if self._verbose:\n                        print_text(\n                            f\"Retrieval entering {node.index_id}: {obj.__class__.__name__}\\n\",\n                            color=\"llama_turquoise\",\n                        )\n                    # TODO: Add concurrent execution via `run_jobs()` ?\n                    retrieved_nodes.extend(\n                        await self._aretrieve_from_object(\n                            obj, query_bundle=query_bundle, score=score\n                        )\n                    )\n                else:\n                    retrieved_nodes.append(n)\n            else:\n                retrieved_nodes.append(n)\n\n        # remove any duplicates based on hash and ref_doc_id\n        seen = set()\n        return [\n            n\n            for n in retrieved_nodes\n            if not (\n                (n.node.hash, n.node.ref_doc_id) in seen\n                or seen.add((n.node.hash, n.node.ref_doc_id))  # type: ignore[func-returns-value]\n            )\n        ]\n\n    @dispatcher.span\n    def retrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\n\n        Args:\n            str_or_query_bundle (QueryType): Either a query string or\n                a QueryBundle object.\n\n        \"\"\"\n        self._check_callback_manager()\n        dispatcher.event(\n            RetrievalStartEvent(\n                str_or_query_bundle=str_or_query_bundle,\n            )\n        )\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n        with self.callback_manager.as_trace(\"query\"):\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as retrieve_event:\n                nodes = self._retrieve(query_bundle)\n                nodes = self._handle_recursive_retrieval(query_bundle, nodes)\n                retrieve_event.on_end(\n                    payload={EventPayload.NODES: nodes},\n                )\n        dispatcher.event(\n            RetrievalEndEvent(\n                str_or_query_bundle=str_or_query_bundle,\n                nodes=nodes,\n            )\n        )\n        return nodes\n\n    @dispatcher.span\n    async def aretrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:\n        self._check_callback_manager()\n\n        dispatcher.event(\n            RetrievalStartEvent(\n                str_or_query_bundle=str_or_query_bundle,\n            )\n        )\n        if isinstance(str_or_query_bundle, str):\n            query_bundle = QueryBundle(str_or_query_bundle)\n        else:\n            query_bundle = str_or_query_bundle\n        with self.callback_manager.as_trace(\"query\"):\n            with self.callback_manager.event(\n                CBEventType.RETRIEVE,\n                payload={EventPayload.QUERY_STR: query_bundle.query_str},\n            ) as retrieve_event:\n                nodes = await self._aretrieve(query_bundle=query_bundle)\n                nodes = await self._ahandle_recursive_retrieval(\n                    query_bundle=query_bundle, nodes=nodes\n                )\n                retrieve_event.on_end(\n                    payload={EventPayload.NODES: nodes},\n                )\n        dispatcher.event(\n            RetrievalEndEvent(\n                str_or_query_bundle=str_or_query_bundle,\n                nodes=nodes,\n            )\n        )\n        return nodes\n\n    @abstractmethod\n    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"\n\n    # TODO: make this abstract\n    # @abstractmethod\n    async def _aretrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n        \"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"\n        return self._retrieve(query_bundle)\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"Return a query component.\"\"\"\n        return RetrieverComponent(retriever=self)\n\n\nclass RetrieverComponent(QueryComponent):\n    \"\"\"Retriever component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    retriever: BaseRetriever = Field(..., description=\"Retriever\")\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.retriever.callback_manager = callback_manager\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # make sure input is a string\n        input[\"input\"] = validate_and_convert_stringable(input[\"input\"])\n        return input\n\n    def _run_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = self.retriever.retrieve(kwargs[\"input\"])\n        return {\"output\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component.\"\"\"\n        output = await self.retriever.aretrieve(kwargs[\"input\"])\n        return {\"output\": output}\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"input\"})\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/embeddings/base.py",
    "filename": "base.py",
    "relpath": "base/embeddings/base.py",
    "start_line": 1,
    "end_line": 464,
    "length": 464,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "mean_agg",
      "similarity",
      "check_callback_manager",
      "_get_query_embedding",
      "_aget_query_embedding",
      "get_query_embedding",
      "aget_query_embedding",
      "get_agg_embedding_from_queries",
      "aget_agg_embedding_from_queries",
      "_get_text_embedding",
      "_aget_text_embedding",
      "_get_text_embeddings",
      "_aget_text_embeddings",
      "get_text_embedding",
      "aget_text_embedding",
      "get_text_embedding_batch",
      "aget_text_embedding_batch",
      "similarity",
      "__call__",
      "acall"
    ],
    "chunk_class_names": [
      "SimilarityMode",
      "BaseEmbedding",
      "for"
    ],
    "document_function_names": [
      "mean_agg",
      "similarity",
      "check_callback_manager",
      "_get_query_embedding",
      "_aget_query_embedding",
      "get_query_embedding",
      "aget_query_embedding",
      "get_agg_embedding_from_queries",
      "aget_agg_embedding_from_queries",
      "_get_text_embedding",
      "_aget_text_embedding",
      "_get_text_embeddings",
      "_aget_text_embeddings",
      "get_text_embedding",
      "aget_text_embedding",
      "get_text_embedding_batch",
      "aget_text_embedding_batch",
      "similarity",
      "__call__",
      "acall"
    ],
    "document_class_names": [
      "SimilarityMode",
      "BaseEmbedding",
      "for"
    ],
    "content": "\"\"\"Base embeddings file.\"\"\"\n\nimport asyncio\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom typing import Any, Callable, Coroutine, List, Optional, Sequence, Tuple\n\nimport numpy as np\nfrom llama_index.core.bridge.pydantic import (\n    Field,\n    ConfigDict,\n    field_validator,\n)\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.callbacks.schema import CBEventType, EventPayload\nfrom llama_index.core.constants import (\n    DEFAULT_EMBED_BATCH_SIZE,\n)\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.schema import BaseNode, MetadataMode, TransformComponent\nfrom llama_index.core.utils import get_tqdm_iterable\nfrom llama_index.core.async_utils import run_jobs\n\n# TODO: change to numpy array\nEmbedding = List[float]\n\n\nfrom llama_index.core.instrumentation.events.embedding import (\n    EmbeddingEndEvent,\n    EmbeddingStartEvent,\n)\nimport llama_index.core.instrumentation as instrument\n\ndispatcher = instrument.get_dispatcher(__name__)\n\n\nclass SimilarityMode(str, Enum):\n    \"\"\"Modes for similarity/distance.\"\"\"\n\n    DEFAULT = \"cosine\"\n    DOT_PRODUCT = \"dot_product\"\n    EUCLIDEAN = \"euclidean\"\n\n\ndef mean_agg(embeddings: List[Embedding]) -> Embedding:\n    \"\"\"Mean aggregation for embeddings.\"\"\"\n    return np.array(embeddings).mean(axis=0).tolist()\n\n\ndef similarity(\n    embedding1: Embedding,\n    embedding2: Embedding,\n    mode: SimilarityMode = SimilarityMode.DEFAULT,\n) -> float:\n    \"\"\"Get embedding similarity.\"\"\"\n    if mode == SimilarityMode.EUCLIDEAN:\n        # Using -euclidean distance as similarity to achieve same ranking order\n        return -float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))\n    elif mode == SimilarityMode.DOT_PRODUCT:\n        return np.dot(embedding1, embedding2)\n    else:\n        product = np.dot(embedding1, embedding2)\n        norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n        return product / norm\n\n\nclass BaseEmbedding(TransformComponent, DispatcherSpanMixin):\n    \"\"\"Base class for embeddings.\"\"\"\n\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    model_name: str = Field(\n        default=\"unknown\", description=\"The name of the embedding model.\"\n    )\n    embed_batch_size: int = Field(\n        default=DEFAULT_EMBED_BATCH_SIZE,\n        description=\"The batch size for embedding calls.\",\n        gt=0,\n        le=2048,\n    )\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n    num_workers: Optional[int] = Field(\n        default=None,\n        description=\"The number of workers to use for async embedding calls.\",\n    )\n\n    @field_validator(\"callback_manager\")\n    @classmethod\n    def check_callback_manager(cls, v: CallbackManager) -> CallbackManager:\n        if v is None:\n            return CallbackManager([])\n        return v\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -> Embedding:\n        \"\"\"\n        Embed the input query synchronously.\n\n        Subclasses should implement this method. Reference get_query_embedding's\n        docstring for more information.\n        \"\"\"\n\n    @abstractmethod\n    async def _aget_query_embedding(self, query: str) -> Embedding:\n        \"\"\"\n        Embed the input query asynchronously.\n\n        Subclasses should implement this method. Reference get_query_embedding's\n        docstring for more information.\n        \"\"\"\n\n    @dispatcher.span\n    def get_query_embedding(self, query: str) -> Embedding:\n        \"\"\"\n        Embed the input query.\n\n        When embedding a query, depending on the model, a special instruction\n        can be prepended to the raw query string. For example, \"Represent the\n        question for retrieving supporting documents: \". If you're curious,\n        other examples of predefined instructions can be found in\n        embeddings/huggingface_utils.py.\n        \"\"\"\n        model_dict = self.to_dict()\n        model_dict.pop(\"api_key\", None)\n        dispatcher.event(\n            EmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n        with self.callback_manager.event(\n            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}\n        ) as event:\n            query_embedding = self._get_query_embedding(query)\n\n            event.on_end(\n                payload={\n                    EventPayload.CHUNKS: [query],\n                    EventPayload.EMBEDDINGS: [query_embedding],\n                },\n            )\n        dispatcher.event(\n            EmbeddingEndEvent(\n                chunks=[query],\n                embeddings=[query_embedding],\n            )\n        )\n        return query_embedding\n\n    @dispatcher.span\n    async def aget_query_embedding(self, query: str) -> Embedding:\n        \"\"\"Get query embedding.\"\"\"\n        model_dict = self.to_dict()\n        model_dict.pop(\"api_key\", None)\n        dispatcher.event(\n            EmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n        with self.callback_manager.event(\n            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}\n        ) as event:\n            query_embedding = await self._aget_query_embedding(query)\n\n            event.on_end(\n                payload={\n                    EventPayload.CHUNKS: [query],\n                    EventPayload.EMBEDDINGS: [query_embedding],\n                },\n            )\n        dispatcher.event(\n            EmbeddingEndEvent(\n                chunks=[query],\n                embeddings=[query_embedding],\n            )\n        )\n        return query_embedding\n\n    def get_agg_embedding_from_queries(\n        self,\n        queries: List[str],\n        agg_fn: Optional[Callable[..., Embedding]] = None,\n    ) -> Embedding:\n        \"\"\"Get aggregated embedding from multiple queries.\"\"\"\n        query_embeddings = [self.get_query_embedding(query) for query in queries]\n        agg_fn = agg_fn or mean_agg\n        return agg_fn(query_embeddings)\n\n    async def aget_agg_embedding_from_queries(\n        self,\n        queries: List[str],\n        agg_fn: Optional[Callable[..., Embedding]] = None,\n    ) -> Embedding:\n        \"\"\"Async get aggregated embedding from multiple queries.\"\"\"\n        query_embeddings = [await self.aget_query_embedding(query) for query in queries]\n        agg_fn = agg_fn or mean_agg\n        return agg_fn(query_embeddings)\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> Embedding:\n        \"\"\"\n        Embed the input text synchronously.\n\n        Subclasses should implement this method. Reference get_text_embedding's\n        docstring for more information.\n        \"\"\"\n\n    async def _aget_text_embedding(self, text: str) -> Embedding:\n        \"\"\"\n        Embed the input text asynchronously.\n\n        Subclasses can implement this method if there is a true async\n        implementation. Reference get_text_embedding's docstring for more\n        information.\n        \"\"\"\n        # Default implementation just falls back on _get_text_embedding\n        return self._get_text_embedding(text)\n\n    def _get_text_embeddings(self, texts: List[str]) -> List[Embedding]:\n        \"\"\"\n        Embed the input sequence of text synchronously.\n\n        Subclasses can implement this method if batch queries are supported.\n        \"\"\"\n        # Default implementation just loops over _get_text_embedding\n        return [self._get_text_embedding(text) for text in texts]\n\n    async def _aget_text_embeddings(self, texts: List[str]) -> List[Embedding]:\n        \"\"\"\n        Embed the input sequence of text asynchronously.\n\n        Subclasses can implement this method if batch queries are supported.\n        \"\"\"\n        return await asyncio.gather(\n            *[self._aget_text_embedding(text) for text in texts]\n        )\n\n    @dispatcher.span\n    def get_text_embedding(self, text: str) -> Embedding:\n        \"\"\"\n        Embed the input text.\n\n        When embedding text, depending on the model, a special instruction\n        can be prepended to the raw text string. For example, \"Represent the\n        document for retrieval: \". If you're curious, other examples of\n        predefined instructions can be found in embeddings/huggingface_utils.py.\n        \"\"\"\n        model_dict = self.to_dict()\n        model_dict.pop(\"api_key\", None)\n        dispatcher.event(\n            EmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n        with self.callback_manager.event(\n            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}\n        ) as event:\n            text_embedding = self._get_text_embedding(text)\n\n            event.on_end(\n                payload={\n                    EventPayload.CHUNKS: [text],\n                    EventPayload.EMBEDDINGS: [text_embedding],\n                }\n            )\n        dispatcher.event(\n            EmbeddingEndEvent(\n                chunks=[text],\n                embeddings=[text_embedding],\n            )\n        )\n        return text_embedding\n\n    @dispatcher.span\n    async def aget_text_embedding(self, text: str) -> Embedding:\n        \"\"\"Async get text embedding.\"\"\"\n        model_dict = self.to_dict()\n        model_dict.pop(\"api_key\", None)\n        dispatcher.event(\n            EmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n        with self.callback_manager.event(\n            CBEventType.EMBEDDING, payload={EventPayload.SERIALIZED: self.to_dict()}\n        ) as event:\n            text_embedding = await self._aget_text_embedding(text)\n\n            event.on_end(\n                payload={\n                    EventPayload.CHUNKS: [text],\n                    EventPayload.EMBEDDINGS: [text_embedding],\n                }\n            )\n        dispatcher.event(\n            EmbeddingEndEvent(\n                chunks=[text],\n                embeddings=[text_embedding],\n            )\n        )\n        return text_embedding\n\n    @dispatcher.span\n    def get_text_embedding_batch(\n        self,\n        texts: List[str],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[Embedding]:\n        \"\"\"Get a list of text embeddings, with batching.\"\"\"\n        cur_batch: List[str] = []\n        result_embeddings: List[Embedding] = []\n\n        queue_with_progress = enumerate(\n            get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")\n        )\n\n        model_dict = self.to_dict()\n        model_dict.pop(\"api_key\", None)\n        for idx, text in queue_with_progress:\n            cur_batch.append(text)\n            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:\n                # flush\n                dispatcher.event(\n                    EmbeddingStartEvent(\n                        model_dict=model_dict,\n                    )\n                )\n                with self.callback_manager.event(\n                    CBEventType.EMBEDDING,\n                    payload={EventPayload.SERIALIZED: self.to_dict()},\n                ) as event:\n                    embeddings = self._get_text_embeddings(cur_batch)\n                    result_embeddings.extend(embeddings)\n                    event.on_end(\n                        payload={\n                            EventPayload.CHUNKS: cur_batch,\n                            EventPayload.EMBEDDINGS: embeddings,\n                        },\n                    )\n                dispatcher.event(\n                    EmbeddingEndEvent(\n                        chunks=cur_batch,\n                        embeddings=embeddings,\n                    )\n                )\n                cur_batch = []\n\n        return result_embeddings\n\n    @dispatcher.span\n    async def aget_text_embedding_batch(\n        self, texts: List[str], show_progress: bool = False\n    ) -> List[Embedding]:\n        \"\"\"Asynchronously get a list of text embeddings, with batching.\"\"\"\n        num_workers = self.num_workers\n\n        model_dict = self.to_dict()\n        model_dict.pop(\"api_key\", None)\n\n        cur_batch: List[str] = []\n        callback_payloads: List[Tuple[str, List[str]]] = []\n        result_embeddings: List[Embedding] = []\n        embeddings_coroutines: List[Coroutine] = []\n        for idx, text in enumerate(texts):\n            cur_batch.append(text)\n            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:\n                # flush\n                dispatcher.event(\n                    EmbeddingStartEvent(\n                        model_dict=model_dict,\n                    )\n                )\n                event_id = self.callback_manager.on_event_start(\n                    CBEventType.EMBEDDING,\n                    payload={EventPayload.SERIALIZED: self.to_dict()},\n                )\n                callback_payloads.append((event_id, cur_batch))\n                embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))\n                cur_batch = []\n\n        # flatten the results of asyncio.gather, which is a list of embeddings lists\n        nested_embeddings = []\n\n        if num_workers and num_workers > 1:\n            nested_embeddings = await run_jobs(\n                embeddings_coroutines,\n                show_progress=show_progress,\n                workers=self.num_workers,\n                desc=\"Generating embeddings\",\n            )\n        else:\n            if show_progress:\n                try:\n                    from tqdm.asyncio import tqdm_asyncio\n\n                    nested_embeddings = await tqdm_asyncio.gather(\n                        *embeddings_coroutines,\n                        total=len(embeddings_coroutines),\n                        desc=\"Generating embeddings\",\n                    )\n                except ImportError:\n                    nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n            else:\n                nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n\n        result_embeddings = [\n            embedding for embeddings in nested_embeddings for embedding in embeddings\n        ]\n\n        for (event_id, text_batch), embeddings in zip(\n            callback_payloads, nested_embeddings\n        ):\n            dispatcher.event(\n                EmbeddingEndEvent(\n                    chunks=text_batch,\n                    embeddings=embeddings,\n                )\n            )\n            self.callback_manager.on_event_end(\n                CBEventType.EMBEDDING,\n                payload={\n                    EventPayload.CHUNKS: text_batch,\n                    EventPayload.EMBEDDINGS: embeddings,\n                },\n                event_id=event_id,\n            )\n\n        return result_embeddings\n\n    def similarity(\n        self,\n        embedding1: Embedding,\n        embedding2: Embedding,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -> float:\n        \"\"\"Get embedding similarity.\"\"\"\n        return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)\n\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -> Sequence[BaseNode]:\n        embeddings = self.get_text_embedding_batch(\n            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],\n            **kwargs,\n        )\n\n        for node, embedding in zip(nodes, embeddings):\n            node.embedding = embedding\n\n        return nodes\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -> Sequence[BaseNode]:\n        embeddings = await self.aget_text_embedding_batch(\n            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],\n            **kwargs,\n        )\n\n        for node, embedding in zip(nodes, embeddings):\n            node.embedding = embedding\n\n        return nodes"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/embeddings/base_sparse.py",
    "filename": "base_sparse.py",
    "relpath": "base/embeddings/base_sparse.py",
    "start_line": 1,
    "end_line": 353,
    "length": 353,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "sparse_similarity",
      "mean_agg",
      "class_name",
      "custom_model_dump",
      "_get_query_embedding",
      "_aget_query_embedding",
      "get_query_embedding",
      "aget_query_embedding",
      "get_agg_embedding_from_queries",
      "aget_agg_embedding_from_queries",
      "_get_text_embedding",
      "_aget_text_embedding",
      "_get_text_embeddings",
      "_aget_text_embeddings",
      "get_text_embedding",
      "aget_text_embedding",
      "get_text_embedding_batch",
      "aget_text_embedding_batch",
      "similarity"
    ],
    "chunk_class_names": [
      "BaseSparseEmbedding",
      "for",
      "name"
    ],
    "document_function_names": [
      "sparse_similarity",
      "mean_agg",
      "class_name",
      "custom_model_dump",
      "_get_query_embedding",
      "_aget_query_embedding",
      "get_query_embedding",
      "aget_query_embedding",
      "get_agg_embedding_from_queries",
      "aget_agg_embedding_from_queries",
      "_get_text_embedding",
      "_aget_text_embedding",
      "_get_text_embeddings",
      "_aget_text_embeddings",
      "get_text_embedding",
      "aget_text_embedding",
      "get_text_embedding_batch",
      "aget_text_embedding_batch",
      "similarity"
    ],
    "document_class_names": [
      "BaseSparseEmbedding",
      "for",
      "name"
    ],
    "content": "\"\"\"Base sparse embeddings file.\"\"\"\n\nimport asyncio\nimport math\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom typing import Any, Callable, Coroutine, Dict, List, Optional\n\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    ConfigDict,\n    model_serializer,\n)\nfrom llama_index.core.constants import DEFAULT_EMBED_BATCH_SIZE\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.instrumentation.events.embedding import (\n    SparseEmbeddingEndEvent,\n    SparseEmbeddingStartEvent,\n)\nimport llama_index.core.instrumentation as instrument\nfrom llama_index.core.utils import get_tqdm_iterable\nfrom llama_index.core.async_utils import run_jobs\n\n\ndispatcher = instrument.get_dispatcher(__name__)\n\nSparseEmbedding = Dict[int, float]\n\n\ndef sparse_similarity(\n    embedding1: SparseEmbedding,\n    embedding2: SparseEmbedding,\n) -> float:\n    \"\"\"Get sparse embedding similarity.\"\"\"\n    if not embedding1 or not embedding2:\n        return 0.0\n\n    # Use the smaller embedding as the primary iteration set\n    if len(embedding1) > len(embedding2):\n        embedding1, embedding2 = embedding2, embedding1\n\n    # Precompute norms and find common indices\n    norm1 = norm2 = dot_product = 0.0\n    common_indices = set(embedding1.keys()) & set(embedding2.keys())\n\n    for idx, value in embedding1.items():\n        norm1 += value**2\n        if idx in common_indices:\n            dot_product += value * embedding2[idx]\n\n    for value in embedding2.values():\n        norm2 += value**2\n\n    if norm1 == 0.0 or norm2 == 0.0:\n        return 0.0\n\n    return dot_product / (math.sqrt(norm1) * math.sqrt(norm2))\n\n\ndef mean_agg(embeddings: List[SparseEmbedding]) -> SparseEmbedding:\n    \"\"\"Get mean aggregation of embeddings.\"\"\"\n    if not embeddings:\n        return {}\n\n    sum_dict: Dict[int, float] = defaultdict(float)\n    for embedding in embeddings:\n        for idx, value in embedding.items():\n            sum_dict[idx] += value\n\n    return {idx: value / len(embeddings) for idx, value in sum_dict.items()}\n\n\nclass BaseSparseEmbedding(BaseModel, DispatcherSpanMixin):\n    \"\"\"Base class for embeddings.\"\"\"\n\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    model_name: str = Field(\n        default=\"unknown\", description=\"The name of the embedding model.\"\n    )\n    embed_batch_size: int = Field(\n        default=DEFAULT_EMBED_BATCH_SIZE,\n        description=\"The batch size for embedding calls.\",\n        gt=0,\n        le=2048,\n    )\n    num_workers: Optional[int] = Field(\n        default=None,\n        description=\"The number of workers to use for async embedding calls.\",\n    )\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"BaseSparseEmbedding\"\n\n    @model_serializer(mode=\"wrap\")\n    def custom_model_dump(self, handler: Any) -> Dict[str, Any]:\n        data = handler(self)\n        # add class name\n        data[\"class_name\"] = self.class_name()\n        # del api_key if it exists\n        data.pop(\"api_key\", None)\n        return data\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -> SparseEmbedding:\n        \"\"\"Embed the input query synchronously.\"\"\"\n\n    @abstractmethod\n    async def _aget_query_embedding(self, query: str) -> SparseEmbedding:\n        \"\"\"Embed the input query asynchronously.\"\"\"\n\n    @dispatcher.span\n    def get_query_embedding(self, query: str) -> SparseEmbedding:\n        \"\"\"Embed the input query.\"\"\"\n        model_dict = self.model_dump()\n        dispatcher.event(\n            SparseEmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n\n        query_embedding = self._get_query_embedding(query)\n\n        dispatcher.event(\n            SparseEmbeddingEndEvent(\n                chunks=[query],\n                embeddings=[query_embedding],\n            )\n        )\n        return query_embedding\n\n    @dispatcher.span\n    async def aget_query_embedding(self, query: str) -> SparseEmbedding:\n        \"\"\"Get query embedding.\"\"\"\n        model_dict = self.model_dump()\n        dispatcher.event(\n            SparseEmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n\n        query_embedding = await self._aget_query_embedding(query)\n\n        dispatcher.event(\n            SparseEmbeddingEndEvent(\n                chunks=[query],\n                embeddings=[query_embedding],\n            )\n        )\n        return query_embedding\n\n    def get_agg_embedding_from_queries(\n        self,\n        queries: List[str],\n        agg_fn: Optional[Callable[..., SparseEmbedding]] = None,\n    ) -> SparseEmbedding:\n        \"\"\"Get aggregated embedding from multiple queries.\"\"\"\n        query_embeddings = [self.get_query_embedding(query) for query in queries]\n        agg_fn = agg_fn or mean_agg\n        return agg_fn(query_embeddings)\n\n    async def aget_agg_embedding_from_queries(\n        self,\n        queries: List[str],\n        agg_fn: Optional[Callable[..., SparseEmbedding]] = None,\n    ) -> SparseEmbedding:\n        \"\"\"Async get aggregated embedding from multiple queries.\"\"\"\n        query_embeddings = [await self.aget_query_embedding(query) for query in queries]\n        agg_fn = agg_fn or mean_agg\n        return agg_fn(query_embeddings)\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -> SparseEmbedding:\n        \"\"\"Embed the input text synchronously.\"\"\"\n\n    @abstractmethod\n    async def _aget_text_embedding(self, text: str) -> SparseEmbedding:\n        \"\"\"Embed the input text asynchronously.\"\"\"\n\n    def _get_text_embeddings(self, texts: List[str]) -> List[SparseEmbedding]:\n        \"\"\"\n        Embed the input sequence of text synchronously.\n\n        Subclasses can implement this method if batch queries are supported.\n        \"\"\"\n        # Default implementation just loops over _get_text_embedding\n        return [self._get_text_embedding(text) for text in texts]\n\n    async def _aget_text_embeddings(self, texts: List[str]) -> List[SparseEmbedding]:\n        \"\"\"\n        Embed the input sequence of text asynchronously.\n\n        Subclasses can implement this method if batch queries are supported.\n        \"\"\"\n        return await asyncio.gather(\n            *[self._aget_text_embedding(text) for text in texts]\n        )\n\n    @dispatcher.span\n    def get_text_embedding(self, text: str) -> SparseEmbedding:\n        \"\"\"Embed the input text.\"\"\"\n        model_dict = self.model_dump()\n        dispatcher.event(\n            SparseEmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n\n        text_embedding = self._get_text_embedding(text)\n\n        dispatcher.event(\n            SparseEmbeddingEndEvent(\n                chunks=[text],\n                embeddings=[text_embedding],\n            )\n        )\n        return text_embedding\n\n    @dispatcher.span\n    async def aget_text_embedding(self, text: str) -> SparseEmbedding:\n        \"\"\"Async get text embedding.\"\"\"\n        model_dict = self.model_dump()\n        dispatcher.event(\n            SparseEmbeddingStartEvent(\n                model_dict=model_dict,\n            )\n        )\n\n        text_embedding = await self._aget_text_embedding(text)\n\n        dispatcher.event(\n            SparseEmbeddingEndEvent(\n                chunks=[text],\n                embeddings=[text_embedding],\n            )\n        )\n        return text_embedding\n\n    @dispatcher.span\n    def get_text_embedding_batch(\n        self,\n        texts: List[str],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -> List[SparseEmbedding]:\n        \"\"\"Get a list of text embeddings, with batching.\"\"\"\n        cur_batch: List[str] = []\n        result_embeddings: List[SparseEmbedding] = []\n\n        queue_with_progress = enumerate(\n            get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")\n        )\n\n        model_dict = self.model_dump()\n        for idx, text in queue_with_progress:\n            cur_batch.append(text)\n            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:\n                # flush\n                dispatcher.event(\n                    SparseEmbeddingStartEvent(\n                        model_dict=model_dict,\n                    )\n                )\n\n                embeddings = self._get_text_embeddings(cur_batch)\n                result_embeddings.extend(embeddings)\n\n                dispatcher.event(\n                    SparseEmbeddingEndEvent(\n                        chunks=cur_batch,\n                        embeddings=embeddings,\n                    )\n                )\n                cur_batch = []\n\n        return result_embeddings\n\n    @dispatcher.span\n    async def aget_text_embedding_batch(\n        self, texts: List[str], show_progress: bool = False\n    ) -> List[SparseEmbedding]:\n        \"\"\"Asynchronously get a list of text embeddings, with batching.\"\"\"\n        num_workers = self.num_workers\n\n        model_dict = self.model_dump()\n\n        cur_batch: List[str] = []\n        callback_payloads: List[List[str]] = []\n        result_embeddings: List[SparseEmbedding] = []\n        embeddings_coroutines: List[Coroutine] = []\n        for idx, text in enumerate(texts):\n            cur_batch.append(text)\n            if idx == len(texts) - 1 or len(cur_batch) == self.embed_batch_size:\n                # flush\n                dispatcher.event(\n                    SparseEmbeddingStartEvent(\n                        model_dict=model_dict,\n                    )\n                )\n\n                callback_payloads.append(cur_batch)\n                embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))\n                cur_batch = []\n\n        # flatten the results of asyncio.gather, which is a list of embeddings lists\n        nested_embeddings = []\n\n        if num_workers and num_workers > 1:\n            nested_embeddings = await run_jobs(\n                embeddings_coroutines,\n                show_progress=show_progress,\n                workers=self.num_workers,\n                desc=\"Generating embeddings\",\n            )\n        else:\n            if show_progress:\n                try:\n                    from tqdm.asyncio import tqdm_asyncio\n\n                    nested_embeddings = await tqdm_asyncio.gather(\n                        *embeddings_coroutines,\n                        total=len(embeddings_coroutines),\n                        desc=\"Generating embeddings\",\n                    )\n                except ImportError:\n                    nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n            else:\n                nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n\n        result_embeddings = [\n            embedding for embeddings in nested_embeddings for embedding in embeddings\n        ]\n\n        for text_batch, embeddings in zip(callback_payloads, nested_embeddings):\n            dispatcher.event(\n                SparseEmbeddingEndEvent(\n                    chunks=text_batch,\n                    embeddings=embeddings,\n                )\n            )\n\n        return result_embeddings\n\n    def similarity(\n        self,\n        embedding1: SparseEmbedding,\n        embedding2: SparseEmbedding,\n    ) -> float:\n        \"\"\"Get sparse embedding similarity.\"\"\"\n        return sparse_similarity(embedding1, embedding2)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/response/schema.py",
    "filename": "schema.py",
    "relpath": "base/response/schema.py",
    "start_line": 1,
    "end_line": 236,
    "length": 236,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__str__",
      "get_formatted_sources",
      "__str__",
      "__getattr__",
      "__post_init_post_parse__",
      "get_formatted_sources",
      "get_response",
      "__str__",
      "get_response",
      "print_response_stream",
      "get_formatted_sources",
      "__post_init__",
      "__str__",
      "_async_str",
      "_yield_response",
      "async_response_gen",
      "get_response",
      "print_response_stream",
      "get_formatted_sources"
    ],
    "chunk_class_names": [
      "class",
      "class",
      "is",
      "will",
      "class",
      "class"
    ],
    "document_function_names": [
      "__str__",
      "get_formatted_sources",
      "__str__",
      "__getattr__",
      "__post_init_post_parse__",
      "get_formatted_sources",
      "get_response",
      "__str__",
      "get_response",
      "print_response_stream",
      "get_formatted_sources",
      "__post_init__",
      "__str__",
      "_async_str",
      "_yield_response",
      "async_response_gen",
      "get_response",
      "print_response_stream",
      "get_formatted_sources"
    ],
    "document_class_names": [
      "class",
      "class",
      "is",
      "will",
      "class",
      "class"
    ],
    "content": "\"\"\"Response schema.\"\"\"\n\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom llama_index.core.async_utils import asyncio_run\nfrom llama_index.core.bridge.pydantic import BaseModel\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core.types import TokenGen, TokenAsyncGen\nfrom llama_index.core.utils import truncate_text\n\n\n@dataclass\nclass Response:\n    \"\"\"Response object.\n\n    Returned if streaming=False.\n\n    Attributes:\n        response: The response text.\n\n    \"\"\"\n\n    response: Optional[str]\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None\n\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        return self.response or \"None\"\n\n    def get_formatted_sources(self, length: int = 100) -> str:\n        \"\"\"Get formatted sources text.\"\"\"\n        texts = []\n        for source_node in self.source_nodes:\n            fmt_text_chunk = truncate_text(source_node.node.get_content(), length)\n            doc_id = source_node.node.node_id or \"None\"\n            source_text = f\"> Source (Doc id: {doc_id}): {fmt_text_chunk}\"\n            texts.append(source_text)\n        return \"\\n\\n\".join(texts)\n\n\n@dataclass\nclass PydanticResponse:\n    \"\"\"PydanticResponse object.\n\n    Returned if streaming=False.\n\n    Attributes:\n        response: The response text.\n\n    \"\"\"\n\n    response: Optional[BaseModel]\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None\n\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        return self.response.model_dump_json() if self.response else \"None\"\n\n    def __getattr__(self, name: str) -> Any:\n        \"\"\"Get attribute, but prioritize the pydantic  response object.\"\"\"\n        if self.response is not None and name in self.response.model_dump():\n            return getattr(self.response, name)\n        else:\n            return None\n\n    def __post_init_post_parse__(self) -> None:\n        \"\"\"This method is required.\n\n        According to the Pydantic docs, if a stdlib dataclass (which this class\n        is one) gets mixed with a BaseModel (in the sense that this gets used as a\n        Field in another BaseModel), then this stdlib dataclass will automatically\n        get converted to a pydantic.v1.dataclass.\n\n        However, it appears that in that automatic conversion, this method\n        is left as NoneType, which raises an error. To safeguard against that,\n        we are expilcitly defining this method as something that can be called.\n\n        Sources:\n            - https://docs.pydantic.dev/1.10/usage/dataclasses/#use-of-stdlib-dataclasses-with-basemodel\n            - https://docs.pydantic.dev/1.10/usage/dataclasses/#initialize-hooks\n        \"\"\"\n        return\n\n    def get_formatted_sources(self, length: int = 100) -> str:\n        \"\"\"Get formatted sources text.\"\"\"\n        texts = []\n        for source_node in self.source_nodes:\n            fmt_text_chunk = truncate_text(source_node.node.get_content(), length)\n            doc_id = source_node.node.node_id or \"None\"\n            source_text = f\"> Source (Doc id: {doc_id}): {fmt_text_chunk}\"\n            texts.append(source_text)\n        return \"\\n\\n\".join(texts)\n\n    def get_response(self) -> Response:\n        \"\"\"Get a standard response object.\"\"\"\n        response_txt = self.response.model_dump_json() if self.response else \"None\"\n        return Response(response_txt, self.source_nodes, self.metadata)\n\n\n@dataclass\nclass StreamingResponse:\n    \"\"\"StreamingResponse object.\n\n    Returned if streaming=True.\n\n    Attributes:\n        response_gen: The response generator.\n\n    \"\"\"\n\n    response_gen: TokenGen\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None\n    response_txt: Optional[str] = None\n\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        if self.response_txt is None and self.response_gen is not None:\n            response_txt = \"\"\n            for text in self.response_gen:\n                response_txt += text\n            self.response_txt = response_txt\n        return self.response_txt or \"None\"\n\n    def get_response(self) -> Response:\n        \"\"\"Get a standard response object.\"\"\"\n        if self.response_txt is None and self.response_gen is not None:\n            response_txt = \"\"\n            for text in self.response_gen:\n                response_txt += text\n            self.response_txt = response_txt\n        return Response(self.response_txt, self.source_nodes, self.metadata)\n\n    def print_response_stream(self) -> None:\n        \"\"\"Print the response stream.\"\"\"\n        if self.response_txt is None and self.response_gen is not None:\n            response_txt = \"\"\n            for text in self.response_gen:\n                print(text, end=\"\", flush=True)\n                response_txt += text\n            self.response_txt = response_txt\n        else:\n            print(self.response_txt)\n\n    def get_formatted_sources(self, length: int = 100, trim_text: int = True) -> str:\n        \"\"\"Get formatted sources text.\"\"\"\n        texts = []\n        for source_node in self.source_nodes:\n            fmt_text_chunk = source_node.node.get_content()\n            if trim_text:\n                fmt_text_chunk = truncate_text(fmt_text_chunk, length)\n            node_id = source_node.node.node_id or \"None\"\n            source_text = f\"> Source (Node id: {node_id}): {fmt_text_chunk}\"\n            texts.append(source_text)\n        return \"\\n\\n\".join(texts)\n\n\n@dataclass\nclass AsyncStreamingResponse:\n    \"\"\"AsyncStreamingResponse object.\n\n    Returned if streaming=True while using async.\n\n    Attributes:\n        _async_response_gen: The response async generator.\n\n    \"\"\"\n\n    response_gen: TokenAsyncGen\n    source_nodes: List[NodeWithScore] = field(default_factory=list)\n    metadata: Optional[Dict[str, Any]] = None\n    response_txt: Optional[str] = None\n\n    def __post_init__(self) -> None:\n        self._lock = asyncio.Lock()\n\n    def __str__(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        return asyncio_run(self._async_str())\n\n    async def _async_str(self) -> str:\n        \"\"\"Convert to string representation.\"\"\"\n        async for _ in self._yield_response():\n            ...\n        return self.response_txt or \"None\"\n\n    async def _yield_response(self) -> TokenAsyncGen:\n        \"\"\"Yield the string response.\"\"\"\n        async with self._lock:\n            if self.response_txt is None and self.response_gen is not None:\n                self.response_txt = \"\"\n                async for text in self.response_gen:\n                    self.response_txt += text\n                    yield text\n            else:\n                yield self.response_txt\n\n    async def async_response_gen(self) -> TokenAsyncGen:\n        \"\"\"Yield the string response.\"\"\"\n        async for text in self._yield_response():\n            yield text\n\n    async def get_response(self) -> Response:\n        \"\"\"Get a standard response object.\"\"\"\n        async for _ in self._yield_response():\n            ...\n        return Response(self.response_txt, self.source_nodes, self.metadata)\n\n    async def print_response_stream(self) -> None:\n        \"\"\"Print the response stream.\"\"\"\n        streaming = True\n        async for text in self._yield_response():\n            print(text, end=\"\", flush=True)\n        # do an empty print to print on the next line again next time\n        print()\n\n    def get_formatted_sources(self, length: int = 100, trim_text: int = True) -> str:\n        \"\"\"Get formatted sources text.\"\"\"\n        texts = []\n        for source_node in self.source_nodes:\n            fmt_text_chunk = source_node.node.get_content()\n            if trim_text:\n                fmt_text_chunk = truncate_text(fmt_text_chunk, length)\n            node_id = source_node.node.node_id or \"None\"\n            source_text = f\"> Source (Node id: {node_id}): {fmt_text_chunk}\"\n            texts.append(source_text)\n        return \"\\n\\n\".join(texts)\n\n\nRESPONSE_TYPE = Union[\n    Response, StreamingResponse, AsyncStreamingResponse, PydanticResponse\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/llms/types.py",
    "filename": "types.py",
    "relpath": "base/llms/types.py",
    "start_line": 1,
    "end_line": 382,
    "length": 382,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "urlstr_to_anyurl",
      "image_to_base64",
      "_guess_mimetype",
      "resolve_image",
      "urlstr_to_anyurl",
      "audio_to_base64",
      "_guess_format",
      "resolve_audio",
      "__init__",
      "legacy_additional_kwargs_image",
      "content",
      "content",
      "__str__",
      "from_str",
      "_recursive_serialization",
      "serialize_additional_kwargs",
      "__str__",
      "__str__"
    ],
    "chunk_class_names": [
      "MessageRole",
      "TextBlock",
      "ImageBlock",
      "AudioBlock",
      "ChatMessage",
      "LogProb",
      "ChatResponse",
      "CompletionResponse",
      "LLMMetadata"
    ],
    "document_function_names": [
      "urlstr_to_anyurl",
      "image_to_base64",
      "_guess_mimetype",
      "resolve_image",
      "urlstr_to_anyurl",
      "audio_to_base64",
      "_guess_format",
      "resolve_audio",
      "__init__",
      "legacy_additional_kwargs_image",
      "content",
      "content",
      "__str__",
      "from_str",
      "_recursive_serialization",
      "serialize_additional_kwargs",
      "__str__",
      "__str__"
    ],
    "document_class_names": [
      "MessageRole",
      "TextBlock",
      "ImageBlock",
      "AudioBlock",
      "ChatMessage",
      "LogProb",
      "ChatResponse",
      "CompletionResponse",
      "LLMMetadata"
    ],
    "content": "from __future__ import annotations\n\nimport base64\nimport filetype\nfrom binascii import Error as BinasciiError\nfrom enum import Enum\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import (\n    Annotated,\n    Any,\n    AsyncGenerator,\n    Generator,\n    List,\n    Literal,\n    Optional,\n    Union,\n    cast,\n)\n\nimport filetype\nfrom typing_extensions import Self\n\nfrom llama_index.core.bridge.pydantic import (\n    AnyUrl,\n    BaseModel,\n    ConfigDict,\n    Field,\n    FilePath,\n    field_serializer,\n    field_validator,\n    model_validator,\n)\nfrom llama_index.core.constants import DEFAULT_CONTEXT_WINDOW, DEFAULT_NUM_OUTPUTS\nfrom llama_index.core.schema import ImageDocument\nfrom llama_index.core.utils import resolve_binary\n\n\nclass MessageRole(str, Enum):\n    \"\"\"Message role.\"\"\"\n\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n    FUNCTION = \"function\"\n    TOOL = \"tool\"\n    CHATBOT = \"chatbot\"\n    MODEL = \"model\"\n\n\nclass TextBlock(BaseModel):\n    block_type: Literal[\"text\"] = \"text\"\n    text: str\n\n\nclass ImageBlock(BaseModel):\n    block_type: Literal[\"image\"] = \"image\"\n    image: bytes | None = None\n    path: FilePath | None = None\n    url: AnyUrl | str | None = None\n    image_mimetype: str | None = None\n    detail: str | None = None\n\n    @field_validator(\"url\", mode=\"after\")\n    @classmethod\n    def urlstr_to_anyurl(cls, url: str | AnyUrl | None) -> AnyUrl | None:\n        \"\"\"Store the url as Anyurl.\"\"\"\n        if isinstance(url, AnyUrl):\n            return url\n        if url is None:\n            return None\n\n        return AnyUrl(url=url)\n\n    @model_validator(mode=\"after\")\n    def image_to_base64(self) -> Self:\n        \"\"\"Store the image as base64 and guess the mimetype when possible.\n\n        In case the model was built passing image data but without a mimetype,\n        we try to guess it using the filetype library. To avoid resource-intense\n        operations, we won't load the path or the URL to guess the mimetype.\n        \"\"\"\n        if not self.image:\n            if not self.image_mimetype:\n                path = self.path or self.url\n                if path:\n                    suffix = Path(str(path)).suffix.replace(\".\", \"\") or None\n                    mimetype = filetype.get_type(ext=suffix)\n                    if mimetype and str(mimetype.mime).startswith(\"image/\"):\n                        self.image_mimetype = str(mimetype.mime)\n\n            return self\n\n        try:\n            # Check if self.image is already base64 encoded.\n            # b64decode() can succeed on random binary data, so we\n            # pass verify=True to make sure it's not a false positive\n            decoded_img = base64.b64decode(self.image, validate=True)\n        except BinasciiError:\n            decoded_img = self.image\n            self.image = base64.b64encode(self.image)\n\n        self._guess_mimetype(decoded_img)\n        return self\n\n    def _guess_mimetype(self, img_data: bytes) -> None:\n        if not self.image_mimetype:\n            guess = filetype.guess(img_data)\n            self.image_mimetype = guess.mime if guess else None\n\n    def resolve_image(self, as_base64: bool = False) -> BytesIO:\n        \"\"\"Resolve an image such that PIL can read it.\n\n        Args:\n            as_base64 (bool): whether the resolved image should be returned as base64-encoded bytes\n        \"\"\"\n        return resolve_binary(\n            raw_bytes=self.image,\n            path=self.path,\n            url=str(self.url) if self.url else None,\n            as_base64=as_base64,\n        )\n\n\nclass AudioBlock(BaseModel):\n    block_type: Literal[\"audio\"] = \"audio\"\n    audio: bytes | None = None\n    path: FilePath | None = None\n    url: AnyUrl | str | None = None\n    format: str | None = None\n\n    @field_validator(\"url\", mode=\"after\")\n    @classmethod\n    def urlstr_to_anyurl(cls, url: str | AnyUrl) -> AnyUrl:\n        \"\"\"Store the url as Anyurl.\"\"\"\n        if isinstance(url, AnyUrl):\n            return url\n        return AnyUrl(url=url)\n\n    @model_validator(mode=\"after\")\n    def audio_to_base64(self) -> Self:\n        \"\"\"Store the audio as base64 and guess the mimetype when possible.\n\n        In case the model was built passing audio data but without a mimetype,\n        we try to guess it using the filetype library. To avoid resource-intense\n        operations, we won't load the path or the URL to guess the mimetype.\n        \"\"\"\n        if not self.audio:\n            return self\n\n        try:\n            # Check if audio is already base64 encoded\n            decoded_audio = base64.b64decode(self.audio)\n        except Exception:\n            decoded_audio = self.audio\n            # Not base64 - encode it\n            self.audio = base64.b64encode(self.audio)\n\n        self._guess_format(decoded_audio)\n\n        return self\n\n    def _guess_format(self, audio_data: bytes) -> None:\n        if not self.format:\n            guess = filetype.guess(audio_data)\n            self.format = guess.extension if guess else None\n\n    def resolve_audio(self, as_base64: bool = False) -> BytesIO:\n        \"\"\"Resolve an audio such that PIL can read it.\n\n        Args:\n            as_base64 (bool): whether the resolved audio should be returned as base64-encoded bytes\n        \"\"\"\n        return resolve_binary(\n            raw_bytes=self.audio,\n            path=self.path,\n            url=str(self.url) if self.url else None,\n            as_base64=as_base64,\n        )\n\n\nContentBlock = Annotated[\n    Union[TextBlock, ImageBlock, AudioBlock], Field(discriminator=\"block_type\")\n]\n\n\nclass ChatMessage(BaseModel):\n    \"\"\"Chat message.\"\"\"\n\n    role: MessageRole = MessageRole.USER\n    additional_kwargs: dict[str, Any] = Field(default_factory=dict)\n    blocks: list[ContentBlock] = Field(default_factory=list)\n\n    def __init__(self, /, content: Any | None = None, **data: Any) -> None:\n        \"\"\"Keeps backward compatibility with the old `content` field.\n\n        If content was passed and contained text, store a single TextBlock.\n        If content was passed and it was a list, assume it's a list of content blocks and store it.\n        \"\"\"\n        if content is not None:\n            if isinstance(content, str):\n                data[\"blocks\"] = [TextBlock(text=content)]\n            elif isinstance(content, list):\n                data[\"blocks\"] = content\n\n        super().__init__(**data)\n\n    @model_validator(mode=\"after\")\n    def legacy_additional_kwargs_image(self) -> Self:\n        \"\"\"Provided for backward compatibility.\n\n        If `additional_kwargs` contains an `images` key, assume the value is a list\n        of ImageDocument and convert them into image blocks.\n        \"\"\"\n        if documents := self.additional_kwargs.get(\"images\"):\n            documents = cast(list[ImageDocument], documents)\n            for doc in documents:\n                img_base64_bytes = doc.resolve_image(as_base64=True).read()\n                self.blocks.append(ImageBlock(image=img_base64_bytes))\n        return self\n\n    @property\n    def content(self) -> str | None:\n        \"\"\"Keeps backward compatibility with the old `content` field.\n\n        Returns:\n            The cumulative content of the TextBlock blocks, None if there are none.\n        \"\"\"\n        content = \"\"\n        for block in self.blocks:\n            if isinstance(block, TextBlock):\n                content += block.text\n\n        return content or None\n\n    @content.setter\n    def content(self, content: str) -> None:\n        \"\"\"Keeps backward compatibility with the old `content` field.\n\n        Raises:\n            ValueError: if blocks contains more than a block, or a block that's not TextBlock.\n        \"\"\"\n        if not self.blocks:\n            self.blocks = [TextBlock(text=content)]\n        elif len(self.blocks) == 1 and isinstance(self.blocks[0], TextBlock):\n            self.blocks = [TextBlock(text=content)]\n        else:\n            raise ValueError(\n                \"ChatMessage contains multiple blocks, use 'ChatMessage.blocks' instead.\"\n            )\n\n    def __str__(self) -> str:\n        return f\"{self.role.value}: {self.content}\"\n\n    @classmethod\n    def from_str(\n        cls,\n        content: str,\n        role: Union[MessageRole, str] = MessageRole.USER,\n        **kwargs: Any,\n    ) -> Self:\n        if isinstance(role, str):\n            role = MessageRole(role)\n        return cls(role=role, blocks=[TextBlock(text=content)], **kwargs)\n\n    def _recursive_serialization(self, value: Any) -> Any:\n        if isinstance(value, BaseModel):\n            value.model_rebuild()  # ensures all fields are initialized and serializable\n            return value.model_dump()  # type: ignore\n        if isinstance(value, dict):\n            return {\n                key: self._recursive_serialization(value)\n                for key, value in value.items()\n            }\n        if isinstance(value, list):\n            return [self._recursive_serialization(item) for item in value]\n        return value\n\n    @field_serializer(\"additional_kwargs\", check_fields=False)\n    def serialize_additional_kwargs(self, value: Any, _info: Any) -> Any:\n        return self._recursive_serialization(value)\n\n\nclass LogProb(BaseModel):\n    \"\"\"LogProb of a token.\"\"\"\n\n    token: str = Field(default_factory=str)\n    logprob: float = Field(default_factory=float)\n    bytes: List[int] = Field(default_factory=list)\n\n\n# ===== Generic Model Output - Chat =====\nclass ChatResponse(BaseModel):\n    \"\"\"Chat response.\"\"\"\n\n    message: ChatMessage\n    raw: Optional[Any] = None\n    delta: Optional[str] = None\n    logprobs: Optional[List[List[LogProb]]] = None\n    additional_kwargs: dict = Field(default_factory=dict)\n\n    def __str__(self) -> str:\n        return str(self.message)\n\n\nChatResponseGen = Generator[ChatResponse, None, None]\nChatResponseAsyncGen = AsyncGenerator[ChatResponse, None]\n\n\n# ===== Generic Model Output - Completion =====\nclass CompletionResponse(BaseModel):\n    \"\"\"\n    Completion response.\n\n    Fields:\n        text: Text content of the response if not streaming, or if streaming,\n            the current extent of streamed text.\n        additional_kwargs: Additional information on the response(i.e. token\n            counts, function calling information).\n        raw: Optional raw JSON that was parsed to populate text, if relevant.\n        delta: New text that just streamed in (only relevant when streaming).\n    \"\"\"\n\n    text: str\n    additional_kwargs: dict = Field(default_factory=dict)\n    raw: Optional[Any] = None\n    logprobs: Optional[List[List[LogProb]]] = None\n    delta: Optional[str] = None\n\n    def __str__(self) -> str:\n        return self.text\n\n\nCompletionResponseGen = Generator[CompletionResponse, None, None]\nCompletionResponseAsyncGen = AsyncGenerator[CompletionResponse, None]\n\n\nclass LLMMetadata(BaseModel):\n    model_config = ConfigDict(\n        protected_namespaces=(\"pydantic_model_\",), arbitrary_types_allowed=True\n    )\n    context_window: int = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=(\n            \"Total number of tokens the model can be input and output for one response.\"\n        ),\n    )\n    num_output: int = Field(\n        default=DEFAULT_NUM_OUTPUTS,\n        description=\"Number of tokens the model can output when generating a response.\",\n    )\n    is_chat_model: bool = Field(\n        default=False,\n        description=(\n            \"Set True if the model exposes a chat interface (i.e. can be passed a\"\n            \" sequence of messages, rather than text), like OpenAI's\"\n            \" /v1/chat/completions endpoint.\"\n        ),\n    )\n    is_function_calling_model: bool = Field(\n        default=False,\n        # SEE: https://openai.com/blog/function-calling-and-other-api-updates\n        description=(\n            \"Set True if the model supports function calling messages, similar to\"\n            \" OpenAI's function calling API. For example, converting 'Email Anya to\"\n            \" see if she wants to get coffee next Friday' to a function call like\"\n            \" `send_email(to: string, body: string)`.\"\n        ),\n    )\n    model_name: str = Field(\n        default=\"unknown\",\n        description=(\n            \"The model's name used for logging, testing, and sanity checking. For some\"\n            \" models this can be automatically discerned. For other models, like\"\n            \" locally loaded models, this must be manually specified.\"\n        ),\n    )\n    system_role: MessageRole = Field(\n        default=MessageRole.SYSTEM,\n        description=\"The role this specific LLM provider\"\n        \"expects for system prompt. E.g. 'SYSTEM' for OpenAI, 'CHATBOT' for Cohere\",\n    )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/llms/base.py",
    "filename": "base.py",
    "relpath": "base/llms/base.py",
    "start_line": 1,
    "end_line": 276,
    "length": 276,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "check_callback_manager",
      "metadata",
      "convert_chat_messages",
      "chat",
      "complete",
      "stream_chat",
      "stream_complete",
      "achat",
      "acomplete",
      "astream_chat",
      "astream_complete"
    ],
    "chunk_class_names": [
      "BaseLLM"
    ],
    "document_function_names": [
      "check_callback_manager",
      "metadata",
      "convert_chat_messages",
      "chat",
      "complete",
      "stream_chat",
      "stream_complete",
      "achat",
      "acomplete",
      "astream_chat",
      "astream_complete"
    ],
    "document_class_names": [
      "BaseLLM"
    ],
    "content": "from abc import abstractmethod\nfrom typing import (\n    Any,\n    List,\n    Sequence,\n)\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseAsyncGen,\n    CompletionResponseGen,\n    LLMMetadata,\n    TextBlock,\n)\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n)\nfrom llama_index.core.bridge.pydantic import Field, model_validator, ConfigDict\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.schema import BaseComponent\n\n\nclass BaseLLM(ChainableMixin, BaseComponent, DispatcherSpanMixin):\n    \"\"\"BaseLLM interface.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]), exclude=True\n    )\n\n    @model_validator(mode=\"after\")\n    def check_callback_manager(self) -> \"BaseLLM\":\n        if self.callback_manager is None:\n            self.callback_manager = CallbackManager([])\n        return self\n\n    @property\n    @abstractmethod\n    def metadata(self) -> LLMMetadata:\n        \"\"\"LLM metadata.\n\n        Returns:\n            LLMMetadata: LLM metadata containing various information about the LLM.\n        \"\"\"\n\n    def convert_chat_messages(self, messages: Sequence[ChatMessage]) -> List[Any]:\n        \"\"\"Convert chat messages to an LLM specific message format.\"\"\"\n        converted_messages = []\n        for message in messages:\n            if isinstance(message.content, str):\n                converted_messages.append(message)\n            elif isinstance(message.content, List):\n                content_string = \"\"\n                for block in message.content:\n                    if isinstance(block, TextBlock):\n                        content_string += block.text\n                    else:\n                        raise ValueError(\"LLM only supports text inputs\")\n                message.content = content_string\n                converted_messages.append(message)\n            else:\n                raise ValueError(f\"Invalid message content: {message.content!s}\")\n\n        return converted_messages\n\n    @abstractmethod\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        \"\"\"Chat endpoint for LLM.\n\n        Args:\n            messages (Sequence[ChatMessage]):\n                Sequence of chat messages.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Returns:\n            ChatResponse: Chat response from the LLM.\n\n        Examples:\n            ```python\n            from llama_index.core.llms import ChatMessage\n\n            response = llm.chat([ChatMessage(role=\"user\", content=\"Hello\")])\n            print(response.content)\n            ```\n        \"\"\"\n\n    @abstractmethod\n    def complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n        \"\"\"Completion endpoint for LLM.\n\n        If the LLM is a chat model, the prompt is transformed into a single `user` message.\n\n        Args:\n            prompt (str):\n                Prompt to send to the LLM.\n            formatted (bool, optional):\n                Whether the prompt is already formatted for the LLM, by default False.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Returns:\n            CompletionResponse: Completion response from the LLM.\n\n        Examples:\n            ```python\n            response = llm.complete(\"your prompt\")\n            print(response.text)\n            ```\n        \"\"\"\n\n    @abstractmethod\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        \"\"\"Streaming chat endpoint for LLM.\n\n        Args:\n            messages (Sequence[ChatMessage]):\n                Sequence of chat messages.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Yields:\n            ChatResponse:\n                A generator of ChatResponse objects, each containing a new token of the response.\n\n        Examples:\n            ```python\n            from llama_index.core.llms import ChatMessage\n\n            gen = llm.stream_chat([ChatMessage(role=\"user\", content=\"Hello\")])\n            for response in gen:\n                print(response.delta, end=\"\", flush=True)\n            ```\n        \"\"\"\n\n    @abstractmethod\n    def stream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseGen:\n        \"\"\"Streaming completion endpoint for LLM.\n\n        If the LLM is a chat model, the prompt is transformed into a single `user` message.\n\n        Args:\n            prompt (str):\n                Prompt to send to the LLM.\n            formatted (bool, optional):\n                Whether the prompt is already formatted for the LLM, by default False.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Yields:\n            CompletionResponse:\n                A generator of CompletionResponse objects, each containing a new token of the response.\n\n        Examples:\n            ```python\n            gen = llm.stream_complete(\"your prompt\")\n            for response in gen:\n                print(response.text, end=\"\", flush=True)\n            ```\n        \"\"\"\n\n    # ===== Async Endpoints =====\n    @abstractmethod\n    async def achat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponse:\n        \"\"\"Async chat endpoint for LLM.\n\n        Args:\n            messages (Sequence[ChatMessage]):\n                Sequence of chat messages.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Returns:\n            ChatResponse: Chat response from the LLM.\n\n        Examples:\n            ```python\n            from llama_index.core.llms import ChatMessage\n\n            response = await llm.achat([ChatMessage(role=\"user\", content=\"Hello\")])\n            print(response.content)\n            ```\n        \"\"\"\n\n    @abstractmethod\n    async def acomplete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponse:\n        \"\"\"Async completion endpoint for LLM.\n\n        If the LLM is a chat model, the prompt is transformed into a single `user` message.\n\n        Args:\n            prompt (str):\n                Prompt to send to the LLM.\n            formatted (bool, optional):\n                Whether the prompt is already formatted for the LLM, by default False.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Returns:\n            CompletionResponse: Completion response from the LLM.\n\n        Examples:\n            ```python\n            response = await llm.acomplete(\"your prompt\")\n            print(response.text)\n            ```\n        \"\"\"\n\n    @abstractmethod\n    async def astream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseAsyncGen:\n        \"\"\"Async streaming chat endpoint for LLM.\n\n        Args:\n            messages (Sequence[ChatMessage]):\n                Sequence of chat messages.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Yields:\n            ChatResponse:\n                An async generator of ChatResponse objects, each containing a new token of the response.\n\n        Examples:\n            ```python\n            from llama_index.core.llms import ChatMessage\n\n            gen = await llm.astream_chat([ChatMessage(role=\"user\", content=\"Hello\")])\n            async for response in gen:\n                print(response.delta, end=\"\", flush=True)\n            ```\n        \"\"\"\n\n    @abstractmethod\n    async def astream_complete(\n        self, prompt: str, formatted: bool = False, **kwargs: Any\n    ) -> CompletionResponseAsyncGen:\n        \"\"\"Async streaming completion endpoint for LLM.\n\n        If the LLM is a chat model, the prompt is transformed into a single `user` message.\n\n        Args:\n            prompt (str):\n                Prompt to send to the LLM.\n            formatted (bool, optional):\n                Whether the prompt is already formatted for the LLM, by default False.\n            kwargs (Any):\n                Additional keyword arguments to pass to the LLM.\n\n        Yields:\n            CompletionResponse:\n                An async generator of CompletionResponse objects, each containing a new token of the response.\n\n        Examples:\n            ```python\n            gen = await llm.astream_complete(\"your prompt\")\n            async for response in gen:\n                print(response.text, end=\"\", flush=True)\n            ```\n        \"\"\""
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/llms/generic_utils.py",
    "filename": "generic_utils.py",
    "relpath": "base/llms/generic_utils.py",
    "start_line": 1,
    "end_line": 315,
    "length": 315,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "messages_to_history_str",
      "messages_to_prompt",
      "prompt_to_messages",
      "completion_response_to_chat_response",
      "stream_completion_response_to_chat_response",
      "gen",
      "astream_completion_response_to_chat_response",
      "gen",
      "chat_response_to_completion_response",
      "stream_chat_response_to_completion_response",
      "gen",
      "completion_to_chat_decorator",
      "wrapper",
      "stream_completion_to_chat_decorator",
      "wrapper",
      "chat_to_completion_decorator",
      "wrapper",
      "stream_chat_to_completion_decorator",
      "wrapper",
      "acompletion_to_chat_decorator",
      "wrapper",
      "achat_to_completion_decorator",
      "wrapper",
      "astream_completion_to_chat_decorator",
      "wrapper",
      "astream_chat_to_completion_decorator",
      "wrapper",
      "async_stream_completion_response_to_chat_response",
      "gen",
      "astream_chat_response_to_completion_response",
      "gen",
      "get_from_param_or_env"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "messages_to_history_str",
      "messages_to_prompt",
      "prompt_to_messages",
      "completion_response_to_chat_response",
      "stream_completion_response_to_chat_response",
      "gen",
      "astream_completion_response_to_chat_response",
      "gen",
      "chat_response_to_completion_response",
      "stream_chat_response_to_completion_response",
      "gen",
      "completion_to_chat_decorator",
      "wrapper",
      "stream_completion_to_chat_decorator",
      "wrapper",
      "chat_to_completion_decorator",
      "wrapper",
      "stream_chat_to_completion_decorator",
      "wrapper",
      "acompletion_to_chat_decorator",
      "wrapper",
      "achat_to_completion_decorator",
      "wrapper",
      "astream_completion_to_chat_decorator",
      "wrapper",
      "astream_chat_to_completion_decorator",
      "wrapper",
      "async_stream_completion_response_to_chat_response",
      "gen",
      "astream_chat_response_to_completion_response",
      "gen",
      "get_from_param_or_env"
    ],
    "document_class_names": [],
    "content": "import os\nfrom typing import Any, Awaitable, Callable, List, Optional, Sequence\n\nfrom llama_index.core.base.llms.types import (\n    ChatMessage,\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    CompletionResponse,\n    CompletionResponseAsyncGen,\n    CompletionResponseGen,\n    MessageRole,\n)\n\n\ndef messages_to_history_str(messages: Sequence[ChatMessage]) -> str:\n    \"\"\"Convert messages to a history string.\"\"\"\n    string_messages = []\n    for message in messages:\n        role = message.role\n        content = message.content\n        string_message = f\"{role.value}: {content}\"\n\n        additional_kwargs = message.additional_kwargs\n        if additional_kwargs:\n            string_message += f\"\\n{additional_kwargs}\"\n        string_messages.append(string_message)\n    return \"\\n\".join(string_messages)\n\n\ndef messages_to_prompt(messages: Sequence[ChatMessage]) -> str:\n    \"\"\"Convert messages to a prompt string.\"\"\"\n    string_messages = []\n    for message in messages:\n        role = message.role\n        content = message.content\n        string_message = f\"{role.value}: {content}\"\n\n        additional_kwargs = message.additional_kwargs\n        if additional_kwargs:\n            string_message += f\"\\n{additional_kwargs}\"\n        string_messages.append(string_message)\n\n    string_messages.append(f\"{MessageRole.ASSISTANT.value}: \")\n    return \"\\n\".join(string_messages)\n\n\ndef prompt_to_messages(prompt: str) -> List[ChatMessage]:\n    \"\"\"Convert a string prompt to a sequence of messages.\"\"\"\n    return [ChatMessage(role=MessageRole.USER, content=prompt)]\n\n\ndef completion_response_to_chat_response(\n    completion_response: CompletionResponse,\n) -> ChatResponse:\n    \"\"\"Convert a completion response to a chat response.\"\"\"\n    return ChatResponse(\n        message=ChatMessage(\n            role=MessageRole.ASSISTANT,\n            content=completion_response.text,\n            additional_kwargs=completion_response.additional_kwargs,\n        ),\n        raw=completion_response.raw,\n    )\n\n\ndef stream_completion_response_to_chat_response(\n    completion_response_gen: CompletionResponseGen,\n) -> ChatResponseGen:\n    \"\"\"Convert a stream completion response to a stream chat response.\"\"\"\n\n    def gen() -> ChatResponseGen:\n        for response in completion_response_gen:\n            yield ChatResponse(\n                message=ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=response.text,\n                    additional_kwargs=response.additional_kwargs,\n                ),\n                delta=response.delta,\n                raw=response.raw,\n            )\n\n    return gen()\n\n\ndef astream_completion_response_to_chat_response(\n    completion_response_gen: CompletionResponseAsyncGen,\n) -> ChatResponseAsyncGen:\n    \"\"\"Convert an async stream completion to an async stream chat response.\"\"\"\n\n    async def gen() -> ChatResponseAsyncGen:\n        async for response in completion_response_gen:\n            yield ChatResponse(\n                message=ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=response.text,\n                    additional_kwargs=response.additional_kwargs,\n                ),\n                delta=response.delta,\n                raw=response.raw,\n            )\n\n    return gen()\n\n\ndef chat_response_to_completion_response(\n    chat_response: ChatResponse,\n) -> CompletionResponse:\n    \"\"\"Convert a chat response to a completion response.\"\"\"\n    return CompletionResponse(\n        text=chat_response.message.content or \"\",\n        additional_kwargs=chat_response.message.additional_kwargs,\n        raw=chat_response.raw,\n    )\n\n\ndef stream_chat_response_to_completion_response(\n    chat_response_gen: ChatResponseGen,\n) -> CompletionResponseGen:\n    \"\"\"Convert a stream chat response to a completion response.\"\"\"\n\n    def gen() -> CompletionResponseGen:\n        for response in chat_response_gen:\n            yield CompletionResponse(\n                text=response.message.content or \"\",\n                additional_kwargs=response.message.additional_kwargs,\n                delta=response.delta,\n                raw=response.raw,\n            )\n\n    return gen()\n\n\ndef completion_to_chat_decorator(\n    func: Callable[..., CompletionResponse]\n) -> Callable[..., ChatResponse]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n\n    def wrapper(messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = func(prompt, **kwargs)\n        # normalize output\n        return completion_response_to_chat_response(completion_response)\n\n    return wrapper\n\n\ndef stream_completion_to_chat_decorator(\n    func: Callable[..., CompletionResponseGen]\n) -> Callable[..., ChatResponseGen]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n\n    def wrapper(messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponseGen:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = func(prompt, **kwargs)\n        # normalize output\n        return stream_completion_response_to_chat_response(completion_response)\n\n    return wrapper\n\n\ndef chat_to_completion_decorator(\n    func: Callable[..., ChatResponse]\n) -> Callable[..., CompletionResponse]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n\n    def wrapper(prompt: str, **kwargs: Any) -> CompletionResponse:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = func(messages, **kwargs)\n        # normalize output\n        return chat_response_to_completion_response(chat_response)\n\n    return wrapper\n\n\ndef stream_chat_to_completion_decorator(\n    func: Callable[..., ChatResponseGen]\n) -> Callable[..., CompletionResponseGen]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n\n    def wrapper(prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = func(messages, **kwargs)\n        # normalize output\n        return stream_chat_response_to_completion_response(chat_response)\n\n    return wrapper\n\n\n# ===== Async =====\n\n\ndef acompletion_to_chat_decorator(\n    func: Callable[..., Awaitable[CompletionResponse]]\n) -> Callable[..., Awaitable[ChatResponse]]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n\n    async def wrapper(messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = await func(prompt, **kwargs)\n        # normalize output\n        return completion_response_to_chat_response(completion_response)\n\n    return wrapper\n\n\ndef achat_to_completion_decorator(\n    func: Callable[..., Awaitable[ChatResponse]]\n) -> Callable[..., Awaitable[CompletionResponse]]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n\n    async def wrapper(prompt: str, **kwargs: Any) -> CompletionResponse:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = await func(messages, **kwargs)\n        # normalize output\n        return chat_response_to_completion_response(chat_response)\n\n    return wrapper\n\n\ndef astream_completion_to_chat_decorator(\n    func: Callable[..., Awaitable[CompletionResponseAsyncGen]]\n) -> Callable[..., Awaitable[ChatResponseAsyncGen]]:\n    \"\"\"Convert a completion function to a chat function.\"\"\"\n\n    async def wrapper(\n        messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseAsyncGen:\n        # normalize input\n        prompt = messages_to_prompt(messages)\n        completion_response = await func(prompt, **kwargs)\n        # normalize output\n        return astream_completion_response_to_chat_response(completion_response)\n\n    return wrapper\n\n\ndef astream_chat_to_completion_decorator(\n    func: Callable[..., Awaitable[ChatResponseAsyncGen]]\n) -> Callable[..., Awaitable[CompletionResponseAsyncGen]]:\n    \"\"\"Convert a chat function to a completion function.\"\"\"\n\n    async def wrapper(prompt: str, **kwargs: Any) -> CompletionResponseAsyncGen:\n        # normalize input\n        messages = prompt_to_messages(prompt)\n        chat_response = await func(messages, **kwargs)\n        # normalize output\n        return astream_chat_response_to_completion_response(chat_response)\n\n    return wrapper\n\n\ndef async_stream_completion_response_to_chat_response(\n    completion_response_gen: CompletionResponseAsyncGen,\n) -> ChatResponseAsyncGen:\n    \"\"\"Convert a stream completion response to a stream chat response.\"\"\"\n\n    async def gen() -> ChatResponseAsyncGen:\n        async for response in completion_response_gen:\n            yield ChatResponse(\n                message=ChatMessage(\n                    role=MessageRole.ASSISTANT,\n                    content=response.text,\n                    additional_kwargs=response.additional_kwargs,\n                ),\n                delta=response.delta,\n                raw=response.raw,\n            )\n\n    return gen()\n\n\ndef astream_chat_response_to_completion_response(\n    chat_response_gen: ChatResponseAsyncGen,\n) -> CompletionResponseAsyncGen:\n    \"\"\"Convert a stream chat response to a completion response.\"\"\"\n\n    async def gen() -> CompletionResponseAsyncGen:\n        async for response in chat_response_gen:\n            yield CompletionResponse(\n                text=response.message.content or \"\",\n                additional_kwargs=response.message.additional_kwargs,\n                delta=response.delta,\n                raw=response.raw,\n            )\n\n    return gen()\n\n\ndef get_from_param_or_env(\n    key: str,\n    param: Optional[str] = None,\n    env_key: Optional[str] = None,\n    default: Optional[str] = None,\n) -> str:\n    \"\"\"Get a value from a param or an environment variable.\"\"\"\n    if param is not None:\n        return param\n    elif env_key and env_key in os.environ and os.environ[env_key]:\n        return os.environ[env_key]\n    elif default is not None:\n        return default\n    else:\n        raise ValueError(\n            f\"Did not find {key}, please add an environment variable\"\n            f\" `{env_key}` which contains it, or pass\"\n            f\"  `{key}` as a named parameter.\"\n        )"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/query_pipeline/query.py",
    "filename": "query.py",
    "relpath": "base/query_pipeline/query.py",
    "start_line": 1,
    "end_line": 362,
    "length": 362,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "validate_and_convert_stringable",
      "from_keys",
      "validate_keys",
      "__len__",
      "all",
      "from_keys",
      "validate_keys",
      "_as_query_component",
      "as_query_component",
      "partial",
      "set_callback_manager",
      "free_req_input_keys",
      "_validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_inputs",
      "validate_component_outputs",
      "run_component",
      "arun_component",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components",
      "set_callback_manager",
      "_validate_component_inputs",
      "_arun_component",
      "_input_keys",
      "_optional_input_keys",
      "_output_keys",
      "input_keys",
      "output_keys",
      "__init__",
      "__init__",
      "__repr__",
      "__str__"
    ],
    "chunk_class_names": [
      "InputKeys",
      "OutputKeys",
      "ChainableMixin",
      "QueryComponent",
      "of",
      "CustomQueryComponent",
      "Link",
      "ComponentIntermediates"
    ],
    "document_function_names": [
      "validate_and_convert_stringable",
      "from_keys",
      "validate_keys",
      "__len__",
      "all",
      "from_keys",
      "validate_keys",
      "_as_query_component",
      "as_query_component",
      "partial",
      "set_callback_manager",
      "free_req_input_keys",
      "_validate_component_inputs",
      "_validate_component_outputs",
      "validate_component_inputs",
      "validate_component_outputs",
      "run_component",
      "arun_component",
      "_run_component",
      "_arun_component",
      "input_keys",
      "output_keys",
      "sub_query_components",
      "set_callback_manager",
      "_validate_component_inputs",
      "_arun_component",
      "_input_keys",
      "_optional_input_keys",
      "_output_keys",
      "input_keys",
      "output_keys",
      "__init__",
      "__init__",
      "__repr__",
      "__str__"
    ],
    "document_class_names": [
      "InputKeys",
      "OutputKeys",
      "ChainableMixin",
      "QueryComponent",
      "of",
      "CustomQueryComponent",
      "Link",
      "ComponentIntermediates"
    ],
    "content": "\"\"\"Pipeline schema.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Generator,\n    List,\n    Optional,\n    Set,\n    Union,\n    cast,\n    get_args,\n)\n\nfrom llama_index.core.base.llms.types import (\n    ChatResponse,\n    ChatMessage,\n    CompletionResponse,\n)\nfrom llama_index.core.base.response.schema import Response\nfrom llama_index.core.bridge.pydantic import BaseModel, Field, ConfigDict\nfrom llama_index.core.callbacks.base import CallbackManager\nfrom llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n\n## Define common types used throughout these components\nStringableInput = Union[\n    CompletionResponse,\n    ChatResponse,\n    ChatMessage,\n    str,\n    QueryBundle,\n    Response,\n    Generator,\n    NodeWithScore,\n    TextNode,\n]\n\n\ndef validate_and_convert_stringable(input: Any) -> str:\n    # special handling for generator\n    if isinstance(input, Generator):\n        # iterate through each element, make sure is stringable\n        new_input = \"\"\n        for elem in input:\n            if not isinstance(elem, get_args(StringableInput)):\n                raise ValueError(f\"Input {elem} is not stringable.\")\n            elif isinstance(elem, (ChatResponse, CompletionResponse)):\n                new_input += cast(str, elem.delta)\n            else:\n                new_input += str(elem)\n        return new_input\n    elif isinstance(input, List):\n        # iterate through each element, make sure is stringable\n        # do this recursively\n        new_input_list = []\n        for elem in input:\n            new_input_list.append(validate_and_convert_stringable(elem))\n        return str(new_input_list)\n    elif isinstance(input, ChatResponse):\n        return input.message.content or \"\"\n    elif isinstance(input, NodeWithScore) and isinstance(input.node, TextNode):\n        return input.get_content()\n    elif isinstance(input, get_args(StringableInput)):\n        return str(input)\n    else:\n        raise ValueError(f\"Input {input} is not stringable.\")\n\n\nclass InputKeys(BaseModel):\n    \"\"\"Input keys.\"\"\"\n\n    required_keys: Set[str] = Field(default_factory=set)\n    optional_keys: Set[str] = Field(default_factory=set)\n\n    @classmethod\n    def from_keys(\n        cls, required_keys: Set[str], optional_keys: Optional[Set[str]] = None\n    ) -> \"InputKeys\":\n        \"\"\"Create InputKeys from tuple.\"\"\"\n        return cls(required_keys=required_keys, optional_keys=optional_keys or set())\n\n    def validate_keys(self, input_keys: Set[str]) -> None:\n        \"\"\"Validate input keys.\"\"\"\n        # check if required keys are present, and that keys all are in required or optional\n        if not self.required_keys.issubset(input_keys):\n            raise ValueError(\n                f\"Required keys {self.required_keys} are not present in input keys {input_keys}\"\n            )\n        if not input_keys.issubset(self.required_keys.union(self.optional_keys)):\n            raise ValueError(\n                f\"Input keys {input_keys} contain keys not in required or optional keys {self.required_keys.union(self.optional_keys)}\"\n            )\n\n    def __len__(self) -> int:\n        \"\"\"Length of input keys.\"\"\"\n        return len(self.required_keys) + len(self.optional_keys)\n\n    def all(self) -> Set[str]:\n        \"\"\"Get all input keys.\"\"\"\n        return self.required_keys.union(self.optional_keys)\n\n\nclass OutputKeys(BaseModel):\n    \"\"\"Output keys.\"\"\"\n\n    required_keys: Set[str] = Field(default_factory=set)\n\n    @classmethod\n    def from_keys(\n        cls,\n        required_keys: Set[str],\n    ) -> \"OutputKeys\":\n        \"\"\"Create OutputKeys from tuple.\"\"\"\n        return cls(required_keys=required_keys)\n\n    def validate_keys(self, input_keys: Set[str]) -> None:\n        \"\"\"Validate input keys.\"\"\"\n        # validate that input keys exactly match required keys\n        if input_keys != self.required_keys:\n            raise ValueError(\n                f\"Input keys {input_keys} do not match required keys {self.required_keys}\"\n            )\n\n\nclass ChainableMixin(ABC):\n    \"\"\"Chainable mixin.\n\n    A module that can produce a `QueryComponent` from a set of inputs through\n    `as_query_component`.\n\n    If plugged in directly into a `QueryPipeline`, the `ChainableMixin` will be\n    converted into a `QueryComponent` with default parameters.\n\n    \"\"\"\n\n    @abstractmethod\n    def _as_query_component(self, **kwargs: Any) -> \"QueryComponent\":\n        \"\"\"Get query component.\"\"\"\n\n    def as_query_component(\n        self, partial: Optional[Dict[str, Any]] = None, **kwargs: Any\n    ) -> \"QueryComponent\":\n        \"\"\"Get query component.\"\"\"\n        component = self._as_query_component(**kwargs)\n        component.partial(**(partial or {}))\n        return component\n\n\nclass QueryComponent(BaseModel):\n    \"\"\"Query component.\n\n    Represents a component that can be run in a `QueryPipeline`.\n\n    \"\"\"\n\n    partial_dict: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Partial arguments to run_component\"\n    )\n\n    # TODO: make this a subclass of BaseComponent (e.g. use Pydantic)\n\n    def partial(self, **kwargs: Any) -> None:\n        \"\"\"Update with partial arguments.\"\"\"\n        self.partial_dict.update(kwargs)\n\n    @abstractmethod\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: refactor so that callback_manager is always passed in during runtime.\n\n    @property\n    def free_req_input_keys(self) -> Set[str]:\n        \"\"\"Get free input keys.\"\"\"\n        return self.input_keys.required_keys.difference(self.partial_dict.keys())\n\n    @abstractmethod\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n\n    def _validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs during run_component.\"\"\"\n        # override if needed\n        return output\n\n    def validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs.\"\"\"\n        # make sure set of input keys == self.input_keys\n        self.input_keys.validate_keys(set(input.keys()))\n        return self._validate_component_inputs(input)\n\n    def validate_component_outputs(self, output: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component outputs.\"\"\"\n        # make sure set of output keys == self.output_keys\n        self.output_keys.validate_keys(set(output.keys()))\n        return self._validate_component_outputs(output)\n\n    def run_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        kwargs.update(self.partial_dict)\n        kwargs = self.validate_component_inputs(kwargs)\n        component_outputs = self._run_component(**kwargs)\n        return self.validate_component_outputs(component_outputs)\n\n    async def arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        kwargs.update(self.partial_dict)\n        kwargs = self.validate_component_inputs(kwargs)\n        component_outputs = await self._arun_component(**kwargs)\n        return self.validate_component_outputs(component_outputs)\n\n    @abstractmethod\n    def _run_component(self, **kwargs: Any) -> Dict:\n        \"\"\"Run component.\"\"\"\n\n    @abstractmethod\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n\n    @property\n    @abstractmethod\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n\n    @property\n    @abstractmethod\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n\n    @property\n    def sub_query_components(self) -> List[\"QueryComponent\"]:\n        \"\"\"Get sub query components.\n\n        Certain query components may have sub query components, e.g. a\n        query pipeline will have sub query components, and so will\n        an IfElseComponent.\n\n        \"\"\"\n        return []\n\n\nclass CustomQueryComponent(QueryComponent):\n    \"\"\"Custom query component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    callback_manager: CallbackManager = Field(\n        default_factory=CallbackManager, description=\"Callback manager\"\n    )\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        self.callback_manager = callback_manager\n\n    def _validate_component_inputs(self, input: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        # NOTE: user can override this method to validate inputs\n        # but we do this by default for convenience\n        return input\n\n    async def _arun_component(self, **kwargs: Any) -> Any:\n        \"\"\"Run component (async).\"\"\"\n        raise NotImplementedError(\"This component does not support async run.\")\n\n    @property\n    def _input_keys(self) -> Set[str]:\n        \"\"\"Input keys dict.\"\"\"\n        raise NotImplementedError(\"Not implemented yet. Please override this method.\")\n\n    @property\n    def _optional_input_keys(self) -> Set[str]:\n        \"\"\"Optional input keys dict.\"\"\"\n        return set()\n\n    @property\n    def _output_keys(self) -> Set[str]:\n        \"\"\"Output keys dict.\"\"\"\n        raise NotImplementedError(\"Not implemented yet. Please override this method.\")\n\n    @property\n    def input_keys(self) -> InputKeys:\n        \"\"\"Input keys.\"\"\"\n        # NOTE: user can override this too, but we have them implement an\n        # abstract method to make sure they do it\n\n        return InputKeys.from_keys(\n            required_keys=self._input_keys, optional_keys=self._optional_input_keys\n        )\n\n    @property\n    def output_keys(self) -> OutputKeys:\n        \"\"\"Output keys.\"\"\"\n        # NOTE: user can override this too, but we have them implement an\n        # abstract method to make sure they do it\n        return OutputKeys.from_keys(self._output_keys)\n\n\nclass Link(BaseModel):\n    \"\"\"Link between two components.\"\"\"\n\n    src: str = Field(..., description=\"Source component name\")\n    dest: str = Field(..., description=\"Destination component name\")\n    src_key: Optional[str] = Field(\n        default=None, description=\"Source component output key\"\n    )\n    dest_key: Optional[str] = Field(\n        default=None, description=\"Destination component input key\"\n    )\n\n    condition_fn: Optional[Callable] = Field(\n        default=None, description=\"Condition to determine if link should be followed\"\n    )\n    input_fn: Optional[Callable] = Field(\n        default=None, description=\"Input to destination component\"\n    )\n\n    def __init__(\n        self,\n        src: str,\n        dest: str,\n        src_key: Optional[str] = None,\n        dest_key: Optional[str] = None,\n        condition_fn: Optional[Callable] = None,\n        input_fn: Optional[Callable] = None,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        # NOTE: This is to enable positional args.\n        super().__init__(\n            src=src,\n            dest=dest,\n            src_key=src_key,\n            dest_key=dest_key,\n            condition_fn=condition_fn,\n            input_fn=input_fn,\n        )\n\n\nclass ComponentIntermediates:\n    \"\"\"Component intermediate inputs and outputs.\"\"\"\n\n    def __init__(\n        self,\n        inputs: Dict[str, Any],\n        outputs: Dict[str, Any],\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.inputs = inputs\n        self.outputs = outputs\n\n    def __repr__(self) -> str:\n        return (\n            f\"ComponentIntermediates(inputs={self.inputs!s}, \"\n            f\"outputs={self.outputs!s})\"\n        )\n\n    def __str__(self) -> str:\n        return self.__repr__()\n\n\n# accept both QueryComponent and ChainableMixin as inputs to query pipeline\n# ChainableMixin modules will be converted to components via `as_query_component`\nQUERY_COMPONENT_TYPE = Union[QueryComponent, ChainableMixin]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/base/agent/types.py",
    "filename": "types.py",
    "relpath": "base/agent/types.py",
    "start_line": 1,
    "end_line": 252,
    "length": 252,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_query",
      "_aquery",
      "stream_chat",
      "astream_chat",
      "get_next_step",
      "link_step",
      "__str__",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "initialize_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "afinalize_task",
      "set_callback_manager",
      "as_agent"
    ],
    "chunk_class_names": [
      "BaseAgent",
      "TaskStep",
      "TaskStepOutput",
      "Task",
      "BaseAgentWorker"
    ],
    "document_function_names": [
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "_query",
      "_aquery",
      "stream_chat",
      "astream_chat",
      "get_next_step",
      "link_step",
      "__str__",
      "_get_prompts",
      "_get_prompt_modules",
      "_update_prompts",
      "initialize_step",
      "run_step",
      "arun_step",
      "stream_step",
      "astream_step",
      "finalize_task",
      "afinalize_task",
      "set_callback_manager",
      "as_agent"
    ],
    "document_class_names": [
      "BaseAgent",
      "TaskStep",
      "TaskStepOutput",
      "Task",
      "BaseAgentWorker"
    ],
    "content": "\"\"\"Base agent type.\"\"\"\n\nimport uuid\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, TYPE_CHECKING\n\nfrom llama_index.core.base.base_query_engine import BaseQueryEngine\nfrom llama_index.core.base.llms.types import ChatMessage\nfrom llama_index.core.base.response.schema import RESPONSE_TYPE, Response\nfrom llama_index.core.bridge.pydantic import (\n    BaseModel,\n    Field,\n    SerializeAsAny,\n    ConfigDict,\n)\nfrom llama_index.core.callbacks import CallbackManager, trace_method\nfrom llama_index.core.chat_engine.types import (\n    BaseChatEngine,\n    StreamingAgentChatResponse,\n)\nfrom llama_index.core.instrumentation import DispatcherSpanMixin\nfrom llama_index.core.memory.types import BaseMemory\nfrom llama_index.core.prompts.mixin import PromptDictType, PromptMixin, PromptMixinType\nfrom llama_index.core.schema import QueryBundle\n\nif TYPE_CHECKING:\n    from llama_index.core.agent.runner.base import AgentRunner\n\n\nclass BaseAgent(BaseChatEngine, BaseQueryEngine):\n    \"\"\"Base Agent.\"\"\"\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        # TODO: the ReAct agent does not explicitly specify prompts, would need a\n        # refactor to expose those prompts\n        return {}\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    # ===== Query Engine Interface =====\n    @trace_method(\"query\")\n    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        agent_response = self.chat(\n            query_bundle.query_str,\n            chat_history=[],\n        )\n        return Response(\n            response=str(agent_response), source_nodes=agent_response.source_nodes\n        )\n\n    @trace_method(\"query\")\n    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n        agent_response = await self.achat(\n            query_bundle.query_str,\n            chat_history=[],\n        )\n        return Response(\n            response=str(agent_response), source_nodes=agent_response.source_nodes\n        )\n\n    def stream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        raise NotImplementedError(\"stream_chat not implemented\")\n\n    async def astream_chat(\n        self, message: str, chat_history: Optional[List[ChatMessage]] = None\n    ) -> StreamingAgentChatResponse:\n        raise NotImplementedError(\"astream_chat not implemented\")\n\n\nclass TaskStep(BaseModel):\n    \"\"\"Agent task step.\n\n    Represents a single input step within the execution run (\"Task\") of an agent\n    given a user input.\n\n    The output is returned as a `TaskStepOutput`.\n\n    \"\"\"\n\n    task_id: str = Field(..., description=\"Task ID\")\n    step_id: str = Field(..., description=\"Step ID\")\n    input: Optional[str] = Field(default=None, description=\"User input\")\n    # memory: BaseMemory = Field(\n    #     ..., description=\"Conversational Memory\"\n    # )\n    step_state: Dict[str, Any] = Field(\n        default_factory=dict, description=\"Additional state for a given step.\"\n    )\n\n    # NOTE: the state below may change throughout the course of execution\n    # this tracks the relationships to other steps\n    next_steps: Dict[str, \"TaskStep\"] = Field(\n        default_factory=dict, description=\"Next steps to be executed.\"\n    )\n    prev_steps: Dict[str, \"TaskStep\"] = Field(\n        default_factory=dict,\n        description=\"Previous steps that were dependencies for this step.\",\n    )\n    is_ready: bool = Field(\n        default=True, description=\"Is this step ready to be executed?\"\n    )\n\n    def get_next_step(\n        self,\n        step_id: str,\n        input: Optional[str] = None,\n        step_state: Optional[Dict[str, Any]] = None,\n    ) -> \"TaskStep\":\n        \"\"\"Convenience function to get next step.\n\n        Preserve task_id, memory, step_state.\n\n        \"\"\"\n        return TaskStep(\n            task_id=self.task_id,\n            step_id=step_id,\n            input=input,\n            # memory=self.memory,\n            step_state=step_state or self.step_state,\n        )\n\n    def link_step(\n        self,\n        next_step: \"TaskStep\",\n    ) -> None:\n        \"\"\"Link to next step.\n\n        Add link from this step to next, and from next step to current.\n\n        \"\"\"\n        self.next_steps[next_step.step_id] = next_step\n        next_step.prev_steps[self.step_id] = self\n\n\nclass TaskStepOutput(BaseModel):\n    \"\"\"Agent task step output.\"\"\"\n\n    output: Any = Field(..., description=\"Task step output\")\n    task_step: TaskStep = Field(..., description=\"Task step input\")\n    next_steps: List[TaskStep] = Field(..., description=\"Next steps to be executed.\")\n    is_last: bool = Field(default=False, description=\"Is this the last step?\")\n\n    def __str__(self) -> str:\n        \"\"\"String representation.\"\"\"\n        return str(self.output)\n\n\nclass Task(BaseModel):\n    \"\"\"Agent Task.\n\n    Represents a \"run\" of an agent given a user input.\n\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    task_id: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Task ID\"\n    )\n    input: str = Field(..., description=\"User input\")\n\n    # NOTE: this is state that may be modified throughout the course of execution of the task\n    memory: SerializeAsAny[BaseMemory] = Field(\n        ...,\n        description=(\n            \"Conversational Memory. Maintains state before execution of this task.\"\n        ),\n    )\n\n    callback_manager: CallbackManager = Field(\n        default_factory=lambda: CallbackManager([]),\n        exclude=True,\n        description=\"Callback manager for the task.\",\n    )\n\n    extra_state: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=(\n            \"Additional user-specified state for a given task. \"\n            \"Can be modified throughout the execution of a task.\"\n        ),\n    )\n\n\nclass BaseAgentWorker(PromptMixin, DispatcherSpanMixin):\n    \"\"\"Base agent worker.\"\"\"\n\n    def _get_prompts(self) -> PromptDictType:\n        \"\"\"Get prompts.\"\"\"\n        # TODO: the ReAct agent does not explicitly specify prompts, would need a\n        # refactor to expose those prompts\n        return {}\n\n    def _get_prompt_modules(self) -> PromptMixinType:\n        \"\"\"Get prompt modules.\"\"\"\n        return {}\n\n    def _update_prompts(self, prompts: PromptDictType) -> None:\n        \"\"\"Update prompts.\"\"\"\n\n    @abstractmethod\n    def initialize_step(self, task: Task, **kwargs: Any) -> TaskStep:\n        \"\"\"Initialize step from task.\"\"\"\n\n    @abstractmethod\n    def run_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step.\"\"\"\n\n    @abstractmethod\n    async def arun_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async).\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def stream_step(self, step: TaskStep, task: Task, **kwargs: Any) -> TaskStepOutput:\n        \"\"\"Run step (stream).\"\"\"\n        # TODO: figure out if we need a different type for TaskStepOutput\n        raise NotImplementedError\n\n    @abstractmethod\n    async def astream_step(\n        self, step: TaskStep, task: Task, **kwargs: Any\n    ) -> TaskStepOutput:\n        \"\"\"Run step (async stream).\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def finalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n\n    async def afinalize_task(self, task: Task, **kwargs: Any) -> None:\n        \"\"\"Finalize task, after all the steps are completed.\"\"\"\n        self.finalize_task(task, **kwargs)\n\n    def set_callback_manager(self, callback_manager: CallbackManager) -> None:\n        \"\"\"Set callback manager.\"\"\"\n        # TODO: make this abstractmethod (right now will break some agent impls)\n\n    def as_agent(self, **kwargs: Any) -> \"AgentRunner\":\n        \"\"\"Return as an agent runner.\"\"\"\n        from llama_index.core.agent.runner.base import AgentRunner\n\n        return AgentRunner(self, **kwargs)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/output_parsers/selection.py",
    "filename": "selection.py",
    "relpath": "output_parsers/selection.py",
    "start_line": 1,
    "end_line": 104,
    "length": 104,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_escape_curly_braces",
      "_filter_dict",
      "_format_output",
      "parse",
      "format"
    ],
    "chunk_class_names": [
      "from",
      "class",
      "SelectionOutputParser"
    ],
    "document_function_names": [
      "_escape_curly_braces",
      "_filter_dict",
      "_format_output",
      "parse",
      "format"
    ],
    "document_class_names": [
      "from",
      "class",
      "SelectionOutputParser"
    ],
    "content": "import json\nfrom dataclasses import dataclass\nfrom typing import Any, List\n\nfrom dataclasses_json import DataClassJsonMixin\nfrom llama_index.core.output_parsers.base import (\n    OutputParserException,\n    StructuredOutput,\n)\nfrom llama_index.core.output_parsers.utils import _marshal_llm_to_json\nfrom llama_index.core.types import BaseOutputParser\n\n\ndef _escape_curly_braces(input_string: str) -> str:\n    # Replace '{' with '{{' and '}' with '}}' to escape curly braces\n    return input_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n\n\nFORMAT_STR = \"\"\"The output should be ONLY JSON formatted as a JSON instance.\n\nHere is an example:\n[\n    {\n        choice: 1,\n        reason: \"<insert reason for choice>\"\n    },\n    ...\n]\n\"\"\"\n\n\n@dataclass\nclass Answer(DataClassJsonMixin):\n    choice: int\n    reason: str\n\n\nclass SelectionOutputParser(BaseOutputParser):\n    REQUIRED_KEYS = frozenset(Answer.__annotations__)\n\n    def _filter_dict(self, json_dict: dict) -> dict:\n        \"\"\"Filter recursively until a dictionary matches all REQUIRED_KEYS.\"\"\"\n        output_dict = json_dict\n        for key, val in json_dict.items():\n            if key in self.REQUIRED_KEYS:\n                continue\n            elif isinstance(val, dict):\n                output_dict = self._filter_dict(val)\n            elif isinstance(val, list):\n                for item in val:\n                    if isinstance(item, dict):\n                        output_dict = self._filter_dict(item)\n\n        return output_dict\n\n    def _format_output(self, output: List[dict]) -> List[dict]:\n        output_json = []\n        for json_dict in output:\n            valid = True\n            for key in self.REQUIRED_KEYS:\n                if key not in json_dict:\n                    valid = False\n                    break\n\n            if not valid:\n                json_dict = self._filter_dict(json_dict)\n\n            output_json.append(json_dict)\n\n        return output_json\n\n    def parse(self, output: str) -> Any:\n        json_string = _marshal_llm_to_json(output)\n        try:\n            json_obj = json.loads(json_string)\n        except json.JSONDecodeError as e_json:\n            try:\n                import yaml\n\n                # NOTE: parsing again with pyyaml\n                #       pyyaml is less strict, and allows for trailing commas\n                #       right now we rely on this since guidance program generates\n                #       trailing commas\n                json_obj = yaml.safe_load(json_string)\n            except yaml.YAMLError as e_yaml:\n                raise OutputParserException(\n                    f\"Got invalid JSON object. Error: {e_json} {e_yaml}. \"\n                    f\"Got JSON string: {json_string}\"\n                )\n            except NameError as exc:\n                raise ImportError(\"Please pip install PyYAML.\") from exc\n\n        if isinstance(json_obj, dict):\n            json_obj = [json_obj]\n\n        if not json_obj:\n            raise ValueError(f\"Failed to convert output to JSON: {output!r}\")\n\n        json_output = self._format_output(json_obj)\n        answers = [Answer.from_dict(json_dict) for json_dict in json_output]\n        return StructuredOutput(raw_output=output, parsed_output=answers)\n\n    def format(self, prompt_template: str) -> str:\n        return prompt_template + \"\\n\\n\" + _escape_curly_braces(FORMAT_STR)"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/output_parsers/base.py",
    "filename": "base.py",
    "relpath": "output_parsers/base.py",
    "start_line": 1,
    "end_line": 71,
    "length": 71,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_as_query_component",
      "_run_component",
      "_arun_component",
      "_validate_component_inputs",
      "set_callback_manager",
      "input_keys",
      "output_keys"
    ],
    "chunk_class_names": [
      "from",
      "class",
      "OutputParserException",
      "ChainableOutputParser",
      "OutputParserComponent"
    ],
    "document_function_names": [
      "_as_query_component",
      "_run_component",
      "_arun_component",
      "_validate_component_inputs",
      "set_callback_manager",
      "input_keys",
      "output_keys"
    ],
    "document_class_names": [
      "from",
      "class",
      "OutputParserException",
      "ChainableOutputParser",
      "OutputParserComponent"
    ],
    "content": "\"\"\"Base output parser class.\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional\n\nfrom llama_index.core.base.query_pipeline.query import (\n    ChainableMixin,\n    InputKeys,\n    OutputKeys,\n    QueryComponent,\n    validate_and_convert_stringable,\n)\nfrom llama_index.core.bridge.pydantic import Field, ConfigDict\nfrom llama_index.core.types import BaseOutputParser\n\n\n@dataclass\nclass StructuredOutput:\n    \"\"\"Structured output class.\"\"\"\n\n    raw_output: str\n    parsed_output: Optional[Any] = None\n\n\nclass OutputParserException(Exception):\n    pass\n\n\nclass ChainableOutputParser(BaseOutputParser, ChainableMixin):\n    \"\"\"Chainable output parser.\"\"\"\n\n    # TODO: consolidate with base at some point if possible.\n\n    def _as_query_component(self, **kwargs: Any) -> QueryComponent:\n        \"\"\"Get query component.\"\"\"\n        return OutputParserComponent(output_parser=self)\n\n\nclass OutputParserComponent(QueryComponent):\n    \"\"\"Output parser component.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    output_parser: BaseOutputParser = Field(..., description=\"Output parser.\")\n\n    def _run_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        output = self.output_parser.parse(kwargs[\"input\"])\n        return {\"output\": output}\n\n    async def _arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Run component.\"\"\"\n        # NOTE: no native async for output parser\n        return self._run_component(**kwargs)\n\n    def _validate_component_inputs(self, input: Any) -> Any:\n        \"\"\"Validate component inputs during run_component.\"\"\"\n        input[\"input\"] = validate_and_convert_stringable(input[\"input\"])\n        return input\n\n    def set_callback_manager(self, callback_manager: Any) -> None:\n        \"\"\"Set callback manager.\"\"\"\n\n    @property\n    def input_keys(self) -> Any:\n        \"\"\"Input keys.\"\"\"\n        return InputKeys.from_keys({\"input\"})\n\n    @property\n    def output_keys(self) -> Any:\n        \"\"\"Output keys.\"\"\"\n        return OutputKeys.from_keys({\"output\"})"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/output_parsers/langchain.py",
    "filename": "langchain.py",
    "relpath": "output_parsers/langchain.py",
    "start_line": 1,
    "end_line": 42,
    "length": 42,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "parse",
      "format"
    ],
    "chunk_class_names": [
      "LangchainOutputParser"
    ],
    "document_function_names": [
      "__init__",
      "parse",
      "format"
    ],
    "document_class_names": [
      "LangchainOutputParser"
    ],
    "content": "\"\"\"Base output parser class.\"\"\"\n\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom llama_index.core.output_parsers.base import ChainableOutputParser\nfrom llama_index.core.prompts.utils import SafeFormatter\n\nif TYPE_CHECKING:\n    from llama_index.core.bridge.langchain import (\n        BaseOutputParser as LCOutputParser,\n    )\n\n\nclass LangchainOutputParser(ChainableOutputParser):\n    \"\"\"Langchain output parser.\"\"\"\n\n    def __init__(\n        self, output_parser: \"LCOutputParser\", format_key: Optional[str] = None\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._output_parser = output_parser\n        self._format_key = format_key\n        self._formatter = SafeFormatter()\n\n    def parse(self, output: str) -> Any:\n        \"\"\"Parse, validate, and correct errors programmatically.\"\"\"\n        # Convert output to string if needed, then parse\n        output_str = str(output) if not isinstance(output, str) else output\n        return self._output_parser.parse(output_str)\n\n    def format(self, query: str) -> str:\n        \"\"\"Format a query with structured output formatting instructions.\"\"\"\n        format_instructions = self._output_parser.get_format_instructions()\n\n        if self._format_key is not None:\n            # Use SafeFormatter for query formatting\n            self._formatter.format_dict = {self._format_key: format_instructions}\n            fmt_query = self._formatter.format(query)\n        else:\n            fmt_query = query + \"\\n\\n\" + format_instructions\n\n        return fmt_query"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/output_parsers/__init__.py",
    "filename": "__init__.py",
    "relpath": "output_parsers/__init__.py",
    "start_line": 1,
    "end_line": 13,
    "length": 13,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [],
    "chunk_class_names": [],
    "document_function_names": [],
    "document_class_names": [],
    "content": "\"\"\"Output parsers.\"\"\"\n\nfrom llama_index.core.output_parsers.base import ChainableOutputParser\nfrom llama_index.core.output_parsers.langchain import LangchainOutputParser\nfrom llama_index.core.output_parsers.pydantic import PydanticOutputParser\nfrom llama_index.core.output_parsers.selection import SelectionOutputParser\n\n__all__ = [\n    \"ChainableOutputParser\",\n    \"LangchainOutputParser\",\n    \"PydanticOutputParser\",\n    \"SelectionOutputParser\",\n]"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/output_parsers/utils.py",
    "filename": "utils.py",
    "relpath": "output_parsers/utils.py",
    "start_line": 1,
    "end_line": 119,
    "length": 119,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "_marshal_llm_to_json",
      "parse_json_markdown",
      "parse_code_markdown",
      "extract_json_str"
    ],
    "chunk_class_names": [],
    "document_function_names": [
      "_marshal_llm_to_json",
      "parse_json_markdown",
      "parse_code_markdown",
      "extract_json_str"
    ],
    "document_class_names": [],
    "content": "import contextlib\nimport json\nimport re\nfrom typing import Any, List\n\nwith contextlib.suppress(ImportError):\n    import yaml\n\nfrom llama_index.core.output_parsers.base import OutputParserException\n\n\ndef _marshal_llm_to_json(output: str) -> str:\n    \"\"\"\n    Extract a substring containing valid JSON or array from a string.\n\n    Args:\n        output: A string that may contain a valid JSON object or array surrounded by\n        extraneous characters or information.\n\n    Returns:\n        A string containing a valid JSON object or array.\n    \"\"\"\n    output = output.strip()\n\n    left_square = output.find(\"[\")\n    left_brace = output.find(\"{\")\n\n    if left_square < left_brace and left_square != -1:\n        left = left_square\n        right = output.rfind(\"]\")\n    else:\n        left = left_brace\n        right = output.rfind(\"}\")\n\n    return output[left : right + 1]\n\n\ndef parse_json_markdown(text: str) -> Any:\n    if \"```json\" in text:\n        text = text.split(\"```json\")[1].strip().strip(\"```\").strip()\n\n    json_string = _marshal_llm_to_json(text)\n\n    try:\n        json_obj = json.loads(json_string)\n    except json.JSONDecodeError as e_json:\n        try:\n            # NOTE: parsing again with pyyaml\n            #       pyyaml is less strict, and allows for trailing commas\n            #       right now we rely on this since guidance program generates\n            #       trailing commas\n            json_obj = yaml.safe_load(json_string)\n        except yaml.YAMLError as e_yaml:\n            raise OutputParserException(\n                f\"Got invalid JSON object. Error: {e_json} {e_yaml}. \"\n                f\"Got JSON string: {json_string}\"\n            )\n        except NameError as exc:\n            raise ImportError(\"Please pip install PyYAML.\") from exc\n\n    return json_obj\n\n\ndef parse_code_markdown(text: str, only_last: bool) -> List[str]:\n    # Regular expression pattern to match code within triple-backticks\n    pattern = r\"```(.*?)```\"\n\n    # Regular expression pattern to match code within triple backticks with\n    # a Python marker. Like: ```python df.columns```\n    python_str_pattern = re.compile(r\"^```python\", re.IGNORECASE)\n    text = python_str_pattern.sub(\"```\", text)\n\n    # Find all matches of the pattern in the text\n    matches = re.findall(pattern, text, re.DOTALL)\n\n    # Return the last matched group if requested\n    code = matches[-1] if matches and only_last else matches\n\n    # If empty we optimistically assume the output is the code\n    if not code:\n        # we want to handle cases where the code may start or end with triple\n        # backticks\n        # we also want to handle cases where the code is surrounded by regular\n        # quotes\n        # we can't just remove all backticks due to JS template strings\n\n        candidate = text.strip()\n\n        if candidate.startswith('\"') and candidate.endswith('\"'):\n            candidate = candidate[1:-1]\n\n        if candidate.startswith(\"'\") and candidate.endswith(\"'\"):\n            candidate = candidate[1:-1]\n\n        if candidate.startswith(\"`\") and candidate.endswith(\"`\"):\n            candidate = candidate[1:-1]\n\n        # For triple backticks we split the handling of the start and end\n        # partly because there can be cases where only one and not the other\n        # is present, and partly because we don't need to be so worried\n        # about it being a string in a programming language\n        if candidate.startswith(\"```\"):\n            candidate = re.sub(r\"^```[a-zA-Z]*\", \"\", candidate)\n\n        if candidate.endswith(\"```\"):\n            candidate = candidate[:-3]\n        code = [candidate.strip()]\n\n    return code\n\n\ndef extract_json_str(text: str) -> str:\n    \"\"\"Extract JSON string from text.\"\"\"\n    # NOTE: this regex parsing is taken from langchain.output_parsers.pydantic\n    match = re.search(r\"\\{.*\\}\", text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL)\n    if not match:\n        raise ValueError(f\"Could not extract json string from output: {text}\")\n\n    return match.group()"
  },
  {
    "filepath": "/home/puranjai-garg/Projects/usfca/project02-ragnarok/data/llama_index/llama-index-core/llama_index/core/output_parsers/pydantic.py",
    "filename": "pydantic.py",
    "relpath": "output_parsers/pydantic.py",
    "start_line": 1,
    "end_line": 66,
    "length": 66,
    "language": "python",
    "codebase": "llama_index",
    "chunking_method": "ast",
    "chunk_function_names": [
      "__init__",
      "output_cls",
      "format_string",
      "get_format_string",
      "parse",
      "format"
    ],
    "chunk_class_names": [
      "PydanticOutputParser"
    ],
    "document_function_names": [
      "__init__",
      "output_cls",
      "format_string",
      "get_format_string",
      "parse",
      "format"
    ],
    "document_class_names": [
      "PydanticOutputParser"
    ],
    "content": "\"\"\"Pydantic output parser.\"\"\"\n\nimport json\nfrom typing import Any, Generic, List, Optional, Type\n\nfrom llama_index.core.output_parsers.base import ChainableOutputParser\nfrom llama_index.core.output_parsers.utils import extract_json_str\nfrom llama_index.core.types import Model\n\nPYDANTIC_FORMAT_TMPL = \"\"\"\nHere's a JSON schema to follow:\n{schema}\n\nOutput a valid JSON object but do not repeat the schema.\n\"\"\"\n\n\nclass PydanticOutputParser(ChainableOutputParser, Generic[Model]):\n    \"\"\"Pydantic Output Parser.\n\n    Args:\n        output_cls (BaseModel): Pydantic output class.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        output_cls: Type[Model],\n        excluded_schema_keys_from_format: Optional[List] = None,\n        pydantic_format_tmpl: str = PYDANTIC_FORMAT_TMPL,\n    ) -> None:\n        \"\"\"Init params.\"\"\"\n        self._output_cls = output_cls\n        self._excluded_schema_keys_from_format = excluded_schema_keys_from_format or []\n        self._pydantic_format_tmpl = pydantic_format_tmpl\n\n    @property\n    def output_cls(self) -> Type[Model]:\n        return self._output_cls\n\n    @property\n    def format_string(self) -> str:\n        \"\"\"Format string.\"\"\"\n        return self.get_format_string(escape_json=True)\n\n    def get_format_string(self, escape_json: bool = True) -> str:\n        \"\"\"Format string.\"\"\"\n        schema_dict = self._output_cls.model_json_schema()\n        for key in self._excluded_schema_keys_from_format:\n            del schema_dict[key]\n\n        schema_str = json.dumps(schema_dict)\n        output_str = self._pydantic_format_tmpl.format(schema=schema_str)\n        if escape_json:\n            return output_str.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        else:\n            return output_str\n\n    def parse(self, text: str) -> Any:\n        \"\"\"Parse, validate, and correct errors programmatically.\"\"\"\n        json_str = extract_json_str(text)\n        return self._output_cls.model_validate_json(json_str)\n\n    def format(self, query: str) -> str:\n        \"\"\"Format a query with structured output formatting instructions.\"\"\"\n        return query + \"\\n\\n\" + self.get_format_string(escape_json=True)"
  }
]