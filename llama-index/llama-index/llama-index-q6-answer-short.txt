After retrieval, LlamaIndex uses a ResponseSynthesizer to combine the information from nodes and generate an answer (usually via an LLM). The ResponseSynthesizer takes the list of Node contents (and maybe their metadata) and prompts the language model to produce a coherent response. For example, one can configure a ResponseSynthesizer with certain settings or post-processors: ResponseSynthesizer.from_args(node_postprocessors=[SimilarityPostprocessor(...)])&#8203;:contentReference[oaicite:38]{index=38}. In a custom query pipeline, you might assemble a RetrieverQueryEngine with a retriever and a response_synthesizer​. Internally, the synthesizer will likely format a prompt with the retrieved texts and ask the LLM to answer the user’s query. (In older versions, this was called a response builder). The exact code uses the LLM interface (from llama_index.core.llm) to generate the final answer string from the nodes.
