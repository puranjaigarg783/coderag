How LlamaIndex Builds an Index from Document Nodes
Looking at the provided code snippets, LlamaIndex follows a systematic process to build an index from document nodes:

1. Entry Point: from_documents Method
In llama-index-core/llama_index/core/indices/base.py, the process begins with the from_documents class method:

@classmethod
def from_documents(cls: Type[IndexType], documents: Sequence[Document], ...) -> IndexType:
This method:

Takes a sequence of Document objects
Creates a StorageContext (or uses the provided one)
Stores document hashes in the docstore
Runs transformations to convert documents into nodes
Calls the class constructor with the resulting nodes
2. Document to Node Transformation
The documents are transformed into nodes using the run_transformations function from pipeline.py:

nodes = run_transformations(
    documents,
    transformations,
    show_progress=show_progress,
    **kwargs,
)
This function applies each transformation component sequentially to the nodes:

for transform in transformations:
    # Apply transformation (with optional caching)
    nodes = transform(nodes, **kwargs)
3. Index Construction
After transformation, the index is constructed using the resulting nodes:

return cls(
    nodes=nodes,
    storage_context=storage_context,
    callback_manager=callback_manager,
    show_progress=show_progress,
    transformations=transformations,
    **kwargs,
)
The class constructor (__init__) then:

Validates the input parameters
Sets up storage context and related stores
Builds an index structure from the nodes if not provided
if index_struct is None:
    nodes = nodes or []
    index_struct = self.build_index_from_nodes(
        nodes + objects,
        **kwargs,
    )
4. Building Index from Nodes: Vector Store Example
For specific implementations like VectorStoreIndex, additional steps are performed in vector_store/base.py:

a. Node Filtering
First, nodes without content are filtered out:

content_nodes = [
    node
    for node in nodes
    if node.get_content(metadata_mode=MetadataMode.EMBED) != ""
]
b. Node Embedding
Nodes are embedded using the _get_node_with_embedding method, which calls embed_nodes:

id_to_embed_map = embed_nodes(
    nodes, self._embed_model, show_progress=show_progress
)
This function:

Identifies nodes that don't already have embeddings
Extracts text content from nodes
Generates embeddings in batches
Returns a mapping of node IDs to embeddings
c. Adding Nodes to Storage
The embedded nodes are added to the vector store:

for nodes_batch in iter_batch(nodes, self._insert_batch_size):
    nodes_batch = self._get_node_with_embedding(nodes_batch, show_progress)
    new_ids = self._vector_store.add(nodes_batch, **insert_kwargs)
Depending on vector store capabilities, node content may also be stored in the document store:

if not self._vector_store.stores_text or self._store_nodes_override:
    # Store nodes in document store if vector store doesn't maintain text
    for node, new_id in zip(nodes_batch, new_ids):
        # Remove embedding to avoid duplication
        node_without_embedding = node.model_copy()
        node_without_embedding.embedding = None
        
        # Add to index structure and document store
        index_struct.add_node(node_without_embedding, text_id=new_id)
        self._docstore.add_documents([node_without_embedding], allow_update=True)
Summary
The LlamaIndex indexing process follows these key steps:

Start with documents and transform them into nodes
Filter nodes (removing empty ones)
Generate embeddings for nodes
Store nodes in appropriate storage systems (vector store, document store)
Build and store the index structure
This architecture provides flexibility to support different index types and storage backends while maintaining a consistent indexing pipeline.
