LlamaIndex abstracts vector stores via a BaseVectorStore interface. You can integrate a custom store by creating a storage context with it or using the index’s helper. For example, VectorStoreIndex.from_vector_store(vector_store, embed_model=..., **kwargs) creates a new index backed by an external vector store​. Internally, this method wraps the provided vector_store in a StorageContext (via StorageContext.from_defaults(vector_store=...)) and then initializes the index with that context​. The vector store must comply with the interface (e.g., provide similarity search and possibly store text). Once integrated, the index will use that for inserting and querying embeddings instead of the default in-memory store.
