[
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/base.py",
    "filename": "base.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/base.py",
    "start_line": 198,
    "end_line": 262,
    "length": 65,
    "content": "    @dispatcher.span\n    def synthesize(\n        self,\n        query: QueryTextType,\n        nodes: List[NodeWithScore],\n        additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TYPE:\n        dispatcher.event(\n            SynthesizeStartEvent(\n                query=query,\n            )\n        )\n\n        if len(nodes) == 0:\n            if self._streaming:\n                empty_response_stream = StreamingResponse(\n                    response_gen=empty_response_generator()\n                )\n                dispatcher.event(\n                    SynthesizeEndEvent(\n                        query=query,\n                        response=empty_response_stream,\n                    )\n                )\n                return empty_response_stream\n            else:\n                empty_response = Response(\"Empty Response\")\n                dispatcher.event(\n                    SynthesizeEndEvent(\n                        query=query,\n                        response=empty_response,\n                    )\n                )\n                return empty_response\n\n        if isinstance(query, str):\n            query = QueryBundle(query_str=query)\n\n        with self._callback_manager.event(\n            CBEventType.SYNTHESIZE,\n            payload={EventPayload.QUERY_STR: query.query_str},\n        ) as event:\n            response_str = self.get_response(\n                query_str=query.query_str,\n                text_chunks=[\n                    n.node.get_content(metadata_mode=MetadataMode.LLM) for n in nodes\n                ],\n                **response_kwargs,\n            )\n\n            additional_source_nodes = additional_source_nodes or []\n            source_nodes = list(nodes) + list(additional_source_nodes)\n\n            response = self._prepare_response_output(response_str, source_nodes)\n\n            event.on_end(payload={EventPayload.RESPONSE: response})\n\n        dispatcher.event(\n            SynthesizeEndEvent(\n                query=query,\n                response=response,\n            )\n        )\n        return response"
  },
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/factory.py",
    "filename": "factory.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/factory.py",
    "start_line": 33,
    "end_line": 151,
    "length": 119,
    "content": "def get_response_synthesizer(\n    llm: Optional[LLM] = None,\n    prompt_helper: Optional[PromptHelper] = None,\n    text_qa_template: Optional[BasePromptTemplate] = None,\n    refine_template: Optional[BasePromptTemplate] = None,\n    summary_template: Optional[BasePromptTemplate] = None,\n    simple_template: Optional[BasePromptTemplate] = None,\n    response_mode: ResponseMode = ResponseMode.COMPACT,\n    callback_manager: Optional[CallbackManager] = None,\n    use_async: bool = False,\n    streaming: bool = False,\n    structured_answer_filtering: bool = False,\n    output_cls: Optional[Type[BaseModel]] = None,\n    program_factory: Optional[\n        Callable[[BasePromptTemplate], BasePydanticProgram]\n    ] = None,\n    verbose: bool = False,\n) -> BaseSynthesizer:\n    \"\"\"Get a response synthesizer.\"\"\"\n    text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT_SEL\n    refine_template = refine_template or DEFAULT_REFINE_PROMPT_SEL\n    simple_template = simple_template or DEFAULT_SIMPLE_INPUT_PROMPT\n    summary_template = summary_template or DEFAULT_TREE_SUMMARIZE_PROMPT_SEL\n\n    callback_manager = callback_manager or Settings.callback_manager\n    llm = llm or Settings.llm\n    prompt_helper = (\n        prompt_helper\n        or Settings._prompt_helper\n        or PromptHelper.from_llm_metadata(\n            llm.metadata,\n        )\n    )\n\n    if response_mode == ResponseMode.REFINE:\n        return Refine(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            structured_answer_filtering=structured_answer_filtering,\n            program_factory=program_factory,\n            verbose=verbose,\n        )\n    elif response_mode == ResponseMode.COMPACT:\n        return CompactAndRefine(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            refine_template=refine_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            structured_answer_filtering=structured_answer_filtering,\n            program_factory=program_factory,\n            verbose=verbose,\n        )\n    elif response_mode == ResponseMode.TREE_SUMMARIZE:\n        return TreeSummarize(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            summary_template=summary_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            use_async=use_async,\n            verbose=verbose,\n        )\n    elif response_mode == ResponseMode.SIMPLE_SUMMARIZE:\n        return SimpleSummarize(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            streaming=streaming,\n        )\n    elif response_mode == ResponseMode.GENERATION:\n        return Generation(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            simple_template=simple_template,\n            streaming=streaming,\n        )\n    elif response_mode == ResponseMode.ACCUMULATE:\n        return Accumulate(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            use_async=use_async,\n        )\n    elif response_mode == ResponseMode.COMPACT_ACCUMULATE:\n        return CompactAndAccumulate(\n            llm=llm,\n            callback_manager=callback_manager,\n            prompt_helper=prompt_helper,\n            text_qa_template=text_qa_template,\n            output_cls=output_cls,\n            streaming=streaming,\n            use_async=use_async,\n        )\n    elif response_mode == ResponseMode.NO_TEXT:\n        return NoText(\n            callback_manager=callback_manager,\n            streaming=streaming,\n        )\n    elif response_mode == ResponseMode.CONTEXT_ONLY:\n        return ContextOnly(\n            callback_manager=callback_manager,\n            streaming=streaming,\n        )\n    else:\n        raise ValueError(f\"Unknown mode: {response_mode}\")"
  },
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/refine.py",
    "filename": "refine.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/refine.py",
    "start_line": 162,
    "end_line": 198,
    "length": 37,
    "content": "    @dispatcher.span\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Give response over chunks.\"\"\"\n        dispatcher.event(\n            GetResponseStartEvent(query_str=query_str, text_chunks=text_chunks)\n        )\n        response: Optional[RESPONSE_TEXT_TYPE] = None\n        for text_chunk in text_chunks:\n            if prev_response is None:\n                # if this is the first chunk, and text chunk already\n                # is an answer, then return it\n                response = self._give_response_single(\n                    query_str, text_chunk, **response_kwargs\n                )\n            else:\n                # refine response if possible\n                response = self._refine_response_single(\n                    prev_response, query_str, text_chunk, **response_kwargs\n                )\n            prev_response = response\n        if isinstance(response, str):\n            if self._output_cls is not None:\n                try:\n                    response = self._output_cls.model_validate_json(response)\n                except ValidationError:\n                    pass\n            else:\n                response = response or \"Empty Response\"\n        else:\n            response = cast(Generator, response)\n        dispatcher.event(GetResponseEndEvent())\n        return response"
  },
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/compact_and_refine.py",
    "filename": "compact_and_refine.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/compact_and_refine.py",
    "start_line": 30,
    "end_line": 48,
    "length": 19,
    "content": "    @dispatcher.span\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        prev_response: Optional[RESPONSE_TEXT_TYPE] = None,\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get compact response.\"\"\"\n        # use prompt helper to fix compact text_chunks under the prompt limitation\n        # TODO: This is a temporary fix - reason it's temporary is that\n        # the refine template does not account for size of previous answer.\n        new_texts = self._make_compact_text_chunks(query_str, text_chunks)\n        return super().get_response(\n            query_str=query_str,\n            text_chunks=new_texts,\n            prev_response=prev_response,\n            **response_kwargs,\n        )"
  },
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
    "filename": "tree_summarize.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
    "start_line": 17,
    "end_line": 28,
    "length": 12,
    "content": "class TreeSummarize(BaseSynthesizer):\n    \"\"\"\n    Tree summarize response builder.\n\n    This response builder recursively merges text chunks and summarizes them\n    in a bottom-up fashion (i.e. building a tree from leaves to root).\n\n    More concretely, at each recursively step:\n    1. we repack the text chunks so that each chunk fills the context window of the LLM\n    2. if there is only one chunk, we give the final response\n    3. otherwise, we summarize each chunk and recursively summarize the summaries.\n    \"\"\""
  },
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
    "filename": "tree_summarize.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
    "start_line": 134,
    "end_line": 173,
    "length": 40,
    "content": "    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get tree summarize response.\"\"\"\n        summary_template = self._summary_template.partial_format(query_str=query_str)\n        # repack text_chunks so that each chunk fills the context window\n        text_chunks = self._prompt_helper.repack(\n            summary_template, text_chunks=text_chunks, llm=self._llm\n        )\n\n        if self._verbose:\n            print(f\"{len(text_chunks)} text chunks after repacking\")\n\n        # give final response if there is only one chunk\n        if len(text_chunks) == 1:\n            response: RESPONSE_TEXT_TYPE\n            if self._streaming:\n                response = self._llm.stream(\n                    summary_template, context_str=text_chunks[0], **response_kwargs\n                )\n            else:\n                if self._output_cls is None:\n                    response = self._llm.predict(\n                        summary_template,\n                        context_str=text_chunks[0],\n                        **response_kwargs,\n                    )\n                else:\n                    response = self._llm.structured_predict(\n                        self._output_cls,\n                        summary_template,\n                        context_str=text_chunks[0],\n                        **response_kwargs,\n                    )\n\n            return response"
  },
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
    "filename": "tree_summarize.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/tree_summarize.py",
    "start_line": 174,
    "end_line": 230,
    "length": 57,
    "content": "        else:\n            # summarize each chunk\n            if self._use_async:\n                if self._output_cls is None:\n                    tasks = [\n                        self._llm.apredict(\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n                else:\n                    tasks = [\n                        self._llm.astructured_predict(\n                            self._output_cls,\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n\n                summary_responses = run_async_tasks(tasks)\n\n                if self._output_cls is not None:\n                    summaries = [\n                        summary.model_dump_json() for summary in summary_responses\n                    ]\n                else:\n                    summaries = summary_responses\n            else:\n                if self._output_cls is None:\n                    summaries = [\n                        self._llm.predict(\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n                else:\n                    summaries = [\n                        self._llm.structured_predict(\n                            self._output_cls,\n                            summary_template,\n                            context_str=text_chunk,\n                            **response_kwargs,\n                        )\n                        for text_chunk in text_chunks\n                    ]\n                    summaries = [summary.model_dump_json() for summary in summaries]\n\n            # recursively summarize the summaries\n            return self.get_response(\n                query_str=query_str, text_chunks=summaries, **response_kwargs\n            )"
  },
  {
    "filepath": "/Users/gbenson/Local/git/llama-index/llama-index-core/llama_index/core/response_synthesizers/base.py",
    "filename": "base.py",
    "relpath": "llama-index-core/llama_index/core/response_synthesizers/base.py",
    "start_line": 151,
    "end_line": 196,
    "length": 46,
    "content": "    def _prepare_response_output(\n        self,\n        response_str: Optional[RESPONSE_TEXT_TYPE],\n        source_nodes: List[NodeWithScore],\n    ) -> RESPONSE_TYPE:\n        \"\"\"Prepare response object from response string.\"\"\"\n        response_metadata = self._get_metadata_for_response(\n            [node_with_score.node for node_with_score in source_nodes]\n        )\n\n        if isinstance(self._llm, StructuredLLM):\n            # convert string to output_cls\n            output = self._llm.output_cls.model_validate_json(str(response_str))\n            return PydanticResponse(\n                output,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n\n        if isinstance(response_str, str):\n            return Response(\n                response_str,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n        if isinstance(response_str, Generator):\n            return StreamingResponse(\n                response_str,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n        if isinstance(response_str, AsyncGenerator):\n            return AsyncStreamingResponse(\n                response_str,\n                source_nodes=source_nodes,\n                metadata=response_metadata,\n            )\n\n        if self._output_cls is not None and isinstance(response_str, self._output_cls):\n            return PydanticResponse(\n                response_str, source_nodes=source_nodes, metadata=response_metadata\n            )\n\n        raise ValueError(\n            f\"Response must be a string or a generator. Found {type(response_str)}\"\n        )"
  }
]